diff -ruN linux-2.6.29/arch/powerpc/platforms/cell/spufs/sched.c android_cluster/linux-2.6.29/arch/powerpc/platforms/cell/spufs/sched.c
--- linux-2.6.29/arch/powerpc/platforms/cell/spufs/sched.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/powerpc/platforms/cell/spufs/sched.c	2014-05-27 23:04:08.414069736 -0700
@@ -1093,7 +1093,7 @@
 		LOAD_INT(c), LOAD_FRAC(c),
 		count_active_contexts(),
 		atomic_read(&nr_spu_contexts),
-		current->nsproxy->pid_ns->last_pid);
+		task_active_pid_ns(current)->last_pid);
 	return 0;
 }
 
diff -ruN linux-2.6.29/arch/x86/configs/i386_defconfig android_cluster/linux-2.6.29/arch/x86/configs/i386_defconfig
--- linux-2.6.29/arch/x86/configs/i386_defconfig	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/configs/i386_defconfig	2014-05-27 23:04:08.566066576 -0700
@@ -36,6 +36,7 @@
 CONFIG_ARCH_HAS_CPU_RELAX=y
 CONFIG_ARCH_HAS_CACHE_LINE_SIZE=y
 CONFIG_HAVE_SETUP_PER_CPU_AREA=y
+CONFIG_HAVE_DYNAMIC_PER_CPU_AREA=y
 # CONFIG_HAVE_CPUMASK_OF_CPU_MAP is not set
 CONFIG_ARCH_HIBERNATION_POSSIBLE=y
 CONFIG_ARCH_SUSPEND_POSSIBLE=y
@@ -63,10 +64,17 @@
 CONFIG_INIT_ENV_ARG_LIMIT=32
 CONFIG_LOCALVERSION=""
 # CONFIG_LOCALVERSION_AUTO is not set
+CONFIG_HAVE_KERNEL_GZIP=y
+CONFIG_HAVE_KERNEL_BZIP2=y
+CONFIG_HAVE_KERNEL_LZMA=y
+CONFIG_KERNEL_GZIP=y
+# CONFIG_KERNEL_BZIP2 is not set
+# CONFIG_KERNEL_LZMA is not set
 CONFIG_SWAP=y
 CONFIG_SYSVIPC=y
 CONFIG_SYSVIPC_SYSCTL=y
 CONFIG_POSIX_MQUEUE=y
+CONFIG_POSIX_MQUEUE_SYSCTL=y
 CONFIG_BSD_PROCESS_ACCT=y
 # CONFIG_BSD_PROCESS_ACCT_V3 is not set
 CONFIG_TASKSTATS=y
@@ -102,23 +110,26 @@
 CONFIG_PID_NS=y
 CONFIG_BLK_DEV_INITRD=y
 CONFIG_INITRAMFS_SOURCE=""
+CONFIG_RD_GZIP=y
+CONFIG_RD_BZIP2=y
+CONFIG_RD_LZMA=y
 CONFIG_CC_OPTIMIZE_FOR_SIZE=y
 CONFIG_SYSCTL=y
+CONFIG_ANON_INODES=y
 # CONFIG_EMBEDDED is not set
 CONFIG_UID16=y
 CONFIG_SYSCTL_SYSCALL=y
 CONFIG_KALLSYMS=y
 CONFIG_KALLSYMS_ALL=y
 CONFIG_KALLSYMS_EXTRA_PASS=y
+# CONFIG_STRIP_ASM_SYMS is not set
 CONFIG_HOTPLUG=y
 CONFIG_PRINTK=y
 CONFIG_BUG=y
 CONFIG_ELF_CORE=y
 CONFIG_PCSPKR_PLATFORM=y
-# CONFIG_COMPAT_BRK is not set
 CONFIG_BASE_FULL=y
 CONFIG_FUTEX=y
-CONFIG_ANON_INODES=y
 CONFIG_EPOLL=y
 CONFIG_SIGNALFD=y
 CONFIG_TIMERFD=y
@@ -126,6 +137,7 @@
 CONFIG_SHMEM=y
 CONFIG_VM_EVENT_COUNTERS=y
 CONFIG_SLUB_DEBUG=y
+# CONFIG_COMPAT_BRK is not set
 # CONFIG_SLAB is not set
 CONFIG_SLUB=y
 # CONFIG_SLOB is not set
@@ -224,8 +236,10 @@
 # CONFIG_GENERIC_CPU is not set
 CONFIG_X86_GENERIC=y
 CONFIG_X86_CPU=y
+CONFIG_X86_L1_CACHE_BYTES=64
+CONFIG_X86_INTERNODE_CACHE_BYTES=64
 CONFIG_X86_CMPXCHG=y
-CONFIG_X86_L1_CACHE_SHIFT=7
+CONFIG_X86_L1_CACHE_SHIFT=5
 CONFIG_X86_XADD=y
 # CONFIG_X86_PPRO_FENCE is not set
 CONFIG_X86_WP_WORKS_OK=y
@@ -246,7 +260,6 @@
 CONFIG_SCHED_SMT=y
 CONFIG_SCHED_MC=y
 # CONFIG_PREEMPT_NONE is not set
-CONFIG_PREEMPT_VOLUNTARY=y
 # CONFIG_PREEMPT is not set
 CONFIG_X86_LOCAL_APIC=y
 CONFIG_X86_IO_APIC=y
@@ -259,6 +272,7 @@
 CONFIG_MICROCODE_OLD_INTERFACE=y
 CONFIG_X86_MSR=y
 CONFIG_X86_CPUID=y
+# CONFIG_X86_CPU_DEBUG is not set
 # CONFIG_NOHIGHMEM is not set
 CONFIG_HIGHMEM4G=y
 # CONFIG_HIGHMEM64G is not set
@@ -288,6 +302,7 @@
 CONFIG_X86_PAT=y
 CONFIG_EFI=y
 CONFIG_SECCOMP=y
+# CONFIG_CC_STACKPROTECTOR is not set
 # CONFIG_HZ_100 is not set
 # CONFIG_HZ_250 is not set
 # CONFIG_HZ_300 is not set
@@ -329,6 +344,7 @@
 CONFIG_ACPI_AC=y
 CONFIG_ACPI_BATTERY=y
 CONFIG_ACPI_BUTTON=y
+CONFIG_ACPI_VIDEO=y
 CONFIG_ACPI_FAN=y
 CONFIG_ACPI_DOCK=y
 # CONFIG_ACPI_BAY is not set
@@ -407,6 +423,7 @@
 CONFIG_PCI_DIRECT=y
 CONFIG_PCI_MMCONFIG=y
 CONFIG_PCI_DOMAINS=y
+# CONFIG_DMAR is not set
 CONFIG_PCIEPORTBUS=y
 # CONFIG_HOTPLUG_PCI_PCIE is not set
 CONFIG_PCIEAER=y
@@ -416,6 +433,7 @@
 # CONFIG_PCI_LEGACY is not set
 # CONFIG_PCI_DEBUG is not set
 CONFIG_HT_IRQ=y
+# CONFIG_PCI_IOV is not set
 CONFIG_ISA_DMA_API=y
 # CONFIG_ISA is not set
 # CONFIG_MCA is not set
@@ -539,7 +557,6 @@
 # CONFIG_IPV6_TUNNEL is not set
 # CONFIG_IPV6_MULTIPLE_TABLES is not set
 # CONFIG_IPV6_MROUTE is not set
-CONFIG_NETLABEL=y
 CONFIG_NETWORK_SECMARK=y
 CONFIG_NETFILTER=y
 # CONFIG_NETFILTER_DEBUG is not set
@@ -601,7 +618,15 @@
 CONFIG_IP6_NF_MANGLE=y
 # CONFIG_IP_DCCP is not set
 # CONFIG_IP_SCTP is not set
-# CONFIG_TIPC is not set
+CONFIG_TIPC=y
+# CONFIG_TIPC_ADVANCED is not set
+CONFIG_TIPC_UNICLUSTER_FRIENDLY=y
+CONFIG_TIPC_MULTIPLE_LINKS=y
+CONFIG_TIPC_CONFIG_SERVICE=y
+CONFIG_TIPC_SOCKET_API=y
+CONFIG_TIPC_SYSTEM_MSGS=y
+# CONFIG_TIPC_DEBUG is not set
+CONFIG_KRGRPC=y
 # CONFIG_ATM is not set
 # CONFIG_BRIDGE is not set
 # CONFIG_VLAN_8021Q is not set
@@ -614,6 +639,7 @@
 # CONFIG_LAPB is not set
 # CONFIG_ECONET is not set
 # CONFIG_WAN_ROUTER is not set
+# CONFIG_PHONET is not set
 CONFIG_NET_SCHED=y
 
 #
@@ -666,6 +692,7 @@
 #
 # CONFIG_NET_PKTGEN is not set
 # CONFIG_NET_TCPPROBE is not set
+# CONFIG_NET_DROP_MONITOR is not set
 CONFIG_HAMRADIO=y
 
 #
@@ -702,6 +729,53 @@
 # CONFIG_NET_9P is not set
 
 #
+# Cluster support
+#
+CONFIG_KERRIGHED=y
+# CONFIG_KERRIGHED_DEVEL is not set
+CONFIG_KRG_TOOLS=y
+CONFIG_KRG_COMMUNICATION_FRAMEWORK=y
+CONFIG_KRG_AUTONODEID=y
+CONFIG_KRG_HOTPLUG=y
+# CONFIG_KRG_HOTPLUG_DEL is not set
+# CONFIG_KRG_HOTPLUG_XCH is not set
+# CONFIG_KRG_DISABLE_HEARTBEAT is not set
+CONFIG_KRG_KDDM=y
+# CONFIG_KRG_KDDM_DEBUG is not set
+CONFIG_KRG_CAP=y
+CONFIG_KRG_PROCFS=y
+CONFIG_KRG_MM=y
+CONFIG_KRG_DVFS=y
+CONFIG_KRG_FAF=y
+CONFIG_KRG_IPC=y
+CONFIG_KRG_PROC=y
+CONFIG_KRG_EPM=y
+CONFIG_KRG_IPC_EPM=y
+CONFIG_KRG_SCHED=y
+CONFIG_KRG_SCHED_COMPAT=y
+CONFIG_KRG_SCHED_COMPAT_FORCE=m
+# CONFIG_KRG_SCHED_CPU_PROBE is not set
+# CONFIG_KRG_SCHED_MEM_PROBE is not set
+CONFIG_KRG_SCHED_MIGRATION_PROBE=m
+CONFIG_KRG_SCHED_MOSIX_PROBE=m
+CONFIG_KRG_SCHED_THRESHOLD_FILTER=m
+CONFIG_KRG_SCHED_FREQ_LIMIT_FILTER=m
+CONFIG_KRG_SCHED_REMOTE_CACHE_FILTER=m
+# CONFIG_KRG_SCHED_ECHO_POLICY is not set
+CONFIG_KRG_SCHED_MOSIX_LOAD_BALANCER=m
+CONFIG_KRG_SCHED_ROUND_ROBIN_BALANCER=m
+CONFIG_CLUSTER_WIDE_PROC_INFRA=y
+CONFIG_CLUSTER_WIDE_PROC=y
+CONFIG_CLUSTER_WIDE_PROC_CPUINFO=y
+CONFIG_CLUSTER_WIDE_PROC_MEMINFO=y
+CONFIG_CLUSTER_WIDE_PROC_LOADAVG=y
+CONFIG_CLUSTER_WIDE_PROC_STAT=y
+CONFIG_CLUSTER_WIDE_PROC_UPTIME=y
+# CONFIG_KRG_SYSCALL_EXIT_HOOK is not set
+# CONFIG_DEBUG_SEG_FAULT is not set
+# CONFIG_KRG_DEBUG is not set
+
+#
 # Device Drivers
 #
 
@@ -809,6 +883,7 @@
 # CONFIG_SCSI_LOWLEVEL is not set
 # CONFIG_SCSI_LOWLEVEL_PCMCIA is not set
 # CONFIG_SCSI_DH is not set
+# CONFIG_SCSI_OSD_INITIATOR is not set
 CONFIG_ATA=y
 # CONFIG_ATA_NONSTANDARD is not set
 CONFIG_ATA_ACPI=y
@@ -906,6 +981,7 @@
 CONFIG_MACINTOSH_DRIVERS=y
 CONFIG_MAC_EMUMOUSEBTN=y
 CONFIG_NETDEVICES=y
+CONFIG_COMPAT_NET_DEV_OPS=y
 # CONFIG_IFB is not set
 # CONFIG_DUMMY is not set
 # CONFIG_BONDING is not set
@@ -940,6 +1016,8 @@
 CONFIG_NET_VENDOR_3COM=y
 # CONFIG_VORTEX is not set
 # CONFIG_TYPHOON is not set
+# CONFIG_ETHOC is not set
+# CONFIG_DNET is not set
 CONFIG_NET_TULIP=y
 # CONFIG_DE2104X is not set
 # CONFIG_TULIP is not set
@@ -986,6 +1064,7 @@
 CONFIG_E1000E=y
 # CONFIG_IP1000 is not set
 # CONFIG_IGB is not set
+# CONFIG_IGBVF is not set
 # CONFIG_NS83820 is not set
 # CONFIG_HAMACHI is not set
 # CONFIG_YELLOWFIN is not set
@@ -1006,6 +1085,7 @@
 # CONFIG_IXGBE is not set
 # CONFIG_IXGB is not set
 # CONFIG_S2IO is not set
+# CONFIG_VXGE is not set
 # CONFIG_MYRI10GE is not set
 # CONFIG_NETXEN_NIC is not set
 # CONFIG_NIU is not set
@@ -1013,6 +1093,7 @@
 # CONFIG_TEHUTI is not set
 # CONFIG_BNX2X is not set
 # CONFIG_SFC is not set
+# CONFIG_BE2NET is not set
 CONFIG_TR=y
 # CONFIG_IBMOL is not set
 # CONFIG_IBMLS is not set
@@ -1029,8 +1110,8 @@
 # CONFIG_IPW2200 is not set
 # CONFIG_LIBERTAS is not set
 # CONFIG_AIRO is not set
-# CONFIG_HERMES is not set
 # CONFIG_ATMEL is not set
+# CONFIG_AT76C50X_USB is not set
 # CONFIG_AIRO_CS is not set
 # CONFIG_PCMCIA_WL3501 is not set
 # CONFIG_PRISM54 is not set
@@ -1157,6 +1238,8 @@
 # CONFIG_TABLET_USB_KBTAB is not set
 # CONFIG_TABLET_USB_WACOM is not set
 CONFIG_INPUT_TOUCHSCREEN=y
+# CONFIG_TOUCHSCREEN_AD7879_I2C is not set
+# CONFIG_TOUCHSCREEN_AD7879 is not set
 # CONFIG_TOUCHSCREEN_FUJITSU is not set
 # CONFIG_TOUCHSCREEN_GUNZE is not set
 # CONFIG_TOUCHSCREEN_ELO is not set
@@ -1248,6 +1331,7 @@
 # CONFIG_LEGACY_PTYS is not set
 # CONFIG_IPMI_HANDLER is not set
 CONFIG_HW_RANDOM=y
+# CONFIG_HW_RANDOM_TIMERIOMEM is not set
 CONFIG_HW_RANDOM_INTEL=y
 CONFIG_HW_RANDOM_AMD=y
 CONFIG_HW_RANDOM_GEODE=y
@@ -1336,7 +1420,6 @@
 # CONFIG_SENSORS_PCF8574 is not set
 # CONFIG_PCF8575 is not set
 # CONFIG_SENSORS_PCA9539 is not set
-# CONFIG_SENSORS_PCF8591 is not set
 # CONFIG_SENSORS_MAX6875 is not set
 # CONFIG_SENSORS_TSL2550 is not set
 # CONFIG_I2C_DEBUG_CORE is not set
@@ -1455,7 +1538,7 @@
 # CONFIG_DRM_VIA is not set
 # CONFIG_DRM_SAVAGE is not set
 # CONFIG_VGASTATE is not set
-# CONFIG_VIDEO_OUTPUT_CONTROL is not set
+CONFIG_VIDEO_OUTPUT_CONTROL=y
 CONFIG_FB=y
 # CONFIG_FIRMWARE_EDID is not set
 # CONFIG_FB_DDC is not set
@@ -1508,7 +1591,6 @@
 # CONFIG_FB_3DFX is not set
 # CONFIG_FB_VOODOO1 is not set
 # CONFIG_FB_VT8623 is not set
-# CONFIG_FB_CYBLA is not set
 # CONFIG_FB_TRIDENT is not set
 # CONFIG_FB_ARK is not set
 # CONFIG_FB_PM3 is not set
@@ -1534,7 +1616,12 @@
 CONFIG_VGACON_SOFT_SCROLLBACK=y
 CONFIG_VGACON_SOFT_SCROLLBACK_SIZE=64
 CONFIG_DUMMY_CONSOLE=y
-# CONFIG_FRAMEBUFFER_CONSOLE is not set
+CONFIG_FRAMEBUFFER_CONSOLE=y
+# CONFIG_FRAMEBUFFER_CONSOLE_DETECT_PRIMARY is not set
+# CONFIG_FRAMEBUFFER_CONSOLE_ROTATION is not set
+# CONFIG_FONTS is not set
+CONFIG_FONT_8x8=y
+CONFIG_FONT_8x16=y
 CONFIG_LOGO=y
 # CONFIG_LOGO_LINUX_MONO is not set
 # CONFIG_LOGO_LINUX_VGA16 is not set
@@ -1596,6 +1683,8 @@
 # CONFIG_SND_INDIGO is not set
 # CONFIG_SND_INDIGOIO is not set
 # CONFIG_SND_INDIGODJ is not set
+# CONFIG_SND_INDIGOIOX is not set
+# CONFIG_SND_INDIGODJX is not set
 # CONFIG_SND_EMU10K1 is not set
 # CONFIG_SND_EMU10K1X is not set
 # CONFIG_SND_ENS1370 is not set
@@ -1756,7 +1845,6 @@
 # CONFIG_USB_LED is not set
 # CONFIG_USB_CYPRESS_CY7C63 is not set
 # CONFIG_USB_CYTHERM is not set
-# CONFIG_USB_PHIDGET is not set
 # CONFIG_USB_IDMOUSE is not set
 # CONFIG_USB_FTDI_ELAN is not set
 # CONFIG_USB_APPLEDISPLAY is not set
@@ -1776,8 +1864,10 @@
 # LED drivers
 #
 # CONFIG_LEDS_PCA9532 is not set
+# CONFIG_LEDS_LP5521 is not set
 # CONFIG_LEDS_CLEVO_MAIL is not set
 # CONFIG_LEDS_PCA955X is not set
+# CONFIG_LEDS_BD2802 is not set
 
 #
 # LED Triggers
@@ -1786,6 +1876,10 @@
 # CONFIG_LEDS_TRIGGER_TIMER is not set
 # CONFIG_LEDS_TRIGGER_HEARTBEAT is not set
 # CONFIG_LEDS_TRIGGER_DEFAULT_ON is not set
+
+#
+# iptables trigger is under Netfilter config (LED target)
+#
 # CONFIG_ACCESSIBILITY is not set
 # CONFIG_INFINIBAND is not set
 CONFIG_EDAC=y
@@ -1850,6 +1944,7 @@
 # DMA Devices
 #
 # CONFIG_INTEL_IOATDMA is not set
+# CONFIG_AUXDISPLAY is not set
 # CONFIG_UIO is not set
 
 #
@@ -1869,6 +1964,7 @@
 #
 # CONFIG_EXT2_FS is not set
 CONFIG_EXT3_FS=y
+# CONFIG_EXT3_DEFAULTS_TO_ORDERED is not set
 CONFIG_EXT3_FS_XATTR=y
 CONFIG_EXT3_FS_POSIX_ACL=y
 CONFIG_EXT3_FS_SECURITY=y
@@ -1896,6 +1992,11 @@
 CONFIG_GENERIC_ACL=y
 
 #
+# Caches
+#
+# CONFIG_FSCACHE is not set
+
+#
 # CD-ROM/DVD Filesystems
 #
 CONFIG_ISO9660_FS=y
@@ -1925,7 +2026,7 @@
 CONFIG_TMPFS_POSIX_ACL=y
 CONFIG_HUGETLBFS=y
 CONFIG_HUGETLB_PAGE=y
-# CONFIG_CONFIGFS_FS is not set
+CONFIG_CONFIGFS_FS=y
 
 #
 # Miscellaneous filesystems
@@ -1947,6 +2048,7 @@
 # CONFIG_ROMFS_FS is not set
 # CONFIG_SYSV_FS is not set
 # CONFIG_UFS_FS is not set
+# CONFIG_NILFS2_FS is not set
 CONFIG_NETWORK_FILESYSTEMS=y
 CONFIG_NFS_FS=y
 CONFIG_NFS_V3=y
@@ -2046,6 +2148,7 @@
 CONFIG_DEBUG_KERNEL=y
 # CONFIG_DEBUG_SHIRQ is not set
 # CONFIG_DETECT_SOFTLOCKUP is not set
+# CONFIG_DETECT_HUNG_TASK is not set
 # CONFIG_SCHED_DEBUG is not set
 CONFIG_SCHEDSTATS=y
 CONFIG_TIMER_STATS=y
@@ -2061,6 +2164,7 @@
 # CONFIG_LOCK_STAT is not set
 # CONFIG_DEBUG_SPINLOCK_SLEEP is not set
 # CONFIG_DEBUG_LOCKING_API_SELFTESTS is not set
+CONFIG_STACKTRACE=y
 # CONFIG_DEBUG_KOBJECT is not set
 # CONFIG_DEBUG_HIGHMEM is not set
 CONFIG_DEBUG_BUGVERBOSE=y
@@ -2095,7 +2199,6 @@
 CONFIG_EARLY_PRINTK=y
 CONFIG_DEBUG_STACKOVERFLOW=y
 CONFIG_DEBUG_STACK_USAGE=y
-# CONFIG_DEBUG_PAGEALLOC is not set
 # CONFIG_DEBUG_PER_CPU_MAPS is not set
 # CONFIG_X86_PTDUMP is not set
 CONFIG_DEBUG_RODATA=y
@@ -2103,7 +2206,7 @@
 CONFIG_DEBUG_NX_TEST=m
 # CONFIG_4KSTACKS is not set
 CONFIG_DOUBLEFAULT=y
-# CONFIG_MMIOTRACE is not set
+CONFIG_HAVE_MMIOTRACE_SUPPORT=y
 CONFIG_IO_DELAY_TYPE_0X80=0
 CONFIG_IO_DELAY_TYPE_0XED=1
 CONFIG_IO_DELAY_TYPE_UDELAY=2
@@ -2120,8 +2223,7 @@
 #
 # Security options
 #
-CONFIG_KEYS=y
-CONFIG_KEYS_DEBUG_PROC_KEYS=y
+# CONFIG_KEYS is not set
 CONFIG_SECURITY=y
 CONFIG_SECURITY_NETWORK=y
 # CONFIG_SECURITY_NETWORK_XFRM is not set
@@ -2221,17 +2323,20 @@
 # Compression
 #
 # CONFIG_CRYPTO_DEFLATE is not set
+# CONFIG_CRYPTO_ZLIB is not set
 # CONFIG_CRYPTO_LZO is not set
 CONFIG_CRYPTO_HW=y
 # CONFIG_CRYPTO_DEV_PADLOCK is not set
 # CONFIG_CRYPTO_DEV_GEODE is not set
 # CONFIG_CRYPTO_DEV_HIFN_795X is not set
 CONFIG_HAVE_KVM=y
+CONFIG_HAVE_KVM_IRQCHIP=y
 CONFIG_VIRTUALIZATION=y
 # CONFIG_KVM is not set
 # CONFIG_LGUEST is not set
 # CONFIG_VIRTIO_PCI is not set
 # CONFIG_VIRTIO_BALLOON is not set
+CONFIG_BINARY_PRINTF=y
 
 #
 # Library routines
@@ -2248,7 +2353,11 @@
 # CONFIG_LIBCRC32C is not set
 CONFIG_AUDIT_GENERIC=y
 CONFIG_ZLIB_INFLATE=y
-CONFIG_PLIST=y
+CONFIG_DECOMPRESS_GZIP=y
+CONFIG_DECOMPRESS_BZIP2=y
+CONFIG_DECOMPRESS_LZMA=y
 CONFIG_HAS_IOMEM=y
 CONFIG_HAS_IOPORT=y
 CONFIG_HAS_DMA=y
+CONFIG_NLATTR=y
+CONFIG_MODULE_HOOK=y
diff -ruN linux-2.6.29/arch/x86/configs/x86_64_defconfig android_cluster/linux-2.6.29/arch/x86/configs/x86_64_defconfig
--- linux-2.6.29/arch/x86/configs/x86_64_defconfig	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/configs/x86_64_defconfig	2014-05-27 23:04:08.570066493 -0700
@@ -36,6 +36,7 @@
 CONFIG_ARCH_HAS_CPU_RELAX=y
 CONFIG_ARCH_HAS_CACHE_LINE_SIZE=y
 CONFIG_HAVE_SETUP_PER_CPU_AREA=y
+CONFIG_HAVE_DYNAMIC_PER_CPU_AREA=y
 CONFIG_HAVE_CPUMASK_OF_CPU_MAP=y
 CONFIG_ARCH_HIBERNATION_POSSIBLE=y
 CONFIG_ARCH_SUSPEND_POSSIBLE=y
@@ -63,10 +64,17 @@
 CONFIG_INIT_ENV_ARG_LIMIT=32
 CONFIG_LOCALVERSION=""
 # CONFIG_LOCALVERSION_AUTO is not set
+CONFIG_HAVE_KERNEL_GZIP=y
+CONFIG_HAVE_KERNEL_BZIP2=y
+CONFIG_HAVE_KERNEL_LZMA=y
+CONFIG_KERNEL_GZIP=y
+# CONFIG_KERNEL_BZIP2 is not set
+# CONFIG_KERNEL_LZMA is not set
 CONFIG_SWAP=y
 CONFIG_SYSVIPC=y
 CONFIG_SYSVIPC_SYSCTL=y
 CONFIG_POSIX_MQUEUE=y
+CONFIG_POSIX_MQUEUE_SYSCTL=y
 CONFIG_BSD_PROCESS_ACCT=y
 # CONFIG_BSD_PROCESS_ACCT_V3 is not set
 CONFIG_TASKSTATS=y
@@ -102,23 +110,25 @@
 CONFIG_PID_NS=y
 CONFIG_BLK_DEV_INITRD=y
 CONFIG_INITRAMFS_SOURCE=""
+CONFIG_RD_GZIP=y
+CONFIG_RD_BZIP2=y
+CONFIG_RD_LZMA=y
 CONFIG_CC_OPTIMIZE_FOR_SIZE=y
 CONFIG_SYSCTL=y
+CONFIG_ANON_INODES=y
 # CONFIG_EMBEDDED is not set
-CONFIG_UID16=y
 CONFIG_SYSCTL_SYSCALL=y
 CONFIG_KALLSYMS=y
 CONFIG_KALLSYMS_ALL=y
 CONFIG_KALLSYMS_EXTRA_PASS=y
+# CONFIG_STRIP_ASM_SYMS is not set
 CONFIG_HOTPLUG=y
 CONFIG_PRINTK=y
 CONFIG_BUG=y
 CONFIG_ELF_CORE=y
 CONFIG_PCSPKR_PLATFORM=y
-# CONFIG_COMPAT_BRK is not set
 CONFIG_BASE_FULL=y
 CONFIG_FUTEX=y
-CONFIG_ANON_INODES=y
 CONFIG_EPOLL=y
 CONFIG_SIGNALFD=y
 CONFIG_TIMERFD=y
@@ -126,6 +136,7 @@
 CONFIG_SHMEM=y
 CONFIG_VM_EVENT_COUNTERS=y
 CONFIG_SLUB_DEBUG=y
+# CONFIG_COMPAT_BRK is not set
 # CONFIG_SLAB is not set
 CONFIG_SLUB=y
 # CONFIG_SLOB is not set
@@ -158,10 +169,8 @@
 CONFIG_KMOD=y
 CONFIG_STOP_MACHINE=y
 CONFIG_BLOCK=y
-CONFIG_BLK_DEV_IO_TRACE=y
 CONFIG_BLK_DEV_BSG=y
 # CONFIG_BLK_DEV_INTEGRITY is not set
-CONFIG_BLOCK_COMPAT=y
 
 #
 # IO Schedulers
@@ -220,10 +229,10 @@
 # CONFIG_MCORE2 is not set
 CONFIG_GENERIC_CPU=y
 CONFIG_X86_CPU=y
-CONFIG_X86_L1_CACHE_BYTES=128
-CONFIG_X86_INTERNODE_CACHE_BYTES=128
+CONFIG_X86_L1_CACHE_BYTES=64
+CONFIG_X86_INTERNODE_CACHE_BYTES=64
 CONFIG_X86_CMPXCHG=y
-CONFIG_X86_L1_CACHE_SHIFT=7
+CONFIG_X86_L1_CACHE_SHIFT=6
 CONFIG_X86_WP_WORKS_OK=y
 CONFIG_X86_TSC=y
 CONFIG_X86_CMPXCHG64=y
@@ -242,8 +251,8 @@
 CONFIG_NR_CPUS=64
 CONFIG_SCHED_SMT=y
 CONFIG_SCHED_MC=y
-# CONFIG_PREEMPT_NONE is not set
-CONFIG_PREEMPT_VOLUNTARY=y
+CONFIG_PREEMPT_NONE=y
+# CONFIG_PREEMPT_VOLUNTARY is not set
 # CONFIG_PREEMPT is not set
 CONFIG_X86_LOCAL_APIC=y
 CONFIG_X86_IO_APIC=y
@@ -289,6 +298,7 @@
 CONFIG_X86_PAT=y
 CONFIG_EFI=y
 CONFIG_SECCOMP=y
+# CONFIG_CC_STACKPROTECTOR is not set
 # CONFIG_HZ_100 is not set
 # CONFIG_HZ_250 is not set
 # CONFIG_HZ_300 is not set
@@ -331,6 +341,7 @@
 CONFIG_ACPI_AC=y
 CONFIG_ACPI_BATTERY=y
 CONFIG_ACPI_BUTTON=y
+CONFIG_ACPI_VIDEO=y
 CONFIG_ACPI_FAN=y
 CONFIG_ACPI_DOCK=y
 # CONFIG_ACPI_BAY is not set
@@ -406,6 +417,7 @@
 # CONFIG_PCI_LEGACY is not set
 # CONFIG_PCI_DEBUG is not set
 CONFIG_HT_IRQ=y
+# CONFIG_PCI_IOV is not set
 CONFIG_ISA_DMA_API=y
 CONFIG_K8_NB=y
 CONFIG_PCCARD=y
@@ -529,7 +541,6 @@
 # CONFIG_IPV6_TUNNEL is not set
 # CONFIG_IPV6_MULTIPLE_TABLES is not set
 # CONFIG_IPV6_MROUTE is not set
-CONFIG_NETLABEL=y
 CONFIG_NETWORK_SECMARK=y
 CONFIG_NETFILTER=y
 # CONFIG_NETFILTER_DEBUG is not set
@@ -591,7 +602,15 @@
 CONFIG_IP6_NF_MANGLE=y
 # CONFIG_IP_DCCP is not set
 # CONFIG_IP_SCTP is not set
-# CONFIG_TIPC is not set
+CONFIG_TIPC=y
+# CONFIG_TIPC_ADVANCED is not set
+CONFIG_TIPC_UNICLUSTER_FRIENDLY=y
+CONFIG_TIPC_MULTIPLE_LINKS=y
+CONFIG_TIPC_CONFIG_SERVICE=y
+CONFIG_TIPC_SOCKET_API=y
+CONFIG_TIPC_SYSTEM_MSGS=y
+# CONFIG_TIPC_DEBUG is not set
+CONFIG_KRGRPC=y
 # CONFIG_ATM is not set
 # CONFIG_BRIDGE is not set
 # CONFIG_VLAN_8021Q is not set
@@ -604,6 +623,7 @@
 # CONFIG_LAPB is not set
 # CONFIG_ECONET is not set
 # CONFIG_WAN_ROUTER is not set
+# CONFIG_PHONET is not set
 CONFIG_NET_SCHED=y
 
 #
@@ -656,6 +676,7 @@
 #
 # CONFIG_NET_PKTGEN is not set
 # CONFIG_NET_TCPPROBE is not set
+# CONFIG_NET_DROP_MONITOR is not set
 CONFIG_HAMRADIO=y
 
 #
@@ -692,6 +713,53 @@
 # CONFIG_NET_9P is not set
 
 #
+# Cluster support
+#
+CONFIG_KERRIGHED=y
+# CONFIG_KERRIGHED_DEVEL is not set
+CONFIG_KRG_TOOLS=y
+CONFIG_KRG_COMMUNICATION_FRAMEWORK=y
+CONFIG_KRG_AUTONODEID=y
+CONFIG_KRG_HOTPLUG=y
+# CONFIG_KRG_HOTPLUG_DEL is not set
+# CONFIG_KRG_HOTPLUG_XCH is not set
+# CONFIG_KRG_DISABLE_HEARTBEAT is not set
+CONFIG_KRG_KDDM=y
+# CONFIG_KRG_KDDM_DEBUG is not set
+CONFIG_KRG_CAP=y
+CONFIG_KRG_PROCFS=y
+CONFIG_KRG_MM=y
+CONFIG_KRG_DVFS=y
+CONFIG_KRG_FAF=y
+CONFIG_KRG_IPC=y
+CONFIG_KRG_PROC=y
+CONFIG_KRG_EPM=y
+CONFIG_KRG_IPC_EPM=y
+CONFIG_KRG_SCHED=y
+CONFIG_KRG_SCHED_COMPAT=y
+CONFIG_KRG_SCHED_COMPAT_FORCE=m
+# CONFIG_KRG_SCHED_CPU_PROBE is not set
+# CONFIG_KRG_SCHED_MEM_PROBE is not set
+CONFIG_KRG_SCHED_MIGRATION_PROBE=m
+CONFIG_KRG_SCHED_MOSIX_PROBE=m
+CONFIG_KRG_SCHED_THRESHOLD_FILTER=m
+CONFIG_KRG_SCHED_FREQ_LIMIT_FILTER=m
+CONFIG_KRG_SCHED_REMOTE_CACHE_FILTER=m
+# CONFIG_KRG_SCHED_ECHO_POLICY is not set
+CONFIG_KRG_SCHED_MOSIX_LOAD_BALANCER=m
+CONFIG_KRG_SCHED_ROUND_ROBIN_BALANCER=m
+CONFIG_CLUSTER_WIDE_PROC_INFRA=y
+CONFIG_CLUSTER_WIDE_PROC=y
+CONFIG_CLUSTER_WIDE_PROC_CPUINFO=y
+CONFIG_CLUSTER_WIDE_PROC_MEMINFO=y
+CONFIG_CLUSTER_WIDE_PROC_LOADAVG=y
+CONFIG_CLUSTER_WIDE_PROC_STAT=y
+CONFIG_CLUSTER_WIDE_PROC_UPTIME=y
+# CONFIG_KRG_SYSCALL_EXIT_HOOK is not set
+# CONFIG_DEBUG_SEG_FAULT is not set
+# CONFIG_KRG_DEBUG is not set
+
+#
 # Device Drivers
 #
 
@@ -800,6 +868,7 @@
 # CONFIG_SCSI_LOWLEVEL is not set
 # CONFIG_SCSI_LOWLEVEL_PCMCIA is not set
 # CONFIG_SCSI_DH is not set
+# CONFIG_SCSI_OSD_INITIATOR is not set
 CONFIG_ATA=y
 # CONFIG_ATA_NONSTANDARD is not set
 CONFIG_ATA_ACPI=y
@@ -895,6 +964,7 @@
 CONFIG_MACINTOSH_DRIVERS=y
 CONFIG_MAC_EMUMOUSEBTN=y
 CONFIG_NETDEVICES=y
+CONFIG_COMPAT_NET_DEV_OPS=y
 # CONFIG_IFB is not set
 # CONFIG_DUMMY is not set
 # CONFIG_BONDING is not set
@@ -929,6 +999,8 @@
 CONFIG_NET_VENDOR_3COM=y
 # CONFIG_VORTEX is not set
 # CONFIG_TYPHOON is not set
+# CONFIG_ETHOC is not set
+# CONFIG_DNET is not set
 CONFIG_NET_TULIP=y
 # CONFIG_DE2104X is not set
 # CONFIG_TULIP is not set
@@ -975,6 +1047,7 @@
 # CONFIG_E1000E is not set
 # CONFIG_IP1000 is not set
 # CONFIG_IGB is not set
+# CONFIG_IGBVF is not set
 # CONFIG_NS83820 is not set
 # CONFIG_HAMACHI is not set
 # CONFIG_YELLOWFIN is not set
@@ -995,6 +1068,7 @@
 # CONFIG_IXGBE is not set
 # CONFIG_IXGB is not set
 # CONFIG_S2IO is not set
+# CONFIG_VXGE is not set
 # CONFIG_MYRI10GE is not set
 # CONFIG_NETXEN_NIC is not set
 # CONFIG_NIU is not set
@@ -1002,6 +1076,7 @@
 # CONFIG_TEHUTI is not set
 # CONFIG_BNX2X is not set
 # CONFIG_SFC is not set
+# CONFIG_BE2NET is not set
 CONFIG_TR=y
 # CONFIG_IBMOL is not set
 # CONFIG_3C359 is not set
@@ -1017,8 +1092,8 @@
 # CONFIG_IPW2200 is not set
 # CONFIG_LIBERTAS is not set
 # CONFIG_AIRO is not set
-# CONFIG_HERMES is not set
 # CONFIG_ATMEL is not set
+# CONFIG_AT76C50X_USB is not set
 # CONFIG_AIRO_CS is not set
 # CONFIG_PCMCIA_WL3501 is not set
 # CONFIG_PRISM54 is not set
@@ -1144,6 +1219,8 @@
 # CONFIG_TABLET_USB_KBTAB is not set
 # CONFIG_TABLET_USB_WACOM is not set
 CONFIG_INPUT_TOUCHSCREEN=y
+# CONFIG_TOUCHSCREEN_AD7879_I2C is not set
+# CONFIG_TOUCHSCREEN_AD7879 is not set
 # CONFIG_TOUCHSCREEN_FUJITSU is not set
 # CONFIG_TOUCHSCREEN_GUNZE is not set
 # CONFIG_TOUCHSCREEN_ELO is not set
@@ -1234,6 +1311,7 @@
 # CONFIG_LEGACY_PTYS is not set
 # CONFIG_IPMI_HANDLER is not set
 CONFIG_HW_RANDOM=y
+# CONFIG_HW_RANDOM_TIMERIOMEM is not set
 # CONFIG_HW_RANDOM_INTEL is not set
 # CONFIG_HW_RANDOM_AMD is not set
 CONFIG_NVRAM=y
@@ -1316,7 +1394,6 @@
 # CONFIG_SENSORS_PCF8574 is not set
 # CONFIG_PCF8575 is not set
 # CONFIG_SENSORS_PCA9539 is not set
-# CONFIG_SENSORS_PCF8591 is not set
 # CONFIG_SENSORS_MAX6875 is not set
 # CONFIG_SENSORS_TSL2550 is not set
 # CONFIG_I2C_DEBUG_CORE is not set
@@ -1428,7 +1505,7 @@
 # CONFIG_DRM_VIA is not set
 # CONFIG_DRM_SAVAGE is not set
 # CONFIG_VGASTATE is not set
-# CONFIG_VIDEO_OUTPUT_CONTROL is not set
+CONFIG_VIDEO_OUTPUT_CONTROL=y
 CONFIG_FB=y
 # CONFIG_FIRMWARE_EDID is not set
 # CONFIG_FB_DDC is not set
@@ -1505,7 +1582,12 @@
 CONFIG_VGACON_SOFT_SCROLLBACK=y
 CONFIG_VGACON_SOFT_SCROLLBACK_SIZE=64
 CONFIG_DUMMY_CONSOLE=y
-# CONFIG_FRAMEBUFFER_CONSOLE is not set
+CONFIG_FRAMEBUFFER_CONSOLE=y
+# CONFIG_FRAMEBUFFER_CONSOLE_DETECT_PRIMARY is not set
+# CONFIG_FRAMEBUFFER_CONSOLE_ROTATION is not set
+# CONFIG_FONTS is not set
+CONFIG_FONT_8x8=y
+CONFIG_FONT_8x16=y
 CONFIG_LOGO=y
 # CONFIG_LOGO_LINUX_MONO is not set
 # CONFIG_LOGO_LINUX_VGA16 is not set
@@ -1566,6 +1648,8 @@
 # CONFIG_SND_INDIGO is not set
 # CONFIG_SND_INDIGOIO is not set
 # CONFIG_SND_INDIGODJ is not set
+# CONFIG_SND_INDIGOIOX is not set
+# CONFIG_SND_INDIGODJX is not set
 # CONFIG_SND_EMU10K1 is not set
 # CONFIG_SND_EMU10K1X is not set
 # CONFIG_SND_ENS1370 is not set
@@ -1725,7 +1809,6 @@
 # CONFIG_USB_LED is not set
 # CONFIG_USB_CYPRESS_CY7C63 is not set
 # CONFIG_USB_CYTHERM is not set
-# CONFIG_USB_PHIDGET is not set
 # CONFIG_USB_IDMOUSE is not set
 # CONFIG_USB_FTDI_ELAN is not set
 # CONFIG_USB_APPLEDISPLAY is not set
@@ -1745,8 +1828,10 @@
 # LED drivers
 #
 # CONFIG_LEDS_PCA9532 is not set
+# CONFIG_LEDS_LP5521 is not set
 # CONFIG_LEDS_CLEVO_MAIL is not set
 # CONFIG_LEDS_PCA955X is not set
+# CONFIG_LEDS_BD2802 is not set
 
 #
 # LED Triggers
@@ -1755,6 +1840,10 @@
 # CONFIG_LEDS_TRIGGER_TIMER is not set
 # CONFIG_LEDS_TRIGGER_HEARTBEAT is not set
 # CONFIG_LEDS_TRIGGER_DEFAULT_ON is not set
+
+#
+# iptables trigger is under Netfilter config (LED target)
+#
 # CONFIG_ACCESSIBILITY is not set
 # CONFIG_INFINIBAND is not set
 CONFIG_EDAC=y
@@ -1819,6 +1908,7 @@
 # DMA Devices
 #
 # CONFIG_INTEL_IOATDMA is not set
+# CONFIG_AUXDISPLAY is not set
 # CONFIG_UIO is not set
 
 #
@@ -1838,6 +1928,7 @@
 #
 # CONFIG_EXT2_FS is not set
 CONFIG_EXT3_FS=y
+# CONFIG_EXT3_DEFAULTS_TO_ORDERED is not set
 CONFIG_EXT3_FS_XATTR=y
 CONFIG_EXT3_FS_POSIX_ACL=y
 CONFIG_EXT3_FS_SECURITY=y
@@ -1866,6 +1957,11 @@
 CONFIG_GENERIC_ACL=y
 
 #
+# Caches
+#
+# CONFIG_FSCACHE is not set
+
+#
 # CD-ROM/DVD Filesystems
 #
 CONFIG_ISO9660_FS=y
@@ -1917,6 +2013,7 @@
 # CONFIG_ROMFS_FS is not set
 # CONFIG_SYSV_FS is not set
 # CONFIG_UFS_FS is not set
+# CONFIG_NILFS2_FS is not set
 CONFIG_NETWORK_FILESYSTEMS=y
 CONFIG_NFS_FS=y
 CONFIG_NFS_V3=y
@@ -2016,6 +2113,7 @@
 CONFIG_DEBUG_KERNEL=y
 # CONFIG_DEBUG_SHIRQ is not set
 # CONFIG_DETECT_SOFTLOCKUP is not set
+# CONFIG_DETECT_HUNG_TASK is not set
 # CONFIG_SCHED_DEBUG is not set
 CONFIG_SCHEDSTATS=y
 CONFIG_TIMER_STATS=y
@@ -2031,6 +2129,7 @@
 # CONFIG_LOCK_STAT is not set
 # CONFIG_DEBUG_SPINLOCK_SLEEP is not set
 # CONFIG_DEBUG_LOCKING_API_SELFTESTS is not set
+CONFIG_STACKTRACE=y
 # CONFIG_DEBUG_KOBJECT is not set
 CONFIG_DEBUG_BUGVERBOSE=y
 # CONFIG_DEBUG_INFO is not set
@@ -2169,6 +2268,7 @@
 #
 CONFIG_CRYPTO_AES=y
 # CONFIG_CRYPTO_AES_X86_64 is not set
+# CONFIG_CRYPTO_AES_NI_INTEL is not set
 # CONFIG_CRYPTO_ANUBIS is not set
 CONFIG_CRYPTO_ARC4=y
 # CONFIG_CRYPTO_BLOWFISH is not set
@@ -2190,14 +2290,17 @@
 # Compression
 #
 # CONFIG_CRYPTO_DEFLATE is not set
+# CONFIG_CRYPTO_ZLIB is not set
 # CONFIG_CRYPTO_LZO is not set
 CONFIG_CRYPTO_HW=y
 # CONFIG_CRYPTO_DEV_HIFN_795X is not set
 CONFIG_HAVE_KVM=y
+CONFIG_HAVE_KVM_IRQCHIP=y
 CONFIG_VIRTUALIZATION=y
 # CONFIG_KVM is not set
 # CONFIG_VIRTIO_PCI is not set
 # CONFIG_VIRTIO_BALLOON is not set
+CONFIG_BINARY_PRINTF=y
 
 #
 # Library routines
@@ -2213,7 +2316,11 @@
 # CONFIG_CRC7 is not set
 # CONFIG_LIBCRC32C is not set
 CONFIG_ZLIB_INFLATE=y
-CONFIG_PLIST=y
+CONFIG_DECOMPRESS_GZIP=y
+CONFIG_DECOMPRESS_BZIP2=y
+CONFIG_DECOMPRESS_LZMA=y
 CONFIG_HAS_IOMEM=y
 CONFIG_HAS_IOPORT=y
 CONFIG_HAS_DMA=y
+CONFIG_NLATTR=y
+CONFIG_MODULE_HOOK=y
diff -ruN linux-2.6.29/arch/x86/ia32/ia32_aout.c android_cluster/linux-2.6.29/arch/x86/ia32/ia32_aout.c
--- linux-2.6.29/arch/x86/ia32/ia32_aout.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/ia32/ia32_aout.c	2014-05-27 23:04:08.570066493 -0700
@@ -26,6 +26,9 @@
 #include <linux/personality.h>
 #include <linux/init.h>
 #include <linux/jiffies.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krgsyms.h>
+#endif
 
 #include <asm/system.h>
 #include <asm/uaccess.h>
@@ -531,11 +534,24 @@
 
 static int __init init_aout_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	int retval;
+
+	krgsyms_register(KRGSYMS_BINFMTS_ARCH, &aout_format);
+	retval = register_binfmt(&aout_format);
+	if (retval)
+		krgsyms_unregister(KRGSYMS_BINFMTS_ARCH);
+	return retval;
+#else
 	return register_binfmt(&aout_format);
+#endif
 }
 
 static void __exit exit_aout_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	krgsyms_unregister(KRGSYMS_BINFMTS_ARCH);
+#endif
 	unregister_binfmt(&aout_format);
 }
 
diff -ruN linux-2.6.29/arch/x86/include/asm/current.h android_cluster/linux-2.6.29/arch/x86/include/asm/current.h
--- linux-2.6.29/arch/x86/include/asm/current.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/include/asm/current.h	2014-05-27 23:04:08.574066411 -0700
@@ -34,6 +34,14 @@
 
 #endif /* X86_32 */
 
+#ifdef CONFIG_KRG_EPM
+#define krg_current (get_current()->effective_current)
+#define current ({							\
+	struct task_struct *__cur = get_current();			\
+	__cur->effective_current ? __cur->effective_current : __cur;	\
+})
+#else /* !CONFIG_KRG_EPM */
 #define current get_current()
+#endif /* !CONFIG_KRG_EPM */
 
 #endif /* _ASM_X86_CURRENT_H */
diff -ruN linux-2.6.29/arch/x86/include/asm/elf.h android_cluster/linux-2.6.29/arch/x86/include/asm/elf.h
--- linux-2.6.29/arch/x86/include/asm/elf.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/include/asm/elf.h	2014-05-27 23:04:08.574066411 -0700
@@ -333,4 +333,11 @@
 extern unsigned long arch_randomize_brk(struct mm_struct *mm);
 #define arch_randomize_brk arch_randomize_brk
 
+#ifdef CONFIG_KRG_MM
+struct vm_area_struct;
+
+void import_vdso_context(struct vm_area_struct *vma);
+int import_mm_struct_end(struct mm_struct *mm, struct task_struct *task);
+#endif
+
 #endif /* _ASM_X86_ELF_H */
diff -ruN linux-2.6.29/arch/x86/include/asm/kerrighed/cpuinfo.h android_cluster/linux-2.6.29/arch/x86/include/asm/kerrighed/cpuinfo.h
--- linux-2.6.29/arch/x86/include/asm/kerrighed/cpuinfo.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/arch/x86/include/asm/kerrighed/cpuinfo.h	2014-05-27 23:04:08.578066327 -0700
@@ -0,0 +1,8 @@
+#ifndef __X86_CPUINFO_H__
+#define __X86_CPUINFO_H__
+
+#include <asm/processor.h>
+
+typedef struct cpuinfo_x86 cpuinfo_t;
+
+#endif /* __X86_CPUINFO_H__ */
diff -ruN linux-2.6.29/arch/x86/include/asm/kerrighed/meminfo.h android_cluster/linux-2.6.29/arch/x86/include/asm/kerrighed/meminfo.h
--- linux-2.6.29/arch/x86/include/asm/kerrighed/meminfo.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/arch/x86/include/asm/kerrighed/meminfo.h	2014-05-27 23:04:08.578066327 -0700
@@ -0,0 +1,11 @@
+#ifndef __X86_MEMINFO_H__
+#define __X86_MEMINFO_H__
+
+typedef struct {
+	unsigned long direct_map_4k;
+	unsigned long direct_map_2M;
+	unsigned long direct_map_1G;
+	int direct_gbpages;
+} krg_arch_meminfo_t;
+
+#endif /* __X86_MEMINFO_H__ */
diff -ruN linux-2.6.29/arch/x86/include/asm/pgalloc.h android_cluster/linux-2.6.29/arch/x86/include/asm/pgalloc.h
--- linux-2.6.29/arch/x86/include/asm/pgalloc.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/include/asm/pgalloc.h	2014-05-27 23:04:08.582066244 -0700
@@ -4,6 +4,9 @@
 #include <linux/threads.h>
 #include <linux/mm.h>		/* for struct page */
 #include <linux/pagemap.h>
+#ifdef CONFIG_KRG_MM
+#include <linux/interrupt.h>
+#endif
 
 static inline int  __paravirt_pgd_alloc(struct mm_struct *mm) { return 0; }
 
@@ -69,6 +72,11 @@
 #if PAGETABLE_LEVELS > 2
 static inline pmd_t *pmd_alloc_one(struct mm_struct *mm, unsigned long addr)
 {
+#ifdef CONFIG_KRG_MM
+	if (in_atomic())
+		return (pmd_t *)get_zeroed_page(GFP_ATOMIC);
+	else
+#endif
 	return (pmd_t *)get_zeroed_page(GFP_KERNEL|__GFP_REPEAT);
 }
 
@@ -99,6 +107,11 @@
 
 static inline pud_t *pud_alloc_one(struct mm_struct *mm, unsigned long addr)
 {
+#ifdef CONFIG_KRG_MM
+	if (in_atomic())
+		return (pud_t *)get_zeroed_page(GFP_ATOMIC);
+	else
+#endif
 	return (pud_t *)get_zeroed_page(GFP_KERNEL|__GFP_REPEAT);
 }
 
diff -ruN linux-2.6.29/arch/x86/include/asm/pgtable-2level.h android_cluster/linux-2.6.29/arch/x86/include/asm/pgtable-2level.h
--- linux-2.6.29/arch/x86/include/asm/pgtable-2level.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/include/asm/pgtable-2level.h	2014-05-27 23:04:08.582066244 -0700
@@ -59,6 +59,15 @@
  * Bits _PAGE_BIT_PRESENT, _PAGE_BIT_FILE and _PAGE_BIT_PROTNONE are taken,
  * split up the 29 bits of offset into this range:
  */
+#ifdef CONFIG_KRG_MM
+/* Bit _PAGE_OBJ_ENTRY is taken too */
+#define PTE_FILE_MAX_BITS	28
+
+#define pte_to_pgoff(pte) ((pte_val((pte)) & ~0xf) >> 4)	\
+
+#define pgoff_to_pte(off) ((pte_t) { .pte = ((off) << 4) |	\
+				            _PAGE_FILE })
+#else
 #define PTE_FILE_MAX_BITS	29
 #define PTE_FILE_SHIFT1		(_PAGE_BIT_PRESENT + 1)
 #if _PAGE_BIT_FILE < _PAGE_BIT_PROTNONE
@@ -87,8 +96,14 @@
 	 + (((off) >> (PTE_FILE_BITS1 + PTE_FILE_BITS2))		\
 	    << PTE_FILE_SHIFT3)						\
 	 + _PAGE_FILE })
+#endif /* ! CONFIG_KRG_MM */
+
 
 /* Encode and de-code a swap entry */
+#ifdef CONFIG_KRG_MM
+#define SWP_TYPE_BITS		5
+#define SWP_OFFSET_SHIFT	9
+#else
 #if _PAGE_BIT_FILE < _PAGE_BIT_PROTNONE
 #define SWP_TYPE_BITS (_PAGE_BIT_FILE - _PAGE_BIT_PRESENT - 1)
 #define SWP_OFFSET_SHIFT (_PAGE_BIT_PROTNONE + 1)
@@ -96,15 +111,27 @@
 #define SWP_TYPE_BITS (_PAGE_BIT_PROTNONE - _PAGE_BIT_PRESENT - 1)
 #define SWP_OFFSET_SHIFT (_PAGE_BIT_FILE + 1)
 #endif
+#endif /* ! CONFIG_KRG_MM */
 
-#define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT > SWP_TYPE_BITS)
 
+#define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT > SWP_TYPE_BITS)
+#ifdef CONFIG_KRG_MM
+#define __swp_type(x)			(((x).val >> (_PAGE_BIT_FILE + 1)) \
+					 & ((1U << SWP_TYPE_BITS) - 1))
+#else
 #define __swp_type(x)			(((x).val >> (_PAGE_BIT_PRESENT + 1)) \
 					 & ((1U << SWP_TYPE_BITS) - 1))
+#endif
 #define __swp_offset(x)			((x).val >> SWP_OFFSET_SHIFT)
+#ifdef CONFIG_KRG_MM
+#define __swp_entry(type, offset)	((swp_entry_t) { \
+					 ((type) << (_PAGE_BIT_FILE + 1)) \
+					 | ((offset) << SWP_OFFSET_SHIFT) })
+#else
 #define __swp_entry(type, offset)	((swp_entry_t) { \
 					 ((type) << (_PAGE_BIT_PRESENT + 1)) \
 					 | ((offset) << SWP_OFFSET_SHIFT) })
+#endif
 #define __pte_to_swp_entry(pte)		((swp_entry_t) { (pte).pte_low })
 #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })
 
diff -ruN linux-2.6.29/arch/x86/include/asm/pgtable_64.h android_cluster/linux-2.6.29/arch/x86/include/asm/pgtable_64.h
--- linux-2.6.29/arch/x86/include/asm/pgtable_64.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/include/asm/pgtable_64.h	2014-05-27 23:04:08.582066244 -0700
@@ -244,6 +244,10 @@
 extern int direct_gbpages;
 
 /* Encode and de-code a swap entry */
+#ifdef CONFIG_KRG_MM
+#define SWP_TYPE_BITS		5
+#define SWP_OFFSET_SHIFT	9
+#else
 #if _PAGE_BIT_FILE < _PAGE_BIT_PROTNONE
 #define SWP_TYPE_BITS (_PAGE_BIT_FILE - _PAGE_BIT_PRESENT - 1)
 #define SWP_OFFSET_SHIFT (_PAGE_BIT_PROTNONE + 1)
@@ -251,15 +255,27 @@
 #define SWP_TYPE_BITS (_PAGE_BIT_PROTNONE - _PAGE_BIT_PRESENT - 1)
 #define SWP_OFFSET_SHIFT (_PAGE_BIT_FILE + 1)
 #endif
+#endif /* !CONFIG_KRG_MM */
 
 #define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT > SWP_TYPE_BITS)
 
+#ifdef CONFIG_KRG_MM
+#define __swp_type(x)			(((x).val >> (_PAGE_BIT_FILE + 1)) \
+					 & ((1U << SWP_TYPE_BITS) - 1))
+#else
 #define __swp_type(x)			(((x).val >> (_PAGE_BIT_PRESENT + 1)) \
 					 & ((1U << SWP_TYPE_BITS) - 1))
+#endif
 #define __swp_offset(x)			((x).val >> SWP_OFFSET_SHIFT)
+#ifdef CONFIG_KRG_MM
+#define __swp_entry(type, offset)	((swp_entry_t) { \
+					 ((type) << (_PAGE_BIT_FILE + 1)) \
+					 | ((offset) << SWP_OFFSET_SHIFT) })
+#else
 #define __swp_entry(type, offset)	((swp_entry_t) { \
 					 ((type) << (_PAGE_BIT_PRESENT + 1)) \
 					 | ((offset) << SWP_OFFSET_SHIFT) })
+#endif
 #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })
 #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })
 
diff -ruN linux-2.6.29/arch/x86/include/asm/pgtable.h android_cluster/linux-2.6.29/arch/x86/include/asm/pgtable.h
--- linux-2.6.29/arch/x86/include/asm/pgtable.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/include/asm/pgtable.h	2014-05-27 23:04:08.582066244 -0700
@@ -22,10 +22,22 @@
 #define _PAGE_BIT_NX           63       /* No execute: only valid after cpuid check */
 
 /* If _PAGE_BIT_PRESENT is clear, we use these: */
+#ifdef CONFIG_KRG_MM
+
+/* | Offset | Swap device (5 bits) | FILE    | PROT_NONE     | OBJ_ENTRY = 0| PRESENT = 0| */
+/* | ObjEntry address              | FILE = 0| PROT_NONE = 0 | OBJ_ENTRY = 1| PRESENT = 0| */
+#define _PAGE_BIT_OBJ_ENTRY	1
+#define _PAGE_BIT_PROTNONE	2
+#define _PAGE_BIT_FILE		3
+
+/* while standard Linux is */
+/* | Offset                   |PROT_NONE  |  FILE    | Swap device (6 bits) | PRESENT = 0| */
+#else
 /* - if the user mapped it with PROT_NONE; pte_present gives true */
 #define _PAGE_BIT_PROTNONE	_PAGE_BIT_GLOBAL
 /* - set: nonlinear file mapping, saved PTE; unset:swap */
 #define _PAGE_BIT_FILE		_PAGE_BIT_DIRTY
+#endif
 
 #define _PAGE_PRESENT	(_AT(pteval_t, 1) << _PAGE_BIT_PRESENT)
 #define _PAGE_RW	(_AT(pteval_t, 1) << _PAGE_BIT_RW)
@@ -53,6 +65,9 @@
 
 #define _PAGE_FILE	(_AT(pteval_t, 1) << _PAGE_BIT_FILE)
 #define _PAGE_PROTNONE	(_AT(pteval_t, 1) << _PAGE_BIT_PROTNONE)
+#ifdef CONFIG_KRG_MM
+#define _PAGE_OBJ_ENTRY (_AT(pteval_t, 1) << _PAGE_BIT_OBJ_ENTRY)
+#endif
 
 #define _PAGE_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |	\
 			 _PAGE_ACCESSED | _PAGE_DIRTY)
@@ -604,6 +619,43 @@
        memcpy(dst, src, count * sizeof(pgd_t));
 }
 
+#ifdef CONFIG_KRG_MM
+struct kddm_obj;
+static inline void set_pte_obj_entry(pte_t *ptep, struct kddm_obj *obj)
+{
+	pte_t pte = __pte((unsigned long)obj);
+//	pte = pte_set_flags(pte, _PAGE_OBJ_ENTRY);
+	pte = __pte(pte_val(pte) | _PAGE_OBJ_ENTRY);
+	set_pte(ptep, pte);
+}
+
+static inline void set_swap_pte_obj_entry(pte_t *ptep, struct kddm_obj *obj)
+{
+	pte_t pte = __pte((unsigned long)obj);
+//	pte = pte_set_flags(pte, _PAGE_OBJ_ENTRY | _PAGE_FILE);
+	pte = __pte(pte_val(pte) | _PAGE_OBJ_ENTRY | _PAGE_FILE);
+	set_pte(ptep, pte);
+}
+
+static inline struct kddm_obj *get_pte_obj_entry(pte_t *ptep)
+{
+	return (struct kddm_obj *)(pte_val(*ptep) & (~(_PAGE_OBJ_ENTRY |
+						       _PAGE_FILE)));
+}
+
+static inline int pte_obj_entry(pte_t *ptep)
+{
+	return ((pte_val(*ptep) & _PAGE_OBJ_ENTRY) && (!pte_present(*ptep)));
+}
+
+static inline int swap_pte_obj_entry(pte_t *ptep)
+{
+	return ((pte_val(*ptep) & _PAGE_OBJ_ENTRY) &&
+		(pte_val(*ptep) & _PAGE_FILE) &&
+		(!pte_present(*ptep)));
+}
+
+#endif /* KRG_MM */
 
 #include <asm-generic/pgtable.h>
 #endif	/* __ASSEMBLY__ */
diff -ruN linux-2.6.29/arch/x86/include/asm/processor.h android_cluster/linux-2.6.29/arch/x86/include/asm/processor.h
--- linux-2.6.29/arch/x86/include/asm/processor.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/include/asm/processor.h	2014-05-27 23:04:08.582066244 -0700
@@ -110,6 +110,11 @@
 	/* Index into per_cpu list: */
 	u16			cpu_index;
 #endif
+#ifdef CONFIG_KRG_PROCFS
+	int                     krg_cpu_id;
+	unsigned long           cpu_khz;
+#endif /* CONFIG_KRG_PROCFS */
+
 	unsigned int		x86_hyper_vendor;
 } __attribute__((__aligned__(SMP_CACHE_BYTES)));
 
diff -ruN linux-2.6.29/arch/x86/include/asm/thread_info.h android_cluster/linux-2.6.29/arch/x86/include/asm/thread_info.h
--- linux-2.6.29/arch/x86/include/asm/thread_info.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/include/asm/thread_info.h	2014-05-27 23:04:08.586066161 -0700
@@ -93,6 +93,12 @@
 #define TIF_FORCED_TF		24	/* true if TF in eflags artificially */
 #define TIF_DEBUGCTLMSR		25	/* uses thread_struct.debugctlmsr */
 #define TIF_DS_AREA_MSR		26      /* uses thread_struct.ds_area_msr */
+#ifdef CONFIG_KRG_FAF
+#define TIF_RUACCESS            28
+#endif
+#ifdef CONFIG_KRG_EPM
+#define TIF_MIGRATION		29
+#endif
 
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
@@ -114,6 +120,12 @@
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
 #define _TIF_DEBUGCTLMSR	(1 << TIF_DEBUGCTLMSR)
 #define _TIF_DS_AREA_MSR	(1 << TIF_DS_AREA_MSR)
+#ifdef CONFIG_KRG_FAF
+#define _TIF_RUACCESS           (1 << TIF_RUACCESS)
+#endif
+#ifdef CONFIG_KRG_EPM
+#define _TIF_MIGRATION		(1 << TIF_MIGRATION)
+#endif
 
 /* work to do in syscall_trace_enter() */
 #define _TIF_WORK_SYSCALL_ENTRY	\
diff -ruN linux-2.6.29/arch/x86/include/asm/uaccess_32.h android_cluster/linux-2.6.29/arch/x86/include/asm/uaccess_32.h
--- linux-2.6.29/arch/x86/include/asm/uaccess_32.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/include/asm/uaccess_32.h	2014-05-27 23:04:08.586066161 -0700
@@ -99,13 +99,13 @@
 
 		switch (n) {
 		case 1:
-			__get_user_size(*(u8 *)to, from, 1, ret, 1);
+			__get_user_size(*(u8 *)to, (u8 __user *)from, 1, ret, 1);
 			return ret;
 		case 2:
-			__get_user_size(*(u16 *)to, from, 2, ret, 2);
+			__get_user_size(*(u16 *)to, (u16 __user *)from, 2, ret, 2);
 			return ret;
 		case 4:
-			__get_user_size(*(u32 *)to, from, 4, ret, 4);
+			__get_user_size(*(u32 *)to, (u32 __user *)from, 4, ret, 4);
 			return ret;
 		}
 	}
@@ -143,13 +143,13 @@
 
 		switch (n) {
 		case 1:
-			__get_user_size(*(u8 *)to, from, 1, ret, 1);
+			__get_user_size(*(u8 *)to, (u8 __user *)from, 1, ret, 1);
 			return ret;
 		case 2:
-			__get_user_size(*(u16 *)to, from, 2, ret, 2);
+			__get_user_size(*(u16 *)to, (u16 __user *)from, 2, ret, 2);
 			return ret;
 		case 4:
-			__get_user_size(*(u32 *)to, from, 4, ret, 4);
+			__get_user_size(*(u32 *)to, (u32 __user *)from, 4, ret, 4);
 			return ret;
 		}
 	}
@@ -165,13 +165,13 @@
 
 		switch (n) {
 		case 1:
-			__get_user_size(*(u8 *)to, from, 1, ret, 1);
+			__get_user_size(*(u8 *)to, (u8 __user *)from, 1, ret, 1);
 			return ret;
 		case 2:
-			__get_user_size(*(u16 *)to, from, 2, ret, 2);
+			__get_user_size(*(u16 *)to, (u16 __user *)from, 2, ret, 2);
 			return ret;
 		case 4:
-			__get_user_size(*(u32 *)to, from, 4, ret, 4);
+			__get_user_size(*(u32 *)to, (u32 __user *)from, 4, ret, 4);
 			return ret;
 		}
 	}
diff -ruN linux-2.6.29/arch/x86/include/asm/uaccess_64.h android_cluster/linux-2.6.29/arch/x86/include/asm/uaccess_64.h
--- linux-2.6.29/arch/x86/include/asm/uaccess_64.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/include/asm/uaccess_64.h	2014-05-27 23:04:08.586066161 -0700
@@ -119,7 +119,13 @@
 	int ret = 0;
 
 	might_fault();
+#ifdef CONFIG_KRG_FAF
+	/* Combine two remote accesses into a single one */
+	if (!__builtin_constant_p(size)
+	    || unlikely(test_thread_flag(TIF_RUACCESS)))
+#else
 	if (!__builtin_constant_p(size))
+#endif
 		return copy_user_generic((__force void *)dst,
 					 (__force void *)src, size);
 	switch (size) {
diff -ruN linux-2.6.29/arch/x86/include/asm/uaccess.h android_cluster/linux-2.6.29/arch/x86/include/asm/uaccess.h
--- linux-2.6.29/arch/x86/include/asm/uaccess.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/include/asm/uaccess.h	2014-05-27 23:04:08.586066161 -0700
@@ -186,6 +186,34 @@
 
 
 #ifdef CONFIG_X86_32
+#ifdef CONFIG_KRG_FAF
+
+#define __put_user_asm_u64(x, addr, err)			\
+	asm volatile("1:	movl %%eax,0(%2)\n"			\
+		     "2:	movl %%edx,4(%2)\n"			\
+		     "3:\n"						\
+		     ".section .fixup,\"ax\"\n"				\
+		     "4:	subl $4,%%esp\n"			\
+		     "  movl $8,(%%esp)\n"				\
+		     "	pushl %%edx\n"					\
+		     "	pushl %%eax\n"					\
+		     "	movl %2,%%eax\n"					\
+		     "	call ruaccess_put_user_asm\n"			\
+		     "	testl %%eax,%%eax\n"				\
+		     "	popl %%eax\n"					\
+		     "	popl %%edx\n"					\
+		     "	lea 4(%%esp),%%esp\n"				\
+		     "	jz 3b\n"					\
+		     "5:	movl %3,%0\n"				\
+		     "	jmp 3b\n"					\
+		     ".previous\n"					\
+		     _ASM_EXTABLE(1b, 4b)				\
+		     _ASM_EXTABLE(2b, 5b)				\
+		     : "=r" (err)					\
+		     : "A" (x), "r" (addr), "i" (-EFAULT), "0" (err))
+
+#else /* !CONFIG_KRG_FAF */
+
 #define __put_user_u64(x, addr, err)					\
 	asm volatile("1:	movl %%eax,0(%2)\n"			\
 		     "2:	movl %%edx,4(%2)\n"			\
@@ -199,6 +227,9 @@
 		     : "=r" (err)					\
 		     : "A" (x), "r" (addr), "i" (-EFAULT), "0" (err))
 
+#endif /* !CONFIG_KRG_FAF */
+
+
 #define __put_user_x8(x, ptr, __ret_pu)				\
 	asm volatile("call __put_user_8" : "=a" (__ret_pu)	\
 		     : "A" ((typeof(*(ptr)))(x)), "c" (ptr) : "ebx")
@@ -338,6 +369,37 @@
 	}								\
 } while (0)
 
+#ifdef CONFIG_KRG_FAF
+
+extern int ruaccess_get_user_asm(void);
+
+#define __get_user_asm(x, addr, err, itype, rtype, ltype, errret)	\
+	asm volatile("1:	mov"itype" %2,%"rtype"1\n"		\
+		     "2:\n"						\
+		     ".section .fixup,\"ax\"\n"				\
+		     "3:	sub $16,%%"_ASM_SP"\n"			\
+		     "	push %%"_ASM_AX"\n"				\
+		     "	"_ASM_MOV" %5,16(%%"_ASM_SP")\n"		\
+		     "	lea %2,%%"_ASM_AX"\n"				\
+		     "	call ruaccess_get_user_asm\n"			\
+		     "	testl %%eax,%%eax\n"				\
+		     "	pop %%"_ASM_AX"\n"				\
+		     "	jnz 4f\n"					\
+		     "	mov"itype" (%%"_ASM_SP"),%"rtype"1\n"		\
+		     "	add $16,%%"_ASM_SP"\n"				\
+		     "	jmp 2b\n"					\
+		     "4:	add $16,%%"_ASM_SP"\n"			\
+		     "	mov %3,%0\n"					\
+		     "	xor"itype" %"rtype"1,%"rtype"1\n"		\
+		     "	jmp 2b\n"					\
+		     ".previous\n"					\
+		     _ASM_EXTABLE(1b, 3b)				\
+		     : "=r" (err), ltype (x)				\
+		     : "m" (__m(addr)), "i" (errret), "0" (err),	\
+		       "i" (sizeof(*(addr))))
+
+#else /* !CONFIG_KRG_FAF */
+
 #define __get_user_asm(x, addr, err, itype, rtype, ltype, errret)	\
 	asm volatile("1:	mov"itype" %2,%"rtype"1\n"		\
 		     "2:\n"						\
@@ -350,6 +412,8 @@
 		     : "=r" (err), ltype(x)				\
 		     : "m" (__m(addr)), "i" (errret), "0" (err))
 
+#endif /* !CONFIG_KRG_FAF */
+
 #define __put_user_nocheck(x, ptr, size)			\
 ({								\
 	int __pu_err;						\
@@ -375,6 +439,34 @@
  * we do not write to any memory gcc knows about, so there are no
  * aliasing issues.
  */
+#ifdef CONFIG_KRG_FAF
+
+extern int ruaccess_put_user_asm(void);
+
+#define __put_user_asm(x, addr, err, itype, rtype, ltype, errret)	\
+	asm volatile("1:	mov"itype" %"rtype"1,%2\n"		\
+		     "2:\n"						\
+		     ".section .fixup,\"ax\"\n"				\
+		     "3:	push %%"_ASM_AX"\n"			\
+		     "	sub $16,%%"_ASM_SP"\n"				\
+		     "	mov"itype" %"rtype"1,(%%"_ASM_SP")\n"		\
+		     "	"_ASM_MOV" %5,8(%%"_ASM_SP")\n"			\
+		     "	lea %2,%%"_ASM_AX"\n"				\
+		     "	call ruaccess_put_user_asm\n"			\
+		     "	add $16,%%"_ASM_SP"\n"				\
+		     "	testl %%eax,%%eax\n"				\
+		     "	pop %%"_ASM_AX"\n"				\
+		     "	jz 2b\n"					\
+		     "	mov %3,%0\n"					\
+		     "	jmp 2b\n"					\
+		     ".previous\n"					\
+		     _ASM_EXTABLE(1b, 3b)				\
+		     : "=r"(err)					\
+		     : ltype(x), "m" (__m(addr)), "i" (errret), "0" (err), \
+		       "i" (sizeof(*(addr))))
+
+#else /* CONFIG_KRG_FAF */
+
 #define __put_user_asm(x, addr, err, itype, rtype, ltype, errret)	\
 	asm volatile("1:	mov"itype" %"rtype"1,%2\n"		\
 		     "2:\n"						\
@@ -385,6 +477,9 @@
 		     _ASM_EXTABLE(1b, 3b)				\
 		     : "=r"(err)					\
 		     : ltype(x), "m" (__m(addr)), "i" (errret), "0" (err))
+
+#endif /* CONFIG_KRG_FAF */
+
 /**
  * __get_user: - Get a simple variable from user space, with less checking.
  * @x:   Variable to store result.
@@ -443,6 +538,18 @@
 } ____cacheline_aligned_in_smp movsl_mask;
 #endif
 
+#ifdef CONFIG_KRG_FAF
+static inline int check_ruaccess(void)
+{
+	struct thread_info *ti = current_thread_info();
+	return (unlikely(test_ti_thread_flag(ti, TIF_RUACCESS))
+		&& segment_eq(ti->addr_limit, USER_DS));
+}
+
+#define ARCH_HAS_RUACCESS
+#define ARCH_HAS_RUACCESS_FIXUP
+#endif /* CONFIG_KRG_FAF */
+
 #define ARCH_HAS_NOCACHE_UACCESS 1
 
 #ifdef CONFIG_X86_32
diff -ruN linux-2.6.29/arch/x86/Kconfig android_cluster/linux-2.6.29/arch/x86/Kconfig
--- linux-2.6.29/arch/x86/Kconfig	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/Kconfig	2014-05-27 23:04:08.558066743 -0700
@@ -1982,6 +1982,8 @@
 
 source "net/Kconfig"
 
+source "kerrighed/Kconfig.Kerrighed"
+
 source "drivers/Kconfig"
 
 source "drivers/firmware/Kconfig"
diff -ruN linux-2.6.29/arch/x86/kernel/cpu/proc.c android_cluster/linux-2.6.29/arch/x86/kernel/cpu/proc.c
--- linux-2.6.29/arch/x86/kernel/cpu/proc.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/kernel/cpu/proc.c	2014-05-27 23:04:08.598065911 -0700
@@ -56,8 +56,11 @@
 #ifdef CONFIG_SMP
 	if (c->x86_max_cores * smp_num_siblings > 1) {
 		seq_printf(m, "physical id\t: %d\n", c->phys_proc_id);
+#ifndef CONFIG_KRG_PROCFS
+		/* TODO: implement support for cpu_core_map */
 		seq_printf(m, "siblings\t: %d\n",
 			   cpus_weight(per_cpu(cpu_core_map, cpu)));
+#endif
 		seq_printf(m, "core id\t\t: %d\n", c->cpu_core_id);
 		seq_printf(m, "cpu cores\t: %d\n", c->booted_cores);
 		seq_printf(m, "apicid\t\t: %d\n", c->apicid);
@@ -86,6 +89,10 @@
 #ifdef CONFIG_SMP
 	cpu = c->cpu_index;
 #endif
+#ifdef CONFIG_KRG_PROCFS
+	if (m->op != &cpuinfo_op)
+		cpu = c->krg_cpu_id;
+#endif
 	seq_printf(m, "processor\t: %u\n"
 		   "vendor_id\t: %s\n"
 		   "cpu family\t: %d\n"
@@ -103,10 +110,15 @@
 		seq_printf(m, "stepping\t: unknown\n");
 
 	if (cpu_has(c, X86_FEATURE_TSC)) {
+#ifdef CONFIG_KRG_PROCFS
+		/* TODO: implement support for cpufreq */
+		unsigned int freq = ((m->op == &cpuinfo_op) ? cpu_khz : c->cpu_khz);
+#else
 		unsigned int freq = cpufreq_quick_get(cpu);
 
 		if (!freq)
 			freq = cpu_khz;
+#endif
 		seq_printf(m, "cpu MHz\t\t: %u.%03u\n",
 			   freq / 1000, (freq % 1000));
 	}
diff -ruN linux-2.6.29/arch/x86/kernel/entry_64.S android_cluster/linux-2.6.29/arch/x86/kernel/entry_64.S
--- linux-2.6.29/arch/x86/kernel/entry_64.S	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/kernel/entry_64.S	2014-05-27 23:04:08.602065829 -0700
@@ -417,6 +417,11 @@
 	call schedule_tail			# rdi: 'prev' task parameter
 
 	GET_THREAD_INFO(%rcx)
+#ifdef CONFIG_KRG_EPM
+	btr $TIF_MIGRATION,TI_flags(%rcx)
+	jc ret_from_migration
+	CFI_REMEMBER_STATE
+#endif
 
 	CFI_REMEMBER_STATE
 	RESTORE_REST
@@ -960,6 +965,27 @@
 	jmp exit_intr
 #endif
 
+#ifdef CONFIG_KRG_EPM
+/*
+ * A newly restarted/migrated process directly context switches to
+ * ret_from_fork, calls schedule_tail, and jumps into this.
+ *
+ * Ptregscall stack frame.
+ *
+ * May return from syscall or from interrupt/exception as well.
+ * If return from interrupt/exception, ORIG_RAX was set to -1 before handling
+ * the migration/checkpoint signal, so jumping to int_signal is ok.
+ *
+ * Ignore trace/audit hooks if migration/checkpoint was done before returning
+ * from syscall. Anyway trace/audit hooks should be made cluster aware before.
+ */
+/* rcx: threadinfo */
+ret_from_migration:
+	CFI_RESTORE_STATE
+	movl TI_flags(%rcx),%edx
+	jmp int_signal
+#endif /* CONFIG_KRG_EPM */
+
 	CFI_ENDPROC
 END(common_interrupt)
 
diff -ruN linux-2.6.29/arch/x86/kernel/i386_ksyms_32.c android_cluster/linux-2.6.29/arch/x86/kernel/i386_ksyms_32.c
--- linux-2.6.29/arch/x86/kernel/i386_ksyms_32.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/kernel/i386_ksyms_32.c	2014-05-27 23:04:08.602065829 -0700
@@ -16,11 +16,17 @@
 EXPORT_SYMBOL(__get_user_1);
 EXPORT_SYMBOL(__get_user_2);
 EXPORT_SYMBOL(__get_user_4);
+#ifdef CONFIG_KRG_FAF
+EXPORT_SYMBOL(ruaccess_get_user_asm);
+#endif /* CONFIG_KRG_FAF */
 
 EXPORT_SYMBOL(__put_user_1);
 EXPORT_SYMBOL(__put_user_2);
 EXPORT_SYMBOL(__put_user_4);
 EXPORT_SYMBOL(__put_user_8);
+#ifdef CONFIG_KRG_FAF
+EXPORT_SYMBOL(ruaccess_put_user_asm);
+#endif /* CONFIG_KRG_FAF */
 
 EXPORT_SYMBOL(strstr);
 
diff -ruN linux-2.6.29/arch/x86/kernel/ldt.c android_cluster/linux-2.6.29/arch/x86/kernel/ldt.c
--- linux-2.6.29/arch/x86/kernel/ldt.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/kernel/ldt.c	2014-05-27 23:04:08.606065745 -0700
@@ -28,7 +28,10 @@
 }
 #endif
 
-static int alloc_ldt(mm_context_t *pc, int mincount, int reload)
+#ifndef CONFIG_KRG_MM
+static
+#endif
+int alloc_ldt(mm_context_t *pc, int mincount, int reload)
 {
 	void *oldldt, *newldt;
 	int oldsize;
diff -ruN linux-2.6.29/arch/x86/kernel/process_32.c android_cluster/linux-2.6.29/arch/x86/kernel/process_32.c
--- linux-2.6.29/arch/x86/kernel/process_32.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/kernel/process_32.c	2014-05-27 23:04:08.610065662 -0700
@@ -297,6 +297,10 @@
 
 	childregs = task_pt_regs(p);
 	*childregs = *regs;
+#ifdef CONFIG_KRG_EPM
+	/* Do not corrupt ax in migration/restart */
+	if (!krg_current || in_krg_do_fork())
+#endif
 	childregs->ax = 0;
 	childregs->sp = sp;
 
@@ -305,6 +309,9 @@
 
 	p->thread.ip = (unsigned long) ret_from_fork;
 
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current)
+#endif
 	savesegment(gs, p->thread.gs);
 
 	tsk = current;
@@ -334,8 +341,15 @@
 
 	ds_copy_thread(p, current);
 
+#ifdef CONFIG_KRG_EPM
+	/* Do not corrupt debugctlmsr in migration/restart */
+	if (!krg_current || in_krg_do_fork()) {
+#endif
 	clear_tsk_thread_flag(p, TIF_DEBUGCTLMSR);
 	p->thread.debugctlmsr = 0;
+#ifdef CONFIG_KRG_EPM
+	}
+#endif
 
 	return err;
 }
@@ -625,6 +639,18 @@
  */
 asmlinkage int sys_vfork(struct pt_regs regs)
 {
+#ifdef CONFIG_KRG_EPM
+#ifdef CONFIG_KRG_CAP
+	if (can_use_krg_cap(current, CAP_DISTANT_FORK))
+#endif
+	{
+		int retval = krg_do_fork(CLONE_VFORK | SIGCHLD,
+					 regs->sp, regs, 0,
+					 NULL, NULL, 0);
+		if (retval > 0)
+			return retval;
+	}
+#endif /* CONFIG_KRG_EPM */
 	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs.sp, &regs, 0, NULL, NULL);
 }
 
diff -ruN linux-2.6.29/arch/x86/kernel/process_64.c android_cluster/linux-2.6.29/arch/x86/kernel/process_64.c
--- linux-2.6.29/arch/x86/kernel/process_64.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/kernel/process_64.c	2014-05-27 23:04:08.610065662 -0700
@@ -329,6 +329,10 @@
 			(THREAD_SIZE + task_stack_page(p))) - 1;
 	*childregs = *regs;
 
+#ifdef CONFIG_KRG_EPM
+	/* Do not corrupt ax in migration/restart */
+	if (!krg_current || in_krg_do_fork())
+#endif
 	childregs->ax = 0;
 	childregs->sp = sp;
 	if (sp == ~0UL)
@@ -339,14 +343,28 @@
 	p->thread.usersp = me->thread.usersp;
 
 	set_tsk_thread_flag(p, TIF_FORK);
+#ifdef CONFIG_KRG_EPM
+	/*
+	 * Migration/restart could have rcx, r11, and rflags corrupted by
+	 * ret_from_fork.
+	 */
+	if (krg_current && !in_krg_do_fork())
+		set_tsk_thread_flag(p, TIF_MIGRATION);
+#endif
 
 	p->thread.fs = me->thread.fs;
 	p->thread.gs = me->thread.gs;
 
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current) {
+#endif
 	savesegment(gs, p->thread.gsindex);
 	savesegment(fs, p->thread.fsindex);
 	savesegment(es, p->thread.es);
 	savesegment(ds, p->thread.ds);
+#ifdef CONFIG_KRG_EPM
+	}
+#endif
 
 	if (unlikely(test_tsk_thread_flag(me, TIF_IO_BITMAP))) {
 		p->thread.io_bitmap_ptr = kmalloc(IO_BITMAP_BYTES, GFP_KERNEL);
@@ -376,8 +394,15 @@
 
 	ds_copy_thread(p, me);
 
+#ifdef CONFIG_KRG_EPM
+	/* Do not corrupt debugctlmsr in migration/restart */
+	if (!krg_current || in_krg_do_fork()) {
+#endif
 	clear_tsk_thread_flag(p, TIF_DEBUGCTLMSR);
 	p->thread.debugctlmsr = 0;
+#ifdef CONFIG_KRG_EPM
+	}
+#endif
 
 	err = 0;
 out:
@@ -712,6 +737,18 @@
  */
 asmlinkage long sys_vfork(struct pt_regs *regs)
 {
+#ifdef CONFIG_KRG_EPM
+#ifdef CONFIG_KRG_CAP
+	if (can_use_krg_cap(current, CAP_DISTANT_FORK))
+#endif
+	{
+		int retval = krg_do_fork(CLONE_VFORK | SIGCHLD,
+					 regs->sp, regs, 0,
+					 NULL, NULL, 0);
+		if (retval > 0)
+			return retval;
+	}
+#endif /* CONFIG_KRG_EPM */
 	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->sp, regs, 0,
 		    NULL, NULL);
 }
diff -ruN linux-2.6.29/arch/x86/kernel/x8664_ksyms_64.c android_cluster/linux-2.6.29/arch/x86/kernel/x8664_ksyms_64.c
--- linux-2.6.29/arch/x86/kernel/x8664_ksyms_64.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/kernel/x8664_ksyms_64.c	2014-05-27 23:04:08.618065496 -0700
@@ -23,10 +23,16 @@
 EXPORT_SYMBOL(__get_user_2);
 EXPORT_SYMBOL(__get_user_4);
 EXPORT_SYMBOL(__get_user_8);
+#ifdef CONFIG_KRG_FAF
+EXPORT_SYMBOL(ruaccess_get_user_asm);
+#endif /* CONFIG_KRG_FAF */
 EXPORT_SYMBOL(__put_user_1);
 EXPORT_SYMBOL(__put_user_2);
 EXPORT_SYMBOL(__put_user_4);
 EXPORT_SYMBOL(__put_user_8);
+#ifdef CONFIG_KRG_FAF
+EXPORT_SYMBOL(ruaccess_put_user_asm);
+#endif /* CONFIG_KRG_FAF */
 
 EXPORT_SYMBOL(copy_user_generic);
 EXPORT_SYMBOL(__copy_user_nocache);
diff -ruN linux-2.6.29/arch/x86/kerrighed/ghost.c android_cluster/linux-2.6.29/arch/x86/kerrighed/ghost.c
--- linux-2.6.29/arch/x86/kerrighed/ghost.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/arch/x86/kerrighed/ghost.c	2014-05-27 23:04:08.618065496 -0700
@@ -0,0 +1,157 @@
+/*
+ *  arch/x86/kerrighed/ghost.c
+ *
+ *  Copyright (C) 2006-2007 Arkadiusz Danilecki
+ *                          Pascal Gallard - Kerlabs, Louis Rilling - Kerlabs
+ */
+
+#include <linux/sched.h>
+#include <asm/processor.h>
+#include <asm/system.h>
+#include <asm/uaccess.h>
+#include <asm/i387.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/ghost_helpers.h>
+
+struct epm_action;
+
+void prepare_to_export(struct task_struct *task)
+{
+	unlazy_fpu(task);
+}
+
+/* struct thread_info */
+
+int export_thread_info(struct epm_action *action,
+		       ghost_t *ghost, struct task_struct *task)
+{
+	int r;
+
+	r = ghost_write(ghost, task->stack, sizeof(struct thread_info));
+	if (r)
+		goto error;
+
+	r = export_exec_domain(action, ghost, task);
+	if (r)
+		goto error;
+	r = export_restart_block(action, ghost, task);
+
+error:
+	return r;
+}
+
+static void __free_thread_info(struct thread_info *ti)
+{
+	ti->task->thread.xstate = NULL;
+	free_thread_info(ti);
+}
+
+int import_thread_info(struct epm_action *action,
+		       ghost_t *ghost, struct task_struct *task)
+{
+	struct thread_info *p;
+	int r;
+
+	p = alloc_thread_info(task);
+	if (!p) {
+		r = -ENOMEM;
+		goto exit;
+	}
+
+	r = ghost_read(ghost, p, sizeof(struct thread_info));
+	/* Required by [__]free_thread_info() */
+	p->task = task;
+	if (r)
+		goto exit_free_thread_info;
+
+	p->exec_domain = import_exec_domain(action, ghost);
+
+	p->preempt_count = 0;
+	p->addr_limit = USER_DS;
+
+	r = import_restart_block(action, ghost, &p->restart_block);
+	if (r)
+		goto exit_free_thread_info;
+
+	task->stack = p;
+
+exit:
+	return r;
+
+exit_free_thread_info:
+	__free_thread_info(p);
+	goto exit;
+}
+
+void unimport_thread_info(struct task_struct *task)
+{
+	__free_thread_info(task->stack);
+}
+
+void free_ghost_thread_info(struct task_struct *ghost)
+{
+	free_thread_info(ghost->stack);
+}
+
+int export_thread_struct(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *tsk)
+{
+	int r = -EBUSY;
+
+	if (test_tsk_thread_flag(tsk, TIF_IO_BITMAP))
+		goto out;
+#ifdef CONFIG_X86_DS
+	if (test_tsk_thread_flag(tsk, TIF_DS_AREA_MSR))
+		goto out;
+#endif
+
+#ifdef CONFIG_X86_64
+	savesegment(gs, tsk->thread.gsindex);
+	savesegment(fs, tsk->thread.fsindex);
+	savesegment(es, tsk->thread.es);
+	savesegment(ds, tsk->thread.ds);
+
+#else /* CONFIG_X86_32 */
+	lazy_save_gs(tsk->thread.gs);
+
+	WARN_ON(tsk->thread.vm86_info);
+#endif /* CONFIG_X86_32 */
+
+	r = ghost_write(ghost, &tsk->thread, sizeof (tsk->thread));
+	if (r)
+		goto out;
+	if (tsk->thread.xstate)
+		r = ghost_write(ghost, tsk->thread.xstate, xstate_size);
+
+out:
+	return r;
+}
+
+int import_thread_struct(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *tsk)
+{
+	int r;
+
+	r = ghost_read(ghost, &tsk->thread, sizeof (tsk->thread));
+	if (r)
+		goto out;
+
+	if (tsk->thread.xstate) {
+		r = -ENOMEM;
+		tsk->thread.xstate = kmem_cache_alloc(task_xstate_cachep,
+						      GFP_KERNEL);
+		if (!tsk->thread.xstate)
+			goto out;
+		r = ghost_read(ghost, tsk->thread.xstate, xstate_size);
+		if (r)
+			free_thread_xstate(tsk);
+	}
+
+out:
+	return r;
+}
+
+void unimport_thread_struct(struct task_struct *task)
+{
+	free_thread_xstate(task);
+}
diff -ruN linux-2.6.29/arch/x86/kerrighed/Makefile android_cluster/linux-2.6.29/arch/x86/kerrighed/Makefile
--- linux-2.6.29/arch/x86/kerrighed/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/arch/x86/kerrighed/Makefile	2014-05-27 23:04:08.618065496 -0700
@@ -0,0 +1,3 @@
+obj-$(CONFIG_KRG_EPM) := ghost.o
+
+EXTRA_CFLAGS += -Wall -Werror
diff -ruN linux-2.6.29/arch/x86/lib/copy_user_64.S android_cluster/linux-2.6.29/arch/x86/lib/copy_user_64.S
--- linux-2.6.29/arch/x86/lib/copy_user_64.S	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/lib/copy_user_64.S	2014-05-27 23:04:08.622065413 -0700
@@ -53,7 +53,11 @@
 102:
 	.section .fixup,"ax"
 103:	addl %ecx,%edx			/* ecx is zerorest also */
+#ifdef CONFIG_KRG_FAF
+	jmp copy_user_check_ruaccess
+#else
 	jmp copy_user_handle_tail
+#endif
 	.previous
 
 	.section __ex_table,"a"
@@ -132,6 +136,10 @@
  */
 ENTRY(copy_user_generic_unrolled)
 	CFI_STARTPROC
+#ifdef CONFIG_KRG_FAF
+	pushq_cfi %rdx	/* save count */
+	CFI_REL_OFFSET rdx,0
+#endif
 	cmpl $8,%edx
 	jb 20f		/* less then 8 bytes, go to byte copy loop */
 	ALIGN_DESTINATION
@@ -179,6 +187,10 @@
 	decl %ecx
 	jnz 21b
 23:	xor %eax,%eax
+#ifdef CONFIG_KRG_FAF
+	leaq 8(%rsp),%rsp
+	CFI_ADJUST_CFA_OFFSET -8
+#endif
 	ret
 
 	.section .fixup,"ax"
@@ -188,7 +200,11 @@
 40:	lea (%rdx,%rcx,8),%rdx
 	jmp 60f
 50:	movl %ecx,%edx
+#ifdef CONFIG_KRG_FAF
+60:	jmp copy_user_check_ruaccess
+#else
 60:	jmp copy_user_handle_tail /* ecx is zerorest also */
+#endif
 	.previous
 
 	.section __ex_table,"a"
@@ -239,6 +255,10 @@
 	CFI_STARTPROC
 	andl %edx,%edx
 	jz 4f
+#ifdef CONFIG_KRG_FAF
+	pushq_cfi %rdx	/* save count */
+	CFI_REL_OFFSET rdx,0
+#endif
 	cmpl $8,%edx
 	jb 2f		/* less than 8 bytes, go to byte copy loop */
 	ALIGN_DESTINATION
@@ -250,13 +270,21 @@
 2:	movl %edx,%ecx
 3:	rep
 	movsb
+#ifdef CONFIG_KRG_FAF
+	leaq 8(%rsp),%rsp
+	CFI_ADJUST_CFA_OFFSET -8
+#endif
 4:	xorl %eax,%eax
 	ret
 
 	.section .fixup,"ax"
 11:	lea (%rdx,%rcx,8),%rcx
 12:	movl %ecx,%edx		/* ecx is zerorest also */
+#ifdef CONFIG_KRG_FAF
+	jmp copy_user_check_ruaccess
+#else
 	jmp copy_user_handle_tail
+#endif
 	.previous
 
 	.section __ex_table,"a"
@@ -266,3 +294,70 @@
 	.previous
 	CFI_ENDPROC
 ENDPROC(copy_user_generic_string)
+
+#ifdef CONFIG_KRG_FAF
+ENTRY(copy_user_check_ruaccess)
+	CFI_STARTPROC
+	cmpl %edx,(%rsp)
+	leaq 8(%rsp),%rsp
+	jne copy_user_handle_tail
+	GET_THREAD_INFO(%rax)
+	testl $_TIF_RUACCESS,TI_flags(%rax)
+	jz copy_user_handle_tail
+	cmpq $-1,TI_addr_limit(%rax)
+	je copy_user_handle_tail
+	jmp krg_copy_user_generic
+	CFI_ENDPROC
+ENDPROC(copy_user_check_ruaccess)
+
+/*
+ * Inputs: rax is the address of the hook to call
+ *         the hook's arg (up to 4) are already passed in conventional order
+ * Outputs: rax is 0 if the hook was called, non zero otherwise
+ *	    if rax is 0, rdx is the hook's result.
+ *
+ * Called from inline assembly, so save all possibly clobbered registers
+ */
+ENTRY(usercopy_check_ruaccess)
+	CFI_STARTPROC
+	pushq_cfi %r8
+	CFI_REL_OFFSET r8,0
+	GET_THREAD_INFO(%r8)
+	testl $_TIF_RUACCESS,TI_flags(%r8)
+	jz back_to_local
+	cmpq $-1,TI_addr_limit(%r8)
+	je back_to_local
+	pushq_cfi %rdi
+	CFI_REL_OFFSET rdi,0
+	pushq_cfi %rsi
+	CFI_REL_OFFSET rsi,0
+	pushq_cfi %rcx
+	CFI_REL_OFFSET rcx,0
+	pushq_cfi %r9
+	CFI_REL_OFFSET r9,0
+	pushq_cfi %r10
+	CFI_REL_OFFSET r10,0
+	pushq_cfi %r11
+	CFI_REL_OFFSET r11,0
+	call *%rax
+	movq %rax,%rdx
+	xorl %eax,%eax
+	popq_cfi %r11
+	CFI_RESTORE r11
+	popq_cfi %r10
+	CFI_RESTORE r10
+	popq_cfi %r9
+	CFI_RESTORE r9
+	popq_cfi %rcx
+	CFI_RESTORE rcx
+	popq_cfi %rsi
+	CFI_RESTORE rsi
+	popq_cfi %rdi
+	CFI_RESTORE rdi
+back_to_local:
+	popq_cfi %r8
+	CFI_RESTORE r8
+	ret
+	CFI_ENDPROC
+ENDPROC(usercopy_check_ruaccess)
+#endif
diff -ruN linux-2.6.29/arch/x86/lib/copy_user_nocache_64.S android_cluster/linux-2.6.29/arch/x86/lib/copy_user_nocache_64.S
--- linux-2.6.29/arch/x86/lib/copy_user_nocache_64.S	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/lib/copy_user_nocache_64.S	2014-05-27 23:04:08.622065413 -0700
@@ -33,7 +33,11 @@
 102:
 	.section .fixup,"ax"
 103:	addl %ecx,%edx			/* ecx is zerorest also */
+#ifdef CONFIG_KRG_FAF
+	jmp copy_user_check_ruaccess
+#else
 	jmp copy_user_handle_tail
+#endif
 	.previous
 
 	.section __ex_table,"a"
@@ -50,6 +54,10 @@
  */
 ENTRY(__copy_user_nocache)
 	CFI_STARTPROC
+#ifdef CONFIG_KRG_FAF
+	pushq_cfi %rdx	/* save count */
+	CFI_REL_OFFSET rdx,0
+#endif
 	cmpl $8,%edx
 	jb 20f		/* less then 8 bytes, go to byte copy loop */
 	ALIGN_DESTINATION
@@ -97,6 +105,10 @@
 	decl %ecx
 	jnz 21b
 23:	xorl %eax,%eax
+#ifdef CONFIG_KRG_FAF
+	leaq 8(%rsp),%rsp
+	CFI_ADJUST_CFA_OFFSET -8
+#endif
 	sfence
 	ret
 
@@ -108,7 +120,11 @@
 	jmp 60f
 50:	movl %ecx,%edx
 60:	sfence
+#ifdef CONFIG_KRG_FAF
+	jmp copy_user_check_ruaccess
+#else
 	jmp copy_user_handle_tail
+#endif
 	.previous
 
 	.section __ex_table,"a"
diff -ruN linux-2.6.29/arch/x86/lib/getuser.S android_cluster/linux-2.6.29/arch/x86/lib/getuser.S
--- linux-2.6.29/arch/x86/lib/getuser.S	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/lib/getuser.S	2014-05-27 23:04:08.622065413 -0700
@@ -43,6 +43,11 @@
 1:	movzb (%_ASM_AX),%edx
 	xor %eax,%eax
 	ret
+#ifdef CONFIG_KRG_FAF
+10:
+	mov $1,%_ASM_DX
+	jmp ruaccess_get_user
+#endif
 	CFI_ENDPROC
 ENDPROC(__get_user_1)
 
@@ -56,6 +61,12 @@
 2:	movzwl -1(%_ASM_AX),%edx
 	xor %eax,%eax
 	ret
+#ifdef CONFIG_KRG_FAF
+20:
+	sub $1,%_ASM_AX
+	mov $2,%_ASM_DX
+	jmp ruaccess_get_user
+#endif
 	CFI_ENDPROC
 ENDPROC(__get_user_2)
 
@@ -69,6 +80,12 @@
 3:	mov -3(%_ASM_AX),%edx
 	xor %eax,%eax
 	ret
+#ifdef CONFIG_KRG_FAF
+30:
+	sub $3,%_ASM_AX
+	mov $4,%_ASM_DX
+	jmp ruaccess_get_user
+#endif
 	CFI_ENDPROC
 ENDPROC(__get_user_4)
 
@@ -83,6 +100,12 @@
 4:	movq -7(%_ASM_AX),%_ASM_DX
 	xor %eax,%eax
 	ret
+#ifdef CONFIG_KRG_FAF
+40:
+	sub $7,%_ASM_AX
+	mov $8,%_ASM_DX
+	jmp ruaccess_get_user
+#endif
 	CFI_ENDPROC
 ENDPROC(__get_user_8)
 #endif
@@ -96,9 +119,247 @@
 END(bad_get_user)
 
 .section __ex_table,"a"
+#ifdef CONFIG_KRG_FAF
+	_ASM_PTR 1b,10b
+	_ASM_PTR 2b,20b
+	_ASM_PTR 3b,30b
+#ifdef CONFIG_X86_64
+	_ASM_PTR 4b,40b
+#endif
+.previous
+#else /* CONFIG_KRG_FAF */
 	_ASM_PTR 1b,bad_get_user
 	_ASM_PTR 2b,bad_get_user
 	_ASM_PTR 3b,bad_get_user
 #ifdef CONFIG_X86_64
 	_ASM_PTR 4b,bad_get_user
 #endif
+#endif /* CONFIG_KRG_FAF */
+
+#ifdef CONFIG_KRG_FAF
+
+#ifdef CONFIG_X86_64
+ruaccess_get_user:
+	CFI_STARTPROC
+	pushq_cfi %rcx
+	CFI_REL_OFFSET rcx,0
+	CFI_REMEMBER_STATE
+	GET_THREAD_INFO(%rcx)
+	testl $_TIF_RUACCESS,TI_flags(%rcx)
+	jz no_ruaccess
+	cmpq $-1,TI_addr_limit(%rcx)
+	je no_ruaccess
+	/*
+	 * gcc does not know that we (may) clobber rdi,rsi,rcx,r8,r9,r10,r11,
+	 * and we don't really care about the cost of saving registers here,
+	 * since we are going to wait for the result from a remote host.
+	 */
+	pushq_cfi %rdi
+	CFI_REL_OFFSET rdi,0
+	pushq_cfi %rsi
+	CFI_REL_OFFSET rsi,0
+	pushq_cfi %r8
+	CFI_REL_OFFSET r8,0
+	pushq_cfi %r9
+	CFI_REL_OFFSET r9,0
+	pushq_cfi %r10
+	CFI_REL_OFFSET r10,0
+	pushq_cfi %r11
+	CFI_REL_OFFSET r11,0
+	/* %rdx is zero-filled on success */
+	xorl %ecx,%ecx
+	pushq_cfi %rcx
+	movq %rsp,%rdi
+	movq %rax,%rsi
+	/* %rdx is set by the caller */
+	callq krg_copy_user_generic
+	/* shorter form of 'movq (%rsp),%rdx' */
+	popq_cfi %rdx
+	popq_cfi %r11
+	CFI_RESTORE r11
+	popq_cfi %r10
+	CFI_RESTORE r10
+	popq_cfi %r9
+	CFI_RESTORE r9
+	popq_cfi %r8
+	CFI_RESTORE r8
+	popq_cfi %rsi
+	CFI_RESTORE rsi
+	popq_cfi %rdi
+	CFI_RESTORE rdi
+	popq_cfi %rcx
+	CFI_RESTORE rcx
+	testl %eax,%eax
+	jnz bad_get_user
+	ret
+no_ruaccess:
+	CFI_RESTORE_STATE
+	popq_cfi %rcx
+	CFI_RESTORE rcx
+	jmp bad_get_user
+	CFI_ENDPROC
+END(ruaccess_get_user)
+#else /* CONFIG_X86_32 */
+ruaccess_get_user:
+	CFI_STARTPROC
+	pushl %ecx
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET ecx,0
+	CFI_REMEMBER_STATE
+	GET_THREAD_INFO(%ecx)
+	testl $_TIF_RUACCESS,TI_flags(%ecx)
+	jz no_ruaccess
+	cmpl $-1,TI_addr_limit(%ecx)
+	je no_ruaccess
+	/* %edx is zero-filled on success */
+	xorl %ecx,%ecx
+	pushl %ecx
+	CFI_ADJUST_CFA_OFFSET 4
+	/* C arg4 (zerorest) */
+	pushl %ecx
+	CFI_ADJUST_CFA_OFFSET 4
+	movl %edx,%ecx
+	movl %eax,%edx
+	lea 4(%esp),%eax
+	call krg_copy_user_generic
+	addl $4,%esp
+	/* shorter form of 'movl (%esp),%edx' */
+	popl %edx
+	CFI_ADJUST_CFA_OFFSET -4
+	popl %ecx
+	CFI_ADJUST_CFA_OFFSET -4
+	CFI_RESTORE ecx
+	testl %eax,%eax
+	jnz bad_get_user
+	ret
+no_ruaccess:
+	CFI_RESTORE_STATE
+	popl %ecx
+	CFI_ADJUST_CFA_OFFSET -4
+	CFI_RESTORE ecx
+	jmp bad_get_user
+	CFI_ENDPROC
+END(ruaccess_get_user)
+#endif /* CONFIG_X86_32 */
+
+#ifdef CONFIG_X86_64
+/*
+ * Inputs:	%rax contains the address to copy from
+ *		24(%rsp) is the number of bytes to read
+ *		16(%rsp) contains the remotely read value on success.
+ *
+ * All (possibly) clobbered registers but %rax are saved since we are called
+ * from inline assembly, and don't want to slow down the fast path with added
+ * clobbers.
+ */
+ENTRY(ruaccess_get_user_asm)
+	CFI_STARTPROC
+	pushq_cfi %rdi
+	CFI_REL_OFFSET rdi,0
+	pushq_cfi %rdx
+	CFI_REL_OFFSET rdx,0
+	CFI_REMEMBER_STATE
+	GET_THREAD_INFO(%rdi)
+	/* 24 + 2*8 = 40 */
+	movq 40(%rsp),%rdx
+	testl $_TIF_RUACCESS,TI_flags(%rdi)
+	jz 6f
+	cmpq $-1,TI_addr_limit(%rdi)
+	je 6f
+	pushq_cfi %rsi
+	CFI_REL_OFFSET rsi,0
+	pushq_cfi %rcx
+	CFI_REL_OFFSET rcx,0
+	/* krg_copy_user_generic may clobber r8-r11 */
+	pushq_cfi %r8
+	CFI_REL_OFFSET r8,0
+	pushq_cfi %r9
+	CFI_REL_OFFSET r9,0
+	pushq_cfi %r10
+	CFI_REL_OFFSET r10,0
+	pushq_cfi %r11
+	CFI_REL_OFFSET r11,0
+	/* 16 + 2*8 + 6*8 = 80 */
+	leaq 80(%rsp),%rdi
+	movq %rax,%rsi
+	/* %rdx is already loaded */
+	xorl %ecx,%ecx
+	call krg_copy_user_generic
+	popq_cfi %r11
+	CFI_RESTORE r11
+	popq_cfi %r10
+	CFI_RESTORE r10
+	popq_cfi %r9
+	CFI_RESTORE r9
+	popq_cfi %r8
+	CFI_RESTORE r8
+	popq_cfi %rcx
+	CFI_RESTORE rcx
+	popq_cfi %rsi
+	CFI_RESTORE rsi
+5:
+	popq_cfi %rdx
+	CFI_RESTORE rdx
+	popq_cfi %rdi
+	CFI_RESTORE rdi
+	ret
+6:
+	CFI_RESTORE_STATE
+	movq %rdx,%rax
+	jmp 5b
+	CFI_ENDPROC
+END(ruaccess_get_user_asm)
+#else /* CONFIG_X86_32 */
+/*
+ * Inputs:	%eax contains the address to copy from
+ *		20(%esp) is the number of bytes to read
+ *		8(%esp) contains the remotely read value on success.
+ *
+ * All (possibly) clobbered registers but %eax are saved since we are called
+ * from inline assembly, and don't want to slow down the fast path with added
+ * clobbers.
+ */
+ENTRY(ruaccess_get_user_asm)
+	CFI_STARTPROC
+	pushl %edx
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET edx,0
+	pushl %ecx
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET ecx,0
+	CFI_REMEMBER_STATE
+	GET_THREAD_INFO(%edx)
+	/* C arg3 (n) */
+	/* 20 + 2*4 = 28 */
+	movl 28(%esp),%ecx
+	testl $_TIF_RUACCESS,TI_flags(%edx)
+	jz 6f
+	cmpl $-1,TI_addr_limit(%edx)
+	je 6f
+	/* C arg4 (zerorest) */
+	xorl %edx,%edx
+	pushl %edx
+	CFI_ADJUST_CFA_OFFSET 4
+	movl %eax,%edx
+	/* 8 + 2*4 + 4 = 20 */
+	lea 20(%esp),%eax
+	call krg_copy_user_generic
+	addl $4,%esp
+	CFI_ADJUST_CFA_OFFSET -4
+5:
+	popl %ecx
+	CFI_ADJUST_CFA_OFFSET -4
+	CFI_RESTORE ecx
+	popl %edx
+	CFI_ADJUST_CFA_OFFSET -4
+	CFI_RESTORE edx
+	ret
+6:
+	CFI_RESTORE_STATE
+	movl %ecx,%eax
+	jmp 5b
+	CFI_ENDPROC
+END(ruaccess_get_user_asm)
+#endif /* CONFIG_X86_32 */
+
+#endif /* CONFIG_KRG_FAF */
diff -ruN linux-2.6.29/arch/x86/lib/putuser.S android_cluster/linux-2.6.29/arch/x86/lib/putuser.S
--- linux-2.6.29/arch/x86/lib/putuser.S	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/lib/putuser.S	2014-05-27 23:04:08.622065413 -0700
@@ -41,7 +41,14 @@
 	jae bad_put_user
 1:	movb %al,(%_ASM_CX)
 	xor %eax,%eax
+#ifdef CONFIG_KRG_FAF
+	ret
+10:	mov $1,%_ASM_BX
+	jmp ruaccess_put_user
+	CFI_ENDPROC
+#else
 	EXIT
+#endif
 ENDPROC(__put_user_1)
 
 ENTRY(__put_user_2)
@@ -52,7 +59,14 @@
 	jae bad_put_user
 2:	movw %ax,(%_ASM_CX)
 	xor %eax,%eax
+#ifdef CONFIG_KRG_FAF
+	ret
+20:	mov $2,%_ASM_BX
+	jmp ruaccess_put_user
+	CFI_ENDPROC
+#else
 	EXIT
+#endif
 ENDPROC(__put_user_2)
 
 ENTRY(__put_user_4)
@@ -63,7 +77,14 @@
 	jae bad_put_user
 3:	movl %eax,(%_ASM_CX)
 	xor %eax,%eax
+#ifdef CONFIG_KRG_FAF
+	ret
+30:	mov $4,%_ASM_BX
+	jmp ruaccess_put_user
+	CFI_ENDPROC
+#else
 	EXIT
+#endif
 ENDPROC(__put_user_4)
 
 ENTRY(__put_user_8)
@@ -77,7 +98,14 @@
 5:	movl %edx,4(%_ASM_CX)
 #endif
 	xor %eax,%eax
+#ifdef CONFIG_KRG_FAF
+	ret
+40:	mov $8,%_ASM_BX
+	jmp ruaccess_put_user
+	CFI_ENDPROC
+#else
 	EXIT
+#endif
 ENDPROC(__put_user_8)
 
 bad_put_user:
@@ -87,11 +115,262 @@
 END(bad_put_user)
 
 .section __ex_table,"a"
+#ifdef CONFIG_KRG_FAF
+	_ASM_PTR 1b,10b
+	_ASM_PTR 2b,20b
+	_ASM_PTR 3b,30b
+	_ASM_PTR 4b,40b
+#else /* !CONFIG_KRG_FAF */
 	_ASM_PTR 1b,bad_put_user
 	_ASM_PTR 2b,bad_put_user
 	_ASM_PTR 3b,bad_put_user
 	_ASM_PTR 4b,bad_put_user
+#endif /* !CONFIG_KRG_FAF */
 #ifdef CONFIG_X86_32
 	_ASM_PTR 5b,bad_put_user
 #endif
 .previous
+
+#ifdef CONFIG_KRG_FAF
+
+#ifdef CONFIG_X86_64
+ruaccess_put_user:
+	CFI_STARTPROC
+	pushq_cfi %r8
+	CFI_REL_OFFSET r8,0
+	CFI_REMEMBER_STATE
+	GET_THREAD_INFO(%r8)
+	testl $_TIF_RUACCESS,TI_flags(%r8)
+	jz no_ruaccess
+	cmpq $-1,TI_addr_limit(%r8)
+	je no_ruaccess
+	/*
+	 * gcc does not know that we (may) clobber rdi,rsi,rdx,rcx,r8,r9,r10,r11,
+	 * and we don't really care about the cost of saving registers here,
+	 * since we are going to wait for the result from a remote host.
+	 */
+	pushq_cfi %rdi
+	CFI_REL_OFFSET rdi,0
+	pushq_cfi %rsi
+	CFI_REL_OFFSET rsi,0
+	pushq_cfi %rdx
+	CFI_REL_OFFSET rdx,0
+	pushq_cfi %rcx
+	CFI_REL_OFFSET rcx,0
+	pushq_cfi %r9
+	CFI_REL_OFFSET r9,0
+	pushq_cfi %r10
+	CFI_REL_OFFSET r10,0
+	pushq_cfi %r11
+	CFI_REL_OFFSET r11,0
+	pushq_cfi %rax
+	/* The value to write is at %rsp */
+	movq %rcx,%rdi
+	movq %rsp,%rsi
+	movq %rbx,%rdx
+	xorl %ecx,%ecx
+	callq krg_copy_user_generic
+	addq $8,%rsp
+	CFI_ADJUST_CFA_OFFSET -8
+	popq_cfi %r11
+	CFI_RESTORE r11
+	popq_cfi %r10
+	CFI_RESTORE r10
+	popq_cfi %r9
+	CFI_RESTORE r9
+	popq_cfi %rcx
+	CFI_RESTORE rcx
+	popq_cfi %rdx
+	CFI_RESTORE rdx
+	popq_cfi %rsi
+	CFI_RESTORE rsi
+	popq_cfi %rdi
+	CFI_RESTORE rdi
+	popq_cfi %r8
+	CFI_RESTORE r8
+	testl %eax,%eax
+	jnz bad_put_user
+	ret
+no_ruaccess:
+	CFI_RESTORE_STATE
+	popq_cfi %r8
+	CFI_RESTORE r8
+	jmp bad_put_user
+	CFI_ENDPROC
+END(ruaccess_put_user)
+#else /* CONFIG_X86_32 */
+ruaccess_put_user:
+	CFI_STARTPROC
+	pushl %edx
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET edx,0
+	CFI_REMEMBER_STATE
+	GET_THREAD_INFO(%edx)
+	testl $_TIF_RUACCESS,TI_flags(%edx)
+	jz no_ruaccess
+	cmpq $-1,TI_addr_limit(%edx)
+	je no_ruaccess
+	pushl %eax
+	CFI_ADJUST_CFA_OFFSET 4
+	/* The value to write is at %esp */
+	pushl %ecx
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET ecx,0
+	/* C arg4 (zerorest) */
+	xorl %edx,%edx
+	pushl %edx
+	CFI_ADJUST_CFA_OFFSET 4
+	movl %ecx,%eax
+	movl %ebx,%ecx
+	lea 8(%esp),%edx
+	call krg_copy_user_generic
+	addl $4,%esp
+	CFI_ADJUST_CFA_OFFSET -4
+	popl %ecx
+	CFI_ADJUST_CFA_OFFSET -4
+	CFI_RESTORE ecx
+	addl $4,%esp
+	CFI_ADJUST_CFA_OFFSET -4
+	popl %edx
+	CFI_ADJUST_CFA_OFFSET -4
+	CFI_RESTORE edx
+	testl %eax,%eax
+	jnz bad_put_user
+	ret
+no_ruaccess:
+	CFI_RESTORE_STATE
+	popl %edx
+	CFI_ADJUST_CFA_OFFSET -4
+	CFI_RESTORE edx
+	jmp bad_put_user
+	CFI_ENDPROC
+END(ruaccess_put_user)
+#endif /* CONFIG_X86_32 */
+
+#ifdef CONFIG_X86_64
+/*
+ * Inputs:	%rax contains the address to copy to
+ *		16(%rsp) is the number of bytes to write
+ *		8(%rsp) is the value to write
+ *
+ * Outputs:	%rax contains 0 if a value was successfully written remotely, or
+ *		the number of bytes that were failed to write (either because no
+ *		remote access was setup, or because write failed remotely).
+ *
+ * All (possibly) clobbered registers but %rax are saved since we are called
+ * from inline assembly, and don't want to slow down the fast path with added
+ * clobbers.
+ */
+ENTRY(ruaccess_put_user_asm)
+	CFI_STARTPROC
+	pushq_cfi %rdi
+	CFI_REL_OFFSET rdi,0
+	pushq_cfi %rdx
+	CFI_REL_OFFSET rdx,0
+	CFI_REMEMBER_STATE
+	GET_THREAD_INFO(%rdi)
+	/* 16 + 2*8 = 32 */
+	movq 32(%rsp),%rdx
+	testl $_TIF_RUACCESS,TI_flags(%rdi)
+	jz 6f
+	cmpq $-1,TI_addr_limit(%rdi)
+	je 6f
+	pushq_cfi %rsi
+	CFI_REL_OFFSET rsi,0
+	pushq_cfi %rcx
+	CFI_REL_OFFSET rcx,0
+	/* krg_copy_user_generic may clobber r8-r11 */
+	pushq_cfi %r8
+	CFI_REL_OFFSET r8,0
+	pushq_cfi %r9
+	CFI_REL_OFFSET r9,0
+	pushq_cfi %r10
+	CFI_REL_OFFSET r10,0
+	pushq_cfi %r11
+	CFI_REL_OFFSET r11,0
+	movq %rax,%rdi
+	/* 8 + 2*8 + 6*8 = 72 */
+	leaq 72(%rsp),%rsi
+	/* %rdx is already loaded */
+	xorl %ecx,%ecx
+	call krg_copy_user_generic
+	popq_cfi %r11
+	CFI_RESTORE r11
+	popq_cfi %r10
+	CFI_RESTORE r10
+	popq_cfi %r9
+	CFI_RESTORE r9
+	popq_cfi %r8
+	CFI_RESTORE r8
+	popq_cfi %rcx
+	CFI_RESTORE rcx
+	popq_cfi %rsi
+	CFI_RESTORE rsi
+5:
+	popq_cfi %rdx
+	CFI_RESTORE rdx
+	popq_cfi %rdi
+	CFI_RESTORE rdi
+	ret
+6:
+	CFI_RESTORE_STATE
+	movq %rdx,%rax
+	jmp 5b
+	CFI_ENDPROC
+END(ruaccess_put_user_asm)
+#else /* CONFIG_X86_32 */
+/*
+ * Inputs:	%eax contains the address to copy to
+ *		12(%esp) is the number of bytes to write
+ *		4(%esp) is the value to write
+ *
+ * Outputs:	%eax contains 0 if a value was successfully written remotely, or
+ *		the number of bytes that were failed to write (either because no
+ *		remote access was setup, or because write failed remotely).
+ *
+ * All (possibly) clobbered registers but %eax are saved since we are called
+ * from inline assembly, and don't want to slow down the fast path with added
+ * clobbers.
+ */
+ENTRY(ruaccess_put_user_asm)
+	CFI_STARTPROC
+	pushl %edx
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET edx,0
+	pushl %ecx
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET ecx,0
+	CFI_REMEMBER_STATE
+	GET_THREAD_INFO(%edx)
+	/* 12 + 2*4 = 20 */
+	movl 20(%esp),%ecx
+	testl $_TIF_RUACCESS,TI_flags(%edx)
+	jz 6f
+	cmpl $-1,TI_addr_limit(%edx)
+	je 6f
+	/* C arg4 (zerorest) */
+	xorl %edx,%edx
+	pushl %edx
+	CFI_ADJUST_CFA_OFFSET 4
+	/* 4 + 2*4 + 4 = 16 */
+	lea 16(%esp),%edx
+	call krg_copy_user_generic
+	addl $4,%esp
+	CFI_ADJUST_CFA_OFFSET -4
+5:
+	popl %ecx
+	CFI_ADJUST_CFA_OFFSET -4
+	CFI_RESTORE ecx
+	popl %edx
+	CFI_ADJUST_CFA_OFFSET -4
+	CFI_RESTORE edx
+	ret
+6:
+	CFI_RESTORE_STATE
+	movl %ecx,%eax
+	jmp 5b
+	CFI_ENDPROC
+END(ruaccess_put_user_asm)
+#endif /* CONFIG_X86_32 */
+
+#endif /* CONFIG_KRG_FAF */
diff -ruN linux-2.6.29/arch/x86/lib/usercopy_32.c android_cluster/linux-2.6.29/arch/x86/lib/usercopy_32.c
--- linux-2.6.29/arch/x86/lib/usercopy_32.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/lib/usercopy_32.c	2014-05-27 23:04:08.622065413 -0700
@@ -11,6 +11,9 @@
 #include <linux/module.h>
 #include <linux/backing-dev.h>
 #include <linux/interrupt.h>
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
 #include <asm/uaccess.h>
 #include <asm/mmx.h>
 
@@ -86,6 +89,11 @@
 __strncpy_from_user(char *dst, const char __user *src, long count)
 {
 	long res;
+#ifdef CONFIG_KRG_FAF
+	if (check_ruaccess())
+		res = krg___strncpy_from_user(dst, src, count);
+	else
+#endif
 	__do_strncpy_from_user(dst, src, count, res);
 	return res;
 }
@@ -114,7 +122,16 @@
 {
 	long res = -EFAULT;
 	if (access_ok(VERIFY_READ, src, 1))
+#ifdef CONFIG_KRG_FAF
+	{
+		if (check_ruaccess())
+			res = krg___strncpy_from_user(dst, src, count);
+		else
+#endif
 		__do_strncpy_from_user(dst, src, count, res);
+#ifdef CONFIG_KRG_FAF
+	}
+#endif
 	return res;
 }
 EXPORT_SYMBOL(strncpy_from_user);
@@ -157,7 +174,16 @@
 {
 	might_fault();
 	if (access_ok(VERIFY_WRITE, to, n))
+#ifdef CONFIG_KRG_FAF
+	{
+		if (check_ruaccess())
+			n = krg___clear_user(to, n);
+		else
+#endif
 		__do_clear_user(to, n);
+#ifdef CONFIG_KRG_FAF
+	}
+#endif
 	return n;
 }
 EXPORT_SYMBOL(clear_user);
@@ -176,6 +202,11 @@
 unsigned long
 __clear_user(void __user *to, unsigned long n)
 {
+#ifdef CONFIG_KRG_FAF
+	if (check_ruaccess())
+		n  = krg___clear_user(to, n);
+	else
+#endif
 	__do_clear_user(to, n);
 	return n;
 }
@@ -199,6 +230,10 @@
 
 	might_fault();
 
+#ifdef CONFIG_KRG_FAF
+	if (check_ruaccess())
+		return krg___strnlen_user(s, n);
+#endif
 	__asm__ __volatile__(
 		"	testl %0, %0\n"
 		"	jz 3f\n"
@@ -717,6 +752,10 @@
 unsigned long __copy_to_user_ll(void __user *to, const void *from,
 				unsigned long n)
 {
+#ifdef CONFIG_KRG_FAF
+	if (check_ruaccess())
+		return krg_copy_user_generic(to, from, n, 0);
+#endif
 #ifndef CONFIG_X86_WP_WORKS_OK
 	if (unlikely(boot_cpu_data.wp_works_ok == 0) &&
 			((unsigned long)to) < TASK_SIZE) {
@@ -785,6 +824,11 @@
 unsigned long __copy_from_user_ll(void *to, const void __user *from,
 					unsigned long n)
 {
+#ifdef CONFIG_KRG_FAF
+	if (check_ruaccess())
+		n = krg_copy_user_generic(to, from, n, 1);
+	else
+#endif
 	if (movsl_is_ok(to, from, n))
 		__copy_user_zeroing(to, from, n);
 	else
@@ -796,6 +840,11 @@
 unsigned long __copy_from_user_ll_nozero(void *to, const void __user *from,
 					 unsigned long n)
 {
+#ifdef CONFIG_KRG_FAF
+	if (check_ruaccess())
+		n = krg_copy_user_generic(to, from, n, 0);
+	else
+#endif
 	if (movsl_is_ok(to, from, n))
 		__copy_user(to, from, n);
 	else
@@ -808,6 +857,11 @@
 unsigned long __copy_from_user_ll_nocache(void *to, const void __user *from,
 					unsigned long n)
 {
+#ifdef CONFIG_KRG_FAF
+	if (check_ruaccess())
+		n = krg_copy_user_generic(to, from, n, 1);
+	else
+#endif
 #ifdef CONFIG_X86_INTEL_USERCOPY
 	if (n > 64 && cpu_has_xmm2)
 		n = __copy_user_zeroing_intel_nocache(to, from, n);
@@ -823,6 +877,11 @@
 unsigned long __copy_from_user_ll_nocache_nozero(void *to, const void __user *from,
 					unsigned long n)
 {
+#ifdef CONFIG_KRG_FAF
+	if (check_ruaccess())
+		n = krg_copy_user_generic(to, from, n, 0);
+	else
+#endif
 #ifdef CONFIG_X86_INTEL_USERCOPY
 	if (n > 64 && cpu_has_xmm2)
 		n = __copy_user_intel_nocache(to, from, n);
@@ -852,7 +911,16 @@
 copy_to_user(void __user *to, const void *from, unsigned long n)
 {
 	if (access_ok(VERIFY_WRITE, to, n))
+#ifdef CONFIG_KRG_FAF
+	{
+		if (check_ruaccess())
+			n = kh_copy_user_generic(to, from, n, 0);
+		else
+#endif
 		n = __copy_to_user(to, from, n);
+#ifdef CONFIG_KRG_FAF
+	}
+#endif
 	return n;
 }
 EXPORT_SYMBOL(copy_to_user);
@@ -877,7 +945,16 @@
 copy_from_user(void *to, const void __user *from, unsigned long n)
 {
 	if (access_ok(VERIFY_READ, from, n))
+#ifdef CONFIG_KRG_FAF
+	{
+		if (check_ruaccess())
+			n = kh_copy_user_generic(to, from, n, 1);
+		else
+#endif
 		n = __copy_from_user(to, from, n);
+#ifdef CONFIG_KRG_FAF
+	}
+#endif
 	else
 		memset(to, 0, n);
 	return n;
diff -ruN linux-2.6.29/arch/x86/lib/usercopy_64.c android_cluster/linux-2.6.29/arch/x86/lib/usercopy_64.c
--- linux-2.6.29/arch/x86/lib/usercopy_64.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/lib/usercopy_64.c	2014-05-27 23:04:08.622065413 -0700
@@ -6,12 +6,51 @@
  * Copyright 2002 Andi Kleen <ak@suse.de>
  */
 #include <linux/module.h>
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
 #include <asm/uaccess.h>
 
 /*
  * Copy a null terminated string from userspace.
  */
 
+#ifdef CONFIG_KRG_FAF
+
+#define __do_strncpy_from_user(dst,src,count,res)			   \
+do {									   \
+	long __d0, __d1, __d2;						   \
+	might_sleep();							   \
+	__asm__ __volatile__(						   \
+		"	testq %1,%1\n"					   \
+		"	jz 2f\n"					   \
+		"0:	lodsb\n"					   \
+		"	stosb\n"					   \
+		"	testb %%al,%%al\n"				   \
+		"	jz 1f\n"					   \
+		"	decq %1\n"					   \
+		"	jnz 0b\n"					   \
+		"1:	subq %1,%0\n"					   \
+		"2:\n"							   \
+		".section .fixup,\"ax\"\n"				   \
+		"3:	cmpq %0,%1\n"					   \
+		"	jne 5f\n"					   \
+		"4: 	movq $krg___strncpy_from_user,%%rax\n"		   \
+		"	call usercopy_check_ruaccess\n"			   \
+		" 	testq %%rax,%%rax\n"				   \
+		" 	jz 2b\n"					   \
+		"5:	movq %5,%0\n"					   \
+		"	jmp 2b\n"					   \
+		".previous\n"						   \
+		_ASM_EXTABLE(0b,3b)					   \
+		: "=&d"(res), "=&c"(count), "=&a" (__d0), "=&S" (__d1),	   \
+		  "=&D" (__d2)						   \
+		: "i"(-EFAULT), "0"(count), "1"(count), "3"(src), "4"(dst) \
+		: "memory");						   \
+} while (0)
+
+#else /* !CONFIG_KRG_FAF */
+
 #define __do_strncpy_from_user(dst,src,count,res)			   \
 do {									   \
 	long __d0, __d1, __d2;						   \
@@ -38,6 +77,8 @@
 		: "memory");						   \
 } while (0)
 
+#endif /* !CONFIG_KRG_FAF */
+
 long
 __strncpy_from_user(char *dst, const char __user *src, long count)
 {
@@ -64,9 +105,49 @@
 unsigned long __clear_user(void __user *addr, unsigned long size)
 {
 	long __d0;
+#ifdef CONFIG_KRG_FAF
+	long __d1;
+#endif
 	might_fault();
 	/* no memory constraint because it doesn't change any memory gcc knows
 	   about */
+#ifdef CONFIG_KRG_FAF
+	asm volatile(
+		"	testq  %[size8],%[size8]\n"
+		"	jz     4f\n"
+		"0:	movq %[zero],(%[dst])\n"
+		"	addq   %[eight],%[dst]\n"
+		"	decl %%ecx ; jnz   0b\n"
+		"4:	movq  %[size1],%%rcx\n"
+		"	testl %%ecx,%%ecx\n"
+		"	jz     2f\n"
+		"1:	movb   %b[zero],(%[dst])\n"
+		"	incq   %[dst]\n"
+		"	decl %%ecx ; jnz  1b\n"
+		"2:\n"
+		".section .fixup,\"ax\"\n"
+		"3:	lea 0(%[size1],%[size8],8),%[size8]\n"
+		"4: 	cmpq %[dst],%[orig_dst]\n"
+		"	jne 2b\n"
+		"	pushq %%rax\n"
+		"	pushq %%rdx\n"
+		"	movq $krg___clear_user,%%rax\n"
+		"	movq %[size8],%%rsi\n"
+		"	call usercopy_check_ruaccess\n"
+		"	testq %%rax,%%rax\n"
+		"	jnz 5f\n"
+		"	movq %%rdx,%[size8]\n"
+		"5: 	popq %%rdx\n"
+		"	popq %%rax\n"
+		"	jmp 2b\n"
+		".previous\n"
+		_ASM_EXTABLE(0b,3b)
+		_ASM_EXTABLE(1b,4b)
+		: [size8] "=&c"(size), [dst] "=&D" (__d0), [orig_dst] "=&S" (__d1)
+		: [size1] "r"(size & 7), "[size8]" (size / 8), "[dst]"(addr),
+		  "[orig_dst]" (addr),
+		  [zero] "r" (0UL), [eight] "r" (8UL));
+#else /* !CONFIG_KRG_FAF */
 	asm volatile(
 		"	testq  %[size8],%[size8]\n"
 		"	jz     4f\n"
@@ -89,6 +170,7 @@
 		: [size8] "=&c"(size), [dst] "=&D" (__d0)
 		: [size1] "r"(size & 7), "[size8]" (size / 8), "[dst]"(addr),
 		  [zero] "r" (0UL), [eight] "r" (8UL));
+#endif /* !CONFIG_KRG_FAF */
 	return size;
 }
 EXPORT_SYMBOL(__clear_user);
@@ -115,8 +197,35 @@
 	while (1) {
 		if (res>n)
 			return n+1;
+#ifdef CONFIG_KRG_FAF
+		{
+			long ret = 0;
+			long local_access = 1;
+
+			asm volatile(
+				"1:	movb (%4),%b1\n"
+				"2:\n"
+				".section .fixup,\"ax\"\n"
+				"3:	movq %5,%0\n"
+				"	testq %2,%2\n"
+				"	jnz 2b\n"
+				"	movq $krg___strnlen_user,%%rax\n"
+				"	call usercopy_check_ruaccess\n"
+				"	jmp 2b\n"
+				".previous\n"
+				_ASM_EXTABLE(1b,3b)
+				: "=&r"(ret), "=r"(c), "=d"(res), "=&a"(local_access)
+				: "D"(s), "S"(n), "0"(ret), "2"(res), "3"(local_access));
+			if (ret) {
+				if (!local_access)
+					return res;
+				return 0;
+			}
+		}
+#else /* !CONFIG_KRG_FAF */
 		if (__get_user(c, s))
 			return 0;
+#endif /* !CONFIG_KRG_FAF */
 		if (!c)
 			return res+1;
 		res++;
@@ -139,8 +248,42 @@
 	char c;
 
 	for (;;) {
+#ifdef CONFIG_KRG_FAF
+		{
+			long ret = 0;
+			long local_access = 1;
+			unsigned long fake_limit = ~0UL;
+
+			asm volatile(
+				"	cmpq %6,%4\n"
+				"	jae 4f\n"
+				"1:	movb (%4),%b1\n"
+				"2:\n"
+				".section .fixup,\"ax\"\n"
+				"3:	movq %5,%0\n"
+				"	testq %2,%2\n"
+				"	jnz 2b\n"
+				"	movq $krg___strnlen_user,%%rax\n"
+				"	call usercopy_check_ruaccess\n"
+				"	jmp 2b\n"
+				"4:	movq %5,%0\n"
+				"	jmp 2b\n"
+				".previous\n"
+				_ASM_EXTABLE(1b,3b)
+				: "=&r"(ret), "=r"(c), "=d"(res), "=&a"(local_access)
+				: "D"(s), "S"(fake_limit),
+				  "g"(current_thread_info()->addr_limit.seg),
+				  "0"(ret), "2"(res), "3"(local_access));
+			if (ret) {
+				if (!local_access)
+					return res;
+				return 0;
+			}
+		}
+#else /* !CONFIG_KRG_FAF */
 		if (get_user(c, s))
 			return 0;
+#endif /* !CONFIG_KRG_FAF */
 		if (!c)
 			return res+1;
 		res++;
diff -ruN linux-2.6.29/arch/x86/Makefile android_cluster/linux-2.6.29/arch/x86/Makefile
--- linux-2.6.29/arch/x86/Makefile	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/Makefile	2014-05-27 23:04:08.558066743 -0700
@@ -147,6 +147,9 @@
 # lguest paravirtualization support
 core-$(CONFIG_LGUEST_GUEST) += arch/x86/lguest/
 
+# Kerrighed SSI cluster support
+core-$(CONFIG_KERRIGHED) += arch/x86/kerrighed/
+
 core-y += arch/x86/kernel/
 core-y += arch/x86/mm/
 
diff -ruN linux-2.6.29/arch/x86/mm/pageattr.c android_cluster/linux-2.6.29/arch/x86/mm/pageattr.c
--- linux-2.6.29/arch/x86/mm/pageattr.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/mm/pageattr.c	2014-05-27 23:04:08.634065163 -0700
@@ -11,6 +11,9 @@
 #include <linux/interrupt.h>
 #include <linux/seq_file.h>
 #include <linux/debugfs.h>
+#ifdef CONFIG_KRG_PROCFS
+#include <kerrighed/dynamic_node_info_linker.h>
+#endif
 
 #include <asm/e820.h>
 #include <asm/processor.h>
@@ -82,6 +85,56 @@
 			direct_pages_count[PG_LEVEL_1G] << 20);
 #endif
 }
+
+#ifdef CONFIG_KRG_PROCFS
+void krg_arch_fill_dynamic_node_info(krg_dynamic_node_info_t *info)
+{
+	info->arch_meminfo.direct_map_4k = direct_pages_count[PG_LEVEL_4K];
+	info->arch_meminfo.direct_map_2M = direct_pages_count[PG_LEVEL_2M];
+#ifdef CONFIG_X86_64
+	info->arch_meminfo.direct_map_1G = direct_pages_count[PG_LEVEL_1G];
+	info->arch_meminfo.direct_gbpages = direct_gbpages;
+#else
+	info->arch_meminfo.direct_map_1G = 0;
+	info->arch_meminfo.direct_gbpages = 0;
+#endif
+}
+
+void krg_arch_accumulate_meminfo(const krg_dynamic_node_info_t *local_info,
+				 krg_dynamic_node_info_t *global_info)
+{
+	const krg_arch_meminfo_t *local = &local_info->arch_meminfo;
+	krg_arch_meminfo_t *global = &global_info->arch_meminfo;
+
+	global->direct_map_4k += local->direct_map_4k;
+	global->direct_map_2M += local->direct_map_2M;
+	if (local->direct_gbpages) {
+		global->direct_map_1G += local->direct_map_1G;
+		global->direct_gbpages = 1;
+	}
+}
+
+void krg_arch_report_meminfo(struct seq_file *m,
+			     const krg_dynamic_node_info_t *info)
+{
+	const krg_arch_meminfo_t *arch_info = &info->arch_meminfo;
+
+	seq_printf(m, "DirectMap4k:    %8lu kB\n",
+		   arch_info->direct_map_4k << 2);
+#if defined(CONFIG_X86_64) || defined(CONFIG_X86_PAE)
+	seq_printf(m, "DirectMap2M:    %8lu kB\n",
+		   arch_info->direct_map_2M << 11);
+#else
+	seq_printf(m, "DirectMap4M:    %8lu kB\n",
+		   arch_info->direct_map_2M << 12);
+#endif
+#ifdef CONFIG_X86_64
+	if (arch_info->direct_gbpages)
+		seq_printf(m, "DirectMap1G:    %8lu kB\n",
+			   arch_info->direct_map_1G << 20);
+#endif
+}
+#endif /* CONFIG_KRG_PROCFS */
 #else
 static inline void split_page_count(int level) { }
 #endif
diff -ruN linux-2.6.29/arch/x86/mm/pgtable.c android_cluster/linux-2.6.29/arch/x86/mm/pgtable.c
--- linux-2.6.29/arch/x86/mm/pgtable.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/mm/pgtable.c	2014-05-27 23:04:08.634065163 -0700
@@ -13,6 +13,11 @@
 {
 	struct page *pte;
 
+#ifdef CONFIG_KRG_MM
+	if (in_atomic())
+		pte = alloc_pages(GFP_ATOMIC|__GFP_ZERO, 0);
+	else
+#endif
 #ifdef CONFIG_HIGHPTE
 	pte = alloc_pages(GFP_KERNEL|__GFP_HIGHMEM|__GFP_REPEAT|__GFP_ZERO, 0);
 #else
diff -ruN linux-2.6.29/arch/x86/vdso/vdso32-setup.c android_cluster/linux-2.6.29/arch/x86/vdso/vdso32-setup.c
--- linux-2.6.29/arch/x86/vdso/vdso32-setup.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/vdso/vdso32-setup.c	2014-05-27 23:04:08.638065080 -0700
@@ -309,6 +309,39 @@
 	return 0;
 }
 
+#ifdef CONFIG_KRG_MM
+void import_vdso_context(struct vm_area_struct *vma)
+{
+	if (vdso_enabled != VDSO_ENABLED) {
+		BUG_ON(!vma->vm_mm->context.vdso
+		       && vma->vm_mm->context.vdso != (void *)VDSO_HIGH_BASE);
+		return;
+	}
+
+	if (compat_uses_vma || !compat) {
+		vma->vm_private_data = vdso_pages;
+
+		BUG_ON(vma->vm_start != vma->vm_mm->context.vdso);
+		BUG_ON(vma->vm_end != vma->vm_start + vdso_size);
+	}
+}
+
+int import_mm_struct_end(struct mm_struct *mm, struct task_struct *task)
+{
+	if (vdso_enabled != VDSO_ENABLED) {
+		BUG_ON(!mm->context.vdso
+		       && mm->context.vdso != (void *)VDSO_HIGH_BASE);
+		return 0;
+	}
+
+	task_thread_info(task)->sysenter_return =
+		VDSO32_SYMBOL(mm->context.vdso, SYSENTER_RETURN);
+
+	return 0;
+}
+
+#endif
+
 /* Setup a VMA at program startup for the vsyscall page */
 int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 {
diff -ruN linux-2.6.29/arch/x86/vdso/vma.c android_cluster/linux-2.6.29/arch/x86/vdso/vma.c
--- linux-2.6.29/arch/x86/vdso/vma.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/arch/x86/vdso/vma.c	2014-05-27 23:04:08.638065080 -0700
@@ -96,6 +96,24 @@
 	return addr;
 }
 
+#ifdef CONFIG_KRG_MM
+void import_vdso_context(struct vm_area_struct *vma)
+{
+	if (!vdso_enabled)
+		return;
+
+	vma->vm_private_data = vdso_pages;
+
+	BUG_ON(vma->vm_start != (unsigned long)vma->vm_mm->context.vdso);
+	BUG_ON(vma->vm_end != vma->vm_start + vdso_size);
+}
+
+int import_mm_struct_end(struct mm_struct *mm, struct task_struct *task)
+{
+	return 0;
+}
+#endif
+
 /* Setup a VMA at program startup for the vsyscall page.
    Not called for compat tasks */
 int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
diff -ruN linux-2.6.29/drivers/block/umem.c android_cluster/linux-2.6.29/drivers/block/umem.c
--- linux-2.6.29/drivers/block/umem.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/drivers/block/umem.c	2014-05-27 23:04:08.734063085 -0700
@@ -1076,7 +1076,7 @@
 	.remove		= mm_pci_remove,
 };
 
-static int __init mm_init(void)
+static int __init umem_init(void)
 {
 	int retval, i;
 	int err;
@@ -1123,7 +1123,7 @@
 	return -ENOMEM;
 }
 
-static void __exit mm_cleanup(void)
+static void __exit umem_cleanup(void)
 {
 	int i;
 
@@ -1139,8 +1139,8 @@
 	unregister_blkdev(major_nr, DRIVER_NAME);
 }
 
-module_init(mm_init);
-module_exit(mm_cleanup);
+module_init(umem_init);
+module_exit(umem_cleanup);
 
 MODULE_AUTHOR(DRIVER_AUTHOR);
 MODULE_DESCRIPTION(DRIVER_DESC);
diff -ruN linux-2.6.29/drivers/char/tty_io.c android_cluster/linux-2.6.29/drivers/char/tty_io.c
--- linux-2.6.29/drivers/char/tty_io.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/drivers/char/tty_io.c	2014-05-27 23:04:08.774062253 -0700
@@ -406,7 +406,10 @@
 	return cmd == TIOCSPGRP ? -ENOTTY : -EIO;
 }
 
-static const struct file_operations tty_fops = {
+#ifndef CONFIG_KRG_DVFS
+static
+#endif
+const struct file_operations tty_fops = {
 	.llseek		= no_llseek,
 	.read		= tty_read,
 	.write		= tty_write,
@@ -430,7 +433,10 @@
 	.fasync		= tty_fasync,
 };
 
-static const struct file_operations hung_up_tty_fops = {
+#ifndef CONFIG_KRG_DVFS
+static
+#endif
+const struct file_operations hung_up_tty_fops = {
 	.llseek		= no_llseek,
 	.read		= hung_up_tty_read,
 	.write		= hung_up_tty_write,
diff -ruN linux-2.6.29/drivers/hwmon/coretemp.c android_cluster/linux-2.6.29/drivers/hwmon/coretemp.c
--- linux-2.6.29/drivers/hwmon/coretemp.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/drivers/hwmon/coretemp.c	2014-05-27 23:04:08.814061422 -0700
@@ -157,17 +157,24 @@
 	/* The 100C is default for both mobile and non mobile CPUs */
 
 	int tjmax = 100000;
-	int ismobile = 1;
+	int usemsr_ee = 1;
 	int err;
 	u32 eax, edx;
 
 	/* Early chips have no MSR for TjMax */
 
 	if ((c->x86_model == 0xf) && (c->x86_mask < 4)) {
-		ismobile = 0;
+		usemsr_ee = 0;
 	}
 
-	if ((c->x86_model > 0xe) && (ismobile)) {
+	/* Atoms seems to have TjMax at 90C */
+
+	if (c->x86_model == 0x1c) {
+		usemsr_ee = 0;
+		tjmax = 90000;
+	}
+
+	if ((c->x86_model > 0xe) && (usemsr_ee)) {
 
 		/* Now we can detect the mobile CPU using Intel provided table
 		   http://softwarecommunity.intel.com/Wiki/Mobility/720.htm
@@ -179,13 +186,13 @@
 			dev_warn(dev,
 				 "Unable to access MSR 0x17, assuming desktop"
 				 " CPU\n");
-			ismobile = 0;
+			usemsr_ee = 0;
 		} else if (!(eax & 0x10000000)) {
-			ismobile = 0;
+			usemsr_ee = 0;
 		}
 	}
 
-	if (ismobile) {
+	if (usemsr_ee) {
 
 		err = rdmsr_safe_on_cpu(id, 0xee, &eax, &edx);
 		if (err) {
@@ -195,7 +202,9 @@
 		} else if (eax & 0x40000000) {
 			tjmax = 85000;
 		}
-	} else {
+	/* if we dont use msr EE it means we are desktop CPU (with exeception
+	   of Atom) */
+	} else if (tjmax == 100000) {
 		dev_warn(dev, "Using relative temperature scale!\n");
 	}
 
@@ -248,9 +257,9 @@
 	platform_set_drvdata(pdev, data);
 
 	/* read the still undocumented IA32_TEMPERATURE_TARGET it exists
-	   on older CPUs but not in this register */
+	   on older CPUs but not in this register, Atoms don't have it either */
 
-	if (c->x86_model > 0xe) {
+	if ((c->x86_model > 0xe) && (c->x86_model != 0x1c)) {
 		err = rdmsr_safe_on_cpu(data->id, 0x1a2, &eax, &edx);
 		if (err) {
 			dev_warn(&pdev->dev, "Unable to read"
@@ -413,11 +422,11 @@
 	for_each_online_cpu(i) {
 		struct cpuinfo_x86 *c = &cpu_data(i);
 
-		/* check if family 6, models 0xe, 0xf, 0x16, 0x17, 0x1A */
+		/* check if family 6, models 0xe, 0xf, 0x16, 0x17, 0x1A, 0x1c */
 		if ((c->cpuid_level < 0) || (c->x86 != 0x6) ||
 		    !((c->x86_model == 0xe) || (c->x86_model == 0xf) ||
 			(c->x86_model == 0x16) || (c->x86_model == 0x17) ||
-			(c->x86_model == 0x1A))) {
+			(c->x86_model == 0x1A) || (c->x86_model == 0x1c))) {
 
 			/* supported CPU not found, but report the unknown
 			   family 6 CPU */
diff -ruN linux-2.6.29/drivers/hwmon/Kconfig android_cluster/linux-2.6.29/drivers/hwmon/Kconfig
--- linux-2.6.29/drivers/hwmon/Kconfig	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/drivers/hwmon/Kconfig	2014-05-27 23:04:08.810061505 -0700
@@ -379,12 +379,12 @@
 	  will be called gl520sm.
 
 config SENSORS_CORETEMP
-	tristate "Intel Core (2) Duo/Solo temperature sensor"
+	tristate "Intel Core/Core2/Atom temperature sensor"
 	depends on X86 && EXPERIMENTAL
 	help
 	  If you say yes here you get support for the temperature
-	  sensor inside your CPU. Supported all are all known variants
-	  of Intel Core family.
+	  sensor inside your CPU. Most of the family 6 CPUs
+	  are supported. Check documentation/driver for details.
 
 config SENSORS_IBMAEM
 	tristate "IBM Active Energy Manager temperature/power sensors and control"
diff -ruN linux-2.6.29/drivers/staging/android/binder.c android_cluster/linux-2.6.29/drivers/staging/android/binder.c
--- linux-2.6.29/drivers/staging/android/binder.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/drivers/staging/android/binder.c	2014-06-09 18:19:40.872438059 -0700
@@ -2333,7 +2333,7 @@
 
 		if (t->from) {
 			struct task_struct *sender = t->from->proc->tsk;
-			tr.sender_pid = task_tgid_nr_ns(sender, current->nsproxy->pid_ns);
+			tr.sender_pid = task_tgid_nr_ns(sender, task_active_pid_ns(current));
 		} else {
 			tr.sender_pid = 0;
 		}
diff -ruN linux-2.6.29/fs/aio.c android_cluster/linux-2.6.29/fs/aio.c
--- linux-2.6.29/fs/aio.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/aio.c	2014-05-27 23:04:09.970037398 -0700
@@ -614,7 +614,10 @@
  *	(Note: this routine is intended to be called only
  *	from a kernel thread context)
  */
-static void use_mm(struct mm_struct *mm)
+#ifndef CONFIG_KRG_FAF
+static
+#endif
+void use_mm(struct mm_struct *mm)
 {
 	struct mm_struct *active_mm;
 	struct task_struct *tsk = current;
@@ -638,7 +641,10 @@
  *	(Note: this routine is intended to be called only
  *	from a kernel thread context)
  */
-static void unuse_mm(struct mm_struct *mm)
+#ifndef CONFIG_KRG_FAF
+static
+#endif
+void unuse_mm(struct mm_struct *mm)
 {
 	struct task_struct *tsk = current;
 
diff -ruN linux-2.6.29/fs/binfmt_aout.c android_cluster/linux-2.6.29/fs/binfmt_aout.c
--- linux-2.6.29/fs/binfmt_aout.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/binfmt_aout.c	2014-05-27 23:04:09.974037315 -0700
@@ -24,6 +24,9 @@
 #include <linux/binfmts.h>
 #include <linux/personality.h>
 #include <linux/init.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krgsyms.h>
+#endif
 
 #include <asm/system.h>
 #include <asm/uaccess.h>
@@ -468,11 +471,24 @@
 
 static int __init init_aout_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	int retval;
+
+	krgsyms_register(KRGSYMS_BINFMTS_AOUT, &aout_format);
+	retval = register_binfmt(&aout_format);
+	if (retval)
+		krgsyms_unregister(KRGSYMS_BINFMTS_AOUT);
+	return retval;
+#else
 	return register_binfmt(&aout_format);
+#endif
 }
 
 static void __exit exit_aout_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	krgsyms_unregister(KRGSYMS_BINFMTS_AOUT);
+#endif
 	unregister_binfmt(&aout_format);
 }
 
diff -ruN linux-2.6.29/fs/binfmt_elf.c android_cluster/linux-2.6.29/fs/binfmt_elf.c
--- linux-2.6.29/fs/binfmt_elf.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/binfmt_elf.c	2014-05-27 23:04:09.974037315 -0700
@@ -38,6 +38,10 @@
 #include <linux/random.h>
 #include <linux/elf.h>
 #include <linux/utsname.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/children.h>
+#include <kerrighed/krgsyms.h>
+#endif
 #include <asm/uaccess.h>
 #include <asm/param.h>
 #include <asm/page.h>
@@ -1359,7 +1363,11 @@
 	prstatus->pr_sigpend = p->pending.signal.sig[0];
 	prstatus->pr_sighold = p->blocked.sig[0];
 	prstatus->pr_pid = task_pid_vnr(p);
+#ifdef CONFIG_KRG_EPM
+	prstatus->pr_ppid = krg_get_real_parent_pid(p);
+#else
 	prstatus->pr_ppid = task_pid_vnr(p->real_parent);
+#endif
 	prstatus->pr_pgrp = task_pgrp_vnr(p);
 	prstatus->pr_sid = task_session_vnr(p);
 	if (thread_group_leader(p)) {
@@ -1401,7 +1409,11 @@
 	psinfo->pr_psargs[len] = 0;
 
 	psinfo->pr_pid = task_pid_vnr(p);
+#ifdef CONFIG_KRG_EPM
+	psinfo->pr_ppid = krg_get_real_parent_pid(p);
+#else
 	psinfo->pr_ppid = task_pid_vnr(p->real_parent);
+#endif
 	psinfo->pr_pgrp = task_pgrp_vnr(p);
 	psinfo->pr_sid = task_session_vnr(p);
 
@@ -2086,12 +2098,25 @@
 
 static int __init init_elf_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	int retval;
+
+	krgsyms_register(KRGSYMS_BINFMTS_ELF, &elf_format);
+	retval = register_binfmt(&elf_format);
+	if (retval)
+		krgsyms_unregister(KRGSYMS_BINFMTS_ELF);
+	return retval;
+#else
 	return register_binfmt(&elf_format);
+#endif
 }
 
 static void __exit exit_elf_binfmt(void)
 {
 	/* Remove the COFF and ELF loaders. */
+#ifdef CONFIG_KRG_EPM
+	krgsyms_unregister(KRGSYMS_BINFMTS_ELF);
+#endif
 	unregister_binfmt(&elf_format);
 }
 
diff -ruN linux-2.6.29/fs/binfmt_elf_fdpic.c android_cluster/linux-2.6.29/fs/binfmt_elf_fdpic.c
--- linux-2.6.29/fs/binfmt_elf_fdpic.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/binfmt_elf_fdpic.c	2014-05-27 23:04:09.978037232 -0700
@@ -34,6 +34,10 @@
 #include <linux/elf.h>
 #include <linux/elf-fdpic.h>
 #include <linux/elfcore.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/children.h>
+#include <kerrighed/krgsyms.h>
+#endif
 
 #include <asm/uaccess.h>
 #include <asm/param.h>
@@ -90,11 +94,24 @@
 
 static int __init init_elf_fdpic_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	int retval;
+
+	krgsyms_register(KRGSYMS_BINFMTS_ELF_FDPIC, &elf_fdpic_format);
+	retval = register_binfmt(&elf_fdpic_format);
+	if (retval)
+		krgsyms_unregister(KRGSYMS_BINFMTS_ELF_FDPIC);
+	return retval;
+#else
 	return register_binfmt(&elf_fdpic_format);
+#endif
 }
 
 static void __exit exit_elf_fdpic_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	krgsyms_unregister(KRGSYMS_BINFMTS_ELF_FDPIC);
+#endif
 	unregister_binfmt(&elf_fdpic_format);
 }
 
@@ -1379,7 +1396,11 @@
 	prstatus->pr_sigpend = p->pending.signal.sig[0];
 	prstatus->pr_sighold = p->blocked.sig[0];
 	prstatus->pr_pid = task_pid_vnr(p);
+#ifdef CONFIG_KRG_EPM
+	prstatus->pr_ppid = krg_get_real_parent_pid(p);
+#else
 	prstatus->pr_ppid = task_pid_vnr(p->parent);
+#endif
 	prstatus->pr_pgrp = task_pgrp_vnr(p);
 	prstatus->pr_sid = task_session_vnr(p);
 	if (thread_group_leader(p)) {
@@ -1423,8 +1444,12 @@
 			psinfo->pr_psargs[i] = ' ';
 	psinfo->pr_psargs[len] = 0;
 
-	psinfo->pr_pid = task_pid_vnr(p);
+ 	psinfo->pr_pid = task_pid_vnr(p);
+#ifdef CONFIG_KRG_EPM
+	psinfo->pr_ppid = krg_get_real_parent_pid(p);
+#else
 	psinfo->pr_ppid = task_pid_vnr(p->parent);
+#endif
 	psinfo->pr_pgrp = task_pgrp_vnr(p);
 	psinfo->pr_sid = task_session_vnr(p);
 
diff -ruN linux-2.6.29/fs/binfmt_em86.c android_cluster/linux-2.6.29/fs/binfmt_em86.c
--- linux-2.6.29/fs/binfmt_em86.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/binfmt_em86.c	2014-05-27 23:04:09.978037232 -0700
@@ -18,6 +18,9 @@
 #include <linux/fs.h>
 #include <linux/file.h>
 #include <linux/errno.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krgsyms.h>
+#endif
 
 
 #define EM86_INTERP	"/usr/bin/em86"
@@ -101,11 +104,24 @@
 
 static int __init init_em86_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	int retval;
+
+	krgsyms_register(KRGSYMS_BINFMTS_EM86, &em86_format);
+	retval = register_binfmt(&em86_format);
+	if (retval)
+		krgsyms_unregister(KRGSYMS_BINFMTS_EM86);
+	return retval;
+#else
 	return register_binfmt(&em86_format);
+#endif
 }
 
 static void __exit exit_em86_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	krgsyms_unregister(KRGSYMS_BINFMTS_EM86);
+#endif
 	unregister_binfmt(&em86_format);
 }
 
diff -ruN linux-2.6.29/fs/binfmt_flat.c android_cluster/linux-2.6.29/fs/binfmt_flat.c
--- linux-2.6.29/fs/binfmt_flat.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/binfmt_flat.c	2014-05-27 23:04:09.978037232 -0700
@@ -35,6 +35,9 @@
 #include <linux/init.h>
 #include <linux/flat.h>
 #include <linux/syscalls.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krgsyms.h>
+#endif
 
 #include <asm/byteorder.h>
 #include <asm/system.h>
@@ -917,7 +920,17 @@
 
 static int __init init_flat_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	int retval;
+
+	krgsyms_register(KRGSYMS_BINFMTS_FLAT, &flat_format);
+	retval = register_binfmt(&flat_format);
+	if (retval)
+		krgsyms_unregister(KRGSYMS_BINFMTS_FLAT);
+	return retval;
+#else
 	return register_binfmt(&flat_format);
+#endif
 }
 
 /****************************************************************************/
diff -ruN linux-2.6.29/fs/binfmt_misc.c android_cluster/linux-2.6.29/fs/binfmt_misc.c
--- linux-2.6.29/fs/binfmt_misc.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/binfmt_misc.c	2014-05-27 23:04:09.978037232 -0700
@@ -28,6 +28,9 @@
 #include <linux/mount.h>
 #include <linux/syscalls.h>
 #include <linux/fs.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krgsyms.h>
+#endif
 
 #include <asm/uaccess.h>
 
@@ -723,15 +726,25 @@
 {
 	int err = register_filesystem(&bm_fs_type);
 	if (!err) {
+#ifdef CONFIG_KRG_EPM
+		krgsyms_register(KRGSYMS_BINFMTS_MISC, &misc_format);
+#endif
 		err = register_binfmt(&misc_format);
 		if (err)
 			unregister_filesystem(&bm_fs_type);
+#ifdef CONFIG_KRG_EPM
+		if (err)
+			krgsyms_unregister(KRGSYMS_BINFMTS_MISC);
+#endif
 	}
 	return err;
 }
 
 static void __exit exit_misc_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	krgsyms_unregister(KRGSYMS_BINFMTS_MISC);
+#endif
 	unregister_binfmt(&misc_format);
 	unregister_filesystem(&bm_fs_type);
 }
diff -ruN linux-2.6.29/fs/binfmt_script.c android_cluster/linux-2.6.29/fs/binfmt_script.c
--- linux-2.6.29/fs/binfmt_script.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/binfmt_script.c	2014-05-27 23:04:09.978037232 -0700
@@ -14,6 +14,9 @@
 #include <linux/file.h>
 #include <linux/err.h>
 #include <linux/fs.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krgsyms.h>
+#endif
 
 static int load_script(struct linux_binprm *bprm,struct pt_regs *regs)
 {
@@ -105,11 +108,24 @@
 
 static int __init init_script_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	int retval;
+
+	krgsyms_register(KRGSYMS_BINFMTS_SCRIPT, &script_format);
+	retval = register_binfmt(&script_format);
+	if (retval)
+		krgsyms_unregister(KRGSYMS_BINFMTS_SCRIPT);
+	return retval;
+#else
 	return register_binfmt(&script_format);
+#endif
 }
 
 static void __exit exit_script_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	krgsyms_unregister(KRGSYMS_BINFMTS_SCRIPT);
+#endif
 	unregister_binfmt(&script_format);
 }
 
diff -ruN linux-2.6.29/fs/binfmt_som.c android_cluster/linux-2.6.29/fs/binfmt_som.c
--- linux-2.6.29/fs/binfmt_som.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/binfmt_som.c	2014-05-27 23:04:09.978037232 -0700
@@ -28,6 +28,9 @@
 #include <linux/shm.h>
 #include <linux/personality.h>
 #include <linux/init.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krgsyms.h>
+#endif
 
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
@@ -295,12 +298,25 @@
 
 static int __init init_som_binfmt(void)
 {
+#ifdef CONFIG_KRG_EPM
+	int retval;
+
+	krgsyms_register(KRGSYMS_BINFMTS_SOM, &som_format);
+	retval = register_binfmt(&som_format);
+	if (retval)
+		krgsyms_unregister(KRGSYMS_BINFMTS_SOM);
+	return retval;
+#else
 	return register_binfmt(&som_format);
+#endif
 }
 
 static void __exit exit_som_binfmt(void)
 {
 	/* Remove the SOM loader. */
+#ifdef CONFIG_KRG_EPM
+	krgsyms_unregister(KRGSYMS_BINFMTS_SOM);
+#endif
 	unregister_binfmt(&som_format);
 }
 
diff -ruN linux-2.6.29/fs/configfs/configfs_internal.h android_cluster/linux-2.6.29/fs/configfs/configfs_internal.h
--- linux-2.6.29/fs/configfs/configfs_internal.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/configfs/configfs_internal.h	2014-05-27 23:04:10.002036733 -0700
@@ -39,6 +39,9 @@
 	umode_t			s_mode;
 	struct dentry		* s_dentry;
 	struct iattr		* s_iattr;
+#ifdef CONFIG_LOCKDEP
+	int			s_depth;
+#endif
 };
 
 #define CONFIGFS_ROOT		0x0001
diff -ruN linux-2.6.29/fs/configfs/dir.c android_cluster/linux-2.6.29/fs/configfs/dir.c
--- linux-2.6.29/fs/configfs/dir.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/configfs/dir.c	2014-05-27 23:04:10.002036733 -0700
@@ -78,11 +78,97 @@
 	.d_delete	= configfs_d_delete,
 };
 
+#ifdef CONFIG_LOCKDEP
+
+/*
+ * Helpers to make lockdep happy with our recursive locking of default groups'
+ * inodes (see configfs_attach_group() and configfs_detach_group()).
+ * We put default groups i_mutexes in separate classes according to their depth
+ * from the youngest non-default group ancestor.
+ *
+ * For a non-default group A having default groups A/B, A/C, and A/C/D, default
+ * groups A/B and A/C will have their inode's mutex in class
+ * default_group_class[0], and default group A/C/D will be in
+ * default_group_class[1].
+ *
+ * The lock classes are declared and assigned in inode.c, according to the
+ * s_depth value.
+ * The s_depth value is initialized to -1, adjusted to >= 0 when attaching
+ * default groups, and reset to -1 when all default groups are attached. During
+ * attachment, if configfs_create() sees s_depth > 0, the lock class of the new
+ * inode's mutex is set to default_group_class[s_depth - 1].
+ */
+
+static void configfs_init_dirent_depth(struct configfs_dirent *sd)
+{
+	sd->s_depth = -1;
+}
+
+static void configfs_set_dir_dirent_depth(struct configfs_dirent *parent_sd,
+					  struct configfs_dirent *sd)
+{
+	int parent_depth = parent_sd->s_depth;
+
+	if (parent_depth >= 0)
+		sd->s_depth = parent_depth + 1;
+}
+
+static void
+configfs_adjust_dir_dirent_depth_before_populate(struct configfs_dirent *sd)
+{
+	/*
+	 * item's i_mutex class is already setup, so s_depth is now only
+	 * used to set new sub-directories s_depth, which is always done
+	 * with item's i_mutex locked.
+	 */
+	/*
+	 *  sd->s_depth == -1 iff we are a non default group.
+	 *  else (we are a default group) sd->s_depth > 0 (see
+	 *  create_dir()).
+	 */
+	if (sd->s_depth == -1)
+		/*
+		 * We are a non default group and we are going to create
+		 * default groups.
+		 */
+		sd->s_depth = 0;
+}
+
+static void
+configfs_adjust_dir_dirent_depth_after_populate(struct configfs_dirent *sd)
+{
+	/* We will not create default groups anymore. */
+	sd->s_depth = -1;
+}
+
+#else /* CONFIG_LOCKDEP */
+
+static void configfs_init_dirent_depth(struct configfs_dirent *sd)
+{
+}
+
+static void configfs_set_dir_dirent_depth(struct configfs_dirent *parent_sd,
+					  struct configfs_dirent *sd)
+{
+}
+
+static void
+configfs_adjust_dir_dirent_depth_before_populate(struct configfs_dirent *sd)
+{
+}
+
+static void
+configfs_adjust_dir_dirent_depth_after_populate(struct configfs_dirent *sd)
+{
+}
+
+#endif /* CONFIG_LOCKDEP */
+
 /*
  * Allocates a new configfs_dirent and links it to the parent configfs_dirent
  */
-static struct configfs_dirent *configfs_new_dirent(struct configfs_dirent * parent_sd,
-						void * element)
+static struct configfs_dirent *configfs_new_dirent(struct configfs_dirent *parent_sd,
+						   void *element, int type)
 {
 	struct configfs_dirent * sd;
 
@@ -94,6 +180,8 @@
 	INIT_LIST_HEAD(&sd->s_links);
 	INIT_LIST_HEAD(&sd->s_children);
 	sd->s_element = element;
+	sd->s_type = type;
+	configfs_init_dirent_depth(sd);
 	spin_lock(&configfs_dirent_lock);
 	if (parent_sd->s_type & CONFIGFS_USET_DROPPING) {
 		spin_unlock(&configfs_dirent_lock);
@@ -138,12 +226,11 @@
 {
 	struct configfs_dirent * sd;
 
-	sd = configfs_new_dirent(parent_sd, element);
+	sd = configfs_new_dirent(parent_sd, element, type);
 	if (IS_ERR(sd))
 		return PTR_ERR(sd);
 
 	sd->s_mode = mode;
-	sd->s_type = type;
 	sd->s_dentry = dentry;
 	if (dentry) {
 		dentry->d_fsdata = configfs_get(sd);
@@ -187,6 +274,7 @@
 		error = configfs_make_dirent(p->d_fsdata, d, k, mode,
 					     CONFIGFS_DIR | CONFIGFS_USET_CREATING);
 	if (!error) {
+		configfs_set_dir_dirent_depth(p->d_fsdata, d->d_fsdata);
 		error = configfs_create(d, mode, init_dir);
 		if (!error) {
 			inc_nlink(p->d_inode);
@@ -789,11 +877,13 @@
 		 * error, as rmdir() would.
 		 */
 		mutex_lock_nested(&dentry->d_inode->i_mutex, I_MUTEX_CHILD);
+		configfs_adjust_dir_dirent_depth_before_populate(sd);
 		ret = populate_groups(to_config_group(item));
 		if (ret) {
 			configfs_detach_item(item);
 			dentry->d_inode->i_flags |= S_DEAD;
 		}
+		configfs_adjust_dir_dirent_depth_after_populate(sd);
 		mutex_unlock(&dentry->d_inode->i_mutex);
 		if (ret)
 			d_delete(dentry);
@@ -916,11 +1006,11 @@
  * Note, btw, that this can be called at *any* time, even when a configfs
  * subsystem isn't registered, or when configfs is loading or unloading.
  * Just like configfs_register_subsystem().  So we take the same
- * precautions.  We pin the filesystem.  We lock each i_mutex _in_order_
- * on our way down the tree.  If we can find the target item in the
+ * precautions.  We pin the filesystem.  We lock configfs_dirent_lock.
+ * If we can find the target item in the
  * configfs tree, it must be part of the subsystem tree as well, so we
- * do not need the subsystem semaphore.  Holding the i_mutex chain locks
- * out mkdir() and rmdir(), who might be racing us.
+ * do not need the subsystem semaphore.  Holding configfs_dirent_lock helps
+ * locking out mkdir() and rmdir(), who might be racing us.
  */
 
 /*
@@ -933,17 +1023,21 @@
  * do that so we can unlock it if we find nothing.
  *
  * Here we do a depth-first search of the dentry hierarchy looking for
- * our object.  We take i_mutex on each step of the way down.  IT IS
- * ESSENTIAL THAT i_mutex LOCKING IS ORDERED.  If we come back up a branch,
- * we'll drop the i_mutex.
- *
- * If the target is not found, -ENOENT is bubbled up and we have released
- * all locks.  If the target was found, the locks will be cleared by
- * configfs_depend_rollback().
+ * our object.
+ * We deliberately ignore items tagged as dropping since they are virtually
+ * dead, as well as items in the middle of attachment since they virtually
+ * do not exist yet. This completes the locking out of racing mkdir() and
+ * rmdir().
+ * Note: subdirectories in the middle of attachment start with s_type =
+ * CONFIGFS_DIR|CONFIGFS_USET_CREATING set by create_dir().  When
+ * CONFIGFS_USET_CREATING is set, we ignore the item.  The actual set of
+ * s_type is in configfs_new_dirent(), which has configfs_dirent_lock.
+ *
+ * If the target is not found, -ENOENT is bubbled up.
  *
  * This adds a requirement that all config_items be unique!
  *
- * This is recursive because the locking traversal is tricky.  There isn't
+ * This is recursive.  There isn't
  * much on the stack, though, so folks that need this function - be careful
  * about your stack!  Patches will be accepted to make it iterative.
  */
@@ -955,13 +1049,13 @@
 
 	BUG_ON(!origin || !sd);
 
-	/* Lock this guy on the way down */
-	mutex_lock(&sd->s_dentry->d_inode->i_mutex);
 	if (sd->s_element == target)  /* Boo-yah */
 		goto out;
 
 	list_for_each_entry(child_sd, &sd->s_children, s_sibling) {
-		if (child_sd->s_type & CONFIGFS_DIR) {
+		if ((child_sd->s_type & CONFIGFS_DIR) &&
+		    !(child_sd->s_type & CONFIGFS_USET_DROPPING) &&
+		    !(child_sd->s_type & CONFIGFS_USET_CREATING)) {
 			ret = configfs_depend_prep(child_sd->s_dentry,
 						   target);
 			if (!ret)
@@ -970,33 +1064,12 @@
 	}
 
 	/* We looped all our children and didn't find target */
-	mutex_unlock(&sd->s_dentry->d_inode->i_mutex);
 	ret = -ENOENT;
 
 out:
 	return ret;
 }
 
-/*
- * This is ONLY called if configfs_depend_prep() did its job.  So we can
- * trust the entire path from item back up to origin.
- *
- * We walk backwards from item, unlocking each i_mutex.  We finish by
- * unlocking origin.
- */
-static void configfs_depend_rollback(struct dentry *origin,
-				     struct config_item *item)
-{
-	struct dentry *dentry = item->ci_dentry;
-
-	while (dentry != origin) {
-		mutex_unlock(&dentry->d_inode->i_mutex);
-		dentry = dentry->d_parent;
-	}
-
-	mutex_unlock(&origin->d_inode->i_mutex);
-}
-
 int configfs_depend_item(struct configfs_subsystem *subsys,
 			 struct config_item *target)
 {
@@ -1037,17 +1110,21 @@
 
 	/* Ok, now we can trust subsys/s_item */
 
-	/* Scan the tree, locking i_mutex recursively, return 0 if found */
+	spin_lock(&configfs_dirent_lock);
+	/* Scan the tree, return 0 if found */
 	ret = configfs_depend_prep(subsys_sd->s_dentry, target);
 	if (ret)
-		goto out_unlock_fs;
+		goto out_unlock_dirent_lock;
 
-	/* We hold all i_mutexes from the subsystem down to the target */
+	/*
+	 * We are sure that the item is not about to be removed by rmdir(), and
+	 * not in the middle of attachment by mkdir().
+	 */
 	p = target->ci_dentry->d_fsdata;
 	p->s_dependent_count += 1;
 
-	configfs_depend_rollback(subsys_sd->s_dentry, target);
-
+out_unlock_dirent_lock:
+	spin_unlock(&configfs_dirent_lock);
 out_unlock_fs:
 	mutex_unlock(&configfs_sb->s_root->d_inode->i_mutex);
 
@@ -1072,10 +1149,10 @@
 	struct configfs_dirent *sd;
 
 	/*
-	 * Since we can trust everything is pinned, we just need i_mutex
-	 * on the item.
+	 * Since we can trust everything is pinned, we just need
+	 * configfs_dirent_lock.
 	 */
-	mutex_lock(&target->ci_dentry->d_inode->i_mutex);
+	spin_lock(&configfs_dirent_lock);
 
 	sd = target->ci_dentry->d_fsdata;
 	BUG_ON(sd->s_dependent_count < 1);
@@ -1086,7 +1163,7 @@
 	 * After this unlock, we cannot trust the item to stay alive!
 	 * DO NOT REFERENCE item after this unlock.
 	 */
-	mutex_unlock(&target->ci_dentry->d_inode->i_mutex);
+	spin_unlock(&configfs_dirent_lock);
 }
 EXPORT_SYMBOL(configfs_undepend_item);
 
@@ -1286,13 +1363,6 @@
 	if (sd->s_type & CONFIGFS_USET_DEFAULT)
 		return -EPERM;
 
-	/*
-	 * Here's where we check for dependents.  We're protected by
-	 * i_mutex.
-	 */
-	if (sd->s_dependent_count)
-		return -EBUSY;
-
 	/* Get a working ref until we have the child */
 	parent_item = configfs_get_config_item(dentry->d_parent);
 	subsys = to_config_group(parent_item)->cg_subsys;
@@ -1303,6 +1373,24 @@
 		return -EINVAL;
 	}
 
+#ifdef CONFIG_KRG_SCHED
+	/* Get a working ref for the duration of this function */
+	item = configfs_get_config_item(dentry);
+
+	/* Drop reference from above, item already holds one. */
+	config_item_put(parent_item);
+
+	if (parent_item->ci_type->ct_group_ops
+	    && parent_item->ci_type->ct_group_ops->allow_drop_item) {
+		ret = parent_item->ci_type->ct_group_ops->allow_drop_item(
+			to_config_group(parent_item), item);
+		if (ret) {
+			config_item_put(item);
+			return ret;
+		}
+	}
+#endif
+
 	/* configfs_mkdir() shouldn't have allowed this */
 	BUG_ON(!subsys->su_group.cg_item.ci_type);
 	subsys_owner = subsys->su_group.cg_item.ci_type->ct_owner;
@@ -1316,15 +1404,27 @@
 
 		mutex_lock(&configfs_symlink_mutex);
 		spin_lock(&configfs_dirent_lock);
-		ret = configfs_detach_prep(dentry, &wait_mutex);
-		if (ret)
-			configfs_detach_rollback(dentry);
+		/*
+		 * Here's where we check for dependents.  We're protected by
+		 * configfs_dirent_lock.
+		 * If no dependent, atomically tag the item as dropping.
+		 */
+		ret = sd->s_dependent_count ? -EBUSY : 0;
+		if (!ret) {
+			ret = configfs_detach_prep(dentry, &wait_mutex);
+			if (ret)
+				configfs_detach_rollback(dentry);
+		}
 		spin_unlock(&configfs_dirent_lock);
 		mutex_unlock(&configfs_symlink_mutex);
 
 		if (ret) {
 			if (ret != -EAGAIN) {
+#ifdef CONFIG_KRG_SCHED
+				config_item_put(item);
+#else
 				config_item_put(parent_item);
+#endif
 				return ret;
 			}
 
@@ -1334,11 +1434,13 @@
 		}
 	} while (ret == -EAGAIN);
 
+#ifndef CONFIG_KRG_SCHED
 	/* Get a working ref for the duration of this function */
 	item = configfs_get_config_item(dentry);
 
 	/* Drop reference from above, item already holds one. */
 	config_item_put(parent_item);
+#endif
 
 	if (item->ci_type)
 		dead_item_owner = item->ci_type->ct_owner;
@@ -1429,7 +1531,7 @@
 	 */
 	err = -ENOENT;
 	if (configfs_dirent_is_ready(parent_sd)) {
-		file->private_data = configfs_new_dirent(parent_sd, NULL);
+		file->private_data = configfs_new_dirent(parent_sd, NULL, 0);
 		if (IS_ERR(file->private_data))
 			err = PTR_ERR(file->private_data);
 		else
diff -ruN linux-2.6.29/fs/configfs/inode.c android_cluster/linux-2.6.29/fs/configfs/inode.c
--- linux-2.6.29/fs/configfs/inode.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/configfs/inode.c	2014-05-27 23:04:10.002036733 -0700
@@ -33,10 +33,15 @@
 #include <linux/backing-dev.h>
 #include <linux/capability.h>
 #include <linux/sched.h>
+#include <linux/lockdep.h>
 
 #include <linux/configfs.h>
 #include "configfs_internal.h"
 
+#ifdef CONFIG_LOCKDEP
+static struct lock_class_key default_group_class[MAX_LOCK_DEPTH];
+#endif
+
 extern struct super_block * configfs_sb;
 
 static const struct address_space_operations configfs_aops = {
@@ -150,6 +155,38 @@
 	return inode;
 }
 
+#ifdef CONFIG_LOCKDEP
+
+static void configfs_set_inode_lock_class(struct configfs_dirent *sd,
+					  struct inode *inode)
+{
+	int depth = sd->s_depth;
+
+	if (depth > 0) {
+		if (depth <= ARRAY_SIZE(default_group_class)) {
+			lockdep_set_class(&inode->i_mutex,
+					  &default_group_class[depth - 1]);
+		} else {
+			/*
+			 * In practice the maximum level of locking depth is
+			 * already reached. Just inform about possible reasons.
+			 */
+			printk(KERN_INFO "configfs: Too many levels of inodes"
+			       " for the locking correctness validator.\n");
+			printk(KERN_INFO "Spurious warnings may appear.\n");
+		}
+	}
+}
+
+#else /* CONFIG_LOCKDEP */
+
+static void configfs_set_inode_lock_class(struct configfs_dirent *sd,
+					  struct inode *inode)
+{
+}
+
+#endif /* CONFIG_LOCKDEP */
+
 int configfs_create(struct dentry * dentry, int mode, int (*init)(struct inode *))
 {
 	int error = 0;
@@ -162,6 +199,7 @@
 					struct inode *p_inode = dentry->d_parent->d_inode;
 					p_inode->i_mtime = p_inode->i_ctime = CURRENT_TIME;
 				}
+				configfs_set_inode_lock_class(sd, inode);
 				goto Proceed;
 			}
 			else
diff -ruN linux-2.6.29/fs/configfs/symlink.c android_cluster/linux-2.6.29/fs/configfs/symlink.c
--- linux-2.6.29/fs/configfs/symlink.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/configfs/symlink.c	2014-05-27 23:04:10.002036733 -0700
@@ -163,7 +163,12 @@
 	if (ret)
 		goto out_put;
 
+#ifdef CONFIG_KRG_SCHED
+	ret = type->ct_item_ops->allow_link(parent_item, target_item,
+					    dentry->d_name.name);
+#else
 	ret = type->ct_item_ops->allow_link(parent_item, target_item);
+#endif
 	if (!ret) {
 		mutex_lock(&configfs_symlink_mutex);
 		ret = create_link(parent_item, target_item, dentry);
@@ -202,6 +207,18 @@
 	parent_item = configfs_get_config_item(dentry->d_parent);
 	type = parent_item->ci_type;
 
+#ifdef CONFIG_KRG_SCHED
+	if (type && type->ct_item_ops &&
+	    type->ct_item_ops->allow_drop_link) {
+		ret = type->ct_item_ops->allow_drop_link(parent_item,
+							 sl->sl_target);
+		if (ret) {
+			config_item_put(parent_item);
+			goto out;
+		}
+	}
+#endif
+
 	spin_lock(&configfs_dirent_lock);
 	list_del_init(&sd->s_sibling);
 	spin_unlock(&configfs_dirent_lock);
diff -ruN linux-2.6.29/fs/dcache.c android_cluster/linux-2.6.29/fs/dcache.c
--- linux-2.6.29/fs/dcache.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/dcache.c	2014-05-27 23:04:10.002036733 -0700
@@ -1912,8 +1912,13 @@
  * If path is not reachable from the supplied root, then the value of
  * root is changed (without modifying refcounts).
  */
+#ifdef CONFIG_KRG_DVFS
+char *____d_path(const struct path *path, struct path *root,
+		 char *buffer, int buflen, bool *deleted)
+#else
 char *__d_path(const struct path *path, struct path *root,
 	       char *buffer, int buflen)
+#endif
 {
 	struct dentry *dentry = path->dentry;
 	struct vfsmount *vfsmnt = path->mnt;
@@ -1922,9 +1927,19 @@
 
 	spin_lock(&vfsmount_lock);
 	prepend(&end, &buflen, "\0", 1);
+#ifdef CONFIG_KRG_DVFS
+	if (!IS_ROOT(dentry) && d_unhashed(dentry)) {
+		*deleted = true;
+		if (prepend(&end, &buflen, " (deleted)", 10) != 0)
+			goto Elong;
+	} else {
+		*deleted = false;
+	}
+#else
 	if (!IS_ROOT(dentry) && d_unhashed(dentry) &&
 		(prepend(&end, &buflen, " (deleted)", 10) != 0))
 			goto Elong;
+#endif
 
 	if (buflen < 1)
 		goto Elong;
@@ -1972,6 +1987,34 @@
 	goto out;
 }
 
+#ifdef CONFIG_KRG_DVFS
+char *d_path_check(const struct path *path, char *buf, int buflen, bool *deleted)
+{
+	char *res;
+	struct path root;
+	struct path tmp;
+
+	read_lock(&current->fs->lock);
+	root = current->fs->root;
+	path_get(&root);
+	read_unlock(&current->fs->lock);
+	spin_lock(&dcache_lock);
+	tmp = root;
+	res = ____d_path(path, &tmp, buf, buflen, deleted);
+	spin_unlock(&dcache_lock);
+	path_put(&root);
+	return res;
+}
+
+char *__d_path(const struct path *path, struct path *root,
+	       char *buffer, int buflen)
+{
+	bool deleted;
+
+	return ____d_path(path, root, buffer, buflen, &deleted);
+}
+#endif
+
 /**
  * d_path - return the path of a dentry
  * @path: path to report
diff -ruN linux-2.6.29/fs/eventpoll.c android_cluster/linux-2.6.29/fs/eventpoll.c
--- linux-2.6.29/fs/eventpoll.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/eventpoll.c	2014-05-27 23:04:10.014036484 -0700
@@ -33,6 +33,9 @@
 #include <linux/bitops.h>
 #include <linux/mutex.h>
 #include <linux/anon_inodes.h>
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
 #include <asm/uaccess.h>
 #include <asm/system.h>
 #include <asm/io.h>
@@ -393,6 +396,10 @@
 			list_del_init(&pwq->llink);
 			remove_wait_queue(pwq->whead, &pwq->wait);
 			kmem_cache_free(pwq_cache, pwq);
+#ifdef CONFIG_KRG_FAF
+			if (epi->ffd.file->f_flags & O_FAF_CLT)
+				krg_faf_poll_dequeue(epi->ffd.file);
+#endif
 		}
 	}
 }
@@ -711,9 +718,29 @@
 		list_add_tail(&pwq->llink, &epi->pwqlist);
 		epi->nwait++;
 	} else {
+#ifdef CONFIG_KRG_FAF
+		pwq = NULL;
+#endif
 		/* We have to signal that an error occurred */
 		epi->nwait = -1;
 	}
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & O_FAF_CLT) {
+		if (krg_faf_poll_wait(file, pwq != NULL)) {
+			if (pwq) {
+				/*
+				 * Don't let ep_unregister_pollwait() do the
+				 * cleanup, since it would call
+				 * krg_faf_poll_dequeue().
+				 */
+				list_del(&pwq->llink);
+				remove_wait_queue(whead, &pwq->wait);
+				kmem_cache_free(pwq_cache, pwq);
+				epi->nwait = -1;
+			}
+		}
+	}
+#endif
 }
 
 static void ep_rbtree_insert(struct eventpoll *ep, struct epitem *epi)
diff -ruN linux-2.6.29/fs/exec.c android_cluster/linux-2.6.29/fs/exec.c
--- linux-2.6.29/fs/exec.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/exec.c	2014-05-27 23:04:10.014036484 -0700
@@ -52,6 +52,16 @@
 #include <linux/tracehook.h>
 #include <linux/kmod.h>
 #include <linux/fsnotify.h>
+#ifdef CONFIG_KRG_CAP
+#include <kerrighed/capabilities.h>
+#endif
+#ifdef CONFIG_KRG_PROC
+#include <kerrighed/task.h>
+#include <kerrighed/krginit.h>
+#endif
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/signal.h>
+#endif
 
 #include <asm/uaccess.h>
 #include <asm/mmu_context.h>
@@ -636,6 +646,11 @@
 		}
 	}
 
+#ifdef CONFIG_KRG_MM
+	if (mm->anon_vma_kddm_set)
+		krg_check_vma_link(vma);
+#endif
+
 #ifdef CONFIG_STACK_GROWSUP
 	stack_base = vma->vm_end + EXTRA_STACK_VM_PAGES * PAGE_SIZE;
 #else
@@ -718,6 +733,9 @@
 {
 	struct task_struct *tsk;
 	struct mm_struct * old_mm, *active_mm;
+#ifdef CONFIG_KRG_MM
+	unique_id_t mm_id = 0;
+#endif
 
 	/* Notify parent that we're no longer interested in the old VM */
 	tsk = current;
@@ -731,6 +749,9 @@
 		 * through with the exec.  We must hold mmap_sem around
 		 * checking core_state and changing tsk->mm.
 		 */
+#ifdef CONFIG_KRG_MM
+		mm_id = old_mm->mm_id;
+#endif
 		down_read(&old_mm->mmap_sem);
 		if (unlikely(old_mm->core_state)) {
 			up_read(&old_mm->mmap_sem);
@@ -749,6 +770,10 @@
 		BUG_ON(active_mm != old_mm);
 		mm_update_next_owner(old_mm);
 		mmput(old_mm);
+#ifdef CONFIG_KRG_MM
+		if (mm_id)
+			kh_mm_release(old_mm, 1);
+#endif
 		return 0;
 	}
 	mmdrop(active_mm);
@@ -804,7 +829,16 @@
 	 */
 	if (!thread_group_leader(tsk)) {
 		struct task_struct *leader = tsk->group_leader;
+#ifdef CONFIG_KRG_PROC
+		struct task_kddm_object *obj;
+#endif
+#ifdef CONFIG_KRG_EPM
+		struct children_kddm_object *parent_children_obj;
+#endif
 
+#ifdef CONFIG_KRG_PROC
+		down_read(&kerrighed_init_sem);
+#endif
 		sig->notify_count = -1;	/* for exit_notify() */
 		for (;;) {
 			write_lock_irq(&tasklist_lock);
@@ -815,6 +849,33 @@
 			schedule();
 		}
 
+#ifdef CONFIG_KRG_EPM
+		parent_children_obj = rcu_dereference(tsk->parent_children_obj);
+#endif
+#ifdef CONFIG_KRG_PROC
+		/* tsk's pid will disappear just below. */
+		obj = leader->task_obj;
+		BUG_ON(!obj ^ !tsk->task_obj);
+		if (
+		    obj
+#ifdef CONFIG_KRG_EPM
+		    || parent_children_obj
+#endif
+		   ) {
+			write_unlock_irq(&tasklist_lock);
+
+#ifdef CONFIG_KRG_EPM
+			parent_children_obj =
+				krg_children_prepare_de_thread(tsk);
+#endif
+			krg_task_free(tsk);
+
+			if (obj)
+				__krg_task_writelock(leader);
+
+			write_lock_irq(&tasklist_lock);
+		}
+#endif /* CONFIG_KRG_PROC */
 		/*
 		 * The only record we have of the real-time age of a
 		 * process, regardless of execs it's done, is start_time.
@@ -847,6 +908,12 @@
 		transfer_pid(leader, tsk, PIDTYPE_PGID);
 		transfer_pid(leader, tsk, PIDTYPE_SID);
 		list_replace_rcu(&leader->tasks, &tsk->tasks);
+#ifdef CONFIG_KRG_PROC
+		rcu_assign_pointer(leader->task_obj, NULL);
+		if (obj)
+			rcu_assign_pointer(obj->task, tsk);
+		rcu_assign_pointer(tsk->task_obj, obj);
+#endif
 
 		tsk->group_leader = tsk;
 		leader->group_leader = tsk;
@@ -856,8 +923,19 @@
 		BUG_ON(leader->exit_state != EXIT_ZOMBIE);
 		leader->exit_state = EXIT_DEAD;
 		write_unlock_irq(&tasklist_lock);
+#ifdef CONFIG_KRG_PROC
+		/* tsk has taken leader's pid. */
+		if (obj)
+			__krg_task_unlock(tsk);
+#endif /* CONFIG_KRG_PROC */
+#ifdef CONFIG_KRG_EPM
+		krg_children_finish_de_thread(parent_children_obj, tsk);
+#endif
 
 		release_task(leader);
+#ifdef CONFIG_KRG_PROC
+		up_read(&kerrighed_init_sem);
+#endif
 	}
 
 	sig->group_exit_task = NULL;
@@ -880,6 +958,11 @@
 		atomic_set(&newsighand->count, 1);
 		memcpy(newsighand->action, oldsighand->action,
 		       sizeof(newsighand->action));
+#ifdef CONFIG_KRG_EPM
+		down_read(&kerrighed_init_sem);
+
+		krg_sighand_alloc_unshared(tsk, newsighand);
+#endif
 
 		write_lock_irq(&tasklist_lock);
 		spin_lock(&oldsighand->siglock);
@@ -887,7 +970,13 @@
 		spin_unlock(&oldsighand->siglock);
 		write_unlock_irq(&tasklist_lock);
 
+#ifdef CONFIG_KRG_EPM
+		krg_sighand_cleanup(oldsighand);
+
+		up_read(&kerrighed_init_sem);
+#else
 		__cleanup_sighand(oldsighand);
+#endif
 	}
 
 	BUG_ON(!thread_group_leader(tsk));
@@ -1014,6 +1103,9 @@
 	   group */
 
 	current->self_exec_id++;
+#ifdef CONFIG_KRG_EPM
+	krg_update_self_exec_id(current);
+#endif
 			
 	flush_signal_handlers(current, 0);
 	flush_old_files(current->files);
@@ -1122,6 +1214,12 @@
 		return retval;
 	bprm->cred_prepared = 1;
 
+#ifdef CONFIG_KRG_CAP
+	retval = krg_cap_prepare_binprm(bprm);
+	if (retval)
+		return retval;
+#endif
+
 	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
 	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
 }
@@ -1336,8 +1434,16 @@
 	if (retval < 0)
 		goto out;
 
+#ifdef CONFIG_KRG_MM
+	retval = krg_do_execve(current, current->mm);
+	if (retval)
+		goto out;
+#endif
 	/* execve succeeded */
 	mutex_unlock(&current->cred_exec_mutex);
+#ifdef CONFIG_KRG_CAP
+	krg_cap_finish_exec(bprm);
+#endif
 	acct_update_integrals(current);
 	free_bprm(bprm);
 	if (displaced)
@@ -1345,6 +1451,11 @@
 	return retval;
 
 out:
+#ifdef CONFIG_KRG_EPM
+	/* Quiet the BUG_ON() in mmput() */
+	if (bprm->mm)
+		atomic_dec(&bprm->mm->mm_ltasks);
+#endif
 	if (bprm->mm)
 		mmput (bprm->mm);
 
@@ -1623,6 +1734,11 @@
 	vfork_done = tsk->vfork_done;
 	if (vfork_done) {
 		tsk->vfork_done = NULL;
+#ifdef CONFIG_KRG_EPM
+		if (tsk->remote_vfork_done)
+			krg_vfork_done(vfork_done);
+		else
+#endif
 		complete(vfork_done);
 	}
 
diff -ruN linux-2.6.29/fs/fcntl.c android_cluster/linux-2.6.29/fs/fcntl.c
--- linux-2.6.29/fs/fcntl.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/fcntl.c	2014-05-27 23:04:10.030036152 -0700
@@ -20,6 +20,9 @@
 #include <linux/rcupdate.h>
 #include <linux/pid_namespace.h>
 #include <linux/smp_lock.h>
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
 
 #include <asm/poll.h>
 #include <asm/siginfo.h>
@@ -344,6 +347,14 @@
 	if (!filp)
 		goto out;
 
+#ifdef CONFIG_KRG_FAF
+	if ((filp->f_flags & O_FAF_CLT)
+	    && (cmd != F_DUPFD) && (cmd != F_GETFD) && (cmd != F_SETFD)) {
+		err = krg_faf_fcntl(filp, cmd, arg);
+		fput(filp);
+		goto out;
+       }
+#endif
 	err = security_file_fcntl(filp, cmd, arg);
 	if (err) {
 		fput(filp);
@@ -369,6 +380,13 @@
 	if (!filp)
 		goto out;
 
+#ifdef CONFIG_KRG_FAF
+	if ((filp->f_flags & O_FAF_CLT) && (cmd != F_DUPFD)) {
+		err = krg_faf_fcntl64(filp, cmd, arg);
+		fput(filp);
+		goto out;
+	}
+#endif
 	err = security_file_fcntl(filp, cmd, arg);
 	if (err) {
 		fput(filp);
diff -ruN linux-2.6.29/fs/file.c android_cluster/linux-2.6.29/fs/file.c
--- linux-2.6.29/fs/file.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/file.c	2014-05-27 23:04:10.030036152 -0700
@@ -136,7 +136,10 @@
 	memset((char *)(nfdt->close_on_exec) + cpy, 0, set);
 }
 
-static struct fdtable * alloc_fdtable(unsigned int nr)
+#ifndef CONFIG_KRG_DVFS
+static
+#endif
+struct fdtable * alloc_fdtable(unsigned int nr)
 {
 	struct fdtable *fdt;
 	char *data;
@@ -246,7 +249,12 @@
  * expanded and execution may have blocked.
  * The files->file_lock should be held on entry, and will be held on exit.
  */
+#ifdef CONFIG_KRG_FAF
+static int __expand_files(struct task_struct *task, struct files_struct *files,
+			  int nr)
+#else
 int expand_files(struct files_struct *files, int nr)
+#endif
 {
 	struct fdtable *fdt;
 
@@ -256,7 +264,11 @@
 	 * N.B. For clone tasks sharing a files structure, this test
 	 * will limit the total number of files that can be opened.
 	 */
+#ifdef CONFIG_KRG_FAF
+	if (nr >= task->signal->rlim[RLIMIT_NOFILE].rlim_cur)
+#else
 	if (nr >= current->signal->rlim[RLIMIT_NOFILE].rlim_cur)
+#endif
 		return -EMFILE;
 
 	/* Do we need to expand? */
@@ -271,7 +283,17 @@
 	return expand_fdtable(files, nr);
 }
 
-static int count_open_files(struct fdtable *fdt)
+#ifdef CONFIG_KRG_FAF
+int expand_files(struct files_struct *files, int nr)
+{
+	return __expand_files(current, files, nr);
+}
+#endif
+
+#ifndef CONFIG_KRG_DVFS
+static
+#endif
+int count_open_files(struct fdtable *fdt)
 {
 	int size = fdt->max_fds;
 	int i;
@@ -437,9 +459,18 @@
 /*
  * allocate a file descriptor, mark it busy.
  */
+#ifdef CONFIG_KRG_FAF
+int __alloc_fd(struct task_struct *task,
+	       unsigned start, unsigned flags)
+#else
 int alloc_fd(unsigned start, unsigned flags)
+#endif
 {
+#ifdef CONFIG_KRG_FAF
+	struct files_struct *files = task->files;
+#else
 	struct files_struct *files = current->files;
+#endif
 	unsigned int fd;
 	int error;
 	struct fdtable *fdt;
@@ -455,7 +486,11 @@
 		fd = find_next_zero_bit(fdt->open_fds->fds_bits,
 					   fdt->max_fds, fd);
 
+#ifdef CONFIG_KRG_FAF
+	error = __expand_files(task, files, fd);
+#else
 	error = expand_files(files, fd);
+#endif
 	if (error < 0)
 		goto out;
 
@@ -488,8 +523,22 @@
 	return error;
 }
 
+#ifdef CONFIG_KRG_FAF
+int alloc_fd(unsigned start, unsigned flags)
+{
+	return __alloc_fd(current, start, flags);
+}
+#endif
+
 int get_unused_fd(void)
 {
 	return alloc_fd(0, 0);
 }
 EXPORT_SYMBOL(get_unused_fd);
+
+#ifdef CONFIG_KRG_FAF
+int __get_unused_fd(struct task_struct *task)
+{
+	return __alloc_fd(task, 0, 0);
+}
+#endif
diff -ruN linux-2.6.29/fs/file_table.c android_cluster/linux-2.6.29/fs/file_table.c
--- linux-2.6.29/fs/file_table.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/file_table.c	2014-05-27 23:04:10.030036152 -0700
@@ -24,6 +24,13 @@
 
 #include <asm/atomic.h>
 
+#ifdef CONFIG_KRG_DVFS
+#include <kerrighed/dvfs.h>
+#endif
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
+
 /* sysctl tunables... */
 struct files_stat_struct files_stat = {
 	.max_files = NR_FILE
@@ -47,6 +54,10 @@
 
 static inline void file_free(struct file *f)
 {
+#ifdef CONFIG_KRG_DVFS
+	if (f->f_objid)
+		krg_put_file(f);
+#endif
 	percpu_counter_dec(&nr_files);
 	file_check_state(f);
 	call_rcu(&f->f_u.fu_rcuhead, file_free_rcu);
@@ -260,10 +271,25 @@
 {
 	struct dentry *dentry = file->f_path.dentry;
 	struct vfsmount *mnt = file->f_path.mnt;
+#ifdef CONFIG_KRG_FAF
+	struct inode *inode;
+#else
 	struct inode *inode = dentry->d_inode;
+#endif
 
 	might_sleep();
 
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & O_FAF_CLT) {
+		eventpoll_release(file);
+		security_file_free(file);
+		put_pid(file->f_owner.pid);
+		file_kill(file);
+		file_free(file);
+		return;
+	}
+	inode = dentry->d_inode;
+#endif
 	fsnotify_close(file);
 	/*
 	 * The function eventpoll_release() should be the first called
diff -ruN linux-2.6.29/fs/inode.c android_cluster/linux-2.6.29/fs/inode.c
--- linux-2.6.29/fs/inode.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/inode.c	2014-05-27 23:04:10.050035736 -0700
@@ -147,6 +147,10 @@
 	inode->i_cdev = NULL;
 	inode->i_rdev = 0;
 	inode->dirtied_when = 0;
+#ifdef CONFIG_KRG_DVFS
+	inode->i_objid = 0;
+#endif
+
 	if (security_inode_alloc(inode)) {
 		if (inode->i_sb->s_op->destroy_inode)
 			inode->i_sb->s_op->destroy_inode(inode);
@@ -164,6 +168,9 @@
 	init_rwsem(&inode->i_alloc_sem);
 	lockdep_set_class(&inode->i_alloc_sem, &sb->s_type->i_alloc_sem_key);
 
+#ifdef CONFIG_KRG_DVFS
+	mapping->kddm_set = NULL;
+#endif
 	mapping->a_ops = &empty_aops;
 	mapping->host = inode;
 	mapping->flags = 0;
diff -ruN linux-2.6.29/fs/ioctl.c android_cluster/linux-2.6.29/fs/ioctl.c
--- linux-2.6.29/fs/ioctl.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/ioctl.c	2014-05-27 23:04:10.050035736 -0700
@@ -17,6 +17,9 @@
 #include <linux/buffer_head.h>
 
 #include <asm/ioctls.h>
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
 
 /* So that the fiemap access checks can't overflow on 32 bit machines. */
 #define FIEMAP_MAX_EXTENTS	(UINT_MAX / sizeof(struct fiemap_extent))
@@ -552,6 +555,12 @@
 	if (!filp)
 		goto out;
 
+#ifdef CONFIG_KRG_FAF
+	if (filp->f_flags & O_FAF_CLT) {
+		error = krg_faf_ioctl(filp, cmd, arg);
+		goto out_fput;
+	}
+#endif
 	error = security_file_ioctl(filp, cmd, arg);
 	if (error)
 		goto out_fput;
diff -ruN linux-2.6.29/fs/locks.c android_cluster/linux-2.6.29/fs/locks.c
--- linux-2.6.29/fs/locks.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/locks.c	2014-05-27 23:04:10.074035237 -0700
@@ -129,6 +129,9 @@
 #include <linux/pid_namespace.h>
 
 #include <asm/uaccess.h>
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
 
 #define IS_POSIX(fl)	(fl->fl_flags & FL_POSIX)
 #define IS_FLOCK(fl)	(fl->fl_flags & FL_FLOCK)
@@ -1576,6 +1579,12 @@
 	if (!filp)
 		goto out;
 
+#ifdef CONFIG_KRG_FAF
+	if (filp->f_flags & O_FAF_CLT) {
+		error = krg_faf_flock(filp, cmd);
+		goto out_putf;
+	}
+#endif
 	can_sleep = !(cmd & LOCK_NB);
 	cmd &= ~LOCK_NB;
 	unlock = (cmd == LOCK_UN);
diff -ruN linux-2.6.29/fs/namei.c android_cluster/linux-2.6.29/fs/namei.c
--- linux-2.6.29/fs/namei.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/namei.c	2014-05-27 23:04:10.078035154 -0700
@@ -32,6 +32,9 @@
 #include <linux/fcntl.h>
 #include <linux/device_cgroup.h>
 #include <asm/uaccess.h>
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
 
 #define ACC_MODE(x) ("\000\004\002\006"[(x)&O_ACCMODE])
 
@@ -959,6 +962,9 @@
 			err = do_follow_link(&next, nd);
 			if (err)
 				goto return_err;
+#ifdef CONFIG_KRG_FAF
+                        if (nd->path.dentry)
+#endif
 			inode = nd->path.dentry->d_inode;
 		} else
 			path_to_nameidata(&next, nd);
@@ -1043,6 +1049,20 @@
 		if (!file)
 			goto out_fail;
 
+#ifdef CONFIG_KRG_FAF
+		if (file->f_flags & O_FAF_CLT) {
+			faf_client_data_t *data = file->private_data;
+
+			retval = -ENOTDIR;
+			if (!S_ISDIR(data->i_mode))
+				goto fput_fail;
+
+			retval = krg_faf_do_path_lookup(file, name, flags, nd);
+
+			fput_light(file, fput_needed);
+			return retval;
+		}
+#endif
 		dentry = file->f_path.dentry;
 
 		retval = -ENOTDIR;
@@ -1754,6 +1774,13 @@
 	if (path.dentry->d_inode && S_ISDIR(path.dentry->d_inode->i_mode))
 		goto exit;
 ok:
+#ifdef CONFIG_KRG_FAF
+	if ((!nd.path.dentry) && (nd.path.mnt)) {
+		struct file *file = (struct file *)nd.path.mnt;
+		get_file(file);
+		return file;
+	}
+#endif
 	/*
 	 * Consider:
 	 * 1. may_open() truncates a file
diff -ruN linux-2.6.29/fs/nfs/file.c android_cluster/linux-2.6.29/fs/nfs/file.c
--- linux-2.6.29/fs/nfs/file.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/nfs/file.c	2014-05-27 23:04:10.082035071 -0700
@@ -47,6 +47,9 @@
 					size_t count, unsigned int flags);
 static ssize_t nfs_file_read(struct kiocb *, const struct iovec *iov,
 				unsigned long nr_segs, loff_t pos);
+static ssize_t nfs_file_splice_write(struct pipe_inode_info *pipe,
+					struct file *filp, loff_t *ppos,
+					size_t count, unsigned int flags);
 static ssize_t nfs_file_write(struct kiocb *, const struct iovec *iov,
 				unsigned long nr_segs, loff_t pos);
 static int  nfs_file_flush(struct file *, fl_owner_t id);
@@ -56,7 +59,9 @@
 static int nfs_flock(struct file *filp, int cmd, struct file_lock *fl);
 static int nfs_setlease(struct file *file, long arg, struct file_lock **fl);
 
+#ifndef CONFIG_KRG_MM
 static struct vm_operations_struct nfs_file_vm_ops;
+#endif
 
 const struct file_operations nfs_file_operations = {
 	.llseek		= nfs_file_llseek,
@@ -76,6 +81,7 @@
 	.lock		= nfs_lock,
 	.flock		= nfs_flock,
 	.splice_read	= nfs_file_splice_read,
+	.splice_write	= nfs_file_splice_write,
 	.check_flags	= nfs_check_flags,
 	.setlease	= nfs_setlease,
 };
@@ -486,7 +492,10 @@
 	return ret;
 }
 
-static struct vm_operations_struct nfs_file_vm_ops = {
+#ifndef CONFIG_KRG_MM
+static
+#endif
+struct vm_operations_struct nfs_file_vm_ops = {
 	.fault = filemap_fault,
 	.page_mkwrite = nfs_vm_page_mkwrite,
 };
@@ -550,6 +559,33 @@
 	goto out;
 }
 
+static ssize_t nfs_file_splice_write(struct pipe_inode_info *pipe,
+				     struct file *filp, loff_t *ppos,
+				     size_t count, unsigned int flags)
+{
+	struct dentry *dentry = filp->f_path.dentry;
+	struct inode *inode = dentry->d_inode;
+	ssize_t ret;
+
+	dprintk("NFS splice_write(%s/%s, %lu@%llu)\n",
+		dentry->d_parent->d_name.name, dentry->d_name.name,
+		(unsigned long) count, (unsigned long long) *ppos);
+
+	/*
+	 * The combination of splice and an O_APPEND destination is disallowed.
+	 */
+
+	nfs_add_stats(inode, NFSIOS_NORMALWRITTENBYTES, count);
+
+	ret = generic_file_splice_write(pipe, filp, ppos, count, flags);
+	if (ret >= 0 && nfs_need_sync_write(filp, inode)) {
+		int err = nfs_do_fsync(nfs_file_open_context(filp), inode);
+		if (err < 0)
+			ret = err;
+	}
+	return ret;
+}
+
 static int do_getlk(struct file *filp, int cmd, struct file_lock *fl)
 {
 	struct inode *inode = filp->f_mapping->host;
diff -ruN linux-2.6.29/fs/nfs/inode.c android_cluster/linux-2.6.29/fs/nfs/inode.c
--- linux-2.6.29/fs/nfs/inode.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/nfs/inode.c	2014-05-27 23:04:10.082035071 -0700
@@ -37,6 +37,9 @@
 #include <linux/vfs.h>
 #include <linux/inet.h>
 #include <linux/nfs_xdr.h>
+#ifdef CONFIG_KRG_MM
+#include <kerrighed/krgsyms.h>
+#endif
 
 #include <asm/system.h>
 #include <asm/uaccess.h>
@@ -1337,6 +1340,12 @@
 {
 	int err;
 
+#ifdef CONFIG_KRG_MM
+	err = krgsyms_register(KRGSYMS_VM_OPS_NFS_FILE, &nfs_file_vm_ops);
+	if (err)
+		goto out7;
+#endif
+
 	err = nfsiod_start();
 	if (err)
 		goto out6;
@@ -1389,6 +1398,10 @@
 out5:
 	nfsiod_stop();
 out6:
+#ifdef CONFIG_KRG_MM
+	krgsyms_unregister(KRGSYMS_VM_OPS_NFS_FILE);
+out7:
+#endif
 	return err;
 }
 
@@ -1405,6 +1418,9 @@
 	unregister_nfs_fs();
 	nfs_fs_proc_exit();
 	nfsiod_stop();
+#ifdef CONFIG_KRG_MM
+	krgsyms_unregister(KRGSYMS_VM_OPS_NFS_FILE);
+#endif
 }
 
 /* Not quite true; I just maintain it */
diff -ruN linux-2.6.29/fs/nfs/internal.h android_cluster/linux-2.6.29/fs/nfs/internal.h
--- linux-2.6.29/fs/nfs/internal.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/nfs/internal.h	2014-05-27 23:04:10.082035071 -0700
@@ -108,6 +108,12 @@
 }
 #endif
 
+#ifdef CONFIG_KRG_MM
+struct vm_operations_struct;
+
+extern struct vm_operations_struct nfs_file_vm_ops;
+#endif
+
 /* nfs4namespace.c */
 #ifdef CONFIG_NFS_V4
 extern struct vfsmount *nfs_do_refmount(const struct vfsmount *mnt_parent, struct dentry *dentry);
diff -ruN linux-2.6.29/fs/ocfs2/mmap.c android_cluster/linux-2.6.29/fs/ocfs2/mmap.c
--- linux-2.6.29/fs/ocfs2/mmap.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/ocfs2/mmap.c	2014-05-27 23:04:10.138033908 -0700
@@ -200,7 +200,10 @@
 	return ret;
 }
 
-static struct vm_operations_struct ocfs2_file_vm_ops = {
+#ifndef CONFIG_KRG_MM
+static
+#endif
+struct vm_operations_struct ocfs2_file_vm_ops = {
 	.fault		= ocfs2_fault,
 	.page_mkwrite	= ocfs2_page_mkwrite,
 };
diff -ruN linux-2.6.29/fs/ocfs2/ocfs2_fs.h android_cluster/linux-2.6.29/fs/ocfs2/ocfs2_fs.h
--- linux-2.6.29/fs/ocfs2/ocfs2_fs.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/ocfs2/ocfs2_fs.h	2014-05-27 23:04:10.138033908 -0700
@@ -57,7 +57,11 @@
 #define OCFS2_MAX_BLOCKSIZE		OCFS2_MIN_CLUSTERSIZE
 
 /* Filesystem magic number */
+#ifndef CONFIG_KRG_DVFS
 #define OCFS2_SUPER_MAGIC		0x7461636f
+#else
+#include <linux/magic.h>
+#endif
 
 /* Object signatures */
 #define OCFS2_SUPER_BLOCK_SIGNATURE	"OCFSV2"
diff -ruN linux-2.6.29/fs/ocfs2/super.c android_cluster/linux-2.6.29/fs/ocfs2/super.c
--- linux-2.6.29/fs/ocfs2/super.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/ocfs2/super.c	2014-05-27 23:04:10.142033825 -0700
@@ -46,6 +46,10 @@
 #define MLOG_MASK_PREFIX ML_SUPER
 #include <cluster/masklog.h>
 
+#ifdef CONFIG_KRG_MM
+#include <kerrighed/krgsyms.h>
+#endif
+
 #include "ocfs2.h"
 
 /* this should be the only file to include a version 1 header */
@@ -71,6 +75,12 @@
 
 #include "buffer_head_io.h"
 
+#ifdef CONFIG_KRG_MM
+struct vm_operations_struct;
+
+extern struct vm_operations_struct ocfs2_file_vm_ops;
+#endif
+
 static struct kmem_cache *ocfs2_inode_cachep = NULL;
 struct kmem_cache *ocfs2_dquot_cachep;
 struct kmem_cache *ocfs2_qf_chunk_cachep;
@@ -1330,6 +1340,12 @@
 	if (status)
 		goto leave;
 
+#ifdef CONFIG_KRG_MM
+	status = krgsyms_register(KRGSYMS_VM_OPS_OCFS2_FILE, &ocfs2_file_vm_ops);
+	if (status)
+		goto leave;
+#endif
+
 	ocfs2_set_locking_protocol();
 
 	status = register_quota_format(&ocfs2_quota_format);
@@ -1338,6 +1354,9 @@
 		ocfs2_quota_shutdown();
 		ocfs2_free_mem_caches();
 		exit_ocfs2_uptodate_cache();
+#ifdef CONFIG_KRG_MM
+		krgsyms_unregister(KRGSYMS_VM_OPS_OCFS2_FILE);
+#endif
 	}
 
 	mlog_exit(status);
@@ -1350,6 +1369,10 @@
 
 static void __exit ocfs2_exit(void)
 {
+#ifdef CONFIG_KRG_MM
+	krgsyms_unregister(KRGSYMS_VM_OPS_OCFS2_FILE);
+#endif
+
 	mlog_entry_void();
 
 	ocfs2_quota_shutdown();
diff -ruN linux-2.6.29/fs/open.c android_cluster/linux-2.6.29/fs/open.c
--- linux-2.6.29/fs/open.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/open.c	2014-06-09 18:19:41.072418056 -0700
@@ -29,6 +29,9 @@
 #include <linux/rcupdate.h>
 #include <linux/audit.h>
 #include <linux/falloc.h>
+#ifdef CONFIG_KRG_IPC
+#include <kerrighed/faf.h>
+#endif
 
 int vfs_statfs(struct dentry *dentry, struct kstatfs *buf)
 {
@@ -166,7 +169,14 @@
 	file = fget(fd);
 	if (!file)
 		goto out;
+
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & O_FAF_CLT)
+		error = krg_faf_fstatfs(file, &tmp);
+	else
+#endif
 	error = vfs_statfs_native(file->f_path.dentry, &tmp);
+
 	if (!error && copy_to_user(buf, &tmp, sizeof(tmp)))
 		error = -EFAULT;
 	fput(file);
@@ -980,7 +990,10 @@
 }
 EXPORT_SYMBOL(dentry_open);
 
-static void __put_unused_fd(struct files_struct *files, unsigned int fd)
+#ifndef CONFIG_KRG_FAF
+static
+#endif
+void __put_unused_fd(struct files_struct *files, unsigned int fd)
 {
 	struct fdtable *fdt = files_fdtable(files);
 	__FD_CLR(fd, fdt->open_fds);
@@ -1010,6 +1023,16 @@
  * It should never happen - if we allow dup2() do it, _really_ bad things
  * will follow.
  */
+#ifdef CONFIG_KRG_FAF
+void __fd_install(struct files_struct *files,
+		  unsigned int fd, struct file *file)
+{
+	struct fdtable *fdt;
+	fdt = files_fdtable(files);
+	BUG_ON(fdt->fd[fd] != NULL);
+	rcu_assign_pointer(fdt->fd[fd], file);
+}
+#endif
 
 void fd_install(unsigned int fd, struct file *file)
 {
@@ -1029,6 +1052,12 @@
 	char *tmp = getname(filename);
 	int fd = PTR_ERR(tmp);
 
+#ifdef CONFIG_KRG_FAF
+       /* Flush Kerrighed O_flags to prevent kernel crashes due to wrong
+        * flags passed from userland.
+        */
+	flags = flags & (~O_KRG_FLAGS);
+#endif
 	if (!IS_ERR(tmp)) {
 		fd = get_unused_fd_flags(flags);
 		if (fd >= 0) {
@@ -1037,6 +1066,9 @@
 				put_unused_fd(fd);
 				fd = PTR_ERR(f);
 			} else {
+#ifdef CONFIG_KRG_FAF
+				if (!(f->f_flags & O_FAF_CLT))
+#endif
 				fsnotify_open(f->f_path.dentry);
 				fd_install(fd, f);
 			}
@@ -1093,6 +1125,9 @@
 int filp_close(struct file *filp, fl_owner_t id)
 {
 	int retval = 0;
+#ifdef CONFIG_KRG_FAF
+	int flags = filp->f_flags;
+#endif
 
 	if (!file_count(filp)) {
 		printk(KERN_ERR "VFS: Close: file count is 0\n");
@@ -1102,9 +1137,19 @@
 	if (filp->f_op && filp->f_op->flush)
 		retval = filp->f_op->flush(filp, id);
 
+#ifdef CONFIG_KRG_FAF
+	if (filp->f_flags & O_FAF_CLT) {
+		fput(filp);
+		return retval;
+	}
+#endif
 	dnotify_flush(filp, id);
 	locks_remove_posix(filp, id);
 	fput(filp);
+#ifdef CONFIG_KRG_FAF
+	if ((flags & O_FAF_SRV) && (file_count(filp) == 1))
+		krg_faf_srv_close(filp);
+#endif
 	return retval;
 }
 
diff -ruN linux-2.6.29/fs/pipe.c android_cluster/linux-2.6.29/fs/pipe.c
--- linux-2.6.29/fs/pipe.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/pipe.c	2014-05-27 23:04:10.150033658 -0700
@@ -22,6 +22,14 @@
 #include <asm/uaccess.h>
 #include <asm/ioctls.h>
 
+#ifdef CONFIG_KRG_EPM
+#include <linux/splice.h>
+#include <kerrighed/app_shared.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/ghost_helpers.h>
+#include <kerrighed/regular_file_mgr.h>
+#endif
+
 /*
  * We use a start+len construction, which provides full use of the 
  * allocated memory.
@@ -823,6 +831,10 @@
 		init_waitqueue_head(&pipe->wait);
 		pipe->r_counter = pipe->w_counter = 1;
 		pipe->inode = inode;
+#ifdef CONFIG_KRG_EPM
+		pipe->fread = NULL;
+		pipe->fwrite = NULL;
+#endif
 	}
 
 	return pipe;
@@ -912,11 +924,10 @@
 	return NULL;
 }
 
-struct file *create_write_pipe(int flags)
+struct dentry *__prepare_pipe_dentry(void)
 {
 	int err;
 	struct inode *inode;
-	struct file *f;
 	struct dentry *dentry;
 	struct qstr name = { .name = "" };
 
@@ -939,25 +950,66 @@
 	dentry->d_flags &= ~DCACHE_UNHASHED;
 	d_instantiate(dentry, inode);
 
-	err = -ENFILE;
+	return dentry;
+
+err_inode:
+	free_pipe_info(inode);
+	iput(inode);
+err:
+	return ERR_PTR(err);
+}
+
+struct file *__create_write_pipe(struct dentry *dentry, int flags)
+{
+	struct file *f;
+	int err = -ENFILE;
+#ifdef CONFIG_KRG_EPM
+	struct pipe_inode_info *pipe;
+#endif
+
 	f = alloc_file(pipe_mnt, dentry, FMODE_WRITE, &write_pipefifo_fops);
 	if (!f)
-		goto err_dentry;
-	f->f_mapping = inode->i_mapping;
+		goto err;
+	f->f_mapping = dentry->d_inode->i_mapping;
 
 	f->f_flags = O_WRONLY | (flags & O_NONBLOCK);
 	f->f_version = 0;
+#ifdef CONFIG_KRG_EPM
+	pipe = dentry->d_inode->i_pipe;
+	pipe->fwrite = f;
+#endif
+
+	return f;
+
+err:
+	return ERR_PTR(err);
+}
+
+struct file *create_write_pipe(int flags)
+{
+	int err;
+	struct file *f;
+	struct dentry *dentry;
+
+	dentry = __prepare_pipe_dentry();
+	if (IS_ERR(dentry)) {
+		err = PTR_ERR(dentry);
+		goto err;
+	}
+
+	f = __create_write_pipe(dentry, flags);
+	if (IS_ERR(f)) {
+		err = PTR_ERR(f);
+		goto err_dentry;
+	}
 
 	return f;
 
  err_dentry:
-	free_pipe_info(inode);
+	free_pipe_info(dentry->d_inode);
 	dput(dentry);
 	return ERR_PTR(err);
 
- err_inode:
-	free_pipe_info(inode);
-	iput(inode);
  err:
 	return ERR_PTR(err);
 }
@@ -969,26 +1021,39 @@
 	put_filp(f);
 }
 
-struct file *create_read_pipe(struct file *wrf, int flags)
+struct file *__create_read_pipe(struct dentry *dentry, int flags)
 {
+#ifdef CONFIG_KRG_EPM
+	struct pipe_inode_info *pipe;
+#endif
 	struct file *f = get_empty_filp();
 	if (!f)
 		return ERR_PTR(-ENFILE);
 
 	/* Grab pipe from the writer */
-	f->f_path = wrf->f_path;
-	path_get(&wrf->f_path);
-	f->f_mapping = wrf->f_path.dentry->d_inode->i_mapping;
+	f->f_path.dentry = dget(dentry);
+	f->f_path.mnt = mntget(pipe_mnt);
+
+	f->f_mapping = dentry->d_inode->i_mapping;
 
 	f->f_pos = 0;
 	f->f_flags = O_RDONLY | (flags & O_NONBLOCK);
 	f->f_op = &read_pipefifo_fops;
 	f->f_mode = FMODE_READ;
 	f->f_version = 0;
+#ifdef CONFIG_KRG_EPM
+	pipe = dentry->d_inode->i_pipe;
+	pipe->fread = f;
+#endif
 
 	return f;
 }
 
+struct file *create_read_pipe(struct file *wrf, int flags)
+{
+	return __create_read_pipe(wrf->f_path.dentry, flags);
+}
+
 int do_pipe_flags(int *fd, int flags)
 {
 	struct file *fw, *fr;
@@ -1039,6 +1104,286 @@
 	return do_pipe_flags(fd, 0);
 }
 
+#ifdef CONFIG_KRG_EPM
+
+int cr_add_pipe_inode_to_shared_table(struct task_struct *task,
+				      struct file *file)
+{
+	int r;
+	long key;
+	struct pipe_inode_info *pipe;
+	struct file *other_end;
+	union export_args args;
+
+	args.file_args.index = -1;
+	args.file_args.file = file;
+
+	key = (long)file->f_path.dentry->d_inode;
+
+	r = add_to_shared_objects_list(task->application, PIPE_INODE, key,
+				       LOCAL_ONLY, task, &args, 0);
+	if (r == -ENOKEY) /* the inode was already in the list */
+		r = 0;
+
+	if (r)
+		goto error;
+
+	/* add the other end of the pipe */
+	pipe = file->f_path.dentry->d_inode->i_pipe;
+
+	BUG_ON(!pipe->fread || !pipe->fwrite);
+	if (file == pipe->fread)
+		other_end = pipe->fwrite;
+	else {
+		BUG_ON(file != pipe->fwrite);
+		other_end = pipe->fread;
+	}
+
+	if (other_end->f_flags & O_FAF_SRV)
+		r = _cr_add_file_to_shared_table(task, -1, other_end, false);
+
+error:
+	return r;
+}
+
+/* largely inspired from code from Oren Laadan */
+static int cr_export_pipe_buffer(ghost_t *ghost, struct inode *inode)
+{
+	struct pipe_inode_info *pipe;
+	struct file *ghost_file;
+	loff_t f_pos;
+	int len, ret = -ENOMEM;
+
+	pipe = alloc_pipe_info(NULL);
+	if (!pipe)
+		goto out;
+
+	pipe->readers = 1;	/* bluff link_pipe() below */
+	len = link_pipe(inode->i_pipe, pipe, INT_MAX, SPLICE_F_NONBLOCK);
+	if (len == -EAGAIN)
+		len = 0;
+	if (len < 0) {
+		ret = len;
+		goto out_free_pipe;
+	}
+
+	ret = ghost_write(ghost, &len, sizeof(int));
+	if (ret < 0)
+		goto out_free_pipe;
+
+	ghost_file = ((struct file_ghost_data*)(ghost->data))->file;
+	f_pos = ghost_file->f_pos;
+	if (f_pos & ~PAGE_CACHE_MASK)
+		f_pos = (f_pos + PAGE_CACHE_SIZE) & PAGE_CACHE_MASK;
+
+	ret = do_splice_from(pipe, ghost_file, &f_pos, len, 0);
+	ghost_file->f_pos = f_pos;
+
+	if (ret < 0)
+		goto out_free_pipe;
+	if (ret != len)
+		ret = -EPIPE;  /* can occur due to an error in target file */
+	else
+		ret = 0;
+
+out_free_pipe:
+	__free_pipe_info(pipe);
+out:
+	return ret;
+}
+
+int cr_export_now_pipe_inode(struct epm_action *action, ghost_t *ghost,
+			     struct task_struct *task,
+			     union export_args *args)
+{
+	int r;
+	struct inode *inode = args->file_args.file->f_path.dentry->d_inode;
+
+	r = ghost_write(ghost, &inode->i_atime, sizeof(struct timespec));
+	if (r)
+		goto err;
+	r = ghost_write(ghost, &inode->i_mtime, sizeof(struct timespec));
+	if (r)
+		goto err;
+	r = ghost_write(ghost, &inode->i_ctime, sizeof(struct timespec));
+	if (r)
+		goto err;
+
+	r = cr_export_pipe_buffer(ghost, inode);
+
+err:
+	return r;
+}
+
+/* largely inspired from code from Oren Laadan */
+static int cr_import_pipe_buffer(ghost_t *ghost, struct inode *inode)
+{
+	struct pipe_inode_info *pipe;
+	struct file *ghost_file;
+	loff_t f_pos;
+	int len, ret;
+
+	ret = ghost_read(ghost, &len, sizeof(int));
+	if (ret)
+		goto err;
+
+	ghost_file = ((struct file_ghost_data*)(ghost->data))->file;
+	f_pos = ghost_file->f_pos;
+	if (f_pos & ~PAGE_CACHE_MASK)
+		f_pos = (f_pos + PAGE_CACHE_SIZE) & PAGE_CACHE_MASK;
+
+	pipe = inode->i_pipe;
+	ret = do_splice_to(ghost_file, &f_pos, pipe, len, 0);
+	ghost_file->f_pos = f_pos;
+
+	if (ret >= 0) {
+		if (ret != len)
+			ret = -EPIPE;  /* can occur due to an error in source file */
+		else
+			ret = 0;
+	}
+
+err:
+	return ret;
+}
+
+static int cr_import_now_pipe_inode(struct epm_action *action,
+				    ghost_t *ghost,
+				    struct task_struct *fake,
+				    int local_only,
+				    void **returned_data,
+				    size_t *data_size)
+{
+	int r;
+	struct inode *inode;
+	struct dentry *dentry;
+
+	BUG_ON(!local_only);
+
+	dentry = __prepare_pipe_dentry();
+	if (IS_ERR(dentry)) {
+		r = PTR_ERR(dentry);
+		goto err;
+	}
+
+	inode = dentry->d_inode;
+
+	r = ghost_read(ghost, &inode->i_atime, sizeof(struct timespec));
+	if (r)
+		goto err_inode;
+	r = ghost_read(ghost, &inode->i_mtime, sizeof(struct timespec));
+	if (r)
+		goto err_inode;
+	r = ghost_read(ghost, &inode->i_ctime, sizeof(struct timespec));
+	if (r)
+		goto err_inode;
+
+	r = cr_import_pipe_buffer(ghost, inode);
+	if (r)
+		goto err_inode;
+
+	*returned_data = dentry;
+	data_size = 0;
+
+err:
+	return r;
+
+err_inode:
+	free_pipe_info(dentry->d_inode);
+	dput(dentry);
+	goto err;
+}
+
+int cr_import_complete_pipe_inode(struct task_struct *fake,
+				  void *_pipe_dentry)
+{
+	struct dentry *dentry = _pipe_dentry;
+
+	dput(dentry);
+
+	return 0;
+}
+
+int cr_delete_pipe_inode(struct task_struct *fake,
+				  void *_pipe_dentry)
+{
+	struct dentry *dentry = _pipe_dentry;
+
+	/* destruction of the pipe info would happen
+	   when there is no more user of the pipe */
+	/* free_pipe_info(dentry->d_inode); */
+
+	dput(dentry);
+
+	return 0;
+}
+
+struct shared_object_operations cr_shared_pipe_inode_ops = {
+	.export_now        = cr_export_now_pipe_inode,
+	.export_user_info  = NULL,
+	.import_now        = cr_import_now_pipe_inode,
+	.import_complete   = cr_import_complete_pipe_inode,
+	.delete            = cr_delete_pipe_inode,
+};
+
+/** Return a kerrighed descriptor corresponding to the given file.
+ *  @author Matthieu Fertré
+ *
+ *  @param file       The file to get a Kerrighed descriptor for.
+ *  @param desc       The returned descriptor.
+ *  @param desc_size  Size of the returned descriptor.
+ *
+ *  @return   0 if everything ok.
+ *            Negative value otherwise.
+ */
+int get_pipe_file_krg_desc(struct file *file, void **desc, int *desc_size)
+{
+	struct regular_file_krg_desc *data;
+	int size, r = -ENOENT;
+
+	size = sizeof(struct regular_file_krg_desc);
+
+	data = kmalloc(size, GFP_KERNEL);
+	if (!data) {
+		r = -ENOMEM;
+		goto exit;
+	}
+
+	data->type = PIPE;
+	data->pipe.f_flags = file->f_flags;
+	data->pipe.key = (long)file->f_path.dentry->d_inode;
+	*desc = data;
+	*desc_size = size;
+
+	r = 0;
+exit:
+	return r;
+}
+
+struct file *reopen_pipe_file_entry_from_krg_desc(struct task_struct *task,
+						  void *_desc)
+{
+	struct regular_file_krg_desc *desc = _desc;
+	struct dentry *dentry;
+	struct file *file;
+
+	dentry = get_imported_shared_object(task->application,
+					   PIPE_INODE, desc->pipe.key);
+
+	BUG_ON(!dentry);
+
+	if (desc->pipe.f_flags & O_WRONLY) {
+		file = __create_write_pipe(dentry, desc->pipe.f_flags);
+		if (!IS_ERR(file))
+			dget(dentry);
+	} else
+		file = __create_read_pipe(dentry, desc->pipe.f_flags);
+
+	return file;
+}
+
+#endif
+
 /*
  * sys_pipe() is the normal C calling standard for creating
  * a pipe. It's not the way Unix traditionally does this, though.
diff -ruN linux-2.6.29/fs/proc/array.c android_cluster/linux-2.6.29/fs/proc/array.c
--- linux-2.6.29/fs/proc/array.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/proc/array.c	2014-05-27 23:04:10.150033658 -0700
@@ -81,6 +81,12 @@
 #include <linux/seq_file.h>
 #include <linux/pid_namespace.h>
 #include <linux/tracehook.h>
+#ifdef CONFIG_KRG_PROCFS
+#include <kerrighed/cpu_id.h>
+#endif
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/children.h>
+#endif
 
 #include <asm/pgtable.h>
 #include <asm/processor.h>
@@ -146,6 +152,9 @@
 	unsigned int state = (tsk->state & TASK_REPORT) | tsk->exit_state;
 	const char **p = &task_state_array[0];
 
+#ifdef CONFIG_KRG_EPM
+	state &= ~EXIT_MIGRATION;
+#endif
 	while (state) {
 		p++;
 		state >>= 1;
@@ -163,8 +172,12 @@
 	pid_t ppid, tpid;
 
 	rcu_read_lock();
+#ifdef CONFIG_KRG_EPM
+	ppid = krg_get_real_parent_tgid(p, ns);
+#else
 	ppid = pid_alive(p) ?
 		task_tgid_nr_ns(rcu_dereference(p->real_parent), ns) : 0;
+#endif
 	tpid = 0;
 	if (pid_alive(p)) {
 		struct task_struct *tracer = tracehook_tracer_task(p);
@@ -418,7 +431,17 @@
 		}
 
 		sid = task_session_nr_ns(task, ns);
+#ifdef CONFIG_KRG_EPM
+		/*
+		 * sighand lock is not enough: task or children KDDM objects
+		 * can disappear before release_task() locks sighand.
+		 */
+		rcu_read_lock();
+		ppid = krg_get_real_parent_tgid(task, ns);
+		rcu_read_unlock();
+#else
 		ppid = task_tgid_nr_ns(task->real_parent, ns);
+#endif
 		pgid = task_pgrp_nr_ns(task, ns);
 
 		unlock_task_sighand(task, &flags);
@@ -491,7 +514,11 @@
 		0UL,
 		0UL,
 		task->exit_signal,
+#ifdef CONFIG_KRG_PROCFS
+		krg_cpu_id(task_cpu(task)),
+#else
 		task_cpu(task),
+#endif
 		task->rt_priority,
 		task->policy,
 		(unsigned long long)delayacct_blkio_ticks(task),
diff -ruN linux-2.6.29/fs/proc/base.c android_cluster/linux-2.6.29/fs/proc/base.c
--- linux-2.6.29/fs/proc/base.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/proc/base.c	2014-06-09 18:19:41.180407254 -0700
@@ -80,6 +80,16 @@
 #include <linux/oom.h>
 #include <linux/elf.h>
 #include <linux/pid_namespace.h>
+#ifdef CONFIG_KRG_KDDM
+#include <kerrighed/krgnodemask.h>
+#include <kddm/kddm.h>
+#endif
+#if defined(CONFIG_KRG_PROCFS) && defined(CONFIG_KRG_PROC)
+#include <kerrighed/pid.h>
+#endif
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
 #include "internal.h"
 
 /* NOTE:
@@ -263,7 +273,10 @@
 	return NULL;
 }
 
-static int proc_pid_cmdline(struct task_struct *task, char * buffer)
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+int proc_pid_cmdline(struct task_struct *task, char * buffer)
 {
 	int res = 0;
 	unsigned int len;
@@ -300,7 +313,11 @@
 	return res;
 }
 
-static int proc_pid_auxv(struct task_struct *task, char *buffer)
+
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+int proc_pid_auxv(struct task_struct *task, char *buffer)
 {
 	int res = 0;
 	struct mm_struct *mm = get_task_mm(task);
@@ -324,7 +341,10 @@
  * Provides a wchan file via kallsyms in a proper one-value-per-file format.
  * Returns the resolved symbol.  If that fails, simply return the address.
  */
-static int proc_pid_wchan(struct task_struct *task, char *buffer)
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+int proc_pid_wchan(struct task_struct *task, char *buffer)
 {
 	unsigned long wchan;
 	char symname[KSYM_NAME_LEN];
@@ -342,7 +362,10 @@
 
 #define MAX_STACK_TRACE_DEPTH	64
 
-static int proc_pid_stack(struct seq_file *m, struct pid_namespace *ns,
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+int proc_pid_stack(struct seq_file *m, struct pid_namespace *ns,
 			  struct pid *pid, struct task_struct *task)
 {
 	struct stack_trace trace;
@@ -373,7 +396,10 @@
 /*
  * Provides /proc/PID/schedstat
  */
-static int proc_pid_schedstat(struct task_struct *task, char *buffer)
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+int proc_pid_schedstat(struct task_struct *task, char *buffer)
 {
 	return sprintf(buffer, "%llu %llu %lu\n",
 			(unsigned long long)task->se.sum_exec_runtime,
@@ -450,7 +476,10 @@
 
 /* The badness from the OOM killer */
 unsigned long badness(struct task_struct *p, unsigned long uptime);
-static int proc_oom_score(struct task_struct *task, char *buffer)
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+int proc_oom_score(struct task_struct *task, char *buffer)
 {
 	unsigned long points;
 	struct timespec uptime;
@@ -487,7 +516,10 @@
 };
 
 /* Display limits for a process */
-static int proc_pid_limits(struct task_struct *task, char *buffer)
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+int proc_pid_limits(struct task_struct *task, char *buffer)
 {
 	unsigned int i;
 	int count = 0;
@@ -532,7 +564,10 @@
 }
 
 #ifdef CONFIG_HAVE_ARCH_TRACEHOOK
-static int proc_pid_syscall(struct task_struct *task, char *buffer)
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+int proc_pid_syscall(struct task_struct *task, char *buffer)
 {
 	long nr;
 	unsigned long args[6], sp, pc;
@@ -551,6 +586,84 @@
 }
 #endif /* CONFIG_HAVE_ARCH_TRACEHOOK */
 
+#ifdef CONFIG_KRG_KDDM
+
+static int proc_kddm_print_wq(char *buffer, wait_queue_head_t *q)
+{
+	wait_queue_t *curr;
+	int len = 0;
+
+	list_for_each_entry(curr, &q->task_list, task_list) {
+		struct task_struct *tsk = curr->private;
+
+		len += sprintf (buffer +len, "%s (%d) ", tsk->comm, tsk->pid);
+	}
+	return len;
+}
+
+static int proc_tid_kddm(struct task_struct *task, char *buffer)
+{
+	struct kddm_info_struct info;
+	struct kddm_set *set;
+	struct kddm_obj *obj_entry;
+	int len = 0;
+
+	if (!task->kddm_info)
+		goto done;
+	info = *task->kddm_info;
+
+	len += sprintf (buffer + len, "Get Object:          %ld\n",
+			info.get_object_counter);
+
+	len += sprintf (buffer + len, "Grab Object:         %ld\n",
+			info.grab_object_counter);
+
+	len += sprintf (buffer + len, "Remove Object:       %ld\n",
+			info.remove_object_counter);
+
+	len += sprintf (buffer + len, "Flush Object:        %ld\n",
+			info.flush_object_counter);
+
+
+	obj_entry = get_kddm_obj_entry(info.ns_id, info.set_id, info.obj_id,
+				       &set);
+	if (!set)
+		goto done;
+	if (!obj_entry || obj_entry != info.wait_obj)
+		goto unlock;
+
+	len += sprintf (buffer + len, "Process wait on object "
+			"(%d;%ld;%ld) %p with state %s\n",
+			info.ns_id, info.set_id,
+			info.obj_id, obj_entry,
+			STATE_NAME (OBJ_STATE(obj_entry)));
+
+	len += sprintf (buffer + len, "  * Probe owner:   %d\n",
+			get_prob_owner(obj_entry));
+	len += sprintf (buffer + len, "  * Frozen count:  %d\n",
+			atomic_read(&obj_entry->frozen_count));
+	len += sprintf (buffer + len, "  * Sleeper count: %d\n",
+			atomic_read(&obj_entry->sleeper_count));
+	len += sprintf (buffer + len, "  * Object:        %p\n",
+			obj_entry->object);
+	len += sprintf (buffer + len, "  * Copy set: ");
+	len += krgnodemask_scnprintf(buffer + len, PAGE_SIZE - len,
+				     obj_entry->master_obj.copyset);
+	len += sprintf (buffer + len, "\n  * Remove set: ");
+	len += krgnodemask_scnprintf(buffer + len, PAGE_SIZE - len,
+				     obj_entry->master_obj.copyset);
+	len += sprintf (buffer + len, "\n  * Waiting processes: ");
+	len += proc_kddm_print_wq (buffer + len, &obj_entry->waiting_tsk);
+	len += sprintf (buffer + len, "\n");
+unlock:
+	put_kddm_obj_entry(set, obj_entry, info.obj_id);
+done:
+
+	return len;
+}
+
+#endif /* CONFIG_KRG_KDDM */
+
 /************************************************************************/
 /*                       Here the fs part begins                        */
 /************************************************************************/
@@ -572,7 +685,10 @@
 	return allowed;
 }
 
-static int proc_setattr(struct dentry *dentry, struct iattr *attr)
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+int proc_setattr(struct dentry *dentry, struct iattr *attr)
 {
 	int error;
 	struct inode *inode = dentry->d_inode;
@@ -586,7 +702,10 @@
 	return error;
 }
 
-static const struct inode_operations proc_def_inode_operations = {
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+const struct inode_operations proc_def_inode_operations = {
 	.setattr	= proc_setattr,
 };
 
@@ -1358,7 +1477,10 @@
 	return ERR_PTR(error);
 }
 
-static int do_proc_readlink(struct path *path, char __user *buffer, int buflen)
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+int do_proc_readlink(struct path *path, char __user *buffer, int buflen)
 {
 	char *tmp = (char*)__get_free_page(GFP_TEMPORARY);
 	char *pathname;
@@ -1367,6 +1489,11 @@
 	if (!tmp)
 		return -ENOMEM;
 
+#ifdef CONFIG_KRG_FAF
+	if (!path->dentry && path->mnt)
+		pathname = krg_faf_d_path((struct file *)path->mnt, tmp, PAGE_SIZE, NULL);
+	else
+#endif
 	pathname = d_path(path, tmp, PAGE_SIZE);
 	len = PTR_ERR(pathname);
 	if (IS_ERR(pathname))
@@ -1397,6 +1524,12 @@
 		goto out;
 
 	error = do_proc_readlink(&path, buffer, buflen);
+#ifdef CONFIG_KRG_FAF
+	if (!path.dentry && path.mnt) {
+		fput((struct file *)path.mnt);
+		goto out;
+	}
+#endif
 	path_put(&path);
 out:
 	return error;
@@ -1515,7 +1648,11 @@
 	struct task_struct *task = get_proc_task(inode);
 	const struct cred *cred;
 
+#if defined(CONFIG_KRG_PROCFS) && defined(CONFIG_KRG_EPM)
+	if (task && task->exit_state != EXIT_MIGRATION) {
+#else
 	if (task) {
+#endif
 		if ((inode->i_mode == (S_IFDIR|S_IRUGO|S_IXUGO)) ||
 		    task_dumpable(task)) {
 			rcu_read_lock();
@@ -1656,13 +1793,28 @@
 			if (path) {
 				*path = file->f_path;
 				path_get(&file->f_path);
+#ifdef CONFIG_KRG_FAF
+				if (file->f_flags & O_FAF_CLT) {
+					get_file(file);
+					path->mnt = (struct vfsmount *)file;
+					/* path->dentry = NULL; */
+				}
+#endif
 			}
 			if (info)
+#ifdef CONFIG_KRG_FAF
+				snprintf(info, PROC_FDINFO_MAX,
+					 "pos:\t%lli\n"
+					 "flags:\t0%o\n",
+					 (long long) file->f_pos,
+					 (unsigned int)(file->f_flags & ~(unsigned long)O_KRG_FLAGS));
+#else
 				snprintf(info, PROC_FDINFO_MAX,
 					 "pos:\t%lli\n"
 					 "flags:\t0%o\n",
 					 (long long) file->f_pos,
 					 file->f_flags);
+#endif
 			spin_unlock(&files->file_lock);
 			put_files_struct(files);
 			return 0;
@@ -2464,13 +2616,19 @@
 	return do_io_accounting(task, buffer, 0);
 }
 
-static int proc_tgid_io_accounting(struct task_struct *task, char *buffer)
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+int proc_tgid_io_accounting(struct task_struct *task, char *buffer)
 {
 	return do_io_accounting(task, buffer, 1);
 }
 #endif /* CONFIG_TASK_IO_ACCOUNTING */
 
-static int proc_pid_personality(struct seq_file *m, struct pid_namespace *ns,
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+int proc_pid_personality(struct seq_file *m, struct pid_namespace *ns,
 				struct pid *pid, struct task_struct *task)
 {
 	seq_printf(m, "%08x\n", task->personality);
@@ -2725,9 +2883,36 @@
 		get_task_struct(task);
 	rcu_read_unlock();
 	if (!task)
-		goto out;
+#if defined(CONFIG_KRG_PROCFS) && defined(CONFIG_KRG_PROC)
+	{
+		if (current->nsproxy->krg_ns
+		    && is_krg_pid_ns_root(ns) && (tgid & GLOBAL_PID_MASK))
+			result = krg_proc_pid_lookup(dir, dentry, tgid);
+#endif
+                goto out;
+#if defined(CONFIG_KRG_PROCFS) && defined(CONFIG_KRG_PROC)
+	}
+#endif
 
 	result = proc_pid_instantiate(dir, dentry, task, NULL);
+#if defined(CONFIG_KRG_PROCFS) && defined(CONFIG_KRG_EPM)
+	if (current->nsproxy->krg_ns
+	    && IS_ERR(result) && task->exit_state == EXIT_MIGRATION) {
+		/*
+		 * proc_pid_instantiate() may have instantiated dentry, but we
+		 * don't know, so restart with a fresh one.
+		 */
+		result = ERR_PTR(-ENOMEM);
+		dentry = d_alloc(dentry->d_parent, &dentry->d_name);
+		if (dentry) {
+			result = krg_proc_pid_lookup(dir, dentry, tgid);
+			if (!result)
+				result = dentry;
+			else
+				dput(dentry);
+		}
+	}
+#endif
 	put_task_struct(task);
 out:
 	return result;
@@ -2737,10 +2922,14 @@
  * Find the first task with tgid >= tgid
  *
  */
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
 struct tgid_iter {
 	unsigned int tgid;
 	struct task_struct *task;
 };
+#else
+/* moved into include/linux/procfs_internal.h */
+#endif
 static struct tgid_iter next_tgid(struct pid_namespace *ns, struct tgid_iter iter)
 {
 	struct pid *pid;
@@ -2778,7 +2967,10 @@
 
 #define TGID_OFFSET (FIRST_PROCESS_ENTRY + ARRAY_SIZE(proc_base_stuff))
 
-static int proc_pid_fill_cache(struct file *filp, void *dirent, filldir_t filldir,
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
+static
+#endif
+int proc_pid_fill_cache(struct file *filp, void *dirent, filldir_t filldir,
 	struct tgid_iter iter)
 {
 	char name[PROC_NUMBUF];
@@ -2807,6 +2999,13 @@
 	ns = filp->f_dentry->d_sb->s_fs_info;
 	iter.task = NULL;
 	iter.tgid = filp->f_pos - TGID_OFFSET;
+#if defined(CONFIG_KRG_PROCFS) && defined(CONFIG_KRG_PROC)
+	if (current->nsproxy->krg_ns && is_krg_pid_ns_root(ns)) {
+		/* All filling is done by krg_proc_pid_readdir */
+		if (krg_proc_pid_readdir(filp, dirent, filldir, TGID_OFFSET))
+			goto out;
+	} else
+#endif
 	for (iter = next_tgid(ns, iter);
 	     iter.task;
 	     iter.tgid += 1, iter = next_tgid(ns, iter)) {
@@ -2816,7 +3015,11 @@
 			goto out;
 		}
 	}
+#if defined(CONFIG_KRG_PROCFS) && defined(CONFIG_KRG_PROC)
+	filp->f_pos = KERRIGHED_PID_MAX_LIMIT + TGID_OFFSET;
+#else
 	filp->f_pos = PID_MAX_LIMIT + TGID_OFFSET;
+#endif
 out:
 	put_task_struct(reaper);
 out_no_task:
@@ -2843,6 +3046,9 @@
 	INF("cmdline",   S_IRUGO, proc_pid_cmdline),
 	ONE("stat",      S_IRUGO, proc_tid_stat),
 	ONE("statm",     S_IRUGO, proc_pid_statm),
+#ifdef CONFIG_KRG_KDDM
+	INF("kddm",      S_IRUGO, proc_tid_kddm),
+#endif
 	REG("maps",      S_IRUGO, proc_maps_operations),
 #ifdef CONFIG_NUMA
 	REG("numa_maps", S_IRUGO, proc_numa_maps_operations),
diff -ruN linux-2.6.29/fs/proc/inode.c android_cluster/linux-2.6.29/fs/proc/inode.c
--- linux-2.6.29/fs/proc/inode.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/proc/inode.c	2014-05-27 23:04:10.150033658 -0700
@@ -18,6 +18,9 @@
 #include <linux/module.h>
 #include <linux/smp_lock.h>
 #include <linux/sysctl.h>
+#if defined(CONFIG_KRG_PROCFS) && defined(CONFIG_KRG_PROC)
+#include <kerrighed/task.h>
+#endif
 
 #include <asm/system.h>
 #include <asm/uaccess.h>
@@ -55,6 +58,10 @@
 
 	/* Stop tracking associated processes */
 	put_pid(PROC_I(inode)->pid);
+#if defined(CONFIG_KRG_PROCFS) && defined(CONFIG_KRG_PROC)
+	krg_task_put(PROC_I(inode)->distant_proc.task_obj);
+	PROC_I(inode)->distant_proc.task_obj = NULL;
+#endif
 
 	/* Let go of any associated proc directory entry */
 	de = PROC_I(inode)->pde;
@@ -84,6 +91,9 @@
 	ei->fd = 0;
 	ei->op.proc_get_link = NULL;
 	ei->pde = NULL;
+#if defined(CONFIG_KRG_PROCFS) && defined(CONFIG_KRG_PROC)
+	ei->distant_proc.task_obj = NULL;
+#endif
 	ei->sysctl = NULL;
 	ei->sysctl_entry = NULL;
 	inode = &ei->vfs_inode;
@@ -459,6 +469,9 @@
 		inode->i_mtime = inode->i_atime = inode->i_ctime = CURRENT_TIME;
 		PROC_I(inode)->fd = 0;
 		PROC_I(inode)->pde = de;
+#ifdef CONFIG_KRG_PROCFS
+		PROC_I(inode)->krg_procfs_private = de->data;
+#endif /* CONFIG_KRG_PROCFS */
 
 		if (de->mode) {
 			inode->i_mode = de->mode;
diff -ruN linux-2.6.29/fs/proc/internal.h android_cluster/linux-2.6.29/fs/proc/internal.h
--- linux-2.6.29/fs/proc/internal.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/proc/internal.h	2014-05-27 23:04:10.150033658 -0700
@@ -23,13 +23,16 @@
 static inline int proc_net_init(void) { return 0; }
 #endif
 
+#ifndef CONFIG_KRG_PROCFS
 struct vmalloc_info {
 	unsigned long	used;
 	unsigned long	largest_chunk;
 };
+#endif /* !CONFIG_KRG_PROCFS */
 
 extern struct mm_struct *mm_for_maps(struct task_struct *);
 
+#ifndef CONFIG_KRG_PROCFS
 #ifdef CONFIG_MMU
 #define VMALLOC_TOTAL (VMALLOC_END - VMALLOC_START)
 extern void get_vmalloc_info(struct vmalloc_info *vmi);
@@ -42,15 +45,23 @@
 	(vmi)->largest_chunk = 0;		\
 } while(0)
 #endif
+#else /* CONFIG_KRG_PROCFS */
+/* Moved into: */
+#include <linux/procfs_internal.h>
+#endif /* CONFIG_KRG_PROCFS */
 
 extern int proc_tid_stat(struct seq_file *m, struct pid_namespace *ns,
 				struct pid *pid, struct task_struct *task);
+#if !defined(CONFIG_KRG_PROCFS) || !defined(CONFIG_KRG_PROC)
 extern int proc_tgid_stat(struct seq_file *m, struct pid_namespace *ns,
 				struct pid *pid, struct task_struct *task);
 extern int proc_pid_status(struct seq_file *m, struct pid_namespace *ns,
 				struct pid *pid, struct task_struct *task);
 extern int proc_pid_statm(struct seq_file *m, struct pid_namespace *ns,
 				struct pid *pid, struct task_struct *task);
+#else /* CONFIG_KRG_PROCFS && CONFIG_KRG_PROC */
+/* moved into include/linux/procfs_internal.h */
+#endif /* CONFIG_KRG_PROCFS && CONFIG_KRG_PROC */
 extern loff_t mem_lseek(struct file *file, loff_t offset, int orig);
 
 extern const struct file_operations proc_maps_operations;
diff -ruN linux-2.6.29/fs/proc/loadavg.c android_cluster/linux-2.6.29/fs/proc/loadavg.c
--- linux-2.6.29/fs/proc/loadavg.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/proc/loadavg.c	2014-05-27 23:04:10.150033658 -0700
@@ -10,7 +10,10 @@
 #define LOAD_INT(x) ((x) >> FSHIFT)
 #define LOAD_FRAC(x) LOAD_INT(((x) & (FIXED_1-1)) * 100)
 
-static int loadavg_proc_show(struct seq_file *m, void *v)
+#ifndef CONFIG_KRG_PROCFS
+static
+#endif
+int loadavg_proc_show(struct seq_file *m, void *v)
 {
 	int a, b, c;
 	unsigned long seq;
diff -ruN linux-2.6.29/fs/proc/meminfo.c android_cluster/linux-2.6.29/fs/proc/meminfo.c
--- linux-2.6.29/fs/proc/meminfo.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/proc/meminfo.c	2014-05-27 23:04:10.150033658 -0700
@@ -19,7 +19,10 @@
 {
 }
 
-static int meminfo_proc_show(struct seq_file *m, void *v)
+#ifndef CONFIG_KRG_PROCFS
+static
+#endif
+int meminfo_proc_show(struct seq_file *m, void *v)
 {
 	struct sysinfo i;
 	unsigned long committed;
@@ -64,6 +67,10 @@
 		"Inactive(anon): %8lu kB\n"
 		"Active(file):   %8lu kB\n"
 		"Inactive(file): %8lu kB\n"
+#ifdef CONFIG_KRG_MM
+		"Active(migr):   %8lu kB\n"
+		"Inactive(migr): %8lu kB\n"
+#endif
 #ifdef CONFIG_UNEVICTABLE_LRU
 		"Unevictable:    %8lu kB\n"
 		"Mlocked:        %8lu kB\n"
@@ -103,12 +110,23 @@
 		K(i.bufferram),
 		K(cached),
 		K(total_swapcache_pages),
+#ifdef CONFIG_KRG_MM
+		K(pages[LRU_ACTIVE_ANON]   + pages[LRU_ACTIVE_FILE] +
+		  pages[LRU_ACTIVE_MIGR]),
+		K(pages[LRU_INACTIVE_ANON] + pages[LRU_INACTIVE_FILE] +
+		  pages[LRU_INACTIVE_MIGR]),
+#else
 		K(pages[LRU_ACTIVE_ANON]   + pages[LRU_ACTIVE_FILE]),
 		K(pages[LRU_INACTIVE_ANON] + pages[LRU_INACTIVE_FILE]),
+#endif
 		K(pages[LRU_ACTIVE_ANON]),
 		K(pages[LRU_INACTIVE_ANON]),
 		K(pages[LRU_ACTIVE_FILE]),
 		K(pages[LRU_INACTIVE_FILE]),
+#ifdef CONFIG_KRG_MM
+		K(pages[LRU_ACTIVE_MIGR]),
+		K(pages[LRU_INACTIVE_MIGR]),
+#endif
 #ifdef CONFIG_UNEVICTABLE_LRU
 		K(pages[LRU_UNEVICTABLE]),
 		K(global_page_state(NR_MLOCK)),
diff -ruN linux-2.6.29/fs/proc/root.c android_cluster/linux-2.6.29/fs/proc/root.c
--- linux-2.6.29/fs/proc/root.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/proc/root.c	2014-05-27 23:04:10.150033658 -0700
@@ -57,7 +57,7 @@
 	if (flags & MS_KERNMOUNT)
 		ns = (struct pid_namespace *)data;
 	else
-		ns = current->nsproxy->pid_ns;
+		ns = task_active_pid_ns(current);
 
 	sb = sget(fs_type, proc_test_super, proc_set_super, ns);
 	if (IS_ERR(sb))
diff -ruN linux-2.6.29/fs/proc/stat.c android_cluster/linux-2.6.29/fs/proc/stat.c
--- linux-2.6.29/fs/proc/stat.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/proc/stat.c	2014-05-27 23:04:10.150033658 -0700
@@ -19,7 +19,10 @@
 #define arch_irq_stat() 0
 #endif
 
-static int show_stat(struct seq_file *p, void *v)
+#ifndef CONFIG_KRG_PROCFS
+static
+#endif
+int show_stat(struct seq_file *p, void *v)
 {
 	int i, j;
 	unsigned long jif;
diff -ruN linux-2.6.29/fs/proc/uptime.c android_cluster/linux-2.6.29/fs/proc/uptime.c
--- linux-2.6.29/fs/proc/uptime.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/proc/uptime.c	2014-05-27 23:04:10.150033658 -0700
@@ -18,7 +18,10 @@
 	return len;
 }
 
-static int uptime_read_proc(char *page, char **start, off_t off, int count,
+#ifndef CONFIG_KRG_PROCFS
+static
+#endif
+int uptime_read_proc(char *page, char **start, off_t off, int count,
 			    int *eof, void *data)
 {
 	struct timespec uptime;
diff -ruN linux-2.6.29/fs/read_write.c android_cluster/linux-2.6.29/fs/read_write.c
--- linux-2.6.29/fs/read_write.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/read_write.c	2014-05-27 23:04:10.154033575 -0700
@@ -21,6 +21,13 @@
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
 
+#ifdef CONFIG_KRG_DVFS
+#include <kerrighed/dvfs.h>
+#endif
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
+
 const struct file_operations generic_ro_fops = {
 	.llseek		= generic_file_llseek,
 	.read		= do_sync_read,
@@ -137,13 +144,25 @@
 {
 	loff_t (*fn)(struct file *, loff_t, int);
 
+#ifdef CONFIG_KRG_DVFS
+	loff_t pos;
+	if (file->f_flags & O_KRG_SHARED)
+		file->f_pos = krg_file_pos_read(file);
+#endif
 	fn = no_llseek;
 	if (file->f_mode & FMODE_LSEEK) {
 		fn = default_llseek;
 		if (file->f_op && file->f_op->llseek)
 			fn = file->f_op->llseek;
 	}
+#ifdef CONFIG_KRG_DVFS
+	pos = fn(file, offset, origin);
+	if (file->f_flags & O_KRG_SHARED)
+		krg_file_pos_write(file, file->f_pos);
+	return pos;
+#else
 	return fn(file, offset, origin);
+#endif
 }
 EXPORT_SYMBOL(vfs_llseek);
 
@@ -158,6 +177,13 @@
 	if (!file)
 		goto bad;
 
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & O_FAF_CLT) {
+		retval = krg_faf_lseek(file, offset, origin);
+		fput_light(file, fput_needed);
+		return retval;
+	}
+#endif
 	retval = -EINVAL;
 	if (origin <= SEEK_MAX) {
 		loff_t res = vfs_llseek(file, offset, origin);
@@ -189,10 +215,19 @@
 	if (origin > SEEK_MAX)
 		goto out_putf;
 
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & O_FAF_CLT) {
+		retval = krg_faf_llseek(file, offset_high, offset_low,
+					&offset, origin);
+	} else {
+#endif
 	offset = vfs_llseek(file, ((loff_t) offset_high << 32) | offset_low,
 			origin);
 
 	retval = (int)offset;
+#ifdef CONFIG_KRG_FAF
+	}
+#endif
 	if (offset >= 0) {
 		retval = -EFAULT;
 		if (!copy_to_user(result, &offset, sizeof(offset)))
@@ -280,6 +315,10 @@
 
 	if (!(file->f_mode & FMODE_READ))
 		return -EBADF;
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & O_FAF_CLT)
+		return krg_faf_read(file, buf, count, pos);
+#endif
 	if (!file->f_op || (!file->f_op->read && !file->f_op->aio_read))
 		return -EINVAL;
 	if (unlikely(!access_ok(VERIFY_WRITE, buf, count)))
@@ -335,6 +374,10 @@
 
 	if (!(file->f_mode & FMODE_WRITE))
 		return -EBADF;
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & O_FAF_CLT)
+		return krg_faf_write(file, buf, count, pos);
+#endif
 	if (!file->f_op || (!file->f_op->write && !file->f_op->aio_write))
 		return -EINVAL;
 	if (unlikely(!access_ok(VERIFY_READ, buf, count)))
@@ -359,6 +402,7 @@
 
 EXPORT_SYMBOL(vfs_write);
 
+#ifndef CONFIG_KRG_DVFS
 static inline loff_t file_pos_read(struct file *file)
 {
 	return file->f_pos;
@@ -368,6 +412,7 @@
 {
 	file->f_pos = pos;
 }
+#endif
 
 SYSCALL_DEFINE3(read, unsigned int, fd, char __user *, buf, size_t, count)
 {
@@ -668,6 +713,10 @@
 {
 	if (!(file->f_mode & FMODE_READ))
 		return -EBADF;
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & O_FAF_CLT)
+		return krg_faf_readv(file, vec, vlen, pos);
+#endif
 	if (!file->f_op || (!file->f_op->aio_read && !file->f_op->read))
 		return -EINVAL;
 
@@ -681,6 +730,10 @@
 {
 	if (!(file->f_mode & FMODE_WRITE))
 		return -EBADF;
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & O_FAF_CLT)
+		return krg_faf_writev(file, vec, vlen, pos);
+#endif
 	if (!file->f_op || (!file->f_op->aio_write && !file->f_op->write))
 		return -EINVAL;
 
diff -ruN linux-2.6.29/fs/select.c android_cluster/linux-2.6.29/fs/select.c
--- linux-2.6.29/fs/select.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/select.c	2014-05-27 23:04:10.162033409 -0700
@@ -25,6 +25,12 @@
 #include <linux/fs.h>
 #include <linux/rcupdate.h>
 #include <linux/hrtimer.h>
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krgsyms.h>
+#endif
 
 #include <asm/uaccess.h>
 
@@ -119,6 +125,10 @@
 static void free_poll_entry(struct poll_table_entry *entry)
 {
 	remove_wait_queue(entry->wait_address, &entry->wait);
+#ifdef CONFIG_KRG_FAF
+	if (entry->filp->f_flags & O_FAF_CLT)
+		krg_faf_poll_dequeue(entry->filp);
+#endif
 	fput(entry->filp);
 }
 
@@ -194,6 +204,27 @@
 	return default_wake_function(&dummy_wait, mode, sync, key);
 }
 
+#ifdef CONFIG_KRG_FAF
+static void poll_put_entry(poll_table *_p, struct poll_table_entry *entry)
+{
+	struct poll_wqueues *p = container_of(_p, struct poll_wqueues, pt);
+	struct poll_table_page *table = p->table;
+
+	if (!table) {
+		p->inline_index--;
+	} else {
+		table->entry--;
+		if (table->entry == table->entries) {
+			p->table = table->next;
+			free_page((unsigned long)table);
+		}
+	}
+
+	p->error = -ENOMEM;
+	__set_current_state(TASK_RUNNING);
+}
+#endif
+
 /* Add a new entry */
 static void __pollwait(struct file *filp, wait_queue_head_t *wait_address,
 				poll_table *p)
@@ -201,13 +232,33 @@
 	struct poll_wqueues *pwq = container_of(p, struct poll_wqueues, pt);
 	struct poll_table_entry *entry = poll_get_entry(pwq);
 	if (!entry)
+#ifndef CONFIG_KRG_FAF
 		return;
+#else
+	        goto check_faf;
+#endif
 	get_file(filp);
 	entry->filp = filp;
 	entry->wait_address = wait_address;
 	init_waitqueue_func_entry(&entry->wait, pollwake);
 	entry->wait.private = pwq;
 	add_wait_queue(wait_address, &entry->wait);
+#ifdef CONFIG_KRG_FAF
+check_faf:
+	if (filp->f_flags & O_FAF_CLT) {
+		if (krg_faf_poll_wait(filp, entry != NULL)) {
+			if (entry) {
+				/*
+				 * Don't call free_poll_entry() since it would
+				 * call krg_faf_poll_dequeue().
+				 */
+				remove_wait_queue(wait_address, &entry->wait);
+				fput(filp);
+				poll_put_entry(p, entry);
+			}
+		}
+	}
+#endif
 }
 
 int poll_schedule_timeout(struct poll_wqueues *pwq, int state,
@@ -941,3 +992,15 @@
 	return ret;
 }
 #endif /* HAVE_SET_RESTORE_SIGMASK */
+
+#ifdef CONFIG_KRG_EPM
+int select_krgsyms_register(void)
+{
+	return krgsyms_register(KRGSYMS_DO_RESTART_POLL, do_restart_poll);
+}
+
+int select_krgsyms_unregister(void)
+{
+	return krgsyms_unregister(KRGSYMS_DO_RESTART_POLL);
+}
+#endif /* CONFIG_KRG_EPM */
diff -ruN linux-2.6.29/fs/splice.c android_cluster/linux-2.6.29/fs/splice.c
--- linux-2.6.29/fs/splice.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/splice.c	2014-05-27 23:04:10.162033409 -0700
@@ -888,7 +888,10 @@
 /*
  * Attempt to initiate a splice from pipe to file.
  */
-static long do_splice_from(struct pipe_inode_info *pipe, struct file *out,
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+long do_splice_from(struct pipe_inode_info *pipe, struct file *out,
 			   loff_t *ppos, size_t len, unsigned int flags)
 {
 	int ret;
@@ -912,7 +915,10 @@
 /*
  * Attempt to initiate a splice from a file to a pipe.
  */
-static long do_splice_to(struct file *in, loff_t *ppos,
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+long do_splice_to(struct file *in, loff_t *ppos,
 			 struct pipe_inode_info *pipe, size_t len,
 			 unsigned int flags)
 {
@@ -1574,7 +1580,10 @@
 /*
  * Link contents of ipipe to opipe.
  */
-static int link_pipe(struct pipe_inode_info *ipipe,
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+int link_pipe(struct pipe_inode_info *ipipe,
 		     struct pipe_inode_info *opipe,
 		     size_t len, unsigned int flags)
 {
diff -ruN linux-2.6.29/fs/stat.c android_cluster/linux-2.6.29/fs/stat.c
--- linux-2.6.29/fs/stat.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/stat.c	2014-05-27 23:04:10.166033326 -0700
@@ -14,6 +14,9 @@
 #include <linux/security.h>
 #include <linux/syscalls.h>
 #include <linux/pagemap.h>
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -61,10 +64,20 @@
 	int error;
 
 	error = user_path_at(dfd, name, LOOKUP_FOLLOW, &path);
-	if (!error) {
-		error = vfs_getattr(path.mnt, path.dentry, stat);
-		path_put(&path);
+	if (error)
+		goto out;
+#ifdef CONFIG_KRG_FAF
+	if ((!path.dentry) && (path.mnt)) {
+		struct file *file = (struct file *)path.mnt;
+		get_file (file);
+		error = krg_faf_fstat(file, stat);
+		fput(file);
+		return error;
 	}
+#endif
+	error = vfs_getattr(path.mnt, path.dentry, stat);
+	path_put(&path);
+out:
 	return error;
 }
 
@@ -101,6 +114,13 @@
 	int error = -EBADF;
 
 	if (f) {
+#ifdef CONFIG_KRG_FAF
+		if (f->f_flags & O_FAF_CLT) {
+			error = krg_faf_fstat(f, stat);
+			fput(f);
+			return error;
+		}
+#endif
 		error = vfs_getattr(f->f_path.mnt, f->f_path.dentry, stat);
 		fput(f);
 	}
diff -ruN linux-2.6.29/fs/sync.c android_cluster/linux-2.6.29/fs/sync.c
--- linux-2.6.29/fs/sync.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/fs/sync.c	2014-05-27 23:04:10.166033326 -0700
@@ -13,6 +13,9 @@
 #include <linux/pagemap.h>
 #include <linux/quotaops.h>
 #include <linux/buffer_head.h>
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
 
 #define VALID_FLAGS (SYNC_FILE_RANGE_WAIT_BEFORE|SYNC_FILE_RANGE_WRITE| \
 			SYNC_FILE_RANGE_WAIT_AFTER)
@@ -100,6 +103,12 @@
 	 * don't have a struct file available.  Damn nfsd..
 	 */
 	if (file) {
+#ifdef CONFIG_KRG_FAF
+		if (file->f_flags & O_FAF_CLT) {
+			ret = krg_faf_fsync(file);
+			goto out;
+		}
+#endif
 		mapping = file->f_mapping;
 		fop = file->f_op;
 	} else {
diff -ruN linux-2.6.29/.gitignore android_cluster/linux-2.6.29/.gitignore
--- linux-2.6.29/.gitignore	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/.gitignore	1969-12-31 16:00:00.000000000 -0800
@@ -1,66 +0,0 @@
-#
-# NOTE! Don't add files that are generated in specific
-# subdirectories here. Add them in the ".gitignore" file
-# in that subdirectory instead.
-#
-# NOTE! Please use 'git-ls-files -i --exclude-standard'
-# command after changing this file, to see if there are
-# any tracked files which get ignored after the change.
-#
-# Normal rules
-#
-.*
-*.o
-*.o.*
-*.a
-*.s
-*.ko
-*.so
-*.so.dbg
-*.mod.c
-*.i
-*.lst
-*.symtypes
-*.order
-*.elf
-*.bin
-*.gz
-
-#
-# Top-level generic files
-#
-tags
-TAGS
-vmlinux
-System.map
-Module.markers
-Module.symvers
-!.gitignore
-!.mailmap
-
-#
-# Generated include files
-#
-include/asm
-include/asm-*/asm-offsets.h
-include/config
-include/linux/autoconf.h
-include/linux/compile.h
-include/linux/version.h
-include/linux/utsrelease.h
-include/linux/bounds.h
-
-# stgit generated dirs
-patches-*
-
-# quilt's files
-patches
-series
-
-# cscope files
-cscope.*
-ncscope.*
-
-*.orig
-*~
-\#*#
diff -ruN linux-2.6.29/include/asm-generic/siginfo.h android_cluster/linux-2.6.29/include/asm-generic/siginfo.h
--- linux-2.6.29/include/asm-generic/siginfo.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/asm-generic/siginfo.h	2014-05-27 23:04:10.226032079 -0700
@@ -142,6 +142,9 @@
  */
 #define SI_USER		0		/* sent by kill, sigsend, raise */
 #define SI_KERNEL	0x80		/* sent by the kernel from somewhere */
+#ifdef CONFIG_KRG_EPM
+#define SI_KERRIGHED	0x90		/* sent by Kerrighed for an EPM action */
+#endif
 #define SI_QUEUE	-1		/* sent by sigqueue */
 #define SI_TIMER __SI_CODE(__SI_TIMER,-2) /* sent by timer expiration */
 #define SI_MESGQ __SI_CODE(__SI_MESGQ,-3) /* sent by real time mesq state change */
diff -ruN linux-2.6.29/include/kddm/io_linker.h android_cluster/linux-2.6.29/include/kddm/io_linker.h
--- linux-2.6.29/include/kddm/io_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/io_linker.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,298 @@
+/** KDDM IO linker interface.
+ *  @file io_linker.h
+ *
+ *  Create link between KDDM and io linkers.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __IO_LINKER__
+#define __IO_LINKER__
+
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/sys/types.h>
+
+#include <kddm/kddm_types.h>
+#include <kddm/object.h>
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                 MACROS                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** IO linker types  */
+
+enum
+  {
+    MEMORY_LINKER,
+    FILE_LINKER,
+    DIR_LINKER,
+    SHM_MEMORY_LINKER,
+    INODE_LINKER,
+    FILE_STRUCT_LINKER,
+    TASK_LINKER,
+    SIGNAL_STRUCT_LINKER,
+    SIGHAND_STRUCT_LINKER,
+    STATIC_NODE_INFO_LINKER,
+    STATIC_CPU_INFO_LINKER,
+    DYNAMIC_NODE_INFO_LINKER,
+    DYNAMIC_CPU_INFO_LINKER,
+    STREAM_LINKER,
+    SOCKET_LINKER,
+    APP_LINKER,
+    FUTEX_LINKER,
+    IPCMAP_LINKER,
+    SHMID_LINKER,
+    SHMKEY_LINKER,
+    SEMARRAY_LINKER,
+    SEMUNDO_LINKER,
+    SEMKEY_LINKER,
+    MSG_LINKER,
+    MSGKEY_LINKER,
+    MSGMASTER_LINKER,
+    DSTREAM_LINKER,
+    DSOCKET_LINKER,
+    PID_LINKER,
+    CHILDREN_LINKER,
+    DVFS_FILE_STRUCT_LINKER,
+    GLOBAL_LOCK_LINKER,
+    STRING_LIST_LINKER,
+    KDDM_TEST_LINKER,
+    MM_STRUCT_LINKER,
+    PIDMAP_MAP_LINKER,
+    MAX_IO_LINKER, /* MUST always be the last one */
+  } ;
+
+
+
+#define KDDM_IO_KEEP_OBJECT 1
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+struct rpc_desc;
+
+/** IO linker struct
+ *  Describe IO linker interface functions, name, etc.
+ */
+
+struct iolinker_struct {
+  int (*instantiate) (struct kddm_set * set, void *private_data, int master);
+  void (*uninstantiate) (struct kddm_set * set, int destroy);
+  int (*first_touch) (struct kddm_obj * obj_entry, struct kddm_set * set,
+		      objid_t objid, int flags);
+  int (*remove_object) (void *object, struct kddm_set * set,
+                        objid_t objid);
+  int (*invalidate_object) (struct kddm_obj * obj_entry, struct kddm_set * set,
+                            objid_t objid);
+  int (*flush_object) (struct kddm_obj * obj_entry, struct kddm_set * set,
+		       objid_t objid);
+  int (*insert_object) (struct kddm_obj * obj_entry, struct kddm_set * set,
+                        objid_t objid);
+  int (*put_object) (struct kddm_obj * obj_entry, struct kddm_set * set,
+		     objid_t objid);
+  int (*sync_object) (struct kddm_obj * obj_entry, struct kddm_set * set,
+                      objid_t objid);
+  void (*change_state) (struct kddm_obj * obj_entry, struct kddm_set * set,
+                         objid_t objid, kddm_obj_state_t state);
+  int (*alloc_object) (struct kddm_obj * obj_entry, struct kddm_set * set,
+                       objid_t objid);
+  int (*import_object) (struct rpc_desc *desc, struct kddm_set *set,
+			struct kddm_obj *obj_entry, objid_t objid, int flags);
+  int (*export_object) (struct rpc_desc *desc, struct kddm_set *set,
+			struct kddm_obj *obj_entry, objid_t objid, int flags);
+  kerrighed_node_t (*default_owner) (struct kddm_set * set, objid_t objid,
+                                     const krgnodemask_t * nodes, int nr_nodes);
+  char linker_name[16];
+  iolinker_id_t linker_id;
+};
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Initialize IO linkers.
+ *  @author Renaud Lottiaux
+ */
+void io_linker_init (void);
+void io_linker_finalize (void);
+
+
+
+/** Register a new kddm IO linker.
+ *  @author Renaud Lottiaux
+ *
+ *  @param io_linker_id
+ *  @param linker
+ */
+int register_io_linker (int linker_id, struct iolinker_struct *io_linker);
+
+
+
+/** Instantiate a kddm set.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set           KDDM set to instantiate
+ *  @param link          Node linked to the kddm set
+ *  @param iolinker_id   Id of the iolinker to link to the kddm set
+ *  @param private_data  Data used by the instantiator...
+ *
+ *  @return error code or 0 if everything ok.
+ */
+int kddm_io_instantiate (struct kddm_set * set, kerrighed_node_t link,
+			 iolinker_id_t iolinker_id, void *private_data,
+			 int data_size, int master);
+
+
+
+/** Uninstantiate a KDDM set.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm set to uninstantiate
+ */
+void kddm_io_uninstantiate (struct kddm_set * set, int destroy);
+
+
+
+/** Do an object first touch.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to first touch.
+ *  @param obj_entry    Object entry the object belong to.
+ *  @param objectState  Initial state of the object.
+ */
+int kddm_io_first_touch_object (struct kddm_obj * obj_entry,
+				struct kddm_set * set, objid_t objid,
+				int flags);
+
+
+
+/** Put a KDDM object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to put.
+ *  @param obj_entry    Object entry the object belong to.
+ */
+int kddm_io_put_object (struct kddm_obj * obj_entry, struct kddm_set * set,
+                        objid_t objid);
+
+
+
+/** Insert an object in a kddm set.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to insert.
+ *  @param obj_entry    Object entry the object belong to.
+ */
+int kddm_io_insert_object (struct kddm_obj * obj_entry, struct kddm_set * set,
+                           objid_t objid);
+
+
+
+/** Request an IO linker to invalidate an object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to invalidate.
+ *  @param obj_entry    Object entry the object belong to.
+ */
+int kddm_io_invalidate_object (struct kddm_obj * obj_entry, struct kddm_set * set,
+                               objid_t objid);
+
+
+
+/** Request an IO linker to remove an object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to remove.
+ *  @param obj_entry    Object entry the object belong to.
+ */
+int kddm_io_remove_object_and_unlock (struct kddm_obj * obj_entry, struct kddm_set * set,
+				      objid_t objid);
+
+int kddm_io_remove_object (void *object, struct kddm_set * set, objid_t objid);
+
+
+
+/** Request an IO linker to sync an object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to sync.
+ *  @param obj_entry    Object entry the object belong to.
+ */
+int kddm_io_sync_object (struct kddm_obj * obj_entry, struct kddm_set * set,
+                         objid_t objid);
+
+
+
+/** Inform an IO linker that an object state has changed.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry    Object entry the object belong to.
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to sync.
+ *  @param new_state    New state for the object.
+ */
+int kddm_io_change_state (struct kddm_obj * obj_entry,
+			  struct kddm_set * set,
+			  objid_t objid,
+			  kddm_obj_state_t new_state);
+
+
+
+/** Request an IO linker to import data into an object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param obj_entry    Object entry to import data into.
+ *  @param buffer       Buffer containing data to import.
+ */
+int kddm_io_import_object (struct rpc_desc *desc, struct kddm_set *set,
+			   struct kddm_obj *obj_entry, objid_t objid,
+			   int flags);
+
+/** Request an IO linker to export data from an object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param obj_entry    Object entry to export data from.
+ *  @param buffer       Buffer to export data to.
+ */
+int kddm_io_export_object (struct rpc_desc *desc, struct kddm_set *set,
+			   struct kddm_obj *obj_entry, objid_t objid,
+			   int flags);
+kerrighed_node_t kddm_io_default_owner (struct kddm_set * set, objid_t objid);
+
+/** Request an IO linker to allocate an object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry   Object entry to export data from.
+ *  @param set         Kddm Set the object belong to.
+ */
+int kddm_io_alloc_object (struct kddm_obj * obj_entry, struct kddm_set * set,
+			  objid_t objid);
+
+#endif // __IO_LINKER__
diff -ruN linux-2.6.29/include/kddm/kddm_find_object.h android_cluster/linux-2.6.29/include/kddm/kddm_find_object.h
--- linux-2.6.29/include/kddm/kddm_find_object.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/kddm_find_object.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,43 @@
+/** KDDM find object.
+ *  @file kddm_find_object.h
+ *
+ *  Definition of KDDM interface.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KDDM_FIND_OBJECT__
+#define __KDDM_FIND_OBJECT__
+
+#include <kddm/kddm_set.h>
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Check the presence of a given object in local physical memory. */
+void *kddm_find_object (struct kddm_ns *ns, kddm_set_id_t set_id,
+			objid_t objid);
+
+void *_kddm_find_object (struct kddm_set *set, objid_t objid);
+
+static inline void *_kddm_find_object_raw (struct kddm_set *set, objid_t objid)
+{
+	struct kddm_obj *obj_entry;
+	void *obj = NULL;
+
+	obj_entry = __get_kddm_obj_entry(set, objid);
+	if (obj_entry) {
+		obj = obj_entry->object;
+		put_kddm_obj_entry(set, obj_entry, objid);
+	}
+
+	return obj;
+}
+
+#endif
diff -ruN linux-2.6.29/include/kddm/kddm_flush_object.h android_cluster/linux-2.6.29/include/kddm/kddm_flush_object.h
--- linux-2.6.29/include/kddm/kddm_flush_object.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/kddm_flush_object.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,29 @@
+/** KDDM flush object.
+ *  @file kddm_flush_object.h
+ *
+ *  Definition of KDDM interface.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KDDM_FLUSH_OBJECT__
+#define __KDDM_FLUSH_OBJECT__
+
+#include <kddm/kddm_set.h>
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Flush an object from local memory */
+int kddm_flush_object(struct kddm_ns *ns, kddm_set_id_t set_id, objid_t objid,
+		      kerrighed_node_t dest);
+
+int _kddm_flush_object(struct kddm_set *set, objid_t objid,
+		       kerrighed_node_t dest);
+
+#endif
diff -ruN linux-2.6.29/include/kddm/kddm_get_object.h android_cluster/linux-2.6.29/include/kddm/kddm_get_object.h
--- linux-2.6.29/include/kddm/kddm_get_object.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/kddm_get_object.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,65 @@
+/** KDDM get object.
+ *  @file kddm_get_object.h
+ *
+ *  Definition of KDDM interface.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KDDM_GET_OBJECT__
+#define __KDDM_GET_OBJECT__
+
+#include <kddm/kddm_set.h>
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Place a read-only copy of a given object in local physical memory. */
+void *kddm_get_object(struct kddm_ns *ns, kddm_set_id_t set_id, objid_t objid);
+
+void *_kddm_get_object(struct kddm_set *set, objid_t objid);
+
+
+
+/** Asynchronous version of the get_object function. */
+void *async_kddm_get_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+			    objid_t objid);
+
+void *_async_kddm_get_object(struct kddm_set *set, objid_t objid);
+
+
+
+/** Place a existing copy of a given object in local physical memory. */
+void *kddm_get_object_no_ft(struct kddm_ns *ns, kddm_set_id_t set_id,
+			    objid_t objid);
+
+void *_kddm_get_object_no_ft(struct kddm_set *set, objid_t objid);
+
+
+
+/** Prepare an object to be manually filled by the function called */
+void *kddm_get_object_manual_ft(struct kddm_ns *ns, kddm_set_id_t set_id,
+				objid_t objid);
+
+void *_kddm_get_object_manual_ft(struct kddm_set *set, objid_t objid);
+
+
+
+/** Place a existing copy of a given object in local physical memory. */
+void *kddm_get_object_no_lock(struct kddm_ns *ns, kddm_set_id_t set_id,
+			      objid_t objid);
+
+void *_kddm_get_object_no_lock(struct kddm_set *set, objid_t objid);
+
+/** Generic get functions with free use of KDDM flags */
+void *fkddm_get_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+		       objid_t objid, int flags);
+
+void *_fkddm_get_object(struct kddm_set *set, objid_t objid, int flags);
+
+#endif
diff -ruN linux-2.6.29/include/kddm/kddm_grab_object.h android_cluster/linux-2.6.29/include/kddm/kddm_grab_object.h
--- linux-2.6.29/include/kddm/kddm_grab_object.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/kddm_grab_object.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,91 @@
+/** KDDM grab object.
+ *  @file kddm_grab_object.h
+ *
+ *  Definition of KDDM interface.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KDDM_GRAB_OBJECT__
+#define __KDDM_GRAB_OBJECT__
+
+#include <kddm/kddm_set.h>
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Place a write copy of a given object in local physical memory. */
+void *kddm_grab_object(struct kddm_ns *ns, kddm_set_id_t set_id, objid_t objid);
+
+void *_kddm_grab_object(struct kddm_set *set, objid_t objid);
+
+void *__kddm_grab_object(struct kddm_set *set, struct kddm_obj *obj_entry,
+			 objid_t objid);
+
+/** Asynchronous version of the grab_object function. */
+void *async_kddm_grab_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+			     objid_t objid);
+
+void *_async_kddm_grab_object(struct kddm_set *set, objid_t objid);
+
+void *__async_kddm_grab_object(struct kddm_set *set,
+			       struct kddm_obj *obj_entry, objid_t objid);
+
+/** Place a existing copy of a given object in local physical memory. */
+void *kddm_grab_object_no_ft(struct kddm_ns *ns, kddm_set_id_t set_id,
+			     objid_t objid);
+
+void *_kddm_grab_object_no_ft(struct kddm_set *set, objid_t objid);
+
+void *__kddm_grab_object_no_ft(struct kddm_set *set,
+			       struct kddm_obj *obj_entry, objid_t objid);
+
+/** Place a existing copy of a given object in local physical memory. */
+void *async_kddm_grab_object_no_ft(struct kddm_ns *ns, kddm_set_id_t set_id,
+			     objid_t objid);
+
+void *_async_kddm_grab_object_no_ft(struct kddm_set *set, objid_t objid);
+
+void *__async_kddm_grab_object_no_ft(struct kddm_set *set,
+				     struct kddm_obj *obj_entry,objid_t objid);
+
+/** Prepare an object to be manually filled by the function called */
+void *kddm_grab_object_manual_ft(struct kddm_ns *ns, kddm_set_id_t set_id,
+				 objid_t objid);
+
+void *_kddm_grab_object_manual_ft(struct kddm_set *set, objid_t objid);
+
+void *__kddm_grab_object_manual_ft(struct kddm_set *set,
+				   struct kddm_obj *obj_entry,
+				   objid_t objid);
+
+/** Place a existing copy of a given object in local physical memory. */
+void *kddm_grab_object_no_lock(struct kddm_ns *ns, kddm_set_id_t set_id,
+			       objid_t objid);
+
+void *_kddm_grab_object_no_lock(struct kddm_set *set, objid_t objid);
+
+void *__kddm_grab_object_no_lock(struct kddm_set *set,
+				 struct kddm_obj *obj_entry, objid_t objid);
+
+/** Place a existing copy of a given object in local physical memory. */
+void *kddm_try_grab_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+			   objid_t objid);
+
+void *_kddm_try_grab_object(struct kddm_set *set, objid_t objid);
+
+void *__kddm_try_grab_object(struct kddm_set *set,
+			     struct kddm_obj *obj_entry, objid_t objid);
+
+void *_kddm_grab_object_cow(struct kddm_set *set, objid_t objid);
+
+/** Generic grab function with free use of KDDM flags */
+void *fkddm_grab_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+			objid_t objid, int flags);
+
+#endif
diff -ruN linux-2.6.29/include/kddm/kddm.h android_cluster/linux-2.6.29/include/kddm/kddm.h
--- linux-2.6.29/include/kddm/kddm.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/kddm.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,104 @@
+/** KDDM interface.
+ *  @file kddm.h
+ *
+ *  Definition of KDDM interface.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KDDM__
+#define __KDDM__
+
+#include <kddm/kddm_types.h>
+#include <kddm/io_linker.h>
+#include <kddm/object.h>
+#include <kddm/kddm_set.h>
+#include <kddm/kddm_find_object.h>
+#include <kddm/kddm_put_object.h>
+#include <kddm/kddm_get_object.h>
+#include <kddm/kddm_grab_object.h>
+#include <kddm/kddm_set_object.h>
+#include <kddm/kddm_flush_object.h>
+#include <kddm/kddm_remove_object.h>
+#include <kddm/kddm_sync_object.h>
+
+#include <kerrighed/debug.h>
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                             MACRO CONSTANTS                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Print an error message concerning a problem in the state machine */
+#define STATE_MACHINE_ERROR(set_id, objid, obj_entry) \
+{ \
+  if (OBJ_STATE_INDEX(OBJ_STATE(obj_entry)) < NB_OBJ_STATE) \
+    PANIC ("Receive a object on %s object (%ld;%ld) \n", \
+	   STATE_NAME(OBJ_STATE(obj_entry)), set_id, objid) ; \
+  else \
+    PANIC( "Object (%ld;%ld) : unknown object state\n", set_id, objid) ; \
+}
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+extern event_counter_t total_get_object_counter;
+extern event_counter_t total_grab_object_counter;
+extern event_counter_t total_remove_object_counter;
+extern event_counter_t total_flush_object_counter;
+
+
+
+/*********************** KDDM set Counter tools ************************/
+
+int initialize_kddm_info_struct (struct task_struct *task);
+
+
+static inline void inc_get_object_counter(struct kddm_set *set)
+{
+	total_get_object_counter++;
+	set->get_object_counter++;
+	if (!current->kddm_info)
+		initialize_kddm_info_struct(current);
+	current->kddm_info->get_object_counter++;
+}
+
+static inline void inc_grab_object_counter(struct kddm_set *set)
+{
+	total_grab_object_counter++;
+	set->grab_object_counter++;
+	if (!current->kddm_info)
+		initialize_kddm_info_struct(current);
+	current->kddm_info->grab_object_counter++;
+}
+
+static inline void inc_remove_object_counter(struct kddm_set *set)
+{
+	total_remove_object_counter++;
+	set->remove_object_counter++;
+	if (!current->kddm_info)
+		initialize_kddm_info_struct(current);
+	current->kddm_info->remove_object_counter++;
+}
+
+static inline void inc_flush_object_counter(struct kddm_set *set)
+{
+	total_flush_object_counter++;
+	set->flush_object_counter++;
+	if (!current->kddm_info)
+		initialize_kddm_info_struct(current);
+	current->kddm_info->flush_object_counter++;
+}
+
+#endif
diff -ruN linux-2.6.29/include/kddm/kddm_info.h android_cluster/linux-2.6.29/include/kddm/kddm_info.h
--- linux-2.6.29/include/kddm/kddm_info.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/kddm_info.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,9 @@
+#ifndef __KDDM_INFO_H__
+#define __KDDM_INFO_H__
+
+extern int (*kh_copy_kddm_info)(unsigned long clone_flags,
+				struct task_struct * tsk);
+
+extern struct kmem_cache *kddm_info_cachep;
+
+#endif /* __KDDM_INFO_H__ */
diff -ruN linux-2.6.29/include/kddm/kddm_put_object.h android_cluster/linux-2.6.29/include/kddm/kddm_put_object.h
--- linux-2.6.29/include/kddm/kddm_put_object.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/kddm_put_object.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,29 @@
+/** KDDM put object.
+ *  @file kddm_put_object.h
+ *
+ *  Definition of KDDM interface.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KDDM_PUT_OBJECT__
+#define __KDDM_PUT_OBJECT__
+
+#include <kddm/kddm_set.h>
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Release a kddm object acquired by a find, get or grab object. */
+
+void kddm_put_object(struct kddm_ns *ns, kddm_set_id_t set_id, objid_t objid);
+
+void _kddm_put_object(struct kddm_set *set, objid_t objid);
+
+#endif
diff -ruN linux-2.6.29/include/kddm/kddm_remove_object.h android_cluster/linux-2.6.29/include/kddm/kddm_remove_object.h
--- linux-2.6.29/include/kddm/kddm_remove_object.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/kddm_remove_object.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,36 @@
+
+
+
+/** KDDM remove object.
+ *  @file kddm_remove_object.h
+ *
+ *  Definition of KDDM interface.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KDDM_REMOVE_OBJECT__
+#define __KDDM_REMOVE_OBJECT__
+
+#include <kddm/kddm_set.h>
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Remove an object from a kddm set cluster wide */
+int kddm_remove_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+		       objid_t objid);
+
+int _kddm_remove_object(struct kddm_set *set, objid_t objid);
+
+int kddm_remove_frozen_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+			      objid_t objid);
+
+int _kddm_remove_frozen_object(struct kddm_set *set, objid_t objid);
+
+#endif
diff -ruN linux-2.6.29/include/kddm/kddm_set.h android_cluster/linux-2.6.29/include/kddm/kddm_set.h
--- linux-2.6.29/include/kddm/kddm_set.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/kddm_set.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,270 @@
+/** KDDM kddm interface.
+ *  @file kddm_set.h
+ *
+ *  Definition of KDDM set interface.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KDDM_SET__
+#define __KDDM_SET__
+
+#include <linux/socket.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/krgsyms.h>
+
+#include <kddm/kddm_types.h>
+#include <kddm/name_space.h>
+#include <kddm/kddm_tree.h>
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                             MACRO CONSTANTS                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/* KDDM set state */
+enum
+  {
+    KDDM_SET_UNINITIALIZED,
+    KDDM_SET_NEED_LOOKUP,
+    KDDM_SET_INVALID,
+    KDDM_SET_LOCKED,
+    KDDM_SET_READY,
+  };
+
+
+
+#define KDDM_ALLOC_STRUCT 1
+#define KDDM_CHECK_UNIQUE 2
+#define KDDM_LOCK_FREE 4
+
+
+/** Return the manager id of the given kddm set */
+#define KDDM_SET_MGR(set) ((kerrighed_node_t)(set->id >> UNIQUE_ID_NODE_SHIFT))
+
+#define MAX_PRIVATE_DATA_SIZE (PAGE_SIZE-sizeof(msg_kddm_set_t))
+
+/** Default size of a kddm set hash table */
+#define KDDM_SET_HASH_TABLE_SIZE 1024
+
+/** Default size for readahead windows */
+#define DEFAULT_READAHEAD_WINDOW_SIZE 8
+
+/* Kddm set with round robin distributed default owner */
+#define KDDM_RR_DEF_OWNER ((kerrighed_node_t)(KERRIGHED_MAX_NODES + 1))
+
+/* Kddm set with default owner based on unique ID */
+#define KDDM_UNIQUE_ID_DEF_OWNER ((kerrighed_node_t)(KERRIGHED_MAX_NODES + 2))
+
+/* Kddm set with a custom default owner policy */
+#define KDDM_CUSTOM_DEF_OWNER ((kerrighed_node_t)(KERRIGHED_MAX_NODES + 3))
+
+/* MUST ALWAYS BE THE LAST ONE and equal to the highest possible value */
+#define KDDM_MAX_DEF_OWNER ((kerrighed_node_t)(KERRIGHED_MAX_NODES + 4))
+
+/* Kddm set id reserved for internal system usage (sys_kddm_ns name space). */
+enum
+  {
+    KDDM_SET_UNUSED,                  //  0
+    TASK_KDDM_ID,                     //  1
+    SIGNAL_STRUCT_KDDM_ID,            //  2
+    SIGHAND_STRUCT_KDDM_ID,           //  3
+    STATIC_NODE_INFO_KDDM_ID,         //  4
+    STATIC_CPU_INFO_KDDM_ID,          //  5
+    DYNAMIC_NODE_INFO_KDDM_ID,        //  6
+    DYNAMIC_CPU_INFO_KDDM_ID,         //  7
+    APP_KDDM_ID,                      //  8
+    SHMID_KDDM_ID,                    //  9
+    SHMKEY_KDDM_ID,                   // 10
+    SHMMAP_KDDM_ID,                   // 11
+    SEMARRAY_KDDM_ID,                 // 12
+    SEMKEY_KDDM_ID,                   // 13
+    SEMMAP_KDDM_ID,                   // 14
+    SEMUNDO_KDDM_ID,                  // 15
+    MSG_KDDM_ID,                      // 16
+    MSGKEY_KDDM_ID,                   // 17
+    MSGMAP_KDDM_ID,                   // 18
+    MSGMASTER_KDDM_ID,                // 19
+    PID_KDDM_ID,                      // 20
+    CHILDREN_KDDM_ID,                 // 21
+    DVFS_FILE_STRUCT_KDDM_ID,         // 22
+    GLOBAL_LOCK_KDDM_SET_ID,	      // 23
+    GLOBAL_CONFIG_KDDM_SET_ID,        // 24
+    KDDM_TEST4_DIST,                  // 25
+    KDDM_TEST4_LOC,                   // 26
+    KDDM_TEST4096,                    // 27
+    MM_STRUCT_KDDM_ID,                // 28
+    PIDMAP_MAP_KDDM_ID,               // 29
+    MIN_KDDM_ID,           /* MUST always be the last one */
+  };
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** kddm set manager message type.
+ *  Used to store informations to sent to the KDDM set manager server.
+ */
+typedef struct {
+	int kddm_ns;               /**< KDDM name space identifier */
+	kddm_set_id_t kddm_set_id; /**< KDDM set identifier */
+	unsigned long flags;       /**< Kddm Set flags */
+	kerrighed_node_t link;     /**< Node linked to the kddm set */
+	int obj_size;              /**< Size of objects stored in kddm set */
+	iolinker_id_t linker_id;   /**< Identifier of the io linker  */
+	unsigned long data_size;   /**< Size of set private data to receive */
+	krgsyms_val_t set_ops;     /**< KDDM set operations struct ID */
+	char private_data[1];
+} msg_kddm_set_t;
+
+
+
+typedef struct {
+	int ns_id;                   /**< KDDM name space identifier */
+	kddm_set_id_t set_id;        /**< KDDM set identifier */
+} kddm_id_msg_t;
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+void kddm_set_init(void);
+void kddm_set_finalize(void);
+
+struct kddm_set *__create_new_kddm_set(struct kddm_ns *ns,
+				       kddm_set_id_t kddm_set_id,
+				       struct kddm_set_ops *set_ops,
+				       void *tree_init_data,
+				       iolinker_id_t linker_id,
+				       kerrighed_node_t def_owner,
+				       int obj_size,
+				       void *private_data,
+				       unsigned long data_size,
+				       unsigned long flags);
+
+static inline struct kddm_set *_create_new_kddm_set(struct kddm_ns *ns,
+				       kddm_set_id_t kddm_set_id,
+				       iolinker_id_t linker_id,
+				       kerrighed_node_t def_owner,
+				       int obj_size,
+				       void *private_data,
+				       unsigned long data_size,
+				       unsigned long flags)
+{
+	return (struct kddm_set *) __create_new_kddm_set(ns, kddm_set_id,
+						 &kddm_tree_set_ops,
+						 _nlevels_kddm_tree_init_data,
+						 linker_id, def_owner,
+						 obj_size, private_data,
+						 data_size, flags);
+}
+
+static inline struct kddm_set *create_new_kddm_set(struct kddm_ns *ns,
+				       kddm_set_id_t kddm_set_id,
+				       iolinker_id_t linker_id,
+				       kerrighed_node_t def_owner,
+				       int obj_size,
+				       unsigned long flags)
+{
+	return (struct kddm_set *) __create_new_kddm_set(ns, kddm_set_id,
+						 &kddm_tree_set_ops,
+						 _nlevels_kddm_tree_init_data,
+						 linker_id, def_owner,
+						 obj_size, NULL, 0, flags);
+}
+
+int _destroy_kddm_set(struct kddm_set * kddm_set);
+int destroy_kddm_set(struct kddm_ns *ns, kddm_set_id_t set_id);
+
+struct kddm_set *__find_get_kddm_set(struct kddm_ns *ns,
+				     kddm_set_id_t kddm_set_id,
+				     int flags);
+
+static inline struct kddm_set *_find_get_kddm_set(struct kddm_ns *ns,
+						  kddm_set_id_t kddm_set_id)
+{
+	return __find_get_kddm_set(ns, kddm_set_id, 0);
+}
+
+struct kddm_set *find_get_kddm_set(int ns_id,
+				   kddm_set_id_t set_id);
+
+struct kddm_set *generic_local_get_kddm_set(int ns_id,
+					     kddm_set_id_t set_id,
+					     int init_state,
+					     int flags);
+
+struct kddm_set *_generic_local_get_kddm_set(struct kddm_ns *ns,
+					     kddm_set_id_t set_id,
+					     int init_state,
+					     int flags);
+
+/** Different flavors of the get_kddm_set function */
+
+static inline struct kddm_set *_local_get_kddm_set(struct kddm_ns *ns,
+						   kddm_set_id_t set_id)
+{
+	return _generic_local_get_kddm_set(ns, set_id, 0, 0);
+}
+
+static inline struct kddm_set *_local_get_alloc_kddm_set(struct kddm_ns *ns,
+							 kddm_set_id_t set_id,
+							 int init_state)
+{
+	return _generic_local_get_kddm_set(ns, set_id, init_state,
+					   KDDM_ALLOC_STRUCT);
+}
+
+static inline struct kddm_set *local_get_kddm_set(int ns_id,
+						  kddm_set_id_t set_id)
+{
+	return generic_local_get_kddm_set(ns_id, set_id, 0, 0);
+}
+
+static inline struct kddm_set *local_get_alloc_kddm_set(int ns_id,
+							kddm_set_id_t set_id,
+							int init_state)
+{
+	return generic_local_get_kddm_set(ns_id, set_id, init_state,
+					  KDDM_ALLOC_STRUCT);
+}
+
+static inline struct kddm_set *_local_get_alloc_unique_kddm_set(
+	                                          struct kddm_ns *ns,
+						  kddm_set_id_t set_id,
+						  int init_state)
+{
+	return _generic_local_get_kddm_set(ns, set_id, init_state,
+					   KDDM_ALLOC_STRUCT |
+					   KDDM_CHECK_UNIQUE);
+
+}
+
+void put_kddm_set(struct kddm_set *set);
+
+static inline int kddm_set_frozen(struct kddm_set *set)
+{
+	return (set->flags & KDDM_FROZEN);
+}
+
+void freeze_kddm(void);
+void unfreeze_kddm(void);
+
+#endif // __KDDM_NS__
diff -ruN linux-2.6.29/include/kddm/kddm_set_object.h android_cluster/linux-2.6.29/include/kddm/kddm_set_object.h
--- linux-2.6.29/include/kddm/kddm_set_object.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/kddm_set_object.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,34 @@
+/** KDDM set object.
+ *  @file kddm_set_object.h
+ *
+ *  Definition of KDDM interface.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KDDM_SET_OBJECT__
+#define __KDDM_SET_OBJECT__
+
+#include <kddm/kddm_set.h>
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Set the initial value of an object. */
+int _kddm_set_object_state(struct kddm_set *set, objid_t objid, void *object,
+			   kddm_obj_state_t state);
+
+int kddm_set_object_state(struct kddm_ns *ns, kddm_set_id_t set_id,
+			  objid_t objid, void *object, kddm_obj_state_t state);
+
+int _kddm_set_object(struct kddm_set *set, objid_t objid, void *object);
+
+int kddm_set_object(struct kddm_ns *ns, kddm_set_id_t set_id, objid_t objid,
+		    void *object);
+
+#endif
diff -ruN linux-2.6.29/include/kddm/kddm_sync_object.h android_cluster/linux-2.6.29/include/kddm/kddm_sync_object.h
--- linux-2.6.29/include/kddm/kddm_sync_object.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/kddm_sync_object.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,28 @@
+/** KDDM sync object.
+ *  @file kddm_sync_object.h
+ *
+ *  Definition of KDDM interface.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KDDM_SYNC_OBJECT__
+#define __KDDM_SYNC_OBJECT__
+
+#include <kddm/kddm_set.h>
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Sync an object from local memory */
+int kddm_sync_frozen_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+			    objid_t objid);
+
+int _kddm_sync_frozen_object(struct kddm_set *set, objid_t objid);
+
+#endif
diff -ruN linux-2.6.29/include/kddm/kddm_tree.h android_cluster/linux-2.6.29/include/kddm/kddm_tree.h
--- linux-2.6.29/include/kddm/kddm_tree.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/kddm_tree.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,66 @@
+/** Kddm tree implementation.
+ *  @file kddm_tree.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KDDM_TREE__
+#define __KDDM_TREE__
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                 MACROS                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+#define _2LEVELS_KDDM_TREE 0
+#define _NLEVELS_KDDM_TREE 1
+
+#define KDDM_TREE_ADD_ENTRY 1
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN VARIABLES                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+extern struct kddm_set_ops kddm_tree_set_ops;
+extern void *_2levels_kddm_tree_init_data;
+extern void *_nlevels_kddm_tree_init_data;
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                   TYPES                                  *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** KDDM tree level struct type */
+struct kddm_tree_lvl {
+	int nr_obj;
+	struct kddm_tree_lvl **sub_lvl;
+};
+
+/** KDDM tree type */
+struct kddm_tree {
+	struct kddm_tree_lvl *lvl1;
+	unsigned long max_data;
+	int tree_type;
+	int nr_level;
+	int bit_width; /*!< width of index 20, 32, 64 */
+	int bit_size; /*!< normal bits per level, last level is the rest  */
+	int bit_size_last; /*!< bits for last level (zero, if width%size=0) */
+};
+
+#endif // __KDDM_TREE__
diff -ruN linux-2.6.29/include/kddm/kddm_types.h android_cluster/linux-2.6.29/include/kddm/kddm_types.h
--- linux-2.6.29/include/kddm/kddm_types.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/kddm_types.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,263 @@
+#ifndef __KDDM_SET_TYPES__
+#define __KDDM_SET_TYPES__
+
+#include <kddm/kddm_tree.h>
+#include <linux/wait.h>
+#include <kerrighed/types.h>
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              OBJECT TYPES                                *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/*********************     Object states    ***********************/
+//
+//                                            +------------------------------------------- Object state
+//                                            |
+//                         |-----------------------------------|
+// |31 30 29 28 27 26 25 24|23|22|21|20|19 18 17 16 15 14 13 12|11|10|9|8|7|6 5 4 3 2 1 0|
+// |-----------------------|--|--|--|--|-----------------------|--|--|-|-|-|-------------|
+//             |             |  |  |  |            |             |  | | | |      |
+//             |             |  |  |  |            |             |  | | | |      +-------- Reserved
+//             |             |  |  |  |            |             |  | | | +--------------- Pending event
+//             |             |  |  |  |            |             |  | | +----------------- Pinned flag
+//             |             |  |  |  |            |             |  | +------------------- SEND_RM_ACK2 flag
+//             |             |  |  |  |            |             |  +--------------------- Failure Flag
+//             |             |  |  |  |            |             +------------------------ Locked
+//             |             |  |  |  |            +-------------------------------------- Object state index
+//             |             |  |  |  +--------------------------------------------------- Owner flag
+//             |             |  |  +------------------------------------------------------ Read access flag
+//             |             |  +--------------------------------------------------------- Write access flag
+//             |             +------------------------------------------------------------ Unused
+//             +-------------------------------------------------------------------------- Probe Owner
+
+/* Various object flags */
+#define OBJECT_PENDING_EVENT 7  /* An event is pending on the object */
+#define OBJECT_PINNED        8  /* Lock the object to give waiting
+				   processes a change to access
+				   the object before a potential
+				   invalidation */
+#define SEND_RM_ACK2         9  /* The default owner need an ack2 after
+				   a global remove is done */
+#define FAILURE_FLAG        10
+#define OBJECT_LOCKED       11  /* The object is locked */
+
+/* Object state */
+#define STATE_INDEX_MASK    0x000FF000  /* Mask to extract the state index */
+#define STATE_INDEX_SHIFT   12
+
+#define KDDM_OWNER_OBJ      (1 << 20)  /* Object is the master object */
+#define KDDM_READ_OBJ       (1 << 21)  /* Object can be read */
+#define KDDM_WRITE_OBJ      (1 << 22)  /* Object can be write */
+
+#define OBJECT_STATE_MASK   0x00FFF000
+
+/* Probe owner */
+#define PROB_OWNER_MASK     0xFF000000
+#define PROB_OWNER_SHIFT    24
+
+/* Helper macros */
+
+#define SET_OBJECT_PINNED(obj_entry) \
+        set_bit (OBJECT_PINNED, &(obj_entry)->flags)
+#define CLEAR_OBJECT_PINNED(obj_entry) \
+        clear_bit (OBJECT_PINNED, &(obj_entry)->flags)
+#define TEST_OBJECT_PINNED(obj_entry) \
+        test_bit (OBJECT_PINNED, &(obj_entry)->flags)
+
+#define SET_OBJECT_LOCKED(obj_entry) \
+        set_bit(OBJECT_LOCKED, &(obj_entry)->flags)
+#define TEST_AND_SET_OBJECT_LOCKED(obj_entry) \
+        test_and_set_bit(OBJECT_LOCKED, &(obj_entry)->flags)
+#define CLEAR_OBJECT_LOCKED(obj_entry) \
+        clear_bit(OBJECT_LOCKED, &(obj_entry)->flags)
+#define TEST_OBJECT_LOCKED(obj_entry) \
+        test_bit(OBJECT_LOCKED, &(obj_entry)->flags)
+
+#define SET_OBJECT_PENDING(obj_entry) \
+        set_bit(OBJECT_PENDING_EVENT, &(obj_entry)->flags)
+#define CLEAR_OBJECT_PENDING(obj_entry) \
+        clear_bit(OBJECT_PENDING_EVENT, &(obj_entry)->flags)
+#define TEST_OBJECT_PENDING(obj_entry) \
+        test_bit(OBJECT_PENDING_EVENT, &(obj_entry)->flags)
+
+#define SET_OBJECT_RM_ACK2(obj_entry) \
+        set_bit (SEND_RM_ACK2, &(obj_entry)->flags)
+#define CLEAR_OBJECT_RM_ACK2(obj_entry) \
+        clear_bit (SEND_RM_ACK2, &(obj_entry)->flags)
+#define TEST_OBJECT_RM_ACK2(obj_entry) \
+        test_bit (SEND_RM_ACK2, &(obj_entry)->flags)
+
+#define SET_FAILURE_FLAG(obj_entry) \
+        set_bit (FAILURE_FLAG, &(obj_entry)->flags)
+#define CLEAR_FAILURE_FLAG(obj_entry) \
+        clear_bit (FAILURE_FLAG, &(obj_entry)->flags)
+#define TEST_FAILURE_FLAG(obj_entry) \
+        test_bit (FAILURE_FLAG, &(obj_entry)->flags)
+
+#define OBJ_STATE(object) \
+        (int)((object)->flags & OBJECT_STATE_MASK)
+#define OBJ_STATE_INDEX(state) \
+        (((state) & STATE_INDEX_MASK) >> STATE_INDEX_SHIFT)
+#define STATE_NAME(state) \
+        state_name[OBJ_STATE_INDEX(state)]
+#define INC_STATE_COUNTER(state) \
+        atomic_inc (&nr_OBJ_STATE[OBJ_STATE_INDEX(state)])
+#define DEC_STATE_COUNTER(state) \
+        atomic_inc (&nr_OBJ_STATE[OBJ_STATE_INDEX(state)])
+
+
+/** kddm object identifier */
+typedef unsigned long objid_t;
+
+
+/** Master object type.
+ *  Type used to store the copy set.
+ */
+typedef struct {
+	krgnodemask_t copyset;   /**< Machines owning an object to invalidate */
+	krgnodemask_t rmset;     /**< Machines owning an object to remove */
+} masterObj_t;
+
+
+
+/** Kddm object type.
+ *  Used to store local informations on objects.
+ */
+typedef struct kddm_obj {
+	/* flags field must be kept first in the structure */
+	long flags;                    /* Flags, state, prob_owner, etc... */
+	atomic_t count;                /* Reference counter */
+	masterObj_t master_obj;        /* Object informations handled by the
+					  manager */
+	void *object;                  /* Kernel physical object struct */
+	atomic_t frozen_count;         /* Number of task freezing the object */
+	atomic_t sleeper_count;        /* Nunmber of task waiting on the
+					  object */
+	wait_queue_head_t waiting_tsk; /* Process waiting for the object */
+} __attribute__((aligned(8))) kddm_obj_t;
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                               KDDM SET TYPES                             *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+/** KDDM set flags */
+#define _KDDM_LOCAL_EXCLUSIVE  0
+#define _KDDM_FT_LINKED        1
+#define _KDDM_FROZEN           2
+
+#define KDDM_LOCAL_EXCLUSIVE  (1<<_KDDM_LOCAL_EXCLUSIVE)
+#define KDDM_FT_LINKED        (1<<_KDDM_FT_LINKED)
+#define KDDM_FROZEN           (1<<_KDDM_FROZEN)
+
+#define kddm_local_exclusive(kddm) test_bit(_KDDM_LOCAL_EXCLUSIVE, &kddm->flags)
+#define set_kddm_local_exclusive(kddm) set_bit(_KDDM_LOCAL_EXCLUSIVE, &kddm->flags);
+#define clear_kddm_local_exclusive(kddm) clear_bit(_KDDM_LOCAL_EXCLUSIVE, &kddm->flags);
+
+#define kddm_ft_linked(kddm) test_bit(_KDDM_FT_LINKED, &kddm->flags)
+#define set_kddm_ft_linked(kddm) set_bit(_KDDM_FT_LINKED, &kddm->flags);
+#define clear_kddm_ft_linked(kddm) clear_bit(_KDDM_FT_LINKED, &kddm->flags);
+
+#define kddm_frozen(kddm) test_bit(_KDDM_FROZEN, &kddm->flags)
+#define set_kddm_frozen(kddm) set_bit(_KDDM_FROZEN, &kddm->flags);
+#define clear_kddm_frozen(kddm) clear_bit(_KDDM_FROZEN, &kddm->flags);
+
+#define KDDM_BREAK_COW_COPY 1
+#define KDDM_BREAK_COW_INV 2
+
+#define NR_OBJ_ENTRY_LOCKS 16
+
+
+struct kddm_set;
+struct rpc_desc;
+
+typedef struct kddm_set_ops {
+	void *(*obj_set_alloc) (struct kddm_set *set, void *data);
+	void (*obj_set_free) (void *tree,
+			      int (*f)(unsigned long, void *data,void *priv),
+			      void *priv);
+	struct kddm_obj *(*lookup_obj_entry)(struct kddm_set *set,
+					     objid_t objid);
+	struct kddm_obj *(*get_obj_entry)(struct kddm_set *set,
+					  objid_t objid, struct kddm_obj *obj);
+	void (*insert_object)(struct kddm_set * set, objid_t objid,
+			      struct kddm_obj *obj_entry);
+	struct kddm_obj *(*break_cow)(struct kddm_set * set,
+				      struct kddm_obj *obj_entry,objid_t objid,
+				      int break_type);
+	void (*remove_obj_entry) (struct kddm_set *set, objid_t objid);
+	void (*for_each_obj_entry)(struct kddm_set *set,
+				   int(*f)(unsigned long, void *, void*),
+				   void *data);
+	void (*export) (struct rpc_desc* desc, struct kddm_set *set);
+	void *(*import) (struct rpc_desc* desc, int *free_data);
+} kddm_set_ops_t;
+
+
+
+typedef unique_id_t kddm_set_id_t;   /**< Kddm set identifier */
+
+typedef int iolinker_id_t;           /**< IO Linker identifier */
+
+/** KDDM set structure */
+
+typedef struct kddm_set {
+	void *obj_set;               /**< Structure hosting the set objects */
+	spinlock_t table_lock;       /**< Object table lock */
+	struct kddm_ns *ns;          /**< kddm set name space */
+	struct kddm_set_ops *ops;    /**< kddm set operations */
+	kddm_set_id_t id;            /**< kddm set identifier */
+	spinlock_t lock;             /**< Structure lock */
+	unsigned int obj_size;       /**< size of objects in the set */
+	atomic_t nr_objects;         /**< Number of objects locally present */
+	unsigned long flags;         /**< Kddm set flags */
+	int state;                   /**< State of the set (locked, ...) */
+	wait_queue_head_t create_wq; /**< Process waiting for set creation */
+	wait_queue_head_t frozen_wq; /**< Process waiting on a frozen KDDM */
+	atomic_t count;
+	unsigned int last_ra_start;  /**< Start of the last readahead window */
+	int ra_window_size;          /**< Size of the readahead window */
+	kerrighed_node_t def_owner;  /**< Id of default owner node */
+	struct iolinker_struct *iolinker;    /**< IO linker ops */
+	struct proc_dir_entry *procfs_entry; /**< entry in /proc/kerrighed/kddm */
+
+	void *private_data;                  /**< Data used to instantiate */
+	int private_data_size;               /**< Size of private data... */
+
+	spinlock_t obj_lock[NR_OBJ_ENTRY_LOCKS];    /**< Objects lock */
+	struct list_head event_list;
+	spinlock_t event_lock;
+	atomic_t nr_masters;
+	atomic_t nr_copies;
+	atomic_t nr_entries;
+	event_counter_t get_object_counter;
+	event_counter_t grab_object_counter;
+	event_counter_t remove_object_counter;
+	event_counter_t flush_object_counter;
+	void *private;
+} kddm_set_t;
+
+
+
+struct kddm_info_struct {
+	event_counter_t get_object_counter;
+	event_counter_t grab_object_counter;
+	event_counter_t remove_object_counter;
+	event_counter_t flush_object_counter;
+
+	wait_queue_t object_wait_queue_entry;
+	struct kddm_obj *wait_obj;
+	int ns_id;
+	kddm_set_id_t set_id;
+	objid_t obj_id;
+};
+
+#endif // __KDDM_SET_TYPES__
diff -ruN linux-2.6.29/include/kddm/name_space.h android_cluster/linux-2.6.29/include/kddm/name_space.h
--- linux-2.6.29/include/kddm/name_space.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/name_space.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,78 @@
+/** KDDM name space interface.
+ *  @file name_space.h
+ *
+ *  Definition of KDDM name space interface.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KDDM_NS__
+#define __KDDM_NS__
+
+#include <linux/unique_id.h>
+#include <linux/hashtable.h>
+#include <kddm/kddm_types.h>
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+struct kddm_ns;
+
+typedef struct kddm_ns_ops {
+	struct kddm_set *(*kddm_set_lookup)(struct kddm_ns *ns,
+					    kddm_set_id_t set_id);
+} kddm_ns_ops_t;
+
+typedef struct kddm_ns {
+	atomic_t count;
+	struct semaphore table_sem;
+	hashtable_t *kddm_set_table;
+	unique_id_root_t kddm_set_unique_id_root;
+	struct kddm_ns_ops *ops;
+	void *private;
+	int id;
+} kddm_ns_t;
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN VARIABLES                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+#define KDDM_DEF_NS_ID 0
+
+extern struct kddm_ns *kddm_def_ns;
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+void kddm_ns_init(void);
+void kddm_ns_finalize(void);
+
+
+struct kddm_ns * create_kddm_ns(int ns_id, void *private,
+				struct kddm_ns_ops *ops);
+int remove_kddm_ns(int ns_id);
+
+struct kddm_ns *kddm_ns_get(int ns_id);
+void kddm_ns_put(struct kddm_ns *ns);
+
+
+#endif // __KDDM_NS__
diff -ruN linux-2.6.29/include/kddm/object.h android_cluster/linux-2.6.29/include/kddm/object.h
--- linux-2.6.29/include/kddm/object.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/object.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,432 @@
+/** Definition and management of kddm objects.
+ *  @file object.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KDDM_OBJECT__
+#define __KDDM_OBJECT__
+
+#include <linux/highmem.h>
+
+#include <kddm/kddm_types.h>
+#include <kddm/kddm_set.h>
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                   MACROS                                 *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+/** Object states used for the coherence protocol */
+
+typedef enum {
+	INV_COPY = 0,
+	READ_COPY =         1 << STATE_INDEX_SHIFT | KDDM_READ_OBJ,
+
+	INV_OWNER =         2 << STATE_INDEX_SHIFT | KDDM_OWNER_OBJ,
+	READ_OWNER =        3 << STATE_INDEX_SHIFT | KDDM_OWNER_OBJ | KDDM_READ_OBJ,
+	WRITE_OWNER =       4 << STATE_INDEX_SHIFT | KDDM_OWNER_OBJ | KDDM_READ_OBJ | KDDM_WRITE_OBJ,
+	WRITE_GHOST =       5 << STATE_INDEX_SHIFT | KDDM_OWNER_OBJ | KDDM_READ_OBJ | KDDM_WRITE_OBJ,
+
+	WAIT_ACK_INV =      6 << STATE_INDEX_SHIFT | KDDM_OWNER_OBJ | KDDM_READ_OBJ,
+	WAIT_ACK_WRITE =    7 << STATE_INDEX_SHIFT | KDDM_OWNER_OBJ | KDDM_READ_OBJ,
+	WAIT_CHG_OWN_ACK =  8 << STATE_INDEX_SHIFT | KDDM_READ_OBJ,
+
+	WAIT_OBJ_READ =    10 << STATE_INDEX_SHIFT,
+	WAIT_OBJ_WRITE =   11 << STATE_INDEX_SHIFT,
+
+	WAIT_OBJ_RM_DONE = 13 << STATE_INDEX_SHIFT,
+	WAIT_OBJ_RM_ACK =  14 << STATE_INDEX_SHIFT,
+	WAIT_OBJ_RM_ACK2 = 15 << STATE_INDEX_SHIFT,
+
+	INV_FILLING =      16 << STATE_INDEX_SHIFT,
+
+	NB_OBJ_STATE =     17 /* MUST always be the last one */
+} kddm_obj_state_t;
+
+/************************** Copyset management **************************/
+
+/** Get the copyset */
+#define COPYSET(obj_entry) (&(obj_entry)->master_obj.copyset)
+#define RMSET(obj_entry) (&(obj_entry)->master_obj.rmset)
+
+/** Clear the copyset */
+#define CLEAR_SET(set) __krgnodes_clear(set)
+
+/** Duplicate the copyset */
+#define DUP2_SET(set, v) __krgnodes_copy(v, set)
+
+/** Tests the presence of a node in the copyset */
+#define NODE_IN_SET(set,nodeid) __krgnode_isset(nodeid, set)
+
+/** Tests if local node is the object owner */
+
+#define I_AM_OWNER(obj_entry) ((obj_entry)->flags & KDDM_OWNER_OBJ)
+
+/** Tests if the copyset is empty */
+#define SET_IS_EMPTY(set) __krgnodes_empty(set)
+
+/** Tests if the local node own the exclusive copy of the object */
+#define OBJ_EXCLUSIVE(obj_entry) (krgnode_is_unique(kerrighed_node_id, (obj_entry)->master_obj.copyset) || \
+				  krgnode_is_unique(get_prob_owner(obj_entry), (obj_entry)->master_obj.copyset))
+
+#define OBJ_EXCLUSIVE2(set) (__krgnode_is_unique(kerrighed_node_id, set))
+
+/** Add a node in the copyset */
+#define ADD_TO_SET(set,nodeid) __krgnode_set(nodeid, set)
+
+/** Remove a node from the copyset */
+#define REMOVE_FROM_SET(set,nodeid) __krgnode_clear(nodeid, set)
+
+#define I_AM_DEFAULT_OWNER(set, objid) \
+        (kerrighed_node_id == kddm_io_default_owner(set, objid))
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+extern atomic_t nr_master_objects;  /*< Number of local master objects */
+extern atomic_t nr_copy_objects;    /*< Number of local copy objects */
+extern atomic_t nr_OBJ_STATE[]; /*< Number of objects in each possible state */
+extern const char *state_name[]; /*< Printable state name */
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+#define ASSERT_OBJ_PATH_LOCKED(set, objid) assert_spin_locked(&(set)->obj_lock[(objid) % NR_OBJ_ENTRY_LOCKS])
+
+/** Lock the object (take care about the interrupt context) **/
+static inline void kddm_obj_path_lock (struct kddm_set *set,
+				       objid_t objid)
+{
+	spinlock_t *lock = &set->obj_lock[objid % NR_OBJ_ENTRY_LOCKS];
+
+	if (irqs_disabled ())
+		spin_lock (lock);
+	else
+		spin_lock_bh (lock);
+}
+
+static inline void kddm_obj_path_unlock (struct kddm_set *set,
+					 objid_t objid)
+{
+	spinlock_t *lock = &set->obj_lock[objid % NR_OBJ_ENTRY_LOCKS];
+
+	if (irqs_disabled ())
+		spin_unlock (lock);
+	else
+		spin_unlock_bh (lock);
+}
+
+
+
+/** Alloc a new KDDM obj entry structure.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set     Kddm set to create an object for.
+ *  @param objid   Id of the object to create.
+ */
+struct kddm_obj *alloc_kddm_obj_entry(struct kddm_set *set,
+				      objid_t objid);
+
+/** Duplicate a KDDM obj entry structure.
+ *  @author Renaud Lottiaux
+ *
+ *  @param src_obj   The object entry to duplicate
+ */
+struct kddm_obj *dup_kddm_obj_entry(struct kddm_obj *src_obj);
+
+/** Free KDDM obj entry structure.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set        The set the object belongs to.
+ *  @param obj_entry  The structure to free
+ *  @param objid      Id of the object to free.
+ */
+void free_kddm_obj_entry(struct kddm_set *set,
+			 struct kddm_obj *obj_entry,
+			 objid_t objid);
+
+static inline void put_obj_entry_count(struct kddm_set *set,
+					 struct kddm_obj *obj_entry,
+					 objid_t objid)
+{
+	if (atomic_dec_and_test(&obj_entry->count))
+		free_kddm_obj_entry(set, obj_entry, objid);
+}
+
+static inline int obj_entry_count(struct kddm_obj *obj_entry)
+{
+        return atomic_read(&obj_entry->count);
+}
+
+/** Lookup for an object entry in a kddm set.
+ *  @author Renaud Lottiaux
+ *
+ *  @param kddm_set    Kddm set to lookup the object in.
+ *  @param objid       Id of the object to lookup for.
+ *
+ *  @return        Object entry of the object or NULL if the object entry does
+ *                 not exist.
+ */
+struct kddm_obj *__get_kddm_obj_entry (struct kddm_set *kddm_set,
+				       objid_t objid);
+
+static inline struct kddm_obj *_get_kddm_obj_entry (struct kddm_ns *ns,
+						    kddm_set_id_t set_id,
+						    objid_t objid,
+						    struct kddm_set **kddm_set)
+{
+	struct kddm_obj *obj = NULL;
+
+	*kddm_set = _find_get_kddm_set (ns, set_id);
+	if (*kddm_set) {
+		obj = __get_kddm_obj_entry (*kddm_set, objid);
+		put_kddm_set(*kddm_set);
+	}
+	return obj;
+}
+
+static inline struct kddm_obj *get_kddm_obj_entry (int ns_id,
+						   kddm_set_id_t set_id,
+						   objid_t objid,
+						   struct kddm_set **kddm_set)
+{
+	struct kddm_obj *obj = NULL;
+
+	*kddm_set = find_get_kddm_set (ns_id, set_id);
+	if (*kddm_set) {
+		obj = __get_kddm_obj_entry (*kddm_set, objid);
+		put_kddm_set(*kddm_set);
+	}
+	return obj;
+}
+
+static inline void put_kddm_obj_entry (struct kddm_set *set,
+				       struct kddm_obj *obj_entry,
+				       objid_t objid)
+{
+	if (obj_entry)
+		CLEAR_OBJECT_LOCKED(obj_entry);
+
+	kddm_obj_path_unlock (set, objid);
+}
+
+struct kddm_obj *default_get_kddm_obj_entry (struct kddm_set *set,
+					     objid_t objid);
+
+
+
+/** Lookup for an object entry in a kddm set and create it if necessary
+ *  @author Renaud Lottiaux
+ *
+ *  @param kddm_set    Kddm set to lookup the object in.
+ *  @param objid       Id of the object to lookup for.
+ *
+ *  @return        Object entry of the object. If the object does not exist,
+ *                 it is allocated
+ */
+struct kddm_obj *__get_alloc_kddm_obj_entry (struct kddm_set *kddm_set,
+					     objid_t objid);
+
+static inline struct kddm_obj *get_alloc_kddm_obj_entry (int ns_id,
+							 kddm_set_id_t set_id,
+							 objid_t objid,
+							 struct kddm_set **kddm_set)
+{
+	struct kddm_obj *obj = NULL;
+
+	*kddm_set = find_get_kddm_set (ns_id, set_id);
+	if (*kddm_set) {
+		obj = __get_alloc_kddm_obj_entry (*kddm_set, objid);
+		put_kddm_set(*kddm_set);
+	}
+	return obj;
+}
+
+static inline struct kddm_obj *_get_alloc_kddm_obj_entry (struct kddm_ns *ns,
+							  kddm_set_id_t set_id,
+							  objid_t objid,
+							  struct kddm_set **kddm_set)
+{
+	struct kddm_obj *obj = NULL;
+
+	*kddm_set = _find_get_kddm_set (ns, set_id);
+	if (*kddm_set) {
+		obj = __get_alloc_kddm_obj_entry (*kddm_set, objid);
+		put_kddm_set(*kddm_set);
+	}
+	return obj;
+}
+
+
+
+int destroy_kddm_obj_entry (struct kddm_set *kddm_set,
+			    struct kddm_obj *obj_entry,
+			    objid_t objid,
+			    int cluster_wide_remove);
+
+void __for_each_kddm_object(struct kddm_set *kddm_set,
+			    int(*f)(unsigned long, void *, void*),
+			    void *data);
+
+void for_each_kddm_object(int ns_id, kddm_set_id_t set_id,
+			  int(*f)(unsigned long, void*, void*),
+			  void *data);
+
+/** Insert a new object frame in a kddm set.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm set to insert object in.
+ *  @param objid        Id of the object to insert.
+ *  @param state        State of the object to insert.
+ */
+void kddm_insert_object (struct kddm_set *set, objid_t objid,
+                         struct kddm_obj * obj_entry,
+			 kddm_obj_state_t state);
+
+static inline struct kddm_obj *kddm_break_cow_object (struct kddm_set * set,
+					      struct kddm_obj *obj_entry,
+					      objid_t objid,
+					      int break_type)
+{
+	if (set->ops->break_cow)
+		return set->ops->break_cow (set, obj_entry, objid, break_type);
+	return obj_entry;
+}
+
+
+/** Change a kddm object state.
+ *  @author Renaud Lottiaux
+ *
+ *  @param kddm_set   Kddm set hosting the object.
+ *  @param obj_entry  Structure of the object.
+ *  @param objid      Id of the object to modify state.
+ *  @param new_state  New state of the object.
+ */
+void kddm_change_obj_state(struct kddm_set * kddm_set,
+			   struct kddm_obj *obj_entry,
+			   objid_t objid,
+			   kddm_obj_state_t newState);
+
+
+/** Invalidate a object frame from a kddm set.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry  Entry of the object to invalidate.
+ *  @param set        Kddm set hosting the object.
+ *  @param objid      Id of the object to invalidate.
+ */
+void kddm_invalidate_local_object_and_unlock (struct kddm_obj *obj_entry,
+					      struct kddm_set *set,
+					      objid_t objid,
+					      kddm_obj_state_t state);
+
+
+
+/** Indicate if an object is frozen, ie if it should not be modified.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry  Entry of the object to test.
+ */
+int object_frozen (struct kddm_obj * obj_entry, struct kddm_set *set);
+
+int object_frozen_or_pinned (struct kddm_obj * obj_entry,
+			     struct kddm_set * set);
+
+
+
+/** Freeze the given object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry  Entry of the object to freeze.
+ */
+void set_object_frozen (struct kddm_obj * obj_entry, struct kddm_set *set);
+
+
+
+/** Object clear Frozen.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry  Entry of the object to warm.
+ */
+void object_clear_frozen (struct kddm_obj * obj_entry, struct kddm_set *set);
+
+
+
+static inline int change_prob_owner(struct kddm_obj * obj_entry,
+				     kerrighed_node_t new_owner)
+{
+	if (obj_entry)
+		obj_entry->flags = (obj_entry->flags & ~PROB_OWNER_MASK) |
+			(new_owner << PROB_OWNER_SHIFT);
+	return 0;
+}
+
+
+
+static inline kerrighed_node_t get_prob_owner (struct kddm_obj *obj_entry)
+{
+	if (likely(obj_entry))
+		return (obj_entry->flags & PROB_OWNER_MASK) >>PROB_OWNER_SHIFT;
+	else
+		return KERRIGHED_NODE_ID_NONE;
+}
+
+
+
+/** Unlock, and make a process sleep until the corresponding
+ *  object is received.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  set        The kddm set the object belong to.
+ *  @param  obj_entry  The object to wait for.
+ *  @param  objid      Id of the object.
+ */
+void __sleep_on_kddm_obj (struct kddm_set *set,
+			  struct kddm_obj *obj_entry,
+			  objid_t objid,
+			  int flags);
+
+static inline void sleep_on_kddm_obj (struct kddm_set *set,
+				      struct kddm_obj *obj_entry,
+				      objid_t objid,
+				      int flags)
+{
+	__sleep_on_kddm_obj (set, obj_entry, objid, flags);
+}
+
+int check_sleep_on_local_exclusive (struct kddm_set *set,
+				    struct kddm_obj *obj_entry,
+				    objid_t objid,
+				    int flags);
+
+
+/** Wake up the process waiting for the object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  obj_entry  The object to wake up waiting process.
+ */
+static inline void wake_up_on_wait_object (struct kddm_obj *obj_entry,
+                                           struct kddm_set *set)
+{
+	if (atomic_read (&obj_entry->sleeper_count))
+		SET_OBJECT_PINNED (obj_entry);
+	wake_up (&obj_entry->waiting_tsk);
+}
+
+int init_kddm_objects (void);
+
+#endif // __KDDM_OBJECT__
diff -ruN linux-2.6.29/include/kddm/object_server.h android_cluster/linux-2.6.29/include/kddm/object_server.h
--- linux-2.6.29/include/kddm/object_server.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kddm/object_server.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,116 @@
+/** Object server.
+ *  @file object_server.h
+ *
+ *  Definition of the object server interface.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __OBJECT_SERVER__
+#define __OBJECT_SERVER__
+
+#include <kddm/io_linker.h>
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                 MACROS                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+#define NODE_MEM_FREE 1
+#define NODE_MEM_FULL 2
+
+
+#define KDDM_OBJ_COPY_ON_READ  0x00000001
+#define KDDM_OBJ_COPY_ON_WRITE 0x00000002
+#define KDDM_ASYNC_REQ         0x00000004
+#define KDDM_NO_FT_REQ         0x00000008
+#define KDDM_SEND_OWNERSHIP    0x00000010
+#define KDDM_DONT_KILL         0x00000020
+#define KDDM_NEED_OBJ_RM_ACK2  0x00000040
+#define KDDM_NO_FREEZE         0x00000080
+#define KDDM_IO_FLUSH          0x00000100
+#define KDDM_SYNC_OBJECT       0x00000200
+#define KDDM_NO_DATA           0x00000400
+#define KDDM_TRY_GRAB          0x00000800
+#define KDDM_REMOVE_ON_ACK     0x00001000
+#define KDDM_COW_OBJECT        0x00002000
+
+#define KDDM_SET_REQ_TYPE (KDDM_OBJ_COPY_ON_READ | KDDM_OBJ_COPY_ON_WRITE)
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+#define MSG_HEADER \
+	kddm_set_id_t set_id;        /**< Set identifier. */ \
+	int ns_id;                   /**< Name space identifier */ \
+	objid_t objid;               /**< Object id */ \
+	long req_id;                 /**< USED FOR DEBUGGING */
+
+
+/** Structure used to store data sent to the object server. */
+/** WARNING: in this structure, field order matter */
+typedef struct {
+	MSG_HEADER
+	int request_type;
+	int flags;                   /**< No First Touch request ? */
+	kerrighed_node_t reply_node; /**< Identifier of the requesting node */
+	kerrighed_node_t new_owner;  /**< Identifier of the new object owner */
+} msg_server_t;
+
+/** WARNING: in this structure, field order matter */
+typedef struct {
+	MSG_HEADER
+	krgnodemask_t rmset;              /**< The remove set to wait for */
+} rm_done_msg_server_t;
+
+/** Structure used to store data sent for object ownership change. */
+/** WARNING: in this structure, field order matter */
+typedef struct {
+	MSG_HEADER
+	kerrighed_node_t reply_node; /**< Identifier of the requesting node */
+	masterObj_t owner_info;      /**< Object owner information */
+} msg_injection_t;
+
+
+/** Structure used to store data sent to the object server. */
+/** WARNING: in this structure, field order matter */
+typedef struct {
+	MSG_HEADER
+	kddm_obj_state_t object_state; /**< State of the received object */
+	int flags;                     /**< Falgs : synchro, ... */
+} msg_object_receiver_t;
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+/** Object Server Initialisation.
+ *  @author Renaud Lottiaux
+ *
+ *  Launch the Object Server handler thread.
+ */
+void object_server_init (void);
+
+/** Object Server Finalization.
+ *  @author Renaud Lottiaux
+ *
+ *  Kill the Object Server handler thread.
+ */
+void object_server_finalize (void);
+
+
+#endif
diff -ruN linux-2.6.29/include/kerrighed/action.h android_cluster/linux-2.6.29/include/kerrighed/action.h
--- linux-2.6.29/include/kerrighed/action.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/action.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,132 @@
+/*
+ * Management of incompatibilities between Kerrighed actions and
+ * some Linux facilities
+ */
+
+#ifndef __KRG_ACTION_H__
+#define __KRG_ACTION_H__
+
+#ifdef CONFIG_KRG_EPM
+
+#include <linux/sched.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/sys/checkpoint.h>
+
+typedef enum {
+	EPM_NO_ACTION,
+	EPM_MIGRATE,
+	EPM_REMOTE_CLONE,
+	EPM_CHECKPOINT,
+	EPM_ACTION_MAX	   /* Always in last position */
+} krg_epm_action_t;
+
+typedef enum {
+	CR_SAVE_NOW,
+	CR_SAVE_LATER
+} c_shared_obj_option_t;
+
+typedef enum {
+	CR_LOAD_NOW,
+	CR_LINK_ONLY
+} r_shared_obj_option_t;
+
+#define APP_REPLACE_PGRP	1
+#define APP_REPLACE_SID		2
+
+struct task_struct;
+struct completion;
+
+struct epm_action {
+	krg_epm_action_t type;
+	union {
+		struct {
+			pid_t pid;
+			kerrighed_node_t target;
+		} migrate;
+		struct {
+			pid_t from_pid;
+			pid_t from_tgid;
+			kerrighed_node_t target;
+			unsigned long clone_flags;
+			unsigned long stack_start;
+			unsigned long stack_size;
+			int *parent_tidptr;
+			int *child_tidptr;
+			struct completion *vfork;
+		} remote_clone;
+		struct {
+			c_shared_obj_option_t shared;
+		} checkpoint;
+		struct {
+			r_shared_obj_option_t shared;
+			struct app_struct * app;
+			int flags;
+		} restart;
+	};
+};
+
+static inline kerrighed_node_t epm_target_node(struct epm_action *action)
+{
+	switch (action->type) {
+	case EPM_MIGRATE:
+		return action->migrate.target;
+	case EPM_REMOTE_CLONE:
+		return action->remote_clone.target;
+	case EPM_CHECKPOINT:
+		return KERRIGHED_NODE_ID_NONE;
+	default:
+		BUG();
+	}
+}
+
+/*
+ * Nests inside and outside of read_lock(&taskslist_lock), but neither inside
+ * nor outside write_lock(_irq)(&tasklist_lock).
+ * Nests outside sighand->lock.
+ */
+extern rwlock_t krg_action_lock;
+
+static inline void krg_action_block_all(void)
+{
+	read_lock(&krg_action_lock);
+}
+
+static inline void krg_action_unblock_all(void)
+{
+	read_unlock(&krg_action_lock);
+}
+
+static inline int krg_action_any_pending(struct task_struct *task)
+{
+	return task->krg_action_flags;
+}
+
+static inline int krg_action_block_any(struct task_struct *task)
+{
+	int pending;
+
+	krg_action_block_all();
+	pending = krg_action_any_pending(task);
+	if (pending)
+		krg_action_unblock_all();
+	return !pending;
+}
+
+static inline void krg_action_unblock_any(struct task_struct *task)
+{
+	krg_action_unblock_all();
+}
+
+int krg_action_disable(struct task_struct *task, krg_epm_action_t action,
+		       int inheritable);
+int krg_action_enable(struct task_struct *task, krg_epm_action_t action,
+		      int inheritable);
+
+int krg_action_start(struct task_struct *task, krg_epm_action_t action);
+int krg_action_stop(struct task_struct *task, krg_epm_action_t action);
+
+int krg_action_pending(struct task_struct *task, krg_epm_action_t action);
+
+#endif /* CONFIG_KRG_EPM */
+
+#endif /* __KRG_ACTION_H__ */
diff -ruN linux-2.6.29/include/kerrighed/application.h android_cluster/linux-2.6.29/include/kerrighed/application.h
--- linux-2.6.29/include/kerrighed/application.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/application.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,202 @@
+/** Application
+ *  @author Matthieu Fertré
+ */
+
+#ifndef __APPLICATION_H__
+#define __APPLICATION_H__
+
+#ifdef CONFIG_KRG_EPM
+
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/spinlock.h>
+#include <linux/types.h>
+#include <linux/linkage.h>
+#include <linux/dcache.h>
+#include <linux/namei.h>
+#include <linux/completion.h>
+#include <linux/rbtree.h>
+
+#include <kerrighed/sys/types.h>
+#include <kerrighed/types.h>
+
+#include <kerrighed/sys/checkpoint.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/action.h>
+
+struct rpc_desc;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              STRUCTURES                                  *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+enum {
+	PCUS_OPERATION_OK,
+	PCUS_STOP_STEP1,
+	PCUS_STOP_STEP2,
+	PCUS_CHKPT_IN_PROGRESS,
+	PCUS_RUNNING
+};
+
+typedef struct task_identity {
+	pid_t pid;
+	pid_t tgid;
+} task_identity_t;
+
+typedef struct task_and_state {
+
+	struct task_struct *task;
+	union {
+		struct {
+			ghost_t *ghost;
+			int result;
+			struct completion completion;
+		} checkpoint;
+		struct {
+			pid_t pid;
+			pid_t tgid;
+			pid_t parent;
+			pid_t real_parent;
+			pid_t real_parent_tgid;
+
+			pid_t pgrp;
+			pid_t session;
+		} restart;
+	};
+	struct list_head next_task;
+} task_state_t;
+
+typedef enum {
+	APP_FROZEN,
+	APP_RESTARTED,
+	APP_RUNNING,
+	APP_RUNNING_CS /* Application is running but is in a critical section:
+			* Checkpoint is forbidden. */
+} app_state_t;
+
+struct app_kddm_object {
+	long app_id;
+	int chkpt_sn;
+
+	app_state_t state;
+	krgnodemask_t nodes;
+
+	__u64 user_data;
+};
+
+struct app_struct {
+	long app_id;
+	int chkpt_sn;
+
+	struct mutex mutex;
+	struct completion tasks_chkpted;
+
+	/* local processes of the application */
+	struct list_head tasks;
+
+	/* list of structs shared by those processes */
+	/* MUST be empty when no checkpoint is in progress */
+	struct {
+		struct rb_root root;
+		spinlock_t lock;
+	} shared_objects;
+
+	union {
+		struct {
+			int flags;
+
+			struct cr_mm_region *first_mm_region;
+		} checkpoint;
+
+		struct {
+			krgnodemask_t replacing_nodes;
+			pid_t substitution_pgrp;
+			pid_t substitution_sid;
+		} restart;
+	};
+
+	const struct cred *cred;
+};
+
+/*--------------------------------------------------------------------------*/
+
+int create_application(struct task_struct *task);
+
+struct app_struct *new_local_app(long app_id);
+
+void delete_app(struct app_struct *app);
+
+int __delete_local_app(struct app_struct *app);
+
+struct app_struct *find_local_app(long app_id);
+
+/*--------------------------------------------------------------------------*/
+
+task_state_t *alloc_task_state_from_pids(pid_t pid,
+					  pid_t tgid,
+					  pid_t parent,
+					  pid_t real_parent,
+					  pid_t real_parent_tgid,
+					  pid_t pgrp,
+					  pid_t session);
+
+void free_task_state(task_state_t *t);
+
+int register_task_to_app(struct app_struct *app, struct task_struct *task);
+
+void unregister_task_to_app(struct app_struct *app, struct task_struct *task);
+
+/* need to hold lock app->mutex before calling it */
+static inline int local_tasks_list_empty(struct app_struct *app) {
+	return list_empty(&app->tasks);
+}
+
+/* task->application->mutex must be taken */
+task_state_t *__set_task_result(struct task_struct *task, int result);
+task_state_t *set_result_wait(int result);
+int get_local_tasks_chkpt_result(struct app_struct* app);
+
+/*--------------------------------------------------------------------------*/
+
+int krg_copy_application(struct task_struct *task);
+void krg_exit_application(struct task_struct *task);
+
+/*--------------------------------------------------------------------------*/
+
+int export_application(struct epm_action *action,
+		       ghost_t *ghost, struct task_struct *task);
+int import_application(struct epm_action *action,
+		       ghost_t *ghost, struct task_struct *task);
+void unimport_application(struct epm_action *action,
+			  ghost_t *ghost, struct task_struct *task);
+
+/*--------------------------------------------------------------------------*/
+
+int global_stop(struct app_kddm_object *obj);
+
+int global_unfreeze(struct app_kddm_object *obj, int signal);
+
+/*--------------------------------------------------------------------------*/
+
+ghost_t *get_task_chkpt_ghost(struct app_struct *app, struct task_struct *task);
+
+/*--------------------------------------------------------------------------*/
+
+int app_set_userdata(__u64 user_data);
+
+int app_get_userdata(long _appid, int flags, __u64 *user_data);
+
+int app_cr_disable(void);
+
+int app_cr_enable(void);
+
+/*--------------------------------------------------------------------------*/
+
+void application_cr_server_init(void);
+void application_cr_server_finalize(void);
+
+#endif /* CONFIG_KRG_EPM */
+
+#endif /* __APPLICATION_H__ */
diff -ruN linux-2.6.29/include/kerrighed/app_shared.h android_cluster/linux-2.6.29/include/kerrighed/app_shared.h
--- linux-2.6.29/include/kerrighed/app_shared.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/app_shared.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,113 @@
+/** Application management of object(s) shared by several processes
+ *  @author Matthieu Fertré
+ */
+
+#ifndef __APPLICATION_SHARED_H__
+#define __APPLICATION_SHARED_H__
+
+#include <linux/rbtree.h>
+#include <kerrighed/ghost_types.h>
+#include <kerrighed/action.h>
+
+/*--------------------------------------------------------------------------*/
+
+struct app_struct;
+struct app_kddm_object;
+struct rpc_desc;
+
+void clear_shared_objects(struct app_struct *app);
+
+void destroy_shared_objects(struct app_struct *app,
+			    struct task_struct *fake);
+
+int global_chkpt_shared(struct rpc_desc *desc,
+			struct app_kddm_object *obj);
+
+int local_chkpt_shared(struct rpc_desc *desc,
+		       struct app_struct *app,
+		       int chkpt_sn);
+
+int global_restart_shared(struct rpc_desc *desc,
+			  struct app_kddm_object *obj,
+			  struct restart_request *req);
+
+int local_restart_shared(struct rpc_desc *desc,
+			 struct app_struct *app,
+			 struct task_struct *fake,
+			 int chkpt_sn);
+
+int local_restart_shared_complete(struct app_struct *app,
+				  struct task_struct *fake);
+
+/*--------------------------------------------------------------------------*/
+
+enum shared_obj_type {
+	/* things to restore before files */
+	PIPE_INODE,
+
+	/* file descriptors */
+	LOCAL_FILE,
+	DVFS_FILE,
+
+	/* other objects */
+	FILES_STRUCT,
+	FS_STRUCT,
+	MM_STRUCT,
+	SEMUNDO_LIST,
+	SIGHAND_STRUCT,
+	SIGNAL_STRUCT,
+	NO_OBJ
+};
+
+struct file_export_args {
+	int index;
+	struct file *file;
+};
+
+union export_args {
+	struct file_export_args file_args;
+};
+
+struct export_obj_info {
+	struct task_struct *task;
+	struct list_head next;
+	union export_args args;
+};
+
+enum object_locality {
+	LOCAL_ONLY,
+	SHARED_ANY,
+	SHARED_MASTER,
+	SHARED_SLAVE
+};
+
+int add_to_shared_objects_list(struct app_struct *app,
+			       enum shared_obj_type type,
+			       unsigned long key,
+			       enum object_locality locality,
+			       struct task_struct* exporting_task,
+			       union export_args *args,
+			       int force);
+
+void *get_imported_shared_object(struct app_struct *app,
+				 enum shared_obj_type type,
+				 unsigned long key);
+
+struct shared_object_operations {
+	size_t restart_data_size;
+	int (*export_now) (struct epm_action *, ghost_t *, struct task_struct *,
+			   union export_args *);
+
+	/* export_user_info is used to export information to a readable file to
+	 * userspace
+	 */
+	int (*export_user_info) (struct epm_action *, ghost_t *, unsigned long,
+				 struct export_obj_info *);
+
+	int (*import_now) (struct epm_action *, ghost_t *, struct task_struct *,
+			   int, void  **, size_t *);
+	int (*import_complete) (struct task_struct *, void *);
+	int (*delete) (struct task_struct *, void *);
+};
+
+#endif
diff -ruN linux-2.6.29/include/kerrighed/capabilities.h android_cluster/linux-2.6.29/include/kerrighed/capabilities.h
--- linux-2.6.29/include/kerrighed/capabilities.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/capabilities.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,77 @@
+/**
+ * Define Kerrighed Capabilities (not exported outside kernel)
+ * @author Jean Parpaillon (c) Inria 2006
+ */
+
+#ifndef _KERRIGHED_CAPABILITIES_H_INTERNAL
+#define _KERRIGHED_CAPABILITIES_H_INTERNAL
+
+#ifdef CONFIG_KRG_CAP
+
+#include <linux/capability.h>
+#include <kerrighed/sys/capabilities.h>
+
+typedef struct kernel_krg_cap_struct {
+	kernel_cap_t effective;
+	kernel_cap_t permitted;
+	kernel_cap_t inheritable_permitted;
+	kernel_cap_t inheritable_effective;
+} kernel_krg_cap_t;
+
+/*
+ * MACROS
+ */
+#define __KRG_CAP_SUPPORTED_BASE CAP_TO_MASK(CAP_CHANGE_KERRIGHED_CAP)
+#ifdef CONFIG_CLUSTER_WIDE_PROC_INFRA
+#define __KRG_CAP_SUPPORTED_PROCFS CAP_TO_MASK(CAP_SEE_LOCAL_PROC_STAT)
+#else
+#define __KRG_CAP_SUPPORTED_PROCFS 0
+#endif
+#ifdef CONFIG_KRG_MM
+#define __KRG_CAP_SUPPORTED_MM CAP_TO_MASK(CAP_USE_REMOTE_MEMORY)
+#else
+#define __KRG_CAP_SUPPORTED_MM 0
+#endif
+#ifdef CONFIG_KRG_EPM
+#define __KRG_CAP_SUPPORTED_EPM CAP_TO_MASK(CAP_CAN_MIGRATE)	 \
+				|CAP_TO_MASK(CAP_DISTANT_FORK)   \
+				|CAP_TO_MASK(CAP_CHECKPOINTABLE)
+#else
+#define __KRG_CAP_SUPPORTED_EPM 0
+#endif
+#define __KRG_CAP_SUPPORTED_DEBUG 0
+#ifdef CONFIG_KRG_SYSCALL_EXIT_HOOK
+#define __KRG_CAP_SUPPORTED_SEH CAP_TO_MASK(CAP_SYSCALL_EXIT_HOOK)
+#else
+#define __KRG_CAP_SUPPORTED_SEH 0
+#endif
+
+#if _KERNEL_CAPABILITY_U32S != 2
+#error Fix up hand-coded capability macro initializers
+#endif
+
+#define KRG_CAP_SUPPORTED ((kernel_cap_t){{ __KRG_CAP_SUPPORTED_BASE   \
+					   |__KRG_CAP_SUPPORTED_PROCFS \
+					   |__KRG_CAP_SUPPORTED_MM     \
+					   |__KRG_CAP_SUPPORTED_EPM    \
+					   |__KRG_CAP_SUPPORTED_DEBUG  \
+					   |__KRG_CAP_SUPPORTED_SEH, 0 }})
+
+#define KRG_CAP_INIT_PERM_SET KRG_CAP_SUPPORTED
+#define KRG_CAP_INIT_EFF_SET \
+	((kernel_cap_t){{ CAP_TO_MASK(CAP_CHANGE_KERRIGHED_CAP), 0 }})
+#define KRG_CAP_INIT_INH_PERM_SET KRG_CAP_INIT_PERM_SET
+#define KRG_CAP_INIT_INH_EFF_SET KRG_CAP_INIT_EFF_SET
+
+struct task_struct;
+struct linux_binprm;
+
+int can_use_krg_cap(struct task_struct *task, int cap);
+
+void krg_cap_fork(struct task_struct *task, unsigned long clone_flags);
+int krg_cap_prepare_binprm(struct linux_binprm *bprm);
+void krg_cap_finish_exec(struct linux_binprm *bprm);
+
+#endif /* CONFIG_KRG_CAP */
+
+#endif /* _KERRIGHED_CAPABILITIES_H_INTERNAL */
diff -ruN linux-2.6.29/include/kerrighed/children.h android_cluster/linux-2.6.29/include/kerrighed/children.h
--- linux-2.6.29/include/kerrighed/children.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/children.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,88 @@
+#ifndef __KRG_CHILDREN_H__
+#define __KRG_CHILDREN_H__
+
+#ifdef CONFIG_KRG_EPM
+
+#include <linux/types.h>
+
+struct children_kddm_object;
+struct task_struct;
+struct pid_namespace;
+struct pid;
+
+struct children_kddm_object *krg_children_alloc(struct task_struct *task);
+void krg_children_share(struct task_struct *task);
+void krg_children_exit(struct task_struct *task);
+void krg_children_get(struct children_kddm_object *obj);
+void krg_children_put(struct children_kddm_object *obj);
+int krg_new_child(struct children_kddm_object *obj,
+		  pid_t parent_pid,
+		  struct task_struct *child);
+void __krg_set_child_pgid(struct children_kddm_object *obj,
+			  pid_t pid, pid_t pgid);
+void krg_set_child_pgid(struct children_kddm_object *obj,
+			struct task_struct *child);
+int krg_set_child_ptraced(struct children_kddm_object *obj,
+			  struct task_struct *child, int ptraced);
+void krg_set_child_exit_signal(struct children_kddm_object *obj,
+			       struct task_struct *child);
+void krg_set_child_exit_state(struct children_kddm_object *obj,
+			      struct task_struct *child);
+void krg_set_child_location(struct children_kddm_object *obj,
+			    struct task_struct *child);
+void krg_remove_child(struct children_kddm_object *obj,
+		      struct task_struct *child);
+void krg_forget_original_remote_parent(struct task_struct *parent,
+				       struct task_struct *reaper);
+pid_t krg_get_real_parent_tgid(struct task_struct *task,
+			       struct pid_namespace *ns);
+pid_t krg_get_real_parent_pid(struct task_struct *task);
+int __krg_get_parent(struct children_kddm_object *obj, pid_t pid,
+		     pid_t *parent_pid, pid_t *real_parent_pid);
+int krg_get_parent(struct children_kddm_object *obj, struct task_struct *child,
+		     pid_t *parent_pid, pid_t *real_parent_pid);
+struct children_kddm_object *krg_children_writelock(pid_t tgid);
+struct children_kddm_object *__krg_children_writelock(struct task_struct *task);
+struct children_kddm_object *krg_children_writelock_nested(pid_t tgid);
+struct children_kddm_object *krg_children_readlock(pid_t tgid);
+struct children_kddm_object *__krg_children_readlock(struct task_struct *task);
+struct children_kddm_object *
+krg_parent_children_writelock(struct task_struct *task,
+			      pid_t *parent_tgid);
+struct children_kddm_object *
+krg_parent_children_readlock(struct task_struct *task,
+			     pid_t *parent_tgid);
+void krg_children_unlock(struct children_kddm_object *obj);
+void krg_update_self_exec_id(struct task_struct *task);
+u32 krg_get_real_parent_self_exec_id(struct task_struct *task,
+				     struct children_kddm_object *obj);
+
+/* fork() hooks */
+int krg_children_prepare_fork(struct task_struct *task,
+			      struct pid *pid,
+			      unsigned long clone_flags);
+int krg_children_fork(struct task_struct *task,
+		      struct pid *pid,
+		      unsigned long clone_flags);
+void krg_children_commit_fork(struct task_struct *task);
+void krg_children_abort_fork(struct task_struct *task);
+
+/* exit()/release_task() hooks */
+void krg_reparent_to_local_child_reaper(struct task_struct *task);
+void krg_children_cleanup(struct task_struct *task);
+
+/* de_thread() hooks */
+struct children_kddm_object *
+krg_children_prepare_de_thread(struct task_struct *task);
+void krg_children_finish_de_thread(struct children_kddm_object *obj,
+				   struct task_struct *task);
+
+/* Used by krg_prepare_exit_notify() and krg_delayed_notify_parent() */
+void
+krg_update_parents(struct task_struct *task, pid_t parent, pid_t real_parent);
+/* Used by krg_release_task() */
+void krg_unhash_process(struct task_struct *tsk);
+
+#endif /* CONFIG_KRG_EPM */
+
+#endif /* __KRG_CHILDREN_H__ */
diff -ruN linux-2.6.29/include/kerrighed/cpu_id.h android_cluster/linux-2.6.29/include/kerrighed/cpu_id.h
--- linux-2.6.29/include/kerrighed/cpu_id.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/cpu_id.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,36 @@
+#ifndef __KKRG_CPU_ID_H__
+#define __KKRG_CPU_ID_H__
+
+#include <linux/threads.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+
+static inline int __krg_cpu_id(kerrighed_node_t node, int cpu_id)
+{
+	return node * NR_CPUS + cpu_id;
+}
+
+static inline int krg_cpu_id(int local_cpu_id)
+{
+	return __krg_cpu_id(kerrighed_node_id, local_cpu_id);
+}
+
+static inline int krg_cpu_is_local(int krg_cpu_id)
+{
+	int min_local_cpu = kerrighed_node_id * NR_CPUS;
+
+	return krg_cpu_id >= min_local_cpu
+		&& krg_cpu_id < min_local_cpu + NR_CPUS;
+}
+
+static inline kerrighed_node_t krg_cpu_node(int krg_cpu_id)
+{
+	return krg_cpu_id / NR_CPUS;
+}
+
+static inline int local_cpu_id(int krg_cpu_id)
+{
+	return krg_cpu_id % NR_CPUS;
+}
+
+#endif /* __KKRG_CPU_ID_H__ */
diff -ruN linux-2.6.29/include/kerrighed/debug.h android_cluster/linux-2.6.29/include/kerrighed/debug.h
--- linux-2.6.29/include/kerrighed/debug.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/debug.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,62 @@
+#ifndef __KRG_DEBUG__
+#define __KRG_DEBUG__
+
+#include <linux/mm.h>
+#include <linux/mmzone.h>
+#include <linux/hardirq.h>
+
+//#define NO_PANIC
+//#define NO_WARNING
+
+#ifndef PANIC
+static inline void print_mem_info(void)
+{
+#if 0
+	int i;
+
+/*   printk ("nr_free_pages : %d", nr_free_pages()) ; */
+
+	for (i = 0; i < MAX_NR_ZONES; i++) {
+		struct zone *zone;
+
+		zone = zone_table[i];
+		if (!zone)
+			break;
+
+		printk("Zone %d (%s) - free=%ld - active=%ld - inactive=%ld\n",
+		       i, zone->name,
+		       zone->free_pages, zone->nr_active, zone->nr_inactive);
+	}
+#else
+		printk("print_mem_info: todo\n");
+#endif
+}
+#endif
+
+#ifdef NO_PANIC
+#define PANIC(format, args...) do {} while(0)
+#else
+#ifndef __arch_um__
+#define PANIC(format, args...) do {printk ("<0>-- PANIC -- (%s) : " , __PRETTY_FUNCTION__); printk (format, ## args) ; __krg_panic__=1; print_mem_info(); if(in_interrupt()){BUG();}else{while(1){schedule();}}} while (0)
+#else				/* __arch_um__ */
+#define PANIC(format, args...) do {printk ("<0>-- PANIC -- (%s) : ", __PRETTY_FUNCTION__); printk (format, ## args) ; __krg_panic__=1; BUG(); } while(0)
+#endif
+#endif
+
+#ifdef NO_WARNING
+#define WARNING(format, args...) do {} while(0)
+#else
+#define WARNING(format, args...) do {printk ("<0>-- WARNING -- (%s) : ", __PRETTY_FUNCTION__); printk (format, ## args) ;} while (0)
+#endif
+
+#ifdef NO_EMPTY
+#define EMPTY do {} while(0)
+#else
+#define EMPTY printk("%s: __function empty__\n",__PRETTY_FUNCTION__)
+#endif
+
+#define OOM { printk("OOM in %s: %d\nprocess stop\n", __PRETTY_FUNCTION__, __LINE__); if(in_interrupt()){BUG();}else{while(1) schedule();}; }
+
+extern int __krg_panic__;
+
+#endif				// __KRG_DEBUG__
diff -ruN linux-2.6.29/include/kerrighed/dvfs.h android_cluster/linux-2.6.29/include/kerrighed/dvfs.h
--- linux-2.6.29/include/kerrighed/dvfs.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/dvfs.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,21 @@
+#include <linux/fs.h>
+#include <kerrighed/fcntl.h>
+
+/** Kerrighed Kernel Hooks **/
+loff_t krg_file_pos_read(struct file *file);
+void krg_file_pos_write(struct file *file, loff_t pos);
+void krg_put_file(struct file *file);
+
+static inline loff_t file_pos_read(struct file *file)
+{
+	if (file->f_flags & O_KRG_SHARED)
+		file->f_pos = krg_file_pos_read(file);
+	return file->f_pos;
+}
+
+static inline void file_pos_write(struct file *file, loff_t pos)
+{
+	if (file->f_flags & O_KRG_SHARED)
+		krg_file_pos_write(file, pos);
+	file->f_pos = pos;
+}
diff -ruN linux-2.6.29/include/kerrighed/dynamic_node_info_linker.h android_cluster/linux-2.6.29/include/kerrighed/dynamic_node_info_linker.h
--- linux-2.6.29/include/kerrighed/dynamic_node_info_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/dynamic_node_info_linker.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,110 @@
+/** Dynamic node informations management.
+ *  @file dynamic_node_info_linker.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef DYNAMIC_NODE_INFO_LINKER_H
+#define DYNAMIC_NODE_INFO_LINKER_H
+
+#include <linux/hardirq.h>
+#include <linux/procfs_internal.h>
+#include <kerrighed/sys/types.h>
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+#include <asm/kerrighed/meminfo.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+/* Node related informations */
+
+typedef struct {
+	struct timespec idletime;
+	struct timespec uptime;
+	unsigned long avenrun[3];	/* Load averages */
+	int last_pid;
+	int nr_threads;
+	unsigned long nr_running;
+	unsigned long long nr_context_switches;
+	unsigned long jif;
+	unsigned long total_forks;
+	unsigned long nr_iowait;
+	u64 arch_irq;
+
+	/* Dynamic memory informations */
+
+	unsigned long totalram;
+	unsigned long freeram;
+	unsigned long bufferram;
+	unsigned long totalhigh;
+	unsigned long freehigh;
+	unsigned long totalswap;
+	unsigned long freeswap;
+
+	unsigned long nr_pages[NR_LRU_LISTS - LRU_BASE];
+	unsigned long nr_mlock;
+	unsigned long nr_file_pages;
+	unsigned long nr_file_dirty;
+	unsigned long nr_writeback;
+	unsigned long nr_anon_pages;
+	unsigned long nr_file_mapped;
+	unsigned long nr_page_table_pages;
+	unsigned long nr_slab_reclaimable;
+	unsigned long nr_slab_unreclaimable;
+	unsigned long nr_unstable_nfs;
+	unsigned long nr_bounce;
+	unsigned long nr_writeback_temp;
+
+	unsigned long quicklists;
+
+	struct vmalloc_info vmi;
+	unsigned long vmalloc_total;
+
+	unsigned long allowed;
+	unsigned long commited;
+
+	unsigned long swapcache_pages;
+
+	unsigned long nr_huge_pages;
+	unsigned long free_huge_pages;
+	unsigned long resv_huge_pages;
+	unsigned long surplus_huge_pages;
+
+	krg_arch_meminfo_t arch_meminfo;
+} krg_dynamic_node_info_t;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+extern struct kddm_set *dynamic_node_info_kddm_set;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+int dynamic_node_info_init(void);
+
+/** Helper function to get dynamic node informations.
+ *  @author Renaud Lottiaux
+ *
+ *  @param node_id   Id of the node we want informations on.
+ *
+ *  @return  Structure containing information on the requested node.
+ */
+static inline
+krg_dynamic_node_info_t *get_dynamic_node_info(kerrighed_node_t nodeid)
+{
+	return _fkddm_get_object(dynamic_node_info_kddm_set, nodeid,
+				 KDDM_NO_FREEZE|KDDM_NO_FT_REQ);
+}
+
+#endif /* DYNAMIC_NODE_INFO_LINKER_H */
diff -ruN linux-2.6.29/include/kerrighed/faf_file_mgr.h android_cluster/linux-2.6.29/include/kerrighed/faf_file_mgr.h
--- linux-2.6.29/include/kerrighed/faf_file_mgr.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/faf_file_mgr.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,42 @@
+/** Global management of faf files interface.
+ *  @file faf_file_mgr.h
+ *
+ *  @author Renaud Lottiaux
+ */
+#ifndef __FAF_FILE_MGR__
+#define __FAF_FILE_MGR__
+
+#include <kerrighed/action.h>
+#include <kerrighed/ghost.h>
+
+struct rpc_desc;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+extern struct dvfs_mobility_operations dvfs_mobility_faf_ops;
+extern struct kmem_cache *faf_client_data_cachep;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+struct file *create_faf_file_from_krg_desc(struct task_struct *task,
+					   void *_desc);
+
+int get_faf_file_krg_desc(struct file *file, void **desc, int *desc_size);
+
+/* file will be faffed if not already */
+int send_faf_file_desc(struct rpc_desc *desc, struct file *file);
+
+/* file must be already faffed */
+int __send_faf_file_desc(struct rpc_desc *desc, struct file *file);
+
+struct file *rcv_faf_file_desc(struct rpc_desc *desc);
+
+#endif // __FAF_FILE_MGR__
diff -ruN linux-2.6.29/include/kerrighed/faf.h android_cluster/linux-2.6.29/include/kerrighed/faf.h
--- linux-2.6.29/include/kerrighed/faf.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/faf.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,103 @@
+/** Kerrighed Kernel Hooks **/
+
+#ifndef __FAF_H__
+#define __FAF_H__
+
+#include <linux/types.h>
+#include <linux/namei.h>
+
+struct file;
+struct iovec;
+struct kstat;
+struct statfs;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+typedef struct faf_client_data {
+	kerrighed_node_t server_id;
+	int server_fd;
+	unsigned long f_flags;
+	fmode_t f_mode;
+	loff_t f_pos;
+	wait_queue_head_t poll_wq;
+	unsigned int poll_revents;
+	umode_t i_mode;
+	unsigned int is_named_pipe:1;
+} faf_client_data_t;
+
+off_t krg_faf_lseek(struct file *file, off_t offset,
+		    unsigned int origin);
+long krg_faf_llseek(struct file *file, unsigned long offset_high,
+		    unsigned long offset_low, loff_t *result,
+		    unsigned int origin);
+ssize_t krg_faf_read(struct file *file, char *buf, size_t count, loff_t *pos);
+ssize_t krg_faf_write(struct file *file, const char *buf,
+		      size_t count, loff_t *pos);
+ssize_t krg_faf_readv(struct file *file, const struct iovec __user *vec,
+		      unsigned long vlen, loff_t *pos);
+ssize_t krg_faf_writev(struct file *file, const struct iovec __user *vec,
+		       unsigned long vlen, loff_t *pos);
+long krg_faf_fcntl(struct file *file, unsigned int cmd,
+		   unsigned long arg);
+long krg_faf_fcntl64(struct file *file, unsigned int cmd,
+		     unsigned long arg);
+long krg_faf_ioctl(struct file *file, unsigned int cmd,
+		   unsigned long arg);
+long krg_faf_fstat(struct file *file, struct kstat *stat);
+long krg_faf_fstatfs(struct file *file, struct statfs *statfs);
+long krg_faf_fsync(struct file *file);
+long krg_faf_flock(struct file *file, unsigned int cmd);
+char *krg_faf_d_path(const struct file *file, char *buffer, int size, bool *deleted);
+char *krg_faf_phys_d_path(const struct file *file, char *buff, int size, bool *deleted);
+int krg_faf_do_path_lookup(struct file *file, const char *name,
+			   unsigned int flags, struct nameidata *nd);
+void krg_faf_srv_close(struct file *file);
+
+struct sockaddr;
+struct msghdr;
+
+long krg_faf_bind(struct file *file, struct sockaddr __user *umyaddr,
+		  int addrlen);
+long krg_faf_connect(struct file *file,
+		     struct sockaddr __user *uservaddr, int addrlen);
+long krg_faf_listen(struct file *file, int backlog);
+long krg_faf_accept(struct file *file,
+		    struct sockaddr __user *upeer_sockaddr,
+		    int __user *upeer_addrlen);
+long krg_faf_getsockname(struct file *file,
+			 struct sockaddr __user *usockaddr,
+			 int __user *usockaddr_len);
+long krg_faf_getpeername(struct file *file,
+			 struct sockaddr __user *usockaddr,
+			 int __user *usockaddr_len);
+long krg_faf_shutdown(struct file *file, int how);
+long krg_faf_setsockopt(struct file *file, int level, int optname,
+			char __user *optval, int optlen);
+long krg_faf_getsockopt(struct file *file, int level, int optname,
+			char __user *optval, int __user *optlen);
+ssize_t krg_faf_sendmsg(struct file *file, struct msghdr *msg,
+			size_t total_len);
+ssize_t krg_faf_recvmsg(struct file *file, struct msghdr *msg,
+			size_t total_len, unsigned int flags);
+int krg_faf_poll_wait(struct file *file, int wait);
+void krg_faf_poll_dequeue(struct file *file);
+
+/* Remote user access */
+unsigned long krg_copy_user_generic(void *to, const void *from,
+				    unsigned long n, int zerorest);
+long krg___strncpy_from_user(char *dst, const char __user *src,
+			     unsigned long count);
+unsigned long krg___strnlen_user(const char __user *str,
+					  unsigned long n);
+unsigned long krg___clear_user(void __user *mem, unsigned long len);
+
+/* functions used by other subsystems */
+int setup_faf_file_if_needed(struct file *file);
+
+int setup_faf_file(struct file *file);
+
+#endif // __FAF_H__
diff -ruN linux-2.6.29/include/kerrighed/fcntl.h android_cluster/linux-2.6.29/include/kerrighed/fcntl.h
--- linux-2.6.29/include/kerrighed/fcntl.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/fcntl.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,17 @@
+#ifndef __KKRG_FCNTL__
+#define __KKRG_FCNTL__
+
+#define O_FAF_CLT_BIT_NR        22        /* Client File Access Forwarding flag */
+#define O_FAF_SRV_BIT_NR        23        /* Server File Access Forwarding flag */
+#define O_KRG_SHARED_BIT_NR     24        /* Cluster wide shared file pointer */
+#define O_FAF_TTY_BIT_NR        25        /* The file is faffed and is a tty */
+
+#define O_FAF_CLT               (1<<O_FAF_CLT_BIT_NR)
+#define O_FAF_SRV               (1<<O_FAF_SRV_BIT_NR)
+#define O_KRG_SHARED            (1<<O_KRG_SHARED_BIT_NR)
+#define O_FAF_TTY               (1<<O_FAF_TTY_BIT_NR)
+
+/* Mask for Kerrighed O flags */
+#define O_KRG_FLAGS             (O_FAF_CLT|O_FAF_SRV|O_KRG_SHARED|O_FAF_TTY)
+
+#endif // __KKRG_FCNTL__
diff -ruN linux-2.6.29/include/kerrighed/file_ghost.h android_cluster/linux-2.6.29/include/kerrighed/file_ghost.h
--- linux-2.6.29/include/kerrighed/file_ghost.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/file_ghost.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,79 @@
+/** File ghost interface
+ *  @file file_ghost.h
+ *
+ *  Definition of file ghost structures and functions.
+ *  @author Renaud Lottiaux
+ */
+#ifndef __FILE_GHOST__
+#define __FILE_GHOST__
+
+#include <asm/uaccess.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+/** File ghost private data
+ */
+struct file_ghost_data {
+	struct file *file;              /**< File to save/load data to/from */
+	int from_fd;
+};
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+int mkdir_chkpt_path(long app_id, unsigned int chkpt_sn);
+
+char *get_chkpt_dir(long app_id, unsigned int chkpt_sn);
+
+char *get_chkpt_filebase(long app_id,
+			 unsigned int chkpt_sn,
+			 const char *format,
+			 ...);
+
+/** Create a new file ghost.
+ *  @author Matthieu Fertré, Renaud Lottiaux
+ *
+ *  @param  access   Ghost access (READ/WRITE)
+ *  @param  file     File to read/write data to/from.
+ *
+ *  @return        ghost_t if everything ok
+ *                 ERR_PTR otherwise.
+ */
+ghost_t *create_file_ghost(int access,
+			   long app_id,
+			   unsigned int chkpt_sn,
+			   const char *format,
+			   ...);
+void unlink_file_ghost(ghost_t *ghost);
+
+/** Create a new file ghost.
+ *  @author Matthieu Fertré, Renaud Lottiaux
+ *
+ *  @param  access   Ghost access (READ/WRITE)
+ *  @param  fd       File descriptor to read/write data to/from.
+ *
+ *  @return        ghost_t if everything ok
+ *                 ERR_PTR otherwise.
+ */
+ghost_t *create_file_ghost_from_fd(int access, unsigned int fd);
+
+loff_t get_file_ghost_pos(ghost_t *ghost);
+void set_file_ghost_pos(ghost_t *ghost, loff_t file_pos);
+
+typedef struct {
+	mm_segment_t fs;
+	const struct cred *cred;
+} ghost_fs_t;
+
+void __set_ghost_fs(ghost_fs_t *oldfs);
+int set_ghost_fs(ghost_fs_t *oldfs, uid_t fsuid, gid_t fsgid);
+void unset_ghost_fs(const ghost_fs_t *oldfs);
+
+#endif // __FILE_GHOST__
diff -ruN linux-2.6.29/include/kerrighed/file.h android_cluster/linux-2.6.29/include/kerrighed/file.h
--- linux-2.6.29/include/kerrighed/file.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/file.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,83 @@
+/** DVFS Level 3 - File struct sharing management.
+ *  @file file.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __DVFS_FILE__
+#define __DVFS_FILE__
+
+#include <kddm/kddm.h>
+
+struct epm_action;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+struct dvfs_file_struct {
+	loff_t f_pos;
+	int count;
+	struct file *file;
+};
+
+extern struct kddm_set *dvfs_file_struct_ctnr;
+
+#ifdef CONFIG_KRG_IPC
+extern struct file_operations krg_shm_file_operations;
+extern const struct file_operations shm_file_operations;
+#endif
+
+extern const struct file_operations shmem_file_operations;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+int create_kddm_file_object(struct file *file);
+
+#ifdef CONFIG_KRG_EPM
+void check_file_struct_sharing (int index, struct file *file,
+				struct epm_action *action);
+#endif
+
+void get_dvfs_file(int index, unsigned long objid);
+void put_dvfs_file(int index, struct file *file);
+
+int dvfs_file_init(void);
+void dvfs_file_finalize(void);
+
+static inline struct dvfs_file_struct *grab_dvfs_file_struct(unsigned long file_id)
+{
+	struct dvfs_file_struct * dvfs_file;
+
+	dvfs_file = _kddm_grab_object(dvfs_file_struct_ctnr, file_id);
+	if (dvfs_file && dvfs_file->file) {
+		if (atomic_read (&dvfs_file->file->f_count) == 0)
+			dvfs_file->file = NULL;
+	}
+	return dvfs_file;
+}
+
+static inline struct dvfs_file_struct *get_dvfs_file_struct(unsigned long file_id)
+{
+	struct dvfs_file_struct * dvfs_file;
+
+	dvfs_file = _kddm_get_object(dvfs_file_struct_ctnr, file_id);
+	if (dvfs_file && dvfs_file->file) {
+		if (atomic_read (&dvfs_file->file->f_count) == 0)
+			dvfs_file->file = NULL;
+	}
+	return dvfs_file;
+}
+
+static inline void put_dvfs_file_struct(unsigned long file_id)
+{
+	_kddm_put_object (dvfs_file_struct_ctnr, file_id);
+}
+
+#endif // __KERFS_FILE__
diff -ruN linux-2.6.29/include/kerrighed/file_stat.h android_cluster/linux-2.6.29/include/kerrighed/file_stat.h
--- linux-2.6.29/include/kerrighed/file_stat.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/file_stat.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,57 @@
+/*
+ * Get information about file
+ *
+ * Copyright (C) 2009, Matthieu Fertré, Kerlabs.
+ */
+
+#ifndef __KRG_FILE_STAT__
+#define __KRG_FILE_STAT__
+
+struct file;
+
+int is_pipe(const struct file *file);
+
+int is_anonymous_pipe(const struct file *file);
+
+int is_named_pipe(const struct file *file);
+
+int is_socket(const struct file *file);
+
+int is_shm(const struct file *file);
+
+int is_char_device(const struct file *file);
+
+int is_block_device(const struct file *file);
+
+int is_directory(const struct file *file);
+
+int is_link(const struct file *file);
+
+int is_tty(const struct file *file);
+
+int is_anon_shared_mmap(const struct file *file);
+
+/*
+ * Return the 'physical' name of a file.
+ * The filesystem must be mounted else it return NULL
+ * Returns NULL if file is deleted and del_ok is false
+ *
+ * buffer must have a size of PAGE_SIZE
+ */
+char *get_phys_filename(struct file *file, char *buffer, bool del_ok);
+
+/*
+ * Return the name of a file as visible in /proc/<pid>/fd.
+ * Virtual files such as socket, and anonymous pipe get a name.
+ *
+ * buffer must have a size of PAGE_SIZE
+ */
+char *get_filename(struct file *file, char *buffer);
+
+char *alloc_filename(struct file *file, char **buffer);
+
+void free_filename(char *buffer);
+
+int can_checkpoint_file(const struct file *file);
+
+#endif
diff -ruN linux-2.6.29/include/kerrighed/ghost.h android_cluster/linux-2.6.29/include/kerrighed/ghost.h
--- linux-2.6.29/include/kerrighed/ghost.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/ghost.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,167 @@
+/** Ghost management interface
+ *  @file ghost_api.h
+ *
+ *  Definition of ghost management interface.
+ *  @author Renaud Lottiaux
+ */
+#ifndef __GHOST_API__
+#define __GHOST_API__
+
+#include <asm-generic/errno-base.h>
+#include <kerrighed/ghost_types.h>
+#include <kerrighed/network_ghost.h>
+#include <kerrighed/file_ghost.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+#define MAX_GHOST_STRING 256
+
+/** Create a new ghost struct.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  type  Type of ghost to create (network, file, etc)
+ *
+ *  @return        Struct of the created ghost.
+ *                 NULL in case of error.
+ */
+ghost_t *create_ghost(ghost_type_t type, int access);
+
+/** Free ghost data structures.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  ghost  The ghost to free.
+ */
+void free_ghost(ghost_t *ghost);
+
+/** Generique function to write to a ghost.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  ghost   The ghost to write to.
+ *  @param  buff    Address of data to write in the ghost.
+ *  @param  length  Length of data to write.
+ *
+ *  @return        0 if everything ok
+ *                 Negative value otherwise.
+ */
+static inline
+int __must_check ghost_write(ghost_t *ghost, const void *buff, size_t length)
+{
+	int r = 0 ;
+
+	BUG_ON(!ghost);
+	BUG_ON(!ghost->ops);
+	BUG_ON(!ghost->ops->write);
+	BUG_ON(!buff);
+	BUG_ON(!(ghost->access & GHOST_WRITE));
+
+	r = ghost->ops->write ( ghost, buff, length );
+
+	return r ;
+}
+
+#define ghost_write_type(ghost, v) ghost_write(ghost, &v, sizeof(v))
+
+/** Generic function to write a character string to a ghost.
+ *  @author Matthieu Fertré
+ *
+ *  @param  ghost   The ghost to write to.
+ *  @param  str     The characters string to write in the ghost.
+ *
+ *  @return        0 if everything ok
+ *                 Negative value otherwise.
+ */
+static inline
+int __must_check ghost_write_string(ghost_t *ghost, const char* str)
+{
+	int r;
+	size_t len;
+	BUG_ON(!str);
+
+	len = strlen(str);
+	if (len > MAX_GHOST_STRING)
+		return -E2BIG;
+
+	r = ghost_write(ghost, &len, sizeof(len));
+	if (r)
+		goto err_write;
+	r = ghost_write(ghost, str, len+1);
+err_write:
+	return r;
+}
+
+/** Generic function to read from a ghost.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  ghost   The ghost to read from.
+ *  @param  buff    Address of buffer to write data in.
+ *  @param  length  Length of data to read.
+ *
+ *  @return        0 if everything ok
+ *                 Negative value otherwise.
+ */
+static inline
+int __must_check ghost_read(ghost_t *ghost, void *buff, size_t length)
+{
+	int r = 0 ;
+
+	BUG_ON (!(ghost->access & GHOST_READ));
+
+	r = ghost->ops->read(ghost, buff, length);
+
+	return r ;
+}
+
+#define ghost_read_type(ghost, v) ghost_read(ghost, &v, sizeof(v))
+
+/** Generic function to read a character string from a ghost.
+ *
+ *  @author Matthieu Fertré
+ *
+ *  @param  ghost   The ghost to write to.
+ *  @param  str     The characters string to read from the ghost.
+ *
+ *  @return        0 if everything ok
+ *                 Negative value otherwise.
+ *
+ *  User is responsible for having enough place to write the string in the
+ *  str buffer
+ */
+static inline
+int __must_check ghost_read_string(ghost_t *ghost, char* str)
+{
+	int r;
+	size_t len;
+	BUG_ON(str == NULL);
+
+	r = ghost_read(ghost, &len, sizeof(len));
+	if (r)
+		goto err_read;
+
+	if (len > MAX_GHOST_STRING)
+		return -E2BIG;
+
+	r = ghost_read(ghost, str, len+1);
+err_read:
+	return r;
+}
+
+int __must_check ghost_printf(ghost_t *ghost, char *format, ...);
+
+/** Generic function to close a ghost.
+ *  @author Matthieu Fertré
+ *
+ *  @param  ghost   The ghost to close.
+ *
+ *  @return        0 if everything ok
+ *                 Negative value otherwise.
+ */
+static inline int ghost_close(ghost_t * ghost)
+{
+	return ghost->ops->close(ghost);
+}
+
+#endif // __GHOST_API__
diff -ruN linux-2.6.29/include/kerrighed/ghost_helpers.h android_cluster/linux-2.6.29/include/kerrighed/ghost_helpers.h
--- linux-2.6.29/include/kerrighed/ghost_helpers.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/ghost_helpers.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,333 @@
+#ifndef __GHOST_HELPERS_H__
+#define __GHOST_HELPERS_H__
+
+#include <kerrighed/ghost_types.h>
+#include <linux/hashtable.h>
+
+struct epm_action;
+struct task_struct;
+struct restart_block;
+struct pid_link;
+struct pid;
+enum shared_obj_type;
+
+/* KDDM */
+
+int export_kddm_info_struct (struct epm_action *action,
+			     ghost_t *ghost, struct task_struct *tsk);
+int import_kddm_info_struct (struct epm_action *action,
+			     ghost_t *ghost, struct task_struct *tsk);
+void unimport_kddm_info_struct (struct task_struct *tsk);
+
+/* MM */
+
+/**
+ *  This function exports the virtual memory of a process
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where VM data should be stored.
+ *  @param task    Task to export file data from.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int export_mm_struct (struct epm_action *action,
+		      ghost_t *ghost, struct task_struct *task);
+
+
+/**
+ *  This function imports the virtual memory of a process
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where VM data are stored.
+ *  @param task    Task to load VM data in.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int import_mm_struct (struct epm_action *action,
+		      ghost_t *ghost, struct task_struct *task);
+
+void unimport_mm_struct(struct task_struct *task);
+
+/**
+ *  Free the mm struct of the ghost process.
+ *
+ *  @param task    Task struct of the ghost process.
+ */
+void free_ghost_mm (struct task_struct *task);
+
+int cr_exclude_mm_region(struct app_struct *app, pid_t pid,
+			 unsigned long addr, size_t size);
+
+void cr_free_mm_exclusions(struct app_struct *app);
+
+/* FS */
+
+/** Export an files structure into a ghost.
+ *  @author  Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where files data should be stored.
+ *  @param tsk    Task to export files data from.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int export_files_struct (struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *tsk);
+
+/** Export the fs_struct of a process
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where file data should be stored.
+ *  @param tsk    Task to export file data from.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int export_fs_struct (struct epm_action *action,
+		      ghost_t *ghost, struct task_struct *tsk);
+
+int export_mm_exe_file(struct epm_action *action, ghost_t *ghost,
+		       struct task_struct *tsk);
+
+int export_vma_file (struct epm_action *action, ghost_t * ghost,
+		     struct task_struct *tsk, struct vm_area_struct *vma,
+		     hashtable_t *file_table);
+
+int export_mnt_namespace (struct epm_action *action,
+			  ghost_t *ghost, struct task_struct *tsk);
+
+/** Import a files structure from a ghost.
+ *  @author  Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where files data are stored.
+ *  @param tsk    Task to load files data in.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int import_files_struct (struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *tsk);
+
+/** Import the fs_struct of a process
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where file data are stored.
+ *  @param tsk    Task to import file data in.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int import_fs_struct (struct epm_action *action,
+		      ghost_t *ghost, struct task_struct *tsk);
+
+int import_mm_exe_file(struct epm_action *action, ghost_t *ghost,
+		       struct task_struct *tsk);
+
+int import_vma_file (struct epm_action *action, ghost_t *ghost,
+		     struct task_struct *tsk, struct vm_area_struct *vma,
+		     hashtable_t *file_table);
+
+int import_mnt_namespace (struct epm_action *action,
+			  ghost_t *ghost, struct task_struct *tsk);
+
+void unimport_files_struct(struct task_struct *task);
+void unimport_fs_struct(struct task_struct *task);
+
+void free_ghost_files (struct task_struct *ghost);
+
+void cr_get_file_type_and_key(const struct file *file,
+			      enum shared_obj_type *type,
+			      long *key,
+			      int allow_unsupported);
+
+int cr_add_file_to_shared_table(struct task_struct *task,
+				int index, struct file *file,
+				int allow_unsupported);
+
+int _cr_add_file_to_shared_table(struct task_struct *task,
+				 int index, struct file *file,
+				 int allow_unsupported);
+
+int cr_add_pipe_inode_to_shared_table(struct task_struct *task,
+				      struct file *file);
+
+/* IPC */
+
+int get_shm_file_krg_desc(struct file *file, void **desc, int *desc_size);
+
+struct file *reopen_shm_file_entry_from_krg_desc(struct task_struct *task,
+						 void *desc);
+
+int export_ipc_namespace(struct epm_action *action,
+                         ghost_t *ghost, struct task_struct *task);
+int import_ipc_namespace(struct epm_action *action,
+                         ghost_t *ghost, struct task_struct *task);
+void unimport_ipc_namespace(struct task_struct *task);
+
+int export_sysv_sem(struct epm_action *action,
+                    ghost_t *ghost, struct task_struct *task);
+int import_sysv_sem(struct epm_action *action,
+                    ghost_t *ghost, struct task_struct *task);
+void unimport_sysv_sem(struct task_struct *task);
+
+/* EPM */
+
+/* Arch-dependent helpers */
+
+void prepare_to_export(struct task_struct *task);
+
+int export_thread_info(struct epm_action *action,
+		       ghost_t *ghost, struct task_struct *task);
+int import_thread_info(struct epm_action *action,
+		       ghost_t *ghost, struct task_struct *task);
+void unimport_thread_info(struct task_struct *task);
+void free_ghost_thread_info(struct task_struct *);
+
+int export_thread_struct(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *tsk);
+int import_thread_struct(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *tsk);
+void unimport_thread_struct(struct task_struct *task);
+
+/* Generic helpers for arch-dependent helpers */
+
+int export_exec_domain(struct epm_action *action,
+		       ghost_t *ghost, struct task_struct *tsk);
+struct exec_domain *import_exec_domain(struct epm_action *action,
+				       ghost_t *ghost);
+
+int export_restart_block(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *tsk);
+int import_restart_block(struct epm_action *action,
+			 ghost_t *ghost, struct restart_block *p);
+
+/* Signals */
+
+int export_signal_struct(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task);
+int import_signal_struct(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task);
+void unimport_signal_struct(struct task_struct *task);
+
+int export_private_signals(struct epm_action *action,
+			   ghost_t *ghost,
+			   struct task_struct *task);
+int import_private_signals(struct epm_action *action,
+			   ghost_t *ghost,
+			   struct task_struct *task);
+void unimport_private_signals(struct task_struct *task);
+
+int export_sighand_struct(struct epm_action *action,
+			  ghost_t *ghost, struct task_struct *task);
+int import_sighand_struct(struct epm_action *action,
+			  ghost_t *ghost, struct task_struct *task);
+void unimport_sighand_struct(struct task_struct *task);
+
+/* Pids */
+
+int export_pid(struct epm_action *action,
+	       ghost_t *ghost, struct pid_link *link);
+int export_pid_namespace(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task);
+
+int import_pid(struct epm_action *action,
+	       ghost_t *ghost, struct pid_link *link, enum pid_type type);
+int import_pid_namespace(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task);
+void unimport_pid(struct pid_link *link);
+
+int cr_create_pid_kddm_object(struct pid *pid);
+
+/* Misc */
+
+int export_sched(struct epm_action *action,
+		 ghost_t *ghost, struct task_struct *tsk);
+int import_sched(struct epm_action *action,
+		 ghost_t *ghost, struct task_struct *task);
+static inline void unimport_sched(struct task_struct *task)
+{
+}
+
+int export_vfork_done(struct epm_action *action,
+		      ghost_t *ghost, struct task_struct *tsk);
+int import_vfork_done(struct epm_action *action,
+		      ghost_t *ghost, struct task_struct *task);
+void unimport_vfork_done(struct task_struct *task);
+
+int export_cred(struct epm_action *action,
+		ghost_t *ghost, struct task_struct *tsk);
+int import_cred(struct epm_action *action,
+		ghost_t *ghost, struct task_struct *task);
+void unimport_cred(struct task_struct *task);
+void free_ghost_cred(struct task_struct *ghost);
+
+#ifdef CONFIG_AUDITSYSCALL
+int export_audit_context(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *tsk);
+int import_audit_context(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task);
+void unimport_audit_context(struct task_struct *task);
+void free_ghost_audit_context(struct task_struct *ghost);
+#else /* !CONFIG_AUDITSYSCALL */
+static inline
+int export_audit_context(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *tsk)
+{
+	return 0;
+}
+static inline
+int import_audit_context(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task)
+{
+	return 0;
+}
+static inline void unimport_audit_context(struct task_struct *task)
+{
+}
+static inline void free_ghost_audit_context(struct task_struct *ghost)
+{
+}
+#endif /* !CONFIG_AUDITSYSCALL */
+
+int export_cgroups(struct epm_action *action,
+		   ghost_t *ghost, struct task_struct *task);
+int import_cgroups(struct epm_action *action,
+		   ghost_t *ghost, struct task_struct *task);
+void unimport_cgroups(struct task_struct *task);
+void free_ghost_cgroups(struct task_struct *ghost);
+
+/* SCHED */
+
+#ifdef CONFIG_KRG_SCHED
+
+int export_krg_sched_info(struct epm_action *action, struct ghost *ghost,
+			  struct task_struct *task);
+int import_krg_sched_info(struct epm_action *action, struct ghost *ghost,
+			  struct task_struct *task);
+void post_import_krg_sched_info(struct task_struct *task);
+void unimport_krg_sched_info(struct task_struct *task);
+
+int export_process_set_links_start(struct epm_action *action, ghost_t *ghost,
+				   struct task_struct *task);
+int export_process_set_links(struct epm_action *action, ghost_t *ghost,
+			     struct pid *pid, enum pid_type type);
+void export_process_set_links_end(struct epm_action *action, ghost_t *ghost,
+				  struct task_struct *task);
+
+int import_process_set_links(struct epm_action *action, ghost_t *ghost,
+			     struct pid *pid, enum pid_type type);
+
+#endif /* CONFIG_KRG_SCHED */
+
+void do_ckpt_msg(int err, char *fmt, ...);
+
+#define ckpt_err(ctx, err, fmt, args...) do {				\
+	struct epm_action *_action = ctx;				\
+	if (!_action || _action->type == EPM_CHECKPOINT)		\
+		do_ckpt_msg(err, "[E @ %s:%d : %d] " fmt, __func__,	\
+			    __LINE__, err, ##args);			\
+} while (0)
+
+#endif /* __GHOST_HELPERS_H__ */
diff -ruN linux-2.6.29/include/kerrighed/ghost_types.h android_cluster/linux-2.6.29/include/kerrighed/ghost_types.h
--- linux-2.6.29/include/kerrighed/ghost_types.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/ghost_types.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,51 @@
+/** Ghost management type definition
+ *  @file ghost_types.h
+ *
+ *  Definition of ghost management type.
+ *  @author Renaud Lottiaux
+ */
+#ifndef __GHOST_TYPES__
+#define __GHOST_TYPES__
+
+#include <kerrighed/types.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+enum ghost_access_t {
+	__GHOST_READ,
+	__GHOST_WRITE
+};
+
+#define GHOST_READ (1<<__GHOST_READ)
+#define GHOST_WRITE (1<<__GHOST_WRITE)
+
+typedef enum {
+	GHOST_NETWORK,
+	GHOST_FILE
+} ghost_type_t; /**< Ghost type (network, file, etc) */
+
+struct ghost ;
+
+/** Ghost operation structure
+ */
+typedef struct ghost_operations {
+	int (*write) (struct ghost *ghost, const void *buff, size_t length);
+	int (*read) (struct ghost *ghost, void *buff, size_t length);
+	int (*close) (struct ghost *ghost);
+} ghost_operations_t;
+
+/** Ghost structure
+ */
+typedef struct ghost {
+	ghost_type_t type;         /**< Ghost type (network, file, etc */
+	size_t size;               /**< Size of data stored in the ghost */
+	ghost_operations_t *ops;   /**< Ghost operation (read, write, etc) */
+	void *data;                /**< Ghost private data */
+	int access;                /**< Kind of access to the ghost (read/write) */
+} ghost_t;
+
+#endif // __GHOST_TYPES__
diff -ruN linux-2.6.29/include/kerrighed/hotplug.h android_cluster/linux-2.6.29/include/kerrighed/hotplug.h
--- linux-2.6.29/include/kerrighed/hotplug.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/hotplug.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,88 @@
+#ifndef __HOTPLUG__
+#define __HOTPLUG__
+
+#include <linux/kref.h>
+#include <kerrighed/krgnodemask.h>
+
+enum {
+	HOTPLUG_PRIO_SCHED_POST,
+	HOTPLUG_PRIO_MEMBERSHIP_ONLINE, // should be done after distributed services management
+	HOTPLUG_PRIO_SCHED,
+	HOTPLUG_PRIO_EPM,
+	HOTPLUG_PRIO_PROCFS,
+	HOTPLUG_PRIO_KDDM,
+	HOTPLUG_PRIO_BARRIER,
+	HOTPLUG_PRIO_RPC,
+	HOTPLUG_PRIO_MEMBERSHIP_PRESENT,
+	HOTPLUG_PRIO_MAX // must be the last one
+};
+
+typedef enum {
+	HOTPLUG_NOTIFY_ADD,
+	HOTPLUG_NOTIFY_REMOVE,
+	HOTPLUG_NOTIFY_REMOVE_LOCAL, // node side: local operations
+	HOTPLUG_NOTIFY_REMOVE_ADVERT, // cluster side
+	HOTPLUG_NOTIFY_REMOVE_DISTANT, // node side: remote operations
+	HOTPLUG_NOTIFY_REMOVE_ACK, // cluster side
+	HOTPLUG_NOTIFY_FAIL,
+} hotplug_event_t;
+
+enum {
+	HOTPLUG_NODE_INVALID,
+	HOTPLUG_NODE_POSSIBLE,
+	HOTPLUG_NODE_PRESENT,
+	HOTPLUG_NODE_ONLINE
+};
+
+struct hotplug_node_set {
+	int subclusterid;
+	krgnodemask_t v;
+};
+
+struct hotplug_context {
+	struct krg_namespace *ns;
+	struct hotplug_node_set node_set;
+	struct kref kref;
+};
+
+struct notifier_block;
+
+struct hotplug_context *hotplug_ctx_alloc(struct krg_namespace *ns);
+void hotplug_ctx_release(struct kref *kref);
+
+static inline void hotplug_ctx_get(struct hotplug_context *ctx)
+{
+	kref_get(&ctx->kref);
+}
+
+static inline void hotplug_ctx_put(struct hotplug_context *ctx)
+{
+	kref_put(&ctx->kref, hotplug_ctx_release);
+}
+
+int register_hotplug_notifier(int (*notifier_call)(struct notifier_block *, hotplug_event_t, void *),
+			      int priority);
+
+struct hotplug_node_set;
+int hotplug_add_notify(struct hotplug_context *ctx, hotplug_event_t event);
+int hotplug_remove_notify(struct hotplug_node_set *nodes_set,
+			  hotplug_event_t event);
+int hotplug_failure_notify(struct hotplug_node_set *nodes_set,
+			   hotplug_event_t event);
+
+void hook_register(void *hk, void *f);
+
+struct universe_elem {
+	int state;
+	int subid;
+};
+extern struct universe_elem universe[KERRIGHED_MAX_NODES];
+
+extern void (*kh_cluster_autostart)(void);
+extern void (*kh_node_reachable)(kerrighed_node_t nodeid);
+extern void (*kh_node_unreachable)(kerrighed_node_t nodeid);
+
+void krg_node_arrival(kerrighed_node_t nodeid);
+void krg_node_departure(kerrighed_node_t nodeid);
+
+#endif
diff -ruN linux-2.6.29/include/kerrighed/kerrighed_signal.h android_cluster/linux-2.6.29/include/kerrighed/kerrighed_signal.h
--- linux-2.6.29/include/kerrighed/kerrighed_signal.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/kerrighed_signal.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,23 @@
+#ifndef __KRG_KERRIGHED_SIGNAL_H__
+#define __KRG_KERRIGHED_SIGNAL_H__
+
+/* Kerrighed signal */
+
+#ifdef CONFIG_KRG_EPM
+
+#include <asm/signal.h>
+
+struct siginfo;
+struct pt_regs;
+struct task_struct;
+
+typedef void kerrighed_handler_t(int sig, struct siginfo *info,
+				 struct pt_regs *regs);
+
+extern kerrighed_handler_t *krg_handler[_NSIG];
+
+int send_kerrighed_signal(int sig, struct siginfo *info, struct task_struct *t);
+
+#endif /* CONFIG_KRG_EPM */
+
+#endif /* __KRG_KERRIGHED_SIGNAL_H__ */
diff -ruN linux-2.6.29/include/kerrighed/krg_exit.h android_cluster/linux-2.6.29/include/kerrighed/krg_exit.h
--- linux-2.6.29/include/kerrighed/krg_exit.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/krg_exit.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,59 @@
+#ifndef __KRG_EXIT_H__
+#define __KRG_EXIT_H__
+
+struct task_struct;
+
+#ifdef CONFIG_KRG_EPM
+
+#include <linux/types.h>
+#include <kerrighed/sys/types.h>
+
+struct children_kddm_object;
+enum pid_type;
+struct siginfo;
+struct rusage;
+
+/* do_wait() hook */
+int krg_do_wait(struct children_kddm_object *obj, int *notask_error,
+		enum pid_type type, pid_t pid, int options,
+		struct siginfo __user *infop, int __user *stat_addr,
+		struct rusage __user *ru);
+
+/* Used by krg_do_wait() */
+int krg_wait_task_zombie(pid_t pid, kerrighed_node_t zombie_location,
+			 int options,
+			 struct siginfo __user *infop,
+			 int __user *stat_addr, struct rusage __user *ru);
+
+/* do_notify_parent() hook */
+int krg_do_notify_parent(struct task_struct *task, struct siginfo *info);
+
+/* Used by remote (zombie) child reparenting */
+void notify_remote_child_reaper(pid_t zombie_pid,
+				kerrighed_node_t zombie_location);
+
+/* Delayed do_notify_parent() in release_task() */
+int krg_delayed_notify_parent(struct task_struct *leader);
+
+/* exit_ptrace() hooks */
+struct children_kddm_object *
+krg_prepare_exit_ptrace_task(struct task_struct *tracer,
+			     struct task_struct *task)
+	__acquires(tasklist_lock);
+void krg_finish_exit_ptrace_task(struct task_struct *task,
+				 struct children_kddm_object *obj,
+				 bool dead)
+	__releases(tasklist_lock);
+
+#endif /* CONFIG_KRG_EPM */
+
+#ifdef CONFIG_KRG_PROC
+
+/* exit_notify() hooks */
+
+void *krg_prepare_exit_notify(struct task_struct *task);
+void krg_finish_exit_notify(struct task_struct *task, int signal, void *cookie);
+
+#endif /* CONFIG_KRG_PROC */
+
+#endif /* __KRG_EXIT_H__ */
diff -ruN linux-2.6.29/include/kerrighed/krgflags.h android_cluster/linux-2.6.29/include/kerrighed/krgflags.h
--- linux-2.6.29/include/kerrighed/krgflags.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/krgflags.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,38 @@
+#ifndef __KRGFLAGS_H__
+#define __KRGFLAGS_H__
+
+enum {
+	__KRGFLAGS_LOADED,
+	__KRGFLAGS_STARTING,
+	__KRGFLAGS_RUNNING,
+	__KRGFLAGS_ADDING,
+	__KRGFLAGS_REMOVING,
+	__KRGFLAGS_RECOVERING,
+	__KRGFLAGS_STOPPING,
+	__KRGFLAGS_STOPPED,
+	__KRGFLAGS_FAILED,
+};
+
+#define KRGFLAGS_LOADED (1<<__KRGFLAGS_LOADED)
+#define KRGFLAGS_STARTING (1<<__KRGFLAGS_STARTING)
+#define KRGFLAGS_RUNNING (1<<__KRGFLAGS_RUNNING)
+#define KRGFLAGS_ADDING (1<<__KRGFLAGS_ADDING)
+#define KRGFLAGS_REMOVING (1<<__KRGFLAGS_REMOVING)
+#define KRGFLAGS_RECOVERING (1<<__KRGFLAGS_RECOVERING)
+#define KRGFLAGS_STOPPING (1<<__KRGFLAGS_STOPPING)
+#define KRGFLAGS_STOPPED (1<<__KRGFLAGS_STOPPED)
+#define KRGFLAGS_FAILED (1<<__KRGFLAGS_FAILED)
+
+extern int kerrighed_cluster_flags;
+extern int kerrighed_node_flags;
+
+#define IS_KERRIGHED_CLUSTER(m) (kerrighed_cluster_flags & m)
+#define IS_KERRIGHED_NODE(m) (kerrighed_node_flags & m)
+
+#define SET_KERRIGHED_CLUSTER_FLAGS(m) kerrighed_cluster_flags |= m
+#define SET_KERRIGHED_NODE_FLAGS(m) kerrighed_node_flags |= m
+
+#define CLEAR_KERRIGHED_CLUSTER_FLAGS(m) kerrighed_cluster_flags &= ~m
+#define CLEAR_KERRIGHED_NODE_FLAGS(m) kerrighed_node_flags &= ~m
+
+#endif
diff -ruN linux-2.6.29/include/kerrighed/krginit.h android_cluster/linux-2.6.29/include/kerrighed/krginit.h
--- linux-2.6.29/include/kerrighed/krginit.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/krginit.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,26 @@
+#ifndef __KRGINIT_H__
+#define __KRGINIT_H__
+
+#include <kerrighed/types.h>
+#include <linux/rwsem.h>
+
+enum kerrighed_init_flags_t {
+	KRG_INITFLAGS_NODEID,
+	KRG_INITFLAGS_SESSIONID,
+	KRG_INITFLAGS_AUTONODEID,
+};
+
+/* Tools */
+extern kerrighed_node_t kerrighed_node_id;
+extern kerrighed_node_t kerrighed_nb_nodes;
+extern kerrighed_node_t kerrighed_nb_nodes_min;
+extern kerrighed_session_t kerrighed_session_id;
+extern kerrighed_subsession_t kerrighed_subsession_id;
+extern int kerrighed_init_flags;
+extern struct rw_semaphore kerrighed_init_sem;
+
+#define SET_KRG_INIT_FLAGS(p) kerrighed_init_flags |= (1<<p)
+#define CLR_KRG_INIT_FLAGS(p) kerrighed_init_flags &= ~(1<<p)
+#define ISSET_KRG_INIT_FLAGS(p) (kerrighed_init_flags & (1<<p))
+
+#endif
diff -ruN linux-2.6.29/include/kerrighed/krgnodemask.h android_cluster/linux-2.6.29/include/kerrighed/krgnodemask.h
--- linux-2.6.29/include/kerrighed/krgnodemask.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/krgnodemask.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,453 @@
+#ifndef __KRG_NODEMASK_H
+#define __KRG_NODEMASK_H
+
+/*
+ * This file is nearly a copy/paste of linux/cpumask.h (2.6.20)
+ * Btw this code is very closed to the NUMA related code linux/nodemask.h
+ * It will be a good idea to check if can merge these two files
+ *
+ * nodemasks provide a bitmap suitable for representing the
+ * set of nodes in a system, one bit position per node number.
+ *
+ * See detailed comments in the file linux/bitmap.h describing the
+ * data type on which these krgnodemasks are based.
+ *
+ * For details of krgnodemask_scnprintf() and krgnodemask_parse_user(),
+ * see bitmap_scnprintf() and bitmap_parse_user() in lib/bitmap.c.
+ * For details of krgnodelist_scnprintf() and krgnodelist_parse(), see
+ * bitmap_scnlistprintf() and bitmap_parselist(), also in bitmap.c.
+ * For details of krgnode_remap(), see bitmap_bitremap in lib/bitmap.c
+ * For details of krgnodes_remap(), see bitmap_remap in lib/bitmap.c.
+ *
+ * The available nodemask operations are:
+ *
+ * void krgnode_set(node, mask)		turn on bit 'node' in mask
+ * void krgnode_clear(node, mask)		turn off bit 'node' in mask
+ * void krgnodes_setall(mask)		set all bits
+ * void krgnodes_clear(mask)		clear all bits
+ * int krgnode_isset(node, mask)		true iff bit 'node' set in mask
+ * int krgnode_test_and_set(node, mask)	test and set bit 'node' in mask
+ *
+ * void krgnodes_and(dst, src1, src2)	dst = src1 & src2  [intersection]
+ * void krgnodes_or(dst, src1, src2)	dst = src1 | src2  [union]
+ * void krgnodes_xor(dst, src1, src2)	dst = src1 ^ src2
+ * void krgnodes_andnot(dst, src1, src2)	dst = src1 & ~src2
+ * void krgnodes_complement(dst, src)	dst = ~src
+ *
+ * int krgnodes_equal(mask1, mask2)		Does mask1 == mask2?
+ * int krgnodes_intersects(mask1, mask2)	Do mask1 and mask2 intersect?
+ * int krgnodes_subset(mask1, mask2)	Is mask1 a subset of mask2?
+ * int krgnodes_empty(mask)			Is mask empty (no bits sets)?
+ * int krgnodes_full(mask)			Is mask full (all bits sets)?
+ * int krgnodes_weight(mask)		Hamming weigh - number of set bits
+ *
+ * void krgnodes_shift_right(dst, src, n)	Shift right
+ * void krgnodes_shift_left(dst, src, n)	Shift left
+ *
+ * int first_krgnode(mask)			Number lowest set bit, or KERRIGHED_MAX_NODES
+ * int next_krgnode(node, mask)		Next node past 'node', or KERRIGHED_MAX_NODES
+ *
+ * krgnodemask_t krgnodemask_of_node(node)	Return nodemask with bit 'node' set
+ * KRGNODE_MASK_ALL				Initializer - all bits set
+ * KRGNODE_MASK_NONE			Initializer - no bits set
+ * unsigned long *krgnodes_addr(mask)	Array of unsigned long's in mask
+ *
+ * int krgnodemask_scnprintf(buf, len, mask) Format nodemask for printing
+ * int krgnodemask_parse_user(ubuf, ulen, mask)	Parse ascii string as nodemask
+ * int krgnodelist_scnprintf(buf, len, mask) Format nodemask as list for printing
+ * int krgnodelist_parse(buf, map)		Parse ascii string as nodelist
+ * int krgnode_remap(oldbit, old, new)	newbit = map(old, new)(oldbit)
+ * int krgnodes_remap(dst, src, old, new)	*dst = map(old, new)(src)
+ *
+ * for_each_krgnode_mask(node, mask)		for-loop node over mask
+ *
+ * int num_online_krgnodes()		Number of online NODEs
+ * int num_possible_krgnodes()		Number of all possible NODEs
+ * int num_present_krgnodes()		Number of present NODEs
+ *
+ * int krgnode_online(node)			Is some node online?
+ * int krgnode_possible(node)		Is some node possible?
+ * int krgnode_present(node)			Is some node present (can schedule)?
+ *
+ * int any_online_krgnode(mask)		First online node in mask
+ *
+ * for_each_possible_krgnode(node)		for-loop node over node_possible_map
+ * for_each_online_krgnode(node)		for-loop node over node_online_map
+ * for_each_present_krgnode(node)		for-loop node over node_present_map
+ *
+ * Subtlety:
+ * 1) The 'type-checked' form of node_isset() causes gcc (3.3.2, anyway)
+ *    to generate slightly worse code.  Note for example the additional
+ *    40 lines of assembly code compiling the "for each possible node"
+ *    loops buried in the disk_stat_read() macros calls when compiling
+ *    drivers/block/genhd.c (arch i386, CONFIG_SMP=y).  So use a simple
+ *    one-line #define for node_isset(), instead of wrapping an inline
+ *    inside a macro, the way we do the other calls.
+ */
+
+#include <linux/kernel.h>
+#include <linux/threads.h>
+#include <linux/bitmap.h>
+#include <kerrighed/sys/types.h>
+#if KERRIGHED_MAX_NODES <= 1
+/* kerrighed_node_id is used for some macros in this special case */
+#include <kerrighed/krginit.h>
+#endif
+
+
+typedef struct { DECLARE_BITMAP(bits, KERRIGHED_MAX_NODES); } krgnodemask_t;
+typedef struct { DECLARE_BITMAP(bits, KERRIGHED_HARD_MAX_NODES); } __krgnodemask_t;
+
+extern krgnodemask_t _unused_krgnodemask_arg_;
+
+#define krgnode_set(node, dst) __krgnode_set((node), &(dst))
+static inline void __krgnode_set(int node, volatile krgnodemask_t *dstp)
+{
+	set_bit(node, dstp->bits);
+}
+
+#define krgnode_clear(node, dst) __krgnode_clear((node), &(dst))
+static inline void __krgnode_clear(int node, volatile krgnodemask_t *dstp)
+{
+	clear_bit(node, dstp->bits);
+}
+
+#define krgnodes_setall(dst) __krgnodes_setall(&(dst))
+static inline void __krgnodes_setall(krgnodemask_t *dstp)
+{
+	bitmap_fill(dstp->bits, KERRIGHED_MAX_NODES);
+}
+
+#define krgnodes_clear(dst) __krgnodes_clear(&(dst))
+static inline void __krgnodes_clear(krgnodemask_t *dstp)
+{
+	bitmap_zero(dstp->bits, KERRIGHED_MAX_NODES);
+}
+
+#define krgnodes_copy(dst, src) __krgnodes_copy(&(dst), &(src))
+static inline void __krgnodes_copy(krgnodemask_t *dstp, const krgnodemask_t *srcp)
+{
+	bitmap_copy(dstp->bits, srcp->bits, KERRIGHED_MAX_NODES);
+}
+
+/* No static inline type checking - see Subtlety (1) above. */
+#define krgnode_isset(node, krgnodemask) test_bit((node), (krgnodemask).bits)
+#define __krgnode_isset(node, krgnodemask) test_bit((node), (krgnodemask)->bits)
+
+#define krgnode_test_and_set(node, krgnodemask) __krgnode_test_and_set((node), &(krgnodemask))
+static inline int __krgnode_test_and_set(int node, krgnodemask_t *addr)
+{
+	return test_and_set_bit(node, addr->bits);
+}
+
+#define krgnodes_and(dst, src1, src2) __krgnodes_and(&(dst), &(src1), &(src2), KERRIGHED_MAX_NODES)
+static inline void __krgnodes_and(krgnodemask_t *dstp, const krgnodemask_t *src1p,
+					const krgnodemask_t *src2p, int nbits)
+{
+	bitmap_and(dstp->bits, src1p->bits, src2p->bits, nbits);
+}
+
+#define krgnodes_or(dst, src1, src2) __krgnodes_or(&(dst), &(src1), &(src2), KERRIGHED_MAX_NODES)
+static inline void __krgnodes_or(krgnodemask_t *dstp, const krgnodemask_t *src1p,
+					const krgnodemask_t *src2p, int nbits)
+{
+	bitmap_or(dstp->bits, src1p->bits, src2p->bits, nbits);
+}
+
+#define krgnodes_xor(dst, src1, src2) __krgnodes_xor(&(dst), &(src1), &(src2), KERRIGHED_MAX_NODES)
+static inline void __krgnodes_xor(krgnodemask_t *dstp, const krgnodemask_t *src1p,
+					const krgnodemask_t *src2p, int nbits)
+{
+	bitmap_xor(dstp->bits, src1p->bits, src2p->bits, nbits);
+}
+
+#define krgnodes_andnot(dst, src1, src2) \
+				__krgnodes_andnot(&(dst), &(src1), &(src2), KERRIGHED_MAX_NODES)
+static inline void __krgnodes_andnot(krgnodemask_t *dstp, const krgnodemask_t *src1p,
+					const krgnodemask_t *src2p, int nbits)
+{
+	bitmap_andnot(dstp->bits, src1p->bits, src2p->bits, nbits);
+}
+
+#define krgnodes_complement(dst, src) __krgnodes_complement(&(dst), &(src), KERRIGHED_MAX_NODES)
+static inline void __krgnodes_complement(krgnodemask_t *dstp,
+					const krgnodemask_t *srcp, int nbits)
+{
+	bitmap_complement(dstp->bits, srcp->bits, nbits);
+}
+
+#define krgnodes_equal(src1, src2) __krgnodes_equal(&(src1), &(src2))
+static inline int __krgnodes_equal(const krgnodemask_t *src1p,
+				   const krgnodemask_t *src2p)
+{
+	return bitmap_equal(src1p->bits, src2p->bits, KERRIGHED_MAX_NODES);
+}
+
+#define krgnodes_intersects(src1, src2) __krgnodes_intersects(&(src1), &(src2), KERRIGHED_MAX_NODES)
+static inline int __krgnodes_intersects(const krgnodemask_t *src1p,
+					const krgnodemask_t *src2p, int nbits)
+{
+	return bitmap_intersects(src1p->bits, src2p->bits, nbits);
+}
+
+#define krgnodes_subset(src1, src2) __krgnodes_subset(&(src1), &(src2), KERRIGHED_MAX_NODES)
+static inline int __krgnodes_subset(const krgnodemask_t *src1p,
+					const krgnodemask_t *src2p, int nbits)
+{
+	return bitmap_subset(src1p->bits, src2p->bits, nbits);
+}
+
+#define krgnodes_empty(src) __krgnodes_empty(&(src))
+static inline int __krgnodes_empty(const krgnodemask_t *srcp)
+{
+	return bitmap_empty(srcp->bits, KERRIGHED_MAX_NODES);
+}
+
+#define krgnodes_full(nodemask) __krgnodes_full(&(nodemask), KERRIGHED_MAX_NODES)
+static inline int __krgnodes_full(const krgnodemask_t *srcp, int nbits)
+{
+	return bitmap_full(srcp->bits, nbits);
+}
+
+#define krgnodes_weight(nodemask) __krgnodes_weight(&(nodemask))
+static inline int __krgnodes_weight(const krgnodemask_t *srcp)
+{
+	return bitmap_weight(srcp->bits, KERRIGHED_MAX_NODES);
+}
+
+#define krgnodes_shift_right(dst, src, n) \
+			__krgnodes_shift_right(&(dst), &(src), (n), KERRIGHED_MAX_NODES)
+static inline void __krgnodes_shift_right(krgnodemask_t *dstp,
+					const krgnodemask_t *srcp, int n, int nbits)
+{
+	bitmap_shift_right(dstp->bits, srcp->bits, n, nbits);
+}
+
+#define krgnodes_shift_left(dst, src, n) \
+			__krgnodes_shift_left(&(dst), &(src), (n), KERRIGHED_MAX_NODES)
+static inline void __krgnodes_shift_left(krgnodemask_t *dstp,
+					const krgnodemask_t *srcp, int n, int nbits)
+{
+	bitmap_shift_left(dstp->bits, srcp->bits, n, nbits);
+}
+
+#define first_krgnode(src) __first_krgnode(&(src))
+static inline int __first_krgnode(const krgnodemask_t *srcp)
+{
+	return min_t(int, KERRIGHED_MAX_NODES, find_first_bit(srcp->bits, KERRIGHED_MAX_NODES));
+}
+
+#define next_krgnode(n, src) __next_krgnode((n), &(src))
+static inline int __next_krgnode(int n, const krgnodemask_t *srcp)
+{
+	return min_t(int, KERRIGHED_MAX_NODES,find_next_bit(srcp->bits, KERRIGHED_MAX_NODES, n+1));
+}
+
+#define krgnodemask_of_node(node)						\
+({									\
+	typeof(_unused_krgnodemask_arg_) m;					\
+	if (sizeof(m) == sizeof(unsigned long)) {			\
+		m.bits[0] = 1UL<<(node);					\
+	} else {							\
+		krgnodes_clear(m);						\
+		krgnode_set((node), m);					\
+	}								\
+	m;								\
+})
+
+#define KRGNODE_MASK_LAST_WORD BITMAP_LAST_WORD_MASK(KERRIGHED_MAX_NODES)
+
+#if KERRIGHED_MAX_NODES <= BITS_PER_LONG
+
+#define KRGNODE_MASK_ALL							\
+(krgnodemask_t) { {								\
+	[BITS_TO_LONGS(KERRIGHED_MAX_NODES)-1] = KRGNODE_MASK_LAST_WORD			\
+} }
+
+#else
+
+#define KRGNODE_MASK_ALL							\
+(krgnodemask_t) { {								\
+	[0 ... BITS_TO_LONGS(KERRIGHED_MAX_NODES)-2] = ~0UL,			\
+	[BITS_TO_LONGS(KERRIGHED_MAX_NODES)-1] = KRGNODE_MASK_LAST_WORD			\
+} }
+
+#endif
+
+#define KRGNODE_MASK_NONE							\
+(krgnodemask_t) { {								\
+	[0 ... BITS_TO_LONGS(KERRIGHED_MAX_NODES)-1] =  0UL				\
+} }
+
+#define KRGNODE_MASK_NODE0							\
+(krgnodemask_t) { {								\
+	[0] =  1UL							\
+} }
+
+#define krgnodes_addr(src) ((src).bits)
+
+#define krgnodemask_scnprintf(buf, len, src) \
+			__krgnodemask_scnprintf((buf), (len), &(src), KERRIGHED_MAX_NODES)
+static inline int __krgnodemask_scnprintf(char *buf, int len,
+					const krgnodemask_t *srcp, int nbits)
+{
+	return bitmap_scnprintf(buf, len, srcp->bits, nbits);
+}
+
+#define krgnodemask_parse_user(ubuf, ulen, dst) \
+			__krgnodemask_parse_user((ubuf), (ulen), &(dst), KERRIGHED_MAX_NODES)
+static inline int __krgnodemask_parse_user(const char __user *buf, int len,
+					krgnodemask_t *dstp, int nbits)
+{
+	return bitmap_parse_user(buf, len, dstp->bits, nbits);
+}
+
+#define krgnodelist_scnprintf(buf, len, src) \
+			__krgnodelist_scnprintf((buf), (len), &(src), KERRIGHED_MAX_NODES)
+static inline int __krgnodelist_scnprintf(char *buf, int len,
+					const krgnodemask_t *srcp, int nbits)
+{
+	return bitmap_scnlistprintf(buf, len, srcp->bits, nbits);
+}
+
+#define krgnodelist_parse(buf, dst) __krgnodelist_parse((buf), &(dst), KERRIGHED_MAX_NODES)
+static inline int __krgnodelist_parse(const char *buf, krgnodemask_t *dstp, int nbits)
+{
+	return bitmap_parselist(buf, dstp->bits, nbits);
+}
+
+#define krgnode_remap(oldbit, old, new) \
+		__krgnode_remap((oldbit), &(old), &(new), KERRIGHED_MAX_NODES)
+static inline int __krgnode_remap(int oldbit,
+		const krgnodemask_t *oldp, const krgnodemask_t *newp, int nbits)
+{
+	return bitmap_bitremap(oldbit, oldp->bits, newp->bits, nbits);
+}
+
+#define krgnodes_remap(dst, src, old, new) \
+		__krgnodes_remap(&(dst), &(src), &(old), &(new), KERRIGHED_MAX_NODES)
+static inline void __krgnodes_remap(krgnodemask_t *dstp, const krgnodemask_t *srcp,
+		const krgnodemask_t *oldp, const krgnodemask_t *newp, int nbits)
+{
+	bitmap_remap(dstp->bits, srcp->bits, oldp->bits, newp->bits, nbits);
+}
+
+#if KERRIGHED_MAX_NODES > 1
+#define for_each_krgnode_mask(node, mask)		\
+	for ((node) = first_krgnode(mask);		\
+		(node) < KERRIGHED_MAX_NODES;		\
+		(node) = next_krgnode((node), (mask)))
+#define __for_each_krgnode_mask(node, mask)		\
+	for ((node) = __first_krgnode(mask);		\
+		(node) < KERRIGHED_MAX_NODES;		\
+		(node) = __next_krgnode((node), (mask)))
+
+#else /* KERRIGHED_MAX_NODES == 1 */
+#define for_each_krgnode_mask(node, mask)		\
+	for ((node) = kerrighed_node_id; (node) < (kerrighed_node_id+1); (node)++, (void)mask)
+#define __for_each_krgnode_mask(node, mask)		\
+	for ((node) = kerrighed_node_id; (node) < (kerrighed_node_id+1); (node)++, (void)mask)
+#endif /* KERRIGHED_MAX_NODES */
+
+#define next_krgnode_in_ring(node, v) __next_krgnode_in_ring(node, &(v))
+static inline kerrighed_node_t __next_krgnode_in_ring(kerrighed_node_t node,
+						      const krgnodemask_t *v)
+{
+	kerrighed_node_t res;
+	res = __next_krgnode(node, v);
+
+	if (res < KERRIGHED_MAX_NODES)
+		return res;
+
+	return __first_krgnode(v);
+}
+
+#define nth_krgnode(node, v) __nth_krgnode(node, &(v))
+static inline kerrighed_node_t __nth_krgnode(kerrighed_node_t node,
+					     const krgnodemask_t *v)
+{
+	kerrighed_node_t iter;
+
+	iter = __first_krgnode(v);
+	while (node > 0) {
+		iter = __next_krgnode(iter, v);
+		node--;
+	}
+
+	return iter;
+}
+
+/** Return true if the index is the only one set in the vector */
+#define krgnode_is_unique(node, v) __krgnode_is_unique(node, &(v))
+static inline int __krgnode_is_unique(kerrighed_node_t node,
+				      const krgnodemask_t *v)
+{
+  int i;
+  
+  i = __first_krgnode(v);
+  if(i != node) return 0;
+  
+  i = __next_krgnode(node, v);
+  if(i != KERRIGHED_MAX_NODES) return 0;
+  
+  return 1;
+}
+
+/*
+ * krgnode_online_map: list of nodes available as object injection target
+ * krgnode_present_map: list of nodes ready to be added in a cluster
+ * krgnode_possible_map: list of nodes that may join the cluster in the future
+ */
+
+extern krgnodemask_t krgnode_possible_map;
+extern krgnodemask_t krgnode_online_map;
+extern krgnodemask_t krgnode_present_map;
+
+#if KERRIGHED_MAX_NODES > 1
+#define num_online_krgnodes()	krgnodes_weight(krgnode_online_map)
+#define num_possible_krgnodes()	krgnodes_weight(krgnode_possible_map)
+#define num_present_krgnodes()	krgnodes_weight(krgnode_present_map)
+#define krgnode_online(node)	krgnode_isset((node), krgnode_online_map)
+#define krgnode_possible(node)	krgnode_isset((node), krgnode_possible_map)
+#define krgnode_present(node)	krgnode_isset((node), krgnode_present_map)
+
+#define any_online_krgnode(mask) __any_online_krgnode(&(mask))
+int __any_online_krgnode(const krgnodemask_t *mask);
+
+#else
+
+#define num_online_krgnodes()	1
+#define num_possible_krgnodes()	1
+#define num_present_krgnodes()	1
+#define krgnode_online(node)		((node) == kerrighed_node_id)
+#define krgnode_possible(node)	((node) == kerrighed_node_id)
+#define krgnode_present(node)	((node) == kerrighed_node_id)
+
+#define any_online_krgnode(mask)		kerrighed_node_id
+#endif
+
+#define for_each_possible_krgnode(node)  for_each_krgnode_mask((node), krgnode_possible_map)
+#define for_each_online_krgnode(node)  for_each_krgnode_mask((node), krgnode_online_map)
+#define for_each_present_krgnode(node) for_each_krgnode_mask((node), krgnode_present_map)
+
+#define set_krgnode_possible(node) krgnode_set(node, krgnode_possible_map)
+#define set_krgnode_online(node)   krgnode_set(node, krgnode_online_map)
+#define set_krgnode_present(node)  krgnode_set(node, krgnode_present_map)
+
+#define clear_krgnode_possible(node) krgnode_clear(node, krgnode_possible_map)
+#define clear_krgnode_online(node)   krgnode_clear(node, krgnode_online_map)
+#define clear_krgnode_present(node)  krgnode_clear(node, krgnode_present_map)
+
+#define nth_possible_krgnode(node) nth_krgnode(node, krgnode_possible_map)
+#define nth_online_krgnode(node) nth_krgnode(node, krgnode_online_map)
+#define nth_present_krgnode(node) nth_krgnode(node, krgnode_present_map)
+
+#define krgnode_next_possible(node) next_krgnode(node, krgnode_possible_map)
+#define krgnode_next_online(node) next_krgnode(node, krgnode_online_map)
+#define krgnode_next_present(node) next_krgnode(node, krgnode_present_map)
+
+#define krgnode_next_possible_in_ring(node) next_krgnode_in_ring(node, krgnode_possible_map)
+#define krgnode_next_online_in_ring(node) next_krgnode_in_ring(node, krgnode_online_map)
+#define krgnode_next_present_in_ring(node) next_krgnode_in_ring(node, krgnode_present_map)
+
+#endif /* __KRG_NODEMASK_H */
diff -ruN linux-2.6.29/include/kerrighed/krg_services.h android_cluster/linux-2.6.29/include/kerrighed/krg_services.h
--- linux-2.6.29/include/kerrighed/krg_services.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/krg_services.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,205 @@
+#ifndef __KRG_SERVICES__
+#define __KRG_SERVICES__
+
+#include <linux/ioctl.h>
+#include <kerrighed/types.h>
+#include <kerrighed/krgnodemask.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                 MACROS                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+#define KERRIGHED_PROC_MAGIC 0xD1
+
+#define TOOLS_PROC_BASE 0
+#define COMM_PROC_BASE 32
+#define KERMM_PROC_BASE 96
+#define KERPROC_PROC_BASE 128
+#define EPM_PROC_BASE 192
+#define IPC_PROC_BASE 224
+
+/*
+ * Tools related Kerrighed syscalls
+ */
+
+#define KSYS_SET_CAP          _IOW(KERRIGHED_PROC_MAGIC, \
+                                   TOOLS_PROC_BASE + 0, \
+                                   krg_cap_t )
+
+#define KSYS_GET_CAP          _IOR(KERRIGHED_PROC_MAGIC, \
+                                   TOOLS_PROC_BASE + 1, \
+                                   krg_cap_t )
+
+#define KSYS_SET_PID_CAP      _IOW(KERRIGHED_PROC_MAGIC, \
+                                   TOOLS_PROC_BASE + 2, \
+                                   krg_cap_pid_t )
+
+#define KSYS_GET_PID_CAP      _IOR(KERRIGHED_PROC_MAGIC, \
+                                   TOOLS_PROC_BASE + 3, \
+                                   krg_cap_pid_t )
+
+#define KSYS_SET_FATHER_CAP   _IOW(KERRIGHED_PROC_MAGIC, \
+                                   TOOLS_PROC_BASE + 4, \
+                                   krg_cap_t )
+
+#define KSYS_GET_FATHER_CAP   _IOR(KERRIGHED_PROC_MAGIC, \
+                                   TOOLS_PROC_BASE + 5, \
+                                   krg_cap_t )
+
+#define KSYS_NB_MAX_NODES     _IOR(KERRIGHED_PROC_MAGIC, \
+                                   TOOLS_PROC_BASE + 6,  \
+				   int)
+#define KSYS_NB_MAX_CLUSTERS  _IOR(KERRIGHED_PROC_MAGIC, \
+                                   TOOLS_PROC_BASE + 7,  \
+				   int)
+
+#define KSYS_GET_SUPPORTED_CAP	_IOR(KERRIGHED_PROC_MAGIC, \
+				     TOOLS_PROC_BASE + 8, \
+				     int)
+
+/*
+ * Communications related Kerrighed syscalls
+ */
+
+#define KSYS_GET_NODE_ID       _IOR(KERRIGHED_PROC_MAGIC, \
+				    COMM_PROC_BASE + 0, \
+                                    int)
+#define KSYS_GET_NODES_COUNT   _IOR(KERRIGHED_PROC_MAGIC, \
+				    COMM_PROC_BASE + 1,   \
+                                    int)
+/* Removed: #define KSYS_HOTPLUG_START     _IOW(KERRIGHED_PROC_MAGIC, \ */
+/*                                              COMM_PROC_BASE + 3,   \ */
+/*                                              __krgnodemask_t) */
+#define KSYS_HOTPLUG_RESTART   _IOW(KERRIGHED_PROC_MAGIC, \
+                                    COMM_PROC_BASE + 4,   \
+                                    __krgnodemask_t)
+#define KSYS_HOTPLUG_SHUTDOWN  _IOW(KERRIGHED_PROC_MAGIC, \
+                                    COMM_PROC_BASE + 5,   \
+                                    __krgnodemask_t)
+#define KSYS_HOTPLUG_REBOOT    _IOW(KERRIGHED_PROC_MAGIC, \
+                                    COMM_PROC_BASE + 6,   \
+                                    __krgnodemask_t)
+#define KSYS_HOTPLUG_STATUS    _IOR(KERRIGHED_PROC_MAGIC, \
+                                    COMM_PROC_BASE + 7,   \
+				    struct hotplug_clusters)
+#define KSYS_HOTPLUG_ADD       _IOW(KERRIGHED_PROC_MAGIC, \
+                                    COMM_PROC_BASE + 8,   \
+                                    struct __hotplug_node_set)
+#define KSYS_HOTPLUG_REMOVE    _IOW(KERRIGHED_PROC_MAGIC, \
+                                    COMM_PROC_BASE + 9,   \
+                                    struct __hotplug_node_set)
+#define KSYS_HOTPLUG_FAIL      _IOW(KERRIGHED_PROC_MAGIC, \
+                                    COMM_PROC_BASE + 10,   \
+                                    struct __hotplug_node_set)
+#define KSYS_HOTPLUG_NODES     _IOWR(KERRIGHED_PROC_MAGIC, \
+                                     COMM_PROC_BASE + 11,  \
+				     struct hotplug_nodes)
+#define KSYS_HOTPLUG_POWEROFF  _IOW(KERRIGHED_PROC_MAGIC, \
+                                    COMM_PROC_BASE + 12,  \
+				    struct __hotplug_node_set)
+/* Removed: #define KSYS_HOTPLUG_WAIT_FOR_START _IO(KERRIGHED_PROC_MAGIC, \ */
+/*                                                  COMM_PROC_BASE + 13) */
+#define KSYS_HOTPLUG_SET_CREATOR	_IO(KERRIGHED_PROC_MAGIC, \
+					    COMM_PROC_BASE + 14)
+#define KSYS_HOTPLUG_READY		_IO(KERRIGHED_PROC_MAGIC, \
+					    COMM_PROC_BASE + 15)
+
+
+/*
+ *  Memory related Kerrighed syscalls
+ */
+
+#define KSYS_CHANGE_MAP_LOCAL_VALUE  _IOW(KERRIGHED_PROC_MAGIC, \
+			                  KERMM_PROC_BASE + 0, \
+					   struct kermm_new_local_data)
+
+/*
+ * Enhanced Process Management related kerrighed syscalls
+ */
+
+#define KSYS_PROCESS_MIGRATION         _IOW(KERRIGHED_PROC_MAGIC, \
+                                            EPM_PROC_BASE + 0, \
+                                            migration_infos_t)
+#define KSYS_THREAD_MIGRATION	       _IOW(KERRIGHED_PROC_MAGIC, \
+                                            EPM_PROC_BASE + 1,\
+                                            migration_infos_t)
+#define KSYS_APP_FREEZE                _IOW(KERRIGHED_PROC_MAGIC, \
+                                            EPM_PROC_BASE + 2, \
+                                            struct checkpoint_info)
+#define KSYS_APP_UNFREEZE              _IOW(KERRIGHED_PROC_MAGIC, \
+                                            EPM_PROC_BASE + 3, \
+                                            struct checkpoint_info)
+#define KSYS_APP_CHKPT                 _IOW(KERRIGHED_PROC_MAGIC, \
+                                            EPM_PROC_BASE + 4, \
+                                            struct checkpoint_info)
+#define KSYS_APP_RESTART               _IOW(KERRIGHED_PROC_MAGIC, \
+                                            EPM_PROC_BASE + 5, \
+                                            struct restart_request)
+#define KSYS_APP_SET_USERDATA          _IOW(KERRIGHED_PROC_MAGIC, \
+                                            EPM_PROC_BASE + 6, \
+                                            __u64)
+#define KSYS_APP_GET_USERDATA          _IOW(KERRIGHED_PROC_MAGIC, \
+                                            EPM_PROC_BASE + 7, \
+                                            struct app_userdata_request)
+#define KSYS_APP_CR_DISABLE		_IO(KERRIGHED_PROC_MAGIC, \
+					   EPM_PROC_BASE + 8)
+#define KSYS_APP_CR_ENABLE		_IO(KERRIGHED_PROC_MAGIC, \
+					   EPM_PROC_BASE + 9)
+#define KSYS_APP_CR_EXCLUDE		_IOW(KERRIGHED_PROC_MAGIC,	\
+					     EPM_PROC_BASE + 10,	\
+					     struct cr_mm_region)
+
+
+/*
+ * IPC related kerrighed syscalls
+ */
+#define KSYS_IPC_MSGQ_CHKPT            _IOW(KERRIGHED_PROC_MAGIC,       \
+					    IPC_PROC_BASE + 0,		\
+					    int[2])
+#define KSYS_IPC_MSGQ_RESTART          _IOW(KERRIGHED_PROC_MAGIC, \
+					    IPC_PROC_BASE + 1,	  \
+					    int)
+#define KSYS_IPC_SEM_CHKPT             _IOW(KERRIGHED_PROC_MAGIC,       \
+					    IPC_PROC_BASE + 2,		\
+					    int[2])
+#define KSYS_IPC_SEM_RESTART           _IOW(KERRIGHED_PROC_MAGIC, \
+					    IPC_PROC_BASE + 3,	  \
+					    int)
+#define KSYS_IPC_SHM_CHKPT             _IOW(KERRIGHED_PROC_MAGIC,       \
+					    IPC_PROC_BASE + 4,		\
+					    int[2])
+#define KSYS_IPC_SHM_RESTART           _IOW(KERRIGHED_PROC_MAGIC, \
+					    IPC_PROC_BASE + 5,	  \
+					    int)
+
+/*
+ * HotPlug
+ */
+
+struct hotplug_nodes {
+	char *nodes;
+};
+
+struct hotplug_clusters {
+	char clusters[KERRIGHED_MAX_CLUSTERS];
+};
+
+/* __hotplug_node_set is the ioctl parameter (sized by KERRIGHED_HARD_MAX_NODES)
+   hotplug_node_set is the structure actually used by kernel (size by KERRIGHED_MAX_NODES)
+*/
+struct __hotplug_node_set {
+	int subclusterid;
+	__krgnodemask_t v;
+};
+
+
+/*
+ * Ctnr
+ */
+typedef struct kermm_new_local_data {
+	unsigned long data_place;
+} kermm_new_local_data_t;
+
+#endif
diff -ruN linux-2.6.29/include/kerrighed/krgsyms.h android_cluster/linux-2.6.29/include/kerrighed/krgsyms.h
--- linux-2.6.29/include/kerrighed/krgsyms.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/krgsyms.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,60 @@
+#ifndef __KRGSYMS__
+#define __KRGSYMS__
+
+#ifndef __ASSEMBLY__
+
+typedef enum krgsyms_val {
+	KRGSYMS_UNDEF, // Must be the first one
+	KRGSYMS_VM_OPS_NULL,
+	KRGSYMS_VM_OPS_SHM,
+	KRGSYMS_VM_OPS_SHMEM,
+	KRGSYMS_VM_OPS_FILE_GENERIC,
+	KRGSYMS_VM_OPS_NFS_FILE,
+	KRGSYMS_VM_OPS_OCFS2_FILE,
+	KRGSYMS_VM_OPS_SPECIAL_MAPPING,
+	KRGSYMS_VM_OPS_MEMORY_KDDM_VMOPS,
+	KRGSYMS_ARCH_UNMAP_AREA,
+	KRGSYMS_ARCH_UNMAP_AREA_TOPDOWN,
+	KRGSYMS_ARCH_GET_UNMAP_AREA,
+	KRGSYMS_ARCH_GET_UNMAP_AREA_TOPDOWN,
+
+	/* Bin format structures */
+	KRGSYMS_BINFMTS_AOUT,
+	KRGSYMS_BINFMTS_ELF,
+	KRGSYMS_BINFMTS_ELF_FDPIC,
+	KRGSYMS_BINFMTS_EM86,
+	KRGSYMS_BINFMTS_FLAT,
+	KRGSYMS_BINFMTS_MISC,
+	KRGSYMS_BINFMTS_SCRIPT,
+	KRGSYMS_BINFMTS_SOM,
+	KRGSYMS_BINFMTS_ARCH,
+
+	/* Restart block functions */
+	KRGSYMS_DO_NO_RESTART_SYSCALL,
+	KRGSYMS_COMPAT_NANOSLEEP_RESTART,
+	KRGSYMS_COMPAT_CLOCK_NANOSLEEP_RESTART,
+	KRGSYMS_HRTIMER_NANOSLEEP_RESTART,
+	KRGSYMS_POSIX_CPU_NSLEEP_RESTART,
+	KRGSYMS_DO_RESTART_POLL,
+	KRGSYMS_FUTEX_WAIT_RESTART,
+
+	/* KDDM set operations */
+	KRGSYMS_KDDM_TREE_OPS,
+	KRGSYMS_KDDM_PT_OPS,
+
+	/* DVFS mobility operations */
+	KRGSYMS_DVFS_MOBILITY_FAF_OPS,
+	KRGSYMS_DVFS_MOBILITY_REGULAR_OPS,
+
+	KRGSYMS_TABLE_SIZE // Must be the last one
+} krgsyms_val_t;
+
+int krgsyms_register(enum krgsyms_val v, void* p);
+int krgsyms_unregister(enum krgsyms_val v);
+
+enum krgsyms_val krgsyms_export(void* p);
+void* krgsyms_import(enum krgsyms_val v);
+
+#endif /* __ASSEMBLY__ */
+
+#endif /* __KRGSYMS__ */
diff -ruN linux-2.6.29/include/kerrighed/krg_syscalls.h android_cluster/linux-2.6.29/include/kerrighed/krg_syscalls.h
--- linux-2.6.29/include/kerrighed/krg_syscalls.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/krg_syscalls.h	2014-05-27 23:04:10.246031664 -0700
@@ -0,0 +1,42 @@
+#ifndef __KRG_SYSCALLS__
+
+#define __KRG_SYSCALLS__
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+typedef int (*proc_service_function_t) (void *arg);
+
+struct proc_service_entry {
+	proc_service_function_t fct;
+	char label[32];
+	unsigned long count;
+	bool restricted;
+};
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+extern struct proc_dir_entry *proc_services;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+int __register_proc_service(unsigned int cmd, proc_service_function_t fun,
+			    bool restricted);
+int register_proc_service(unsigned int cmd, proc_service_function_t fun);
+int unregister_proc_service(unsigned int cmd);
+
+int krg_syscalls_init(void);
+int krg_syscalls_finalize(void);
+
+#endif				/* __KRG_SYSCALLS__ */
diff -ruN linux-2.6.29/include/kerrighed/libproc.h android_cluster/linux-2.6.29/include/kerrighed/libproc.h
--- linux-2.6.29/include/kerrighed/libproc.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/libproc.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,10 @@
+#ifndef __LIBPROC_H__
+#define __LIBPROC_H__
+
+#include <kddm/io_linker.h>
+
+kerrighed_node_t global_pid_default_owner(struct kddm_set *set, objid_t objid,
+					  const krgnodemask_t *nodes,
+					  int nr_nodes);
+
+#endif /* __LIBPROC_H__ */
diff -ruN linux-2.6.29/include/kerrighed/migration.h android_cluster/linux-2.6.29/include/kerrighed/migration.h
--- linux-2.6.29/include/kerrighed/migration.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/migration.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,60 @@
+/*
+ *  Migration interface API.
+ *  @file migration.h
+ *
+ *  Implementation of migration functions.
+ *
+ *  @author Geoffroy Vallée
+ */
+
+#ifndef __KRG_MIGRATION_H__
+#define __KRG_MIGRATION_H__
+
+#include <linux/types.h>
+#include <kerrighed/sys/types.h>
+
+struct migration_infos_struct {
+	kerrighed_node_t destination_node_id;
+	union {
+		pid_t process_to_migrate;
+		pid_t thread_to_migrate;
+	};
+};
+
+typedef struct migration_infos_struct migration_infos_t;
+
+#ifdef CONFIG_KRG_EPM
+
+#if defined(CONFIG_KRG_SCHED) && defined(CONFIG_MODULE_HOOK)
+#include <linux/module_hook.h>
+
+extern struct module_hook_desc kmh_migration_start;
+extern struct module_hook_desc kmh_migration_aborted;
+extern struct module_hook_desc kmh_migration_end;
+#endif
+
+struct task_struct;
+
+int __may_migrate(struct task_struct *task);
+int may_migrate(struct task_struct *task);
+
+enum migration_scope {
+	MIGR_THREAD,		/* A single task */
+	MIGR_LOCAL_PROCESS,	/* All local threads of a thread group */
+	MIGR_GLOBAL_PROCESS,	/* All threads (even those
+				 * running on other nodes) of a thread group */
+};
+
+int __migrate_linux_threads(struct task_struct *task_to_migrate,
+			    enum migration_scope scope,
+			    kerrighed_node_t dest_node);
+int migrate_linux_threads(pid_t pid,
+			  enum migration_scope scope,
+			  kerrighed_node_t dest_node);
+
+/* Used by krg_release_task() */
+void migration_aborted(struct task_struct *tsk);
+
+#endif /* CONFIG_KRG_EPM */
+
+#endif /* __KRG_MIGRATION_H__ */
diff -ruN linux-2.6.29/include/kerrighed/mm.h android_cluster/linux-2.6.29/include/kerrighed/mm.h
--- linux-2.6.29/include/kerrighed/mm.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/mm.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,121 @@
+#ifndef __KKRG_MM__
+#define __KKRG_MM__
+
+#include <linux/err.h>
+#include <linux/fs.h>
+#include <linux/sched.h>
+
+#ifdef CONFIG_USERMODE
+#ifdef PTE_MASK
+// At this time (2.6.11) PTE_MASK is not defined in UM, so as soon as this
+// will be defined, we will remove this part
+#warning PTE_MASK already defined
+#else
+#define PTE_MASK PAGE_MASK
+#endif
+#endif
+
+/** Exported Functions **/
+
+int alloc_ldt(mm_context_t *pc, int mincount, int reload) ;
+void exit_mm(struct task_struct * tsk);
+struct vm_area_struct *remove_vma(struct vm_area_struct *vma);
+#define allocate_mm()	(kmem_cache_alloc(mm_cachep, GFP_KERNEL))
+#define free_mm(mm)	(kmem_cache_free(mm_cachep, (mm)))
+struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p);
+int __dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm, int anon_only);
+void detach_vmas_to_be_unmapped(struct mm_struct *mm,
+				struct vm_area_struct *vma,
+				struct vm_area_struct *prev,
+				unsigned long end);
+void unmap_region(struct mm_struct *mm, struct vm_area_struct *vma,
+		  struct vm_area_struct *prev, unsigned long start,
+		  unsigned long end);
+
+void remove_vma_list(struct mm_struct *mm, struct vm_area_struct *vma);
+
+/** Exported Variables **/
+
+extern struct kmem_cache *mm_cachep;
+extern struct vm_operations_struct generic_file_vm_ops ;
+
+int special_mapping_vm_ops_krgsyms_register(void);
+int special_mapping_vm_ops_krgsyms_unregister(void);
+
+static inline void dump_vma(struct task_struct *tsk)
+{
+	struct vm_area_struct *vma;
+
+	vma = tsk->mm->mmap;
+
+	while(vma) {
+		printk ("[0x%08lx:0x%08lx] - flags 0x%08lx - offset 0x%08lx - "
+			"file %p\n", vma->vm_start, vma->vm_end, vma->vm_flags,
+			vma->vm_pgoff, vma->vm_file);
+
+		vma = vma->vm_next;
+	}
+}
+
+static inline int anon_vma(struct vm_area_struct *vma)
+{
+	if (vma->vm_flags & VM_SHARED)
+		return 0;
+
+	if (!vma->vm_file)
+		return 1;
+
+	return (vma->anon_vma || vma->vm_flags & VM_KDDM);
+}
+
+#ifdef CONFIG_KRG_FAF
+void use_mm(struct mm_struct *mm);
+void unuse_mm(struct mm_struct *mm);
+#endif
+
+void mm_struct_pin(struct mm_struct *mm);
+void mm_struct_unpin(struct mm_struct *mm);
+
+/** Kerrighed Kernel Hooks **/
+
+extern void (*kh_mm_get) (struct mm_struct *mm);
+extern void (*kh_mm_release) (struct mm_struct *mm, int notify);
+
+int krg_do_execve(struct task_struct *tsk, struct mm_struct *mm);
+extern struct mm_struct *(*kh_copy_mm)(struct task_struct *tsk,
+				       struct mm_struct *oldmm,
+				       unsigned long clone_flags);
+
+extern void (*kh_fill_pte)(struct mm_struct *mm, unsigned long addr,
+			   pte_t *pte);
+extern void (*kh_zap_pte)(struct mm_struct *mm, unsigned long addr,
+			  pte_t *pte);
+
+int try_to_flush_page(struct page *page);
+
+void krg_notify_mem(int mem_usage);
+
+void krg_check_vma_link(struct vm_area_struct *vma);
+
+void krg_do_mmap_region(struct vm_area_struct *vma, unsigned long flags,
+			unsigned int vm_flags);
+
+void krg_do_munmap(struct mm_struct *mm, unsigned long start, size_t len);
+
+void krg_do_mremap(struct mm_struct *mm, unsigned long addr,
+		   unsigned long old_len, unsigned long new_len,
+		   unsigned long flags, unsigned long new_addr,
+		   unsigned long _new_addr, unsigned long lock_limit);
+
+void krg_do_brk(struct mm_struct *mm, unsigned long brk,
+		unsigned long lock_limit, unsigned long data_limit);
+
+int krg_expand_stack(struct vm_area_struct *vma, unsigned long address);
+
+void krg_do_mprotect(struct mm_struct *mm, unsigned long start, size_t len,
+		     unsigned long prot, int personality);
+
+#define TestClearPageLRU(page)  test_and_clear_bit(PG_lru, &(page)->flags)
+
+#endif // __KKRG_MM__
+
diff -ruN linux-2.6.29/include/kerrighed/namespace.h android_cluster/linux-2.6.29/include/kerrighed/namespace.h
--- linux-2.6.29/include/kerrighed/namespace.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/namespace.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,38 @@
+#ifndef __KRG_NAMESPACE_H__
+#define __KRG_NAMESPACE_H__
+
+#include <linux/nsproxy.h>
+#include <linux/rcupdate.h>
+#include <asm/atomic.h>
+
+struct task_struct;
+
+struct krg_namespace {
+	atomic_t count;
+	struct nsproxy root_nsproxy;
+	struct user_namespace *root_user_ns;
+	struct task_struct *root_task;
+	struct rcu_head rcu;
+};
+
+int copy_krg_ns(struct task_struct *task, struct nsproxy *new);
+void free_krg_ns(struct krg_namespace *ns);
+
+struct krg_namespace *find_get_krg_ns(void);
+
+static inline void get_krg_ns(struct krg_namespace *ns)
+{
+	atomic_inc(&ns->count);
+}
+
+static inline void put_krg_ns(struct krg_namespace *ns)
+{
+	if (atomic_dec_and_test(&ns->count))
+		free_krg_ns(ns);
+}
+
+bool can_create_krg_ns(unsigned long flags);
+
+void krg_ns_root_exit(struct task_struct *task);
+
+#endif /* __KRG_NAMESPACE_H__ */
diff -ruN linux-2.6.29/include/kerrighed/network_ghost.h android_cluster/linux-2.6.29/include/kerrighed/network_ghost.h
--- linux-2.6.29/include/kerrighed/network_ghost.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/network_ghost.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,38 @@
+/** Network ghost interface
+ *  @file network_ghost.h
+ *
+ *  Definition of network ghost structures and functions.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __NETWORK_GHOST_H__
+#define __NETWORK_GHOST_H__
+
+#include <kerrighed/ghost_types.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+struct rpc_desc;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+/** Create a network file ghost.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  access Ghost access (READ/WRITE)
+ *  @param  desc   RPC descriptor to send/receive data on.
+ *
+ *  @return        0 if everything ok
+ *                 Negative value otherwise.
+ */
+ghost_t * create_network_ghost(int access, struct rpc_desc *desc);
+
+#endif /* __NETWORK_GHOST_H__ */
diff -ruN linux-2.6.29/include/kerrighed/page_table_tree.h android_cluster/linux-2.6.29/include/kerrighed/page_table_tree.h
--- linux-2.6.29/include/kerrighed/page_table_tree.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/page_table_tree.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,76 @@
+/** KDDM object tree based on page tables.
+ *  @file page_table_tree.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __PAGE_TABLE_TREE__
+#define __PAGE_TABLE_TREE__
+
+#include <kddm/kddm_types.h>
+#include <linux/pagemap.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN VARIABLES                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+extern struct kddm_set_ops kddm_pt_set_ops;
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+static inline unsigned long mk_swap_pte_page(pte_t *ptep)
+{
+	return (pte_val(*ptep) | 1);
+}
+
+static inline unsigned long swap_pte_page(struct page *page)
+{
+	return ((unsigned long) page) & 1 ;
+}
+
+struct kddm_obj *get_obj_entry_from_pte(struct mm_struct *mm,
+					unsigned long addr, pte_t *ptep,
+					struct kddm_obj *new_obj);
+
+static inline swp_entry_t get_swap_entry_from_page(struct page *page)
+{
+	pte_t pte;
+
+	pte = __pte(((unsigned long) page) & ~1UL);
+	return pte_to_swp_entry(pte);
+}
+
+static inline void wait_lock_page (struct page *page)
+{
+	//while (TestSetPageLocked(page))
+	while (trylock_page(page))
+		cpu_relax();
+}
+
+/* Used to ensure atomicity of operations on kddm_count and obj_entry fields */
+static inline void wait_lock_kddm_page (struct page *page)
+{
+       while (TestSetPageLockedKDDM(page))
+		cpu_relax();
+}
+
+static inline void unlock_kddm_page (struct page *page)
+{
+	ClearPageLockedKDDM(page);
+}
+
+int kddm_pt_invalidate (struct kddm_set *set, objid_t objid,
+			struct kddm_obj *obj_entry, struct page *page);
+
+int kddm_pt_swap_in (struct mm_struct *mm, unsigned long addr, pte_t *orig_pte);
+
+#endif // __PAGE_TABLE_TREE__
diff -ruN linux-2.6.29/include/kerrighed/physical_fs.h android_cluster/linux-2.6.29/include/kerrighed/physical_fs.h
--- linux-2.6.29/include/kerrighed/physical_fs.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/physical_fs.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,43 @@
+/** Access to Physical File System management.
+ *  @file physical_fs.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __PHYSICAL_FS__
+#define __PHYSICAL_FS__
+
+#include <linux/path.h>
+#include <linux/types.h>
+
+struct nsproxy;
+struct file;
+
+struct prev_root {
+	struct path path;
+	struct nsproxy *nsproxy;
+};
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+char *physical_d_path(const struct path *path, char *tmp, bool del_ok);
+
+void get_physical_root(struct path *root);
+
+void chroot_to_physical_root(struct prev_root *prev_root);
+void chroot_to_prev_root(const struct prev_root *prev_root);
+
+struct file *open_physical_file(char *filename,
+				int flags, int mode, uid_t fsuid, gid_t fsgid);
+
+int close_physical_file(struct file *file);
+
+int remove_physical_file(struct file *file);
+
+int remove_physical_dir(struct file *file);
+
+#endif // __PHYSICAL_FS__
diff -ruN linux-2.6.29/include/kerrighed/pid.h android_cluster/linux-2.6.29/include/kerrighed/pid.h
--- linux-2.6.29/include/kerrighed/pid.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/pid.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,184 @@
+#ifndef __KERRIGHED_PID_H__
+#define __KERRIGHED_PID_H__
+
+#ifdef CONFIG_KRG_PROC
+
+#include <asm/page.h> /* Needed by linux/threads.h */
+#include <linux/pid_namespace.h>
+#include <linux/threads.h>
+#include <linux/types.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/krgnodemask.h>
+
+/*
+ * WARNING: procfs and futex need at least the 2 MSbits free (in procfs: 1 for
+ * sign, 1 for upper pid limit; in futex: see linux/futex.h)
+ */
+
+#define GLOBAL_PID_MASK PID_MAX_LIMIT
+#define PID_NODE_SHIFT (NR_BITS_PID_MAX_LIMIT + 1)
+#define INTERNAL_PID_MASK (PID_MAX_LIMIT - 1)
+
+#define GLOBAL_PID_NODE(pid, node) \
+	(((node) << PID_NODE_SHIFT)|GLOBAL_PID_MASK|((pid) & INTERNAL_PID_MASK))
+#define GLOBAL_PID(pid) GLOBAL_PID_NODE(pid, kerrighed_node_id)
+
+/** extract the original linux kernel pid of a Kerrighed PID */
+#define SHORT_PID(pid) ((pid) & INTERNAL_PID_MASK)
+/** extract the original node id of a Kerrighed PID */
+#define ORIG_NODE(pid) ((pid) >> PID_NODE_SHIFT)
+
+#define KERRIGHED_PID_MAX_LIMIT GLOBAL_PID_NODE(0, KERRIGHED_MAX_NODES)
+
+/* Kerrighed container's PID numbers */
+static inline pid_t pid_knr(struct pid *pid)
+{
+	struct pid_namespace *ns = ns_of_pid(pid);
+	if (ns && ns->krg_ns_root)
+		return pid_nr_ns(pid, ns->krg_ns_root);
+	return 0;
+}
+
+static inline pid_t task_pid_knr(struct task_struct *task)
+{
+	return pid_knr(task_pid(task));
+}
+
+static inline pid_t task_tgid_knr(struct task_struct *task)
+{
+	return pid_knr(task_tgid(task));
+}
+
+static inline pid_t task_pgrp_knr(struct task_struct *task)
+{
+	return pid_knr(task_pgrp(task));
+}
+
+static inline pid_t task_session_knr(struct task_struct *task)
+{
+	return pid_knr(task_session(task));
+}
+
+static inline struct pid *find_kpid(int nr)
+{
+	struct pid_namespace *ns = find_get_krg_pid_ns();
+	struct pid *pid = find_pid_ns(nr, ns);
+	put_pid_ns(ns);
+	return pid;
+}
+
+static inline struct task_struct *find_task_by_kpid(pid_t pid)
+{
+	return pid_task(find_kpid(pid), PIDTYPE_PID);
+}
+
+/* PID location */
+#ifdef CONFIG_KRG_EPM
+int krg_set_pid_location(struct task_struct *task);
+int krg_unset_pid_location(struct task_struct *task);
+#endif
+kerrighed_node_t krg_lock_pid_location(pid_t pid);
+void krg_unlock_pid_location(pid_t pid);
+
+/* Global PID, foreign pidmap aware iterator */
+struct pid *krg_find_ge_pid(int nr, struct pid_namespace *pid_ns,
+			    struct pid_namespace *pidmap_ns);
+
+#else /* !CONFIG_KRG_PROC */
+
+static inline pid_t pid_knr(struct pid *pid)
+{
+	return pid_nr(pid);
+}
+
+static
+inline pid_t __task_pid_knr(struct task_struct *task, enum pid_type type)
+{
+	return __task_pid_nr_ns(task, type, &init_pid_ns);
+}
+
+static inline pid_t task_pid_knr(struct task_struct *task)
+{
+	return task->pid;
+}
+
+static inline pid_t task_tgid_knr(struct task_struct *task)
+{
+	return task->tgid;
+}
+
+static inline pid_t task_pgrp_knr(struct task_struct *task)
+{
+	return __task_pid_knr(task, PIDTYPE_PGID);
+}
+
+static inline pid_t task_session_knr(struct task_struct *task)
+{
+	return __task_pid_knr(task, PIDTYPE_SID);
+}
+
+static inline struct pid *find_kpid(int nr)
+{
+	return find_pid_ns(nr, &init_pid_ns);
+}
+
+static inline struct task_struct *find_task_by_kpid(pid_t pid)
+{
+	return find_task_by_pid_ns(pid, &init_pid_ns);
+}
+
+#endif /* !CONFIG_KRG_PROC */
+
+#ifdef CONFIG_KRG_EPM
+
+/* Task KDDM object link */
+struct pid_kddm_object;
+struct task_kddm_object;
+struct pid;
+
+/* Must be called under rcu_read_lock() */
+struct task_kddm_object *krg_pid_task(struct pid *pid);
+
+/* Must be called under rcu_read_lock() */
+void krg_pid_unlink_task(struct pid_kddm_object *obj);
+
+/* Pid reference tracking */
+struct pid *krg_get_pid(int nr);
+void krg_end_get_pid(struct pid *pid);
+void krg_put_pid(struct pid *pid);
+
+/* Foreign pidmaps */
+int pidmap_map_read_lock(void);
+void pidmap_map_read_unlock(void);
+kerrighed_node_t pidmap_node(kerrighed_node_t node);
+struct pid_namespace *node_pidmap(kerrighed_node_t node);
+
+void pidmap_map_cleanup(struct krg_namespace *krg_ns);
+
+void krg_free_pidmap(struct upid *upid);
+
+#elif defined(CONFIG_KRG_PROC)
+
+static inline int pidmap_map_read_lock(void)
+{
+	return 0;
+}
+
+static inline void pidmap_map_read_unlock(void)
+{
+}
+
+static inline kerrighed_node_t pidmap_node(kerrighed_node_t node)
+{
+	return krgnode_online(node) ? node : KERRIGHED_NODE_ID_NONE;
+}
+
+static inline struct pid_namespace *node_pidmap(kerrighed_node_t node)
+{
+	return NULL;
+}
+
+#endif /* CONFIG_KRG_EPM */
+
+#endif /* __KERRIGHED_PID_H__ */
diff -ruN linux-2.6.29/include/kerrighed/procfs.h android_cluster/linux-2.6.29/include/kerrighed/procfs.h
--- linux-2.6.29/include/kerrighed/procfs.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/procfs.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,29 @@
+#ifndef __TOOLS_PROCFS__
+#define __TOOLS_PROCFS__
+
+#ifdef __KERNEL__
+
+#include <linux/ioctl.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+extern struct proc_dir_entry *proc_kerrighed;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+int kerrighed_proc_init(void);
+void kerrighed_proc_finalize(void);
+
+void procfs_deltree(struct proc_dir_entry *entry);
+
+#endif				//  __KERNEL__
+
+#endif				/* __TOOLS_PROCFS__ */
diff -ruN linux-2.6.29/include/kerrighed/regular_file_mgr.h android_cluster/linux-2.6.29/include/kerrighed/regular_file_mgr.h
--- linux-2.6.29/include/kerrighed/regular_file_mgr.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/regular_file_mgr.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,91 @@
+/** Global management of regular files interface.
+ *  @file regular_file_mgr.h
+ *
+ *  @author Renaud Lottiaux
+ */
+#ifndef __REGULAR_FILE_MGR__
+#define __REGULAR_FILE_MGR__
+
+#include <kddm/kddm_types.h>
+#include <kerrighed/ghost.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+enum file_krg_type {
+	FILE,
+	PIPE,
+	SHM
+};
+
+struct regular_file_krg_desc {
+	enum file_krg_type type;
+	union {
+		struct {
+			fmode_t f_mode;
+			int shmid;
+		} shm;
+		struct {
+			unsigned long f_flags;
+			long key;
+		} pipe;
+		struct {
+			umode_t mode;
+			loff_t pos;
+			unsigned int flags;
+			unsigned int uid;
+			unsigned int gid;
+			kddm_set_id_t ctnrid;
+			char *filename;
+		} file;
+	};
+};
+
+struct epm_action;
+struct dvfs_file_struct;
+union export_args;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+extern struct dvfs_mobility_operations dvfs_mobility_regular_ops;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+int ghost_read_file_krg_desc(ghost_t *ghost, void **desc, int *desc_size);
+int ghost_write_file_krg_desc(ghost_t *ghost, void *desc, int desc_size);
+
+struct file *begin_import_dvfs_file(unsigned long dvfs_objid,
+				    struct dvfs_file_struct **dvfs_file);
+
+int end_import_dvfs_file(unsigned long dvfs_objid,
+			 struct dvfs_file_struct *dvfs_file,
+			 struct file *file, int first_import);
+
+int cr_link_to_file(struct epm_action *action, ghost_t *ghost,
+		    struct task_struct *task, struct file **returned_file);
+
+int get_pipe_file_krg_desc(struct file *file, void **desc, int *desc_size);
+
+int get_regular_file_krg_desc(struct file *file, void **desc,
+			      int *desc_size);
+
+int prepare_restart_data_shared_file(struct file *f,
+				     void *fdesc, int fdesc_size,
+				     void **returned_data, size_t *data_size,
+				     bool from_substitution);
+
+struct file *reopen_pipe_file_entry_from_krg_desc(struct task_struct *task,
+						  void *_desc);
+
+#endif // __REGULAR_FILE_MGR__
diff -ruN linux-2.6.29/include/kerrighed/remote_cred.h android_cluster/linux-2.6.29/include/kerrighed/remote_cred.h
--- linux-2.6.29/include/kerrighed/remote_cred.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/remote_cred.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,30 @@
+#ifndef __REMOTE_CRED_H__
+#define __REMOTE_CRED_H__
+
+#include <linux/sched.h>
+#include <linux/cred.h>
+#include <linux/rcupdate.h>
+
+struct rpc_desc;
+
+int pack_creds(struct rpc_desc *desc, const struct cred *cred);
+int unpack_creds(struct rpc_desc *desc, struct cred *cred);
+const struct cred *unpack_override_creds(struct rpc_desc *desc);
+
+static inline int permissions_ok(struct task_struct *task_to_act_on)
+{
+	const struct cred *cred = current_cred();
+	const struct cred *tcred;
+	bool ok;
+
+	rcu_read_lock();
+	tcred = __task_cred(task_to_act_on);
+	ok = ((cred->euid == tcred->uid) ||
+	      (cred->euid == tcred->euid) ||
+	      (cred->euid == 0));
+	rcu_read_unlock();
+
+	return ok;
+}
+
+#endif /* __REMOTE_CRED_H__ */
diff -ruN linux-2.6.29/include/kerrighed/remote_syscall.h android_cluster/linux-2.6.29/include/kerrighed/remote_syscall.h
--- linux-2.6.29/include/kerrighed/remote_syscall.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/remote_syscall.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,26 @@
+#ifndef __REMOTE_SYSCALL_H__
+#define __REMOTE_SYSCALL_H__
+
+#include <linux/types.h>
+
+struct rpc_desc;
+struct pid;
+struct cred;
+
+struct rpc_desc *krg_remote_syscall_begin(int req, pid_t pid,
+					  const void *msg, size_t size);
+void krg_remote_syscall_end(struct rpc_desc *desc, pid_t pid);
+int krg_remote_syscall_simple(int req, pid_t pid, const void *msg, size_t size);
+
+struct pid *krg_handle_remote_syscall_begin(struct rpc_desc *desc,
+					    const void *_msg, size_t size,
+					    void *msg,
+					    const struct cred **old_cred);
+void krg_handle_remote_syscall_end(struct pid *pid,
+				   const struct cred *old_cred);
+
+void remote_signals_init(void);
+void remote_sched_init(void);
+void remote_sys_init(void);
+
+#endif /* __REMOTE_SYSCALL_H__ */
diff -ruN linux-2.6.29/include/kerrighed/scheduler/filter.h android_cluster/linux-2.6.29/include/kerrighed/scheduler/filter.h
--- linux-2.6.29/include/kerrighed/scheduler/filter.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/scheduler/filter.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,786 @@
+#ifndef __KRG_SCHEDULER_FILTER_H__
+#define __KRG_SCHEDULER_FILTER_H__
+
+#include <linux/configfs.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/scheduler/port.h>
+
+/*
+ * A filter is a scheduler_port having a source. A filter collects data/events
+ * from a source connected to its sink, and generates data/events related to
+ * what it collects. Filters sources are connected to scheduler_ports when doing
+ * mkdir in ports directories, and filters sinks are connected to lower sources
+ * the same way as ports.
+ *
+ * A filter can block update propagation as well generating updates.
+ *
+ * A filter can modify the value of the source it is connected to when the sink
+ * it is connected to requests its value, as well as providing values generated
+ * by other means.
+ */
+
+/* Structure representing a filter */
+struct scheduler_filter {
+	struct scheduler_source source;
+	struct scheduler_port port;
+};
+
+#define SCHEDULER_FILTER_ATTR_SIZE SCHEDULER_PORT_ATTR_SIZE
+
+/*
+ * Structure representing a filter attribute
+ * Just an API translation from  port attributes
+ */
+struct scheduler_filter_attribute {
+	struct scheduler_port_attribute port_attr;
+};
+
+/*
+ * Convenience macros to define a scheduler_filter_attribute
+ *
+ * These convenience macros should be used the following way:
+ *
+ * First, implemented methods must be defined using the
+ * DEFINE_SCHEDULER_FILTER_ATTRIBUTE_<method> macros. Second, the
+ * scheduler_filter_attribute must be filled using
+ * {BEGIN,END}_SCHEDULER_FILTER_ATTRIBUTE and SCHEDULER_FILTER_ATTRIBUTE_*
+ * macros:
+ *
+ *	BEGIN_SCHEDULER_FILTER_ATTRIBUTE(var_name, name, mode),
+ * if needed:
+ *		.SCHEDULER_FILTER_ATTRIBUTE_<method>(name),
+ *		...
+ * and finally:
+ *	END_SCHEDULER_FILTER_ATTRIBUTE(name);
+ */
+
+/**
+ * Convenience macro to start the definition of a
+ * scheduler_filter_attribute. The definition must end with
+ * END_SCHEDULER_FILTER_ATTRIBUTE(name).
+ *
+ * @param var_name	name of the scheduler_filter_attribute variable
+ * @param name		name of the attribute entry in the filter directory
+ * @param mode		access mode of the attribute entry
+ */
+#define BEGIN_SCHEDULER_FILTER_ATTRIBUTE(var_name, name, mode)		\
+	struct scheduler_filter_attribute var_name = {			\
+		.port_attr = SCHEDULER_PORT_ATTRIBUTE_INIT(#name, mode,	\
+							   NULL, NULL)
+
+/**
+ * Convenience macro to attach a previously defined show() method to a
+ * scheduler_filter_attribute. The show() method must have been defined earlier
+ * with DEFINE_SCHEDULER_FILTER_ATTRIBUTE_SHOW(name, ...).
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_FILTER_ATTRIBUTE
+ */
+#define SCHEDULER_FILTER_ATTRIBUTE_SHOW(name)	\
+	port_attr.show = name##_port_attr_show
+
+/**
+ * Convenience macro to attach a previously defined store() method to a
+ * scheduler_filter_attribute. The store() method must have been defined earlier
+ * with DEFINE_SCHEDULER_FILTER_ATTRIBUTE_STORE(name, ...).
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_FILTER_ATTRIBUTE
+ */
+#define SCHEDULER_FILTER_ATTRIBUTE_STORE(name)		\
+	port_attr.store = name##_port_attr_store
+
+/**
+ * End the definition of a scheduler_filter_attribute. Must close any
+ * BEGIN_SCHEDULER_FILTER_ATTRIBUTE section.
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_FILTER_ATTRIBUTE
+ */
+#define END_SCHEDULER_FILTER_ATTRIBUTE(name)	\
+	}
+
+/**
+ * Convenience macro to define an show() method for a filter attribute. The
+ * method will be called name_show.
+ *
+ * @param name		name of the filter attribute
+ * @param filter	filter which attribute is read
+ * @param attr		scheduler_filter_attribute describing the attribute
+ * @param page		4Kbytes buffer to fill in
+ */
+#define DEFINE_SCHEDULER_FILTER_ATTRIBUTE_SHOW(name, filter, attr, page)    \
+	static ssize_t name##_show(struct scheduler_filter *,		    \
+				   struct scheduler_filter_attribute *,     \
+				   char *);				    \
+	static ssize_t name##_port_attr_show(				    \
+		struct scheduler_port *port,				    \
+		struct scheduler_port_attribute *port_attr, char *page)	    \
+	{								    \
+		return name##_show(					    \
+			container_of(port, struct scheduler_filter, port),  \
+			container_of(port_attr,				    \
+				     struct scheduler_filter_attribute,     \
+				     port_attr),			    \
+			page);						    \
+	}								    \
+	static ssize_t name##_show(struct scheduler_filter *filter,	    \
+				   struct scheduler_filter_attribute *attr, \
+				   char *page)
+
+/**
+ * Convenience macro to define an store() method for a filter attribute. The
+ * method will be called name_store.
+ *
+ * @param name		name of the filter attribute
+ * @param filter	filter which attribute is to be modified
+ * @param attr		scheduler_filter_attribute describing the attribute
+ * @param page		buffer containing the data to modify attribute
+ * @param count		number of bytes contained in buffer
+ */
+#define DEFINE_SCHEDULER_FILTER_ATTRIBUTE_STORE(name, filter, attr,	     \
+						page, count)		     \
+	static ssize_t name##_store(struct scheduler_filter *,		     \
+				   struct scheduler_filter_attribute *,      \
+				   const char *, size_t);		     \
+	static ssize_t name##_port_attr_store(				     \
+		struct scheduler_port *port,				     \
+		struct scheduler_port_attribute *port_attr,		     \
+		const char *page, size_t count)				     \
+	{								     \
+		return name##_store(					     \
+			container_of(port, struct scheduler_filter, port),   \
+			container_of(port_attr,				     \
+				     struct scheduler_filter_attribute,      \
+				     port_attr),			     \
+			page, count);					     \
+	}								     \
+	static ssize_t name##_store(struct scheduler_filter *filter,	     \
+				    struct scheduler_filter_attribute *attr, \
+				    const char *page, size_t count)
+
+/* End of convenience macros */
+
+/*
+ * Structure describing a type of filter. Filter of this type can be created
+ * once the type is registered.
+ */
+struct scheduler_filter_type {
+	struct scheduler_source_type source_type;
+	struct scheduler_port_type port_type;
+	struct scheduler_filter_attribute **attrs;
+};
+
+/**
+ * Mandatory macro to define a scheduler_filter_type. Can be used through the
+ * BEGIN_SCHEDULER_FILTER_TYPE helper.
+ *
+ * @param filter_type	variable name of the filter type
+ * @param owner		module providing this type
+ * @param name		unique name among scheduler_port_type names
+ * @param new		port_type constructor for this filter type
+ * @param destroy	port_type destructor for this filter type
+ * @param get_value	(source) callback to get the filtered value
+ * @param update_value	(sink) callback called when the source of a filter of
+ *			this type notifies an update
+ * @param show_value	(source) callback to show the value filtered by a filter
+ *			of this type
+ * @param get_remote_value
+ *			(port) callback to get a remote filtered value
+ * @param src_value_type
+ *			string containing the type name of filter's values
+ * @param src_value_type_size
+ *			size in bytes of a src_value_type value
+ * @param src_get_param_type
+ *			string containing the type name of the parameters for
+ *			the filter's get_value method, or NULL
+ * @param src_get_param_type_size
+ *			size in bytes of a src_get_param_type parameter
+ * @param snk_value_type
+ *			string containing the type name of lower source's values
+ * @param snk_value_type_size
+ *			size in bytes of a snk_value_type value
+ * @param snk_get_param_type
+ *			string containing the type name of the parameters for
+ *			the sink's get_value calls, or NULL
+ * @param snk_get_param_type_size
+ *			size in bytes of a snk_get_param_type parameter
+ * @param _attrs	NULL-terminated array of custom
+ *			scheduler_filter_attributes, or NULL
+ */
+#define SCHEDULER_FILTER_TYPE_INIT(filter_type, owner, name,		\
+				   new, destroy,			\
+				   get_value, update_value, show_value, \
+				   get_remote_value,			\
+				   src_value_type,			\
+				   src_value_type_size,			\
+				   src_get_param_type,			\
+				   src_get_param_type_size,		\
+				   snk_value_type,			\
+				   snk_value_type_size,			\
+				   snk_get_param_type,			\
+				   snk_get_param_type_size,		\
+				   _attrs)				\
+	{								\
+		.source_type =						\
+			SCHEDULER_SOURCE_TYPE_INIT(get_value, show_value, \
+						   src_value_type,	\
+						   src_value_type_size, \
+						   src_get_param_type,	\
+						   src_get_param_type_size), \
+		.port_type =						\
+			SCHEDULER_PORT_TYPE_INIT(filter_type.port_type, \
+						 owner, name,		\
+						 update_value,		\
+						 snk_value_type,	\
+						 snk_value_type_size,	\
+						 snk_get_param_type,	\
+						 snk_get_param_type_size, \
+						 &filter_type.source_type, \
+						 get_remote_value,	\
+						 new, destroy),		\
+		.attrs = _attrs,					\
+	}
+
+/*
+ * Convenience macros to define a scheduler_filter_type
+ *
+ * These convenience macros should be used the following way:
+ *
+ * First, implemented methods must be defined using the
+ * DEFINE_SCHEDULER_FILTER_<method> macros. Second, the scheduler_filter_type
+ * must be filled using {BEGIN,END}_SCHEDULER_FILTER_TYPE and SCHEDULER_FILTER_*
+ * macros:
+ *
+ *	BEGIN_SCHEDULER_FILTER_TYPE(name),
+ *		.SCHEDULER_FILTER_SOURCE_VALUE_TYPE(name, type),
+ *		.SCHEDULER_FILTER_PORT_VALUE_TYPE(name, type),
+ * if needed:
+ *		.SCHEDULER_FILTER_<method>(name),
+ *		.SCHEDULER_FILTER_SOURCE_PARAM_TYPE(name, type),
+ *		.SCHEDULER_FILTER_PORT_PARAM_TYPE(name, type),
+ *		.SCHEDULER_FILTER_ATTRS(name, attrs),
+ *		...
+ * and finally:
+ *	END_SCHEDULER_FILTER_TYPE(name);
+ */
+
+/**
+ * Convenience macro to start the definition of a scheduler_filter_type. The
+ * definition must end with END_SCHEDULER_FILTER_TYPE(name). The variable will
+ * be called name_type.
+ *
+ * @param name		name of the scheduler_filter type
+ */
+#define BEGIN_SCHEDULER_FILTER_TYPE(name)			   \
+	struct scheduler_filter_type name##_type = {		   \
+		.source_type = SCHEDULER_SOURCE_TYPE_INIT(	   \
+			scheduler_filter_simple_source_get_value,  \
+			scheduler_filter_simple_source_show_value, \
+			NULL, 0, NULL, 0),			   \
+		.port_type = SCHEDULER_PORT_TYPE_INIT(		   \
+			name##_type.port_type, THIS_MODULE, #name, \
+			scheduler_filter_simple_sink_update_value, \
+			NULL, 0, NULL, 0,			   \
+			&name##_type.source_type,		   \
+			scheduler_port_get_remote_value,	   \
+			name##_port_new, name##_port_destroy)
+
+/**
+ * Convenience macro to attach a previously defined get_value() method to a
+ * scheduler_filter type. The get_value() method must have been defined earlier
+ * with DEFINE_SCHEDULER_FILTER_GET_VALUE(name, ...).
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_FILTER_TYPE
+ */
+#define SCHEDULER_FILTER_GET_VALUE(name)				    \
+	__SCHEDULER_SOURCE_GET_VALUE(source_type., name##_source_get_value)
+
+/**
+ * Convenience macro to attach a previously defined update_value() method to a
+ * scheduler_filter type. The update_value() method must have been defined earlier
+ * with DEFINE_SCHEDULER_FILTER_UPDATE_VALUE(name, ...).
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_FILTER_TYPE
+ */
+#define SCHEDULER_FILTER_UPDATE_VALUE(name)			\
+	__SCHEDULER_PORT_UPDATE_VALUE(port_type., name##_port)
+
+/**
+ * Convenience macro to attach a previously defined show_value() method to a
+ * scheduler_filter type. The show_value() method must have been defined earlier
+ * with DEFINE_SCHEDULER_FILTER_SHOW_VALUE(name, ...).
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_FILTER_TYPE
+ */
+#define SCHEDULER_FILTER_SHOW_VALUE(name)				      \
+	__SCHEDULER_SOURCE_SHOW_VALUE(source_type., name##_source_show_value)
+
+/**
+ * Convenience macro to attach a previously defined get_remote_value() method to
+ * a scheduler_filter type. The get_remote_value() method must have been defined
+ * earlier with DEFINE_SCHEDULER_FILTER_GET_REMOTE_VALUE(name, ...).
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_FILTER_TYPE
+ */
+#define SCHEDULER_FILTER_GET_REMOTE_VALUE(name)				\
+	__SCHEDULER_PORT_GET_REMOTE_VALUE(port_type., name##_port)
+
+/**
+ * Convenience macro to declare the value type generated by a scheduler
+ * filter. Must be used within all BEGIN_SCHEDULER_FILTER_TYPE sections.
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_FILTER_TYPE
+ * @param type		litteral expression of the value type output by the
+ *			source
+ */
+#define SCHEDULER_FILTER_SOURCE_VALUE_TYPE(name, type)		\
+	__SCHEDULER_SOURCE_VALUE_TYPE(source_type., type)
+
+/**
+ * Convenience macro to declare the parameter type of the get_value() method of
+ * a filter's source.
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_FILTER_TYPE
+ * @param type		litteral expression of the parameter type
+ */
+#define SCHEDULER_FILTER_SOURCE_PARAM_TYPE(name, type)		\
+	__SCHEDULER_SOURCE_PARAM_TYPE(source_type., type)
+
+/**
+ * Convenience macro to declare the value type collected by a scheduler
+ * filter. Must be used within all BEGIN_SCHEDULER_FILTER_TYPE sections.
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_FILTER_TYPE
+ * @param type		litteral expression of the value type read by the sink
+ */
+#define SCHEDULER_FILTER_PORT_VALUE_TYPE(name, type)			\
+	__SCHEDULER_PORT_VALUE_TYPE(port_type., name##_port, type)
+
+/**
+ * Convenience macro to declare the parameter type used when calling the
+ * get_value() method of a connected source.
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_FILTER_TYPE
+ * @param type		litteral expression of the parameter type
+ */
+#define SCHEDULER_FILTER_PORT_PARAM_TYPE(name, type)			\
+	__SCHEDULER_PORT_PARAM_TYPE(port_type., name##_port, type)
+
+/**
+ * Convenience macro to attach custom filter attributes to a filter type.
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_FILTER_TYPE
+ * @param attrs		NULL terminated array of pointers to custom filter
+ *			attributes (struct scheduler_filter_attribute)
+ */
+#define SCHEDULER_FILTER_ATTRIBUTES(name, _attrs)	\
+	attrs = _attrs
+
+/**
+ * End the definition of a scheduler_filter_type. Must close any
+ * BEGIN_SCHEDULER_FILTER_TYPE section.
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_FILTER_TYPE
+ */
+#define END_SCHEDULER_FILTER_TYPE(name)		\
+	}
+
+/**
+ * Convenience macro to define a typed get_value method with no parameter.
+ * The typed method will be called name_get_value
+ *
+ * @param name		name of the filter type
+ * @param filter	name of the struct filter *arg of the method
+ * @param type		type of the values output by the filter (eg. int)
+ * @param ptr		name of the type *arg of the method
+ * @param nr		name of the array length arg of the method
+ */
+#define DEFINE_SCHEDULER_FILTER_GET_VALUE(name, filter, type, ptr, nr)	       \
+	static int name##_get_value(struct scheduler_filter *,		       \
+				    type *, unsigned int);		       \
+	static int name##_source_get_value(struct scheduler_source *source,    \
+					   void *__ptr, unsigned int __nr,     \
+					   const void *__in_ptr,	       \
+					   unsigned int __in_nr)	       \
+	{								       \
+		if (!__nr)						       \
+			return 0;					       \
+		return name##_get_value(				       \
+			container_of(source, struct scheduler_filter, source), \
+			__ptr, __nr);					       \
+	}								       \
+	static int name##_get_value(struct scheduler_filter *filter,	       \
+				    type *ptr, unsigned int nr)
+
+/**
+ * Convenience macro to define a typed get_value() method with parameters.
+ * The typed method will be called name_get_value
+ *
+ * @param name		name of the filter type
+ * @param filter	name of the struct filter *arg of the method
+ * @param type		type of the values output by the filter (eg. int)
+ * @param ptr		name of the type *arg of the method
+ * @param nr		name of the array length parameter of the method
+ * @param in_type	type of the parameters of the method (eg. int)
+ * @param in_ptr	name of the in_type *arg of the method
+ * @param in_nr		name of the parameters array length arg of the method
+ */
+#define DEFINE_SCHEDULER_FILTER_GET_VALUE_WITH_INPUT(name, filter,	       \
+						     type, ptr, nr,	       \
+						     in_type, in_ptr, in_nr)   \
+	static int name##_get_value(struct scheduler_filter *,		       \
+				    type *, unsigned int,		       \
+				    const in_type *, unsigned int);	       \
+	static int name##_source_get_value(struct scheduler_source *source,    \
+					   void *__ptr, unsigned int __nr,     \
+					   const void *__in_ptr,	       \
+					   unsigned int __in_nr)	       \
+	{								       \
+		return name##_get_value(				       \
+			container_of(source, struct scheduler_filter, source), \
+			__ptr, __nr, __in_ptr, __in_nr);		       \
+	}								       \
+	static int name##_get_value(struct scheduler_filter *filter,	       \
+				    type *ptr, unsigned int nr,		       \
+				    const in_type *in_ptr,		       \
+				    unsigned int in_nr)
+
+/**
+ * Convenience macro to define a show_value() method.
+ * The method will be called name_show_value
+ *
+ * @param name		name of the filter type
+ * @param filter	name of the struct filter *arg of the method
+ * @param page		name of the buffer arg of the method
+ */
+#define DEFINE_SCHEDULER_FILTER_SHOW_VALUE(name, filter, page)		       \
+	static ssize_t name##_show_value(struct scheduler_filter *filter,      \
+					 char *page);			       \
+	static ssize_t name##_source_show_value(			       \
+		struct scheduler_source *source,			       \
+		char *page)						       \
+	{								       \
+		return name##_show_value(				       \
+			container_of(source, struct scheduler_filter, source), \
+			page);						       \
+	}								       \
+	static ssize_t name##_show_value(struct scheduler_filter *filter,      \
+					 char *page)
+
+/**
+ * Convenience macro to define a update_value() method.
+ * The method will be called name_update_value
+ *
+ * @param name		name of the filter type
+ * @param filter	name of the struct filter *arg of the method
+ */
+#define DEFINE_SCHEDULER_FILTER_UPDATE_VALUE(name, filter)		   \
+	static void name##_update_value(struct scheduler_filter *);	   \
+	static void name##_port_sink_update_value(			   \
+		struct scheduler_sink *sink,				   \
+		struct scheduler_source *source)			   \
+	{								   \
+		name##_update_value(					   \
+			container_of(sink,				   \
+				     struct scheduler_filter, port.sink)); \
+	}								   \
+	static void name##_update_value(struct scheduler_filter *filter)
+
+/**
+ * Convenience macro to define a typed get_remote_value() method with
+ * parameters.  The typed method will be called name_get_remote_value
+ *
+ * @param name		name of the filter type
+ * @param filter	name of the struct filter *arg of the method
+ * @param node		name of the node arg of the method
+ * @param type		type of the values output by the filter (eg. int)
+ * @param ptr		name of the type *arg of the method
+ * @param nr		name of the array length parameter of the method
+ * @param in_type	type of the parameters of the method (eg. int)
+ * @param in_ptr	name of the in_type *arg of the method
+ * @param in_nr		name of the parameters array length arg of the method
+ */
+#define DEFINE_SCHEDULER_FILTER_GET_REMOTE_VALUE(name, filter, node,	     \
+						 type, ptr, nr,		     \
+						 in_type, in_ptr, in_nr)     \
+	static int name##_get_remote_value(struct scheduler_filter *filter,  \
+					   kerrighed_node_t node,	     \
+					   type *ptr, unsigned int nr,	     \
+					   const in_type *in_ptr,	     \
+					   unsigned int in_nr);		     \
+	static int name##_port_get_remote_value(struct scheduler_port *port, \
+						kerrighed_node_t node,	     \
+						void *__ptr,		     \
+						unsigned int __nr,	     \
+						const void *__in_ptr,	     \
+						unsigned int __in_nr)	     \
+	{								     \
+		return name##_get_remote_value(				     \
+			container_of(port, struct scheduler_filter, port),   \
+			node,						     \
+			__ptr, __nr, __in_ptr, __in_nr);		     \
+	}								     \
+	static int name##_get_remote_value(struct scheduler_filter *filter,  \
+					   kerrighed_node_t node,	     \
+					   type *ptr, unsigned int nr,	     \
+					   const in_type *in_ptr,	     \
+					   unsigned int in_nr)
+
+/**
+ * Convenience macro to define the mandatory constructor for a filter type. The
+ * method will be called name_new
+ *
+ * @param name		name of the filter type
+ * @param fname		configfs entry name of the new filter
+ */
+#define DEFINE_SCHEDULER_FILTER_NEW(name, fname)			  \
+	static struct scheduler_filter *name##_new(const char *);	  \
+	static struct scheduler_port *name##_port_new(const char *fname)  \
+	{								  \
+		return &name##_new(fname)->port;			  \
+	}								  \
+	static struct scheduler_filter *name##_new(const char *fname)
+
+/**
+ * Convenience macro to define the mandatory destructor for a filter type. The
+ * method will be called name_destroy
+ *
+ * @param name		name of the filter type
+ * @param filter	filter to destroy
+ */
+#define DEFINE_SCHEDULER_FILTER_DESTROY(name, filter)			    \
+	static void name##_destroy(struct scheduler_filter *);		    \
+	static void name##_port_destroy(struct scheduler_port *port)	    \
+	{								    \
+		name##_destroy(						    \
+			container_of(port, struct scheduler_filter, port)); \
+	}								    \
+	static void name##_destroy(struct scheduler_filter *filter)
+
+/* End of convenience macros */
+
+/**
+ * Register a new filter type
+ *
+ * @param type		type initialized with SCHEDULER_FILTER_TYPE[_INIT] to
+ *			register
+ *
+ * @return		0 is successful,
+ *			-EINVAL if the type is not complete,
+ *			-ENOMEM if not sufficient memory could be allocated,
+ *			-EEXIST if a filter type of the same name is already
+ *			registered
+ */
+int scheduler_filter_type_register(struct scheduler_filter_type *type);
+/**
+ * Unregister a filter type. Must *only* be called at module unloading.
+ *
+ * @param type		The filter type to unregister
+ */
+void scheduler_filter_type_unregister(struct scheduler_filter_type *type);
+
+/**
+ * Initialize a scheduler_filter. Must be called by filter constructors.
+ *
+ * @param filter	filter to initialize
+ * @param name		name of the new filter. Must match the name given as
+ *			argument to the constructor.
+ * @param type		type of the new filter
+ * @param default_groups
+ *			NULL terminated array of custom config_groups displayed
+ *			as subdirs of the filter, or NULL
+ *
+ * @return		0 if successful,
+ *			-ENODEV if the type is not registered
+ */
+int scheduler_filter_init(struct scheduler_filter *filter,
+			  const char *name,
+			  struct scheduler_filter_type *type,
+			  struct config_group **default_groups);
+/**
+ * Cleanup a scheduler filter. Must be called by the filter destructor.
+ *
+ * @param filter	filter to cleanup
+ */
+void scheduler_filter_cleanup(struct scheduler_filter *filter);
+
+/**
+ * Get a reference on a filter.
+ *
+ * @param filter	filter to get the reference on
+ */
+static inline void scheduler_filter_get(struct scheduler_filter *filter)
+{
+	config_group_get(&filter->port.pipe.config);
+}
+
+/**
+ * Put a reference on a filter.
+ *
+ * @param filter	filter to put the reference on
+ */
+static inline void scheduler_filter_put(struct scheduler_filter *filter)
+{
+	config_group_put(&filter->port.pipe.config);
+}
+
+/**
+ * Lock a filter. This will *not* prevent the scheduler_pipe subsystem from
+ * calling update_value and show_value callbacks. This will however block
+ * subscription/unsubscription of sinks to this filter.
+ *
+ * @param filter	filter to lock
+ */
+static inline void scheduler_filter_lock(struct scheduler_filter *filter)
+{
+	scheduler_source_lock(&filter->source);
+}
+
+/**
+ * Unlock a filter.
+ *
+ * @param filter	filter to unlock
+ */
+static inline void scheduler_filter_unlock(struct scheduler_filter *filter)
+{
+	scheduler_source_unlock(&filter->source);
+}
+
+/* Trivial get_value method for a scheduler_filter */
+extern source_get_value_t scheduler_filter_simple_source_get_value;
+
+/**
+ * Get the value from the source connected to a filter
+ *
+ * @param filter	filter to query
+ * @param value_p	array of values to be filled
+ * @param nr		max number of values to fill
+ * @param in_value_p	array of parameters, or NULL
+ * @param in_nr		number of elements in in_value_p
+ *
+ * @return		number of values filled, or
+ *			negative error code
+ */
+static inline
+int
+scheduler_filter_simple_get_value_with_input(struct scheduler_filter *filter,
+					     void *value_p, int nr_value,
+					     void *param_p, int nr_param)
+{
+	/*
+	 * Optimization: one stack frame less than:
+	 * return scheduler_filter_simple_source_get_value(&filter->source,
+	 *						   ...);
+	 */
+	return scheduler_port_get_value(&filter->port,
+					value_p, nr_value, param_p, nr_param);
+}
+/**
+ * Get the value from the source connected to a filter without parameter
+ *
+ * @param filter	filter to query
+ * @param value_p	array of values to be filled
+ * @param nr		max number of values to fill
+ *
+ * @return		number of values filled, or
+ *			negative error code
+ */
+static inline
+int
+scheduler_filter_simple_get_value(struct scheduler_filter *filter,
+				  void *value_p, int nr_value)
+{
+	/*
+	 * Optimization: one stack frame less than:
+	 * return scheduler_filter_simple_source_get_value(&filter->source,
+	 *						   ...);
+	 */
+	return scheduler_port_get_value(&filter->port,
+					value_p, nr_value, NULL, 0);
+}
+
+/* Trivial show_value() method for a scheduler_filter */
+extern source_show_value_t scheduler_filter_simple_source_show_value;
+
+/**
+ * Show the value collected by a filter (from a its connected source) as a string
+ * (for instance through configfs)
+ *
+ * @param filter	filter to query
+ * @param page		buffer to store the value (4 Kbytes size)
+ *
+ * @return		number of bytes written to buffer, or
+ *			negative error code
+ */
+static inline
+ssize_t
+scheduler_filter_simple_show_value(struct scheduler_filter *filter, char *page)
+{
+	/*
+	 * Optimization: one stack frame less than:
+	 * return scheduler_filter_simple_source_show_value(&filter->source,
+	 *						    page);
+	 */
+	return scheduler_port_show_value(&filter->port, page);
+}
+
+/* Trivial update_value() method for a filter */
+extern sink_update_value_t scheduler_filter_simple_sink_update_value;
+
+/**
+ * Propagate a notification to the sink connected to a filter
+ *
+ * @param filter	filter being notified
+ */
+static inline
+void scheduler_filter_simple_update_value(struct scheduler_filter *filter)
+{
+	scheduler_filter_simple_sink_update_value(&filter->port.sink, NULL);
+}
+
+/**
+ * Get the value from the remote peer source of the source connected to a
+ * filter. If the connected source is itself a port and defines a
+ * get_remote_value() method, its get_remote_value() method will be called
+ * instead. If not get_remote_value() method is defined, the call will be
+ * forwarded down until either a source being not a port is found or a
+ * get_remote_value() method is defined.
+ *
+ * @param filter	filter querying a remote source
+ * @param node		node to get the value from
+ * @param value_p	array of values to be filled
+ * @param nr_value	max number of values to fill
+ * @param in_value_p	array of parameters, or NULL
+ * @param in_nr		number of elements in in_value_p
+ *
+ * @return		number of values filled, or
+ *			-EAGAIN if the request is pending and the caller should
+ *			retry later, or
+ *			other negative error code
+ */
+static inline
+int
+scheduler_filter_simple_get_remote_value(struct scheduler_filter *filter,
+					 kerrighed_node_t node,
+					 void *value_p, unsigned int nr_value,
+					 const void *param_p, unsigned int nr_param)
+{
+	return scheduler_port_get_remote_value(&filter->port, node,
+					       value_p, nr_value,
+					       param_p, nr_param);
+}
+
+#endif /* __KRG_SCHEDULER_FILTER_H__ */
diff -ruN linux-2.6.29/include/kerrighed/scheduler/global_config.h android_cluster/linux-2.6.29/include/kerrighed/scheduler/global_config.h
--- linux-2.6.29/include/kerrighed/scheduler/global_config.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/scheduler/global_config.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,262 @@
+#ifndef __KRG_SCHEDULER_GLOBAL_CONFIG_H__
+#define __KRG_SCHEDULER_GLOBAL_CONFIG_H__
+
+#include <linux/workqueue.h>
+#include <linux/list.h>
+#include <kerrighed/ghost_types.h>
+
+struct global_config_item;
+
+typedef void global_config_drop_func_t(struct global_config_item *item);
+
+/**
+ * Operations associated to a global config item
+ */
+struct global_config_drop_operations {
+	/* callback called when globally dropping a global_config_item */
+	global_config_drop_func_t *drop_func;
+	/* not null if the item is symbolic link */
+	int is_symlink;
+};
+
+/**
+ * Structure needed to store informations about a global config object
+ * (config_item or symlink)
+ * Should not be accessed directly.
+ */
+struct global_config_item {
+	struct list_head list;
+	struct delayed_work drop_work;
+	/* operations associated to the global config item */
+	const struct global_config_drop_operations *drop_ops;
+	const char *path;
+	const char *target_path;
+};
+
+/**
+ * Block globalized config operations
+ *
+ * @return		0 if success,
+ *			negative error code otherwise
+ */
+int global_config_freeze(void);
+/**
+ * Un-block globalized config operations
+ */
+void global_config_thaw(void);
+
+/**
+ * Initialize a global_config_item
+ *
+ * @param item		item to initialize
+ * @param ops		operations associated with this item
+ */
+void global_config_item_init(struct global_config_item *item,
+			     const struct global_config_drop_operations *ops);
+
+struct config_item;
+struct string_list_object;
+
+/**
+ * Function that prepares a global mkdir
+ *
+ * @param parent	item under which the operation is done
+ * @param name		name of the new sub-directory
+ *
+ * @return		valid pointer or NULL to be passed to
+ *			global_config_make_item_end or
+ *			global_config_make_item_error, or error
+ */
+struct string_list_object *
+global_config_make_item_begin(struct config_item *parent, const char *name);
+int __global_config_make_item_commit(struct string_list_object *list,
+				     struct config_item *parent,
+				     struct global_config_item *item,
+				     const char *name);
+void __global_config_make_item_end(struct string_list_object *list);
+/**
+ * Commit a global config mkdir
+ *
+ * @param list		pointer returned by global_config_make_item_begin
+ * @param parent	item under which the operation is done
+ * @param item		pointer to the global_config_item for the new dir,
+ *			previously initialized with global_config_item_init
+ * @param name		name of the new dir
+ *
+ * @return		0 on success, or error
+ */
+int global_config_make_item_end(struct string_list_object *list,
+				struct config_item *parent,
+				struct global_config_item *item,
+				const char *name);
+/**
+ * Cleanup a global mkdir if an error occurs before calling
+ * global_config_make_item_end
+ *
+ * @param list		pointer returned by global_config_make_item_begin
+ * @param name		name of the new dir
+ */
+void global_config_make_item_error(struct string_list_object *list,
+				   const char *name);
+
+/**
+ * Function that prepares a global symlink
+ *
+ * @param parent	item under which the link is created
+ * @param name		name of the new link
+ * @param target	target item of the new link
+ *
+ * @return		valid pointer or NULL to be passed to
+ *			global_config_allow_link_end or
+ *			global_config_allow_link_error, or error
+ */
+struct string_list_object *
+global_config_allow_link_begin(struct config_item *parent,
+			       const char *name,
+			       struct config_item *target_name);
+int __global_config_allow_link_commit(struct string_list_object *list,
+				      struct config_item *parent,
+				      struct global_config_item *item,
+				      const char *name,
+				      struct config_item *target);
+void __global_config_allow_link_end(struct string_list_object *list);
+/**
+ * Commit a global config symlink
+ *
+ * @param list		pointer returned by global_config_allow_link_begin
+ * @param parent	item under which the new link is created
+ * @param item		pointer to the global_config_item for the new link,
+ *			previously initialized with global_config_item_init
+ * @param name		name of the new link
+ * @param target	target item of the new link
+ *
+ * @return		0 on success, or error
+ */
+int global_config_allow_link_end(struct string_list_object *list,
+				      struct config_item *parent,
+				      struct global_config_item *item,
+				      const char *name,
+				      struct config_item *target_name);
+/**
+ * Cleanup a global symlink if an error occurs before calling
+ * global_config_allow_link_end
+ *
+ * @param list		pointer returned by global_config_allow_link_begin
+ * @param name		name of the new link
+ * @param target	target item of the new link
+ */
+void global_config_allow_link_error(struct string_list_object *list,
+					 const char *name,
+					 struct config_item *target_name);
+
+struct global_config_attrs {
+	struct list_head head;
+	int valid;
+};
+
+struct config_group;
+
+void global_config_attrs_init_r(struct config_group *group);
+void global_config_attrs_cleanup_r(struct config_group *group);
+
+struct configfs_attribute;
+
+/**
+ * Prepare a global store operation on an attribute.
+ *
+ * @param item		item owning the attribute
+ *
+ * @return		a valid pointer or NULL to be passed to
+ *			global_config_attr_store_end or
+ *			global_config_attr_store_error, or error
+ */
+struct string_list_object *
+global_config_attr_store_begin(struct config_item *item);
+/**
+ * Commit a global store on an attribute
+ *
+ * @param list		pointer returned by global_config_attr_store_begin
+ * @param item		item owning the attribute
+ * @param attr		attribute to modify
+ * @param page		buffer containing the value to store
+ * @param count		number of bytes to store, as can really be stored
+ *			(result from the local store operation for instance).
+ *
+ * @return		number of bytes written on success, or error
+ */
+ssize_t global_config_attr_store_end(struct string_list_object *list,
+				     struct config_item *item,
+				     struct configfs_attribute *attr,
+				     const char *page, size_t count);
+/**
+ * Cleanup a global attribute store if an error occurs before calling
+ * global_config_attr_store_end
+ *
+ * @param list		pointer returned by global_config_attr_store_begin
+ * @param item		item owning the attribute
+ */
+void global_config_attr_store_error(struct string_list_object *list,
+				    struct config_item *item);
+
+/**
+ * Notify a global rmdir or unlink. The drop may be delayed, so the item should
+ * not be freed before the drop callback is called.
+ *
+ * @param item		global_config_item used for the dropped entry
+ */
+void global_config_drop(struct global_config_item *item);
+
+struct rpc_desc;
+/**
+ * Pack information so that a peer config_item can be reached on a peer node.
+ *
+ * @param desc		RPC descriptor to pack item identification info
+ * @param item		local peer item of item to reach on peer nodes
+ *
+ * @return		0 if successful, or
+ *			negative error code
+ */
+int global_config_pack_item(struct rpc_desc *desc, struct config_item *item);
+/**
+ * Find and get a reference on the local peer item of a remote item
+ *
+ * @param desc		RPC descriptor to unpack item identification info
+ *
+ * @return		Valid pointer to a config_item, or
+ *			negative error pointer
+ */
+struct config_item *global_config_unpack_get_item(struct rpc_desc *desc);
+
+struct epm_action;
+
+/**
+ * Export information to a ghost so that a peer config_item can be reached on a
+ * peer node
+ *
+ * @param action	EPM action using the ghost
+ * @param ghost		ghost to export to
+ * @param item		globalized config_item to reach on peer node
+ *
+ * @return		0 if successful, or
+ *			negative error code
+ */
+int export_global_config_item(struct epm_action *action, ghost_t *ghost,
+			      struct config_item *item);
+/**
+ * Import information from a ghost and get a reference on a globalized
+ * config_item
+ * Returns success as long as ghost can still be used. An error in item lookup
+ * is returned in item pointer.
+ *
+ * @param action	EPM action using the ghost
+ * @param ghost		ghost to import from
+ * @param item_p	pointer to a pointer to fill with the found item or
+ *			error item, if ghost info could be successfuly imported
+ *
+ * @return		0 if ghost import successful, or
+ *			negative error code
+ */
+int import_global_config_item(struct epm_action *action, ghost_t *ghost,
+			      struct config_item **item_p);
+
+#endif /* __KRG_SCHEDULER_GLOBAL_CONFIG_H__ */
diff -ruN linux-2.6.29/include/kerrighed/scheduler/hooks.h android_cluster/linux-2.6.29/include/kerrighed/scheduler/hooks.h
--- linux-2.6.29/include/kerrighed/scheduler/hooks.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/scheduler/hooks.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,12 @@
+#ifndef __KRG_SCHEDULER_HOOKS_H__
+#define __KRG_SCHEDULER_HOOKS_H__
+
+#if defined(CONFIG_KRG_SCHED) && defined(CONFIG_MODULE_HOOK)
+#include <linux/module_hook.h>
+
+extern struct module_hook_desc kmh_calc_load;
+extern struct module_hook_desc kmh_process_on;
+extern struct module_hook_desc kmh_process_off;
+#endif
+
+#endif /* __KRG_SCHEDULER_HOOKS_H__ */
diff -ruN linux-2.6.29/include/kerrighed/scheduler/info.h android_cluster/linux-2.6.29/include/kerrighed/scheduler/info.h
--- linux-2.6.29/include/kerrighed/scheduler/info.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/scheduler/info.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,57 @@
+#ifndef __KRG_SCHEDULER_INFO_H__
+#define __KRG_SCHEDULER_INFO_H__
+
+#ifdef CONFIG_KRG_SCHED
+
+#include <linux/list.h>
+
+struct module;
+struct task_struct;
+struct epm_action;
+struct ghost;
+
+struct krg_sched_module_info_type {
+	struct list_head list;		/* reserved for krg_sched_info */
+	struct list_head instance_head;	/* subsystem internal */
+	const char *name;
+	struct module *owner;
+	/* can block */
+	struct krg_sched_module_info *(*copy)(struct task_struct *,
+					      struct krg_sched_module_info *);
+	/* may be called from interrupt context */
+	void (*free)(struct krg_sched_module_info *);
+	/* can block */
+	int (*export)(struct epm_action *, struct ghost *,
+		      struct krg_sched_module_info *);
+	/* can block */
+	struct krg_sched_module_info *(*import)(struct epm_action *,
+						struct ghost *,
+						struct task_struct *);
+};
+
+/* struct to include in module specific task krg_sched_info struct */
+/* modification is reserved for krg_sched_info subsystem internal */
+struct krg_sched_module_info {
+	struct list_head info_list;
+	struct list_head instance_list;
+	struct krg_sched_module_info_type *type;
+};
+
+int krg_sched_module_info_register(struct krg_sched_module_info_type *type);
+/*
+ * must only be called at module unloading (See comment in
+ * krg_sched_info_copy())
+ */
+void krg_sched_module_info_unregister(struct krg_sched_module_info_type *type);
+/* Must be called under rcu_read_lock() */
+struct krg_sched_module_info *
+krg_sched_module_info_get(struct task_struct *task,
+			  struct krg_sched_module_info_type *type);
+
+/* fork() / exit() */
+extern int krg_sched_info_copy(struct task_struct *tsk);
+extern void krg_sched_info_free(struct task_struct *tsk);
+
+#endif /* CONFIG_KRG_SCHED */
+
+#endif /* __KRG_SCHEDULER_INFO_H__ */
diff -ruN linux-2.6.29/include/kerrighed/scheduler/pipe.h android_cluster/linux-2.6.29/include/kerrighed/scheduler/pipe.h
--- linux-2.6.29/include/kerrighed/scheduler/pipe.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/scheduler/pipe.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,696 @@
+#ifndef __KRG_SCHEDULER_PIPE_H__
+#define __KRG_SCHEDULER_PIPE_H__
+
+#include <linux/configfs.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/list.h>
+#include <linux/kref.h>
+#include <linux/types.h>
+#include <kerrighed/scheduler/remote_pipe.h>
+
+/*
+ * A scheduler_source represents a source of information.
+ *
+ * Information can be queried from a scheduler_source, and a scheduler_source
+ * may publish updates to subscribers. Subscribers are represented by
+ * scheduler_sink structures.
+ *
+ * To make its value(s) available, a scheduler_source provides a get_value()
+ * method. The value(s) can be queried using
+ * scheduler_source_get_value(). Values are typed, and an optional, typed array
+ * of parameters can be given to scheduler_source_get_value(). A source can also
+ * make its value(s) available as text through a show_value() method. This
+ * method must be called through the scheduler_source_show_value() function.
+ *
+ * To publish updates to its subscribers (scheduler_sink), a scheduler_source
+ * calls scheduler_source_publish(). To this end a scheduler_sink subscribing to
+ * a scheduler_source must provide an update_value() method.
+ *
+ * To access the values of the source connected to a scheduler_sink, one can use
+ * the scheduler_sink_get_value() (typed) and scheduler_sink_show_value() (text)
+ * methods.
+ *
+ * A scheduler_pipe represents a scheduler_source or a scheduler_sink, or both
+ * as a single directory in configfs. The value of the scheduler_source can be
+ * read in the "value" configfs_attribute of the scheduler_pipe's directory,
+ * provided that the scheduler_source provides a show_value() method. The value
+ * collected by the scheduler_sink from its connected source (if any) can be
+ * read in the "collected_value" configfs_attribute of the scheduler_pipe's
+ * directory.
+ */
+
+/* Structure representing the types used in a get_value method */
+struct get_value_types {
+	const char *out_type; /* type output by the method */
+	size_t out_type_size;
+	const char *in_type; /* parameter type of the method */
+	size_t in_type_size;
+};
+
+/* Type definitions for scheduler_source */
+
+struct scheduler_source;
+
+/**
+ * prototype for the method to get the source's value
+ *
+ * When called through scheduler_source_get_value(), the arguments are
+ * checked so that no array pointer is NULL if its size is not 0, and at least
+ * one array has elements.
+ *
+ * Source internal locking is supposed to be explicitly handled by the method.
+ *
+ * @param source	source from which to read the value
+ * @param value_p	array of values to be filled by the method
+ * @param nr		max number of values to fill
+ * @param in_value_p	array of parameters, or NULL
+ * @param in_nr		number of elements in in_value_p
+ *
+ * @return		number of values filled, or
+ *			negative error code
+ */
+typedef int source_get_value_t(struct scheduler_source *source,
+			       void *value_p, unsigned int nr,
+			       const void *in_value_p, unsigned int in_nr);
+/**
+ * prototype for the method to show the source's value as text
+ *
+ * Source internal locking is supposed to be explicitly handled by the method.
+ *
+ * @param source	source from which to read the value
+ * @param page		buffer to write the value to (4 Kbytes)
+ *
+ * @return		number of bytes written to buffer, or
+ *			negative error code
+ */
+typedef ssize_t source_show_value_t(struct scheduler_source *source,
+				    char *page);
+
+/* To initialize with SCHEDULER_SOURCE_TYPE[_INIT]. */
+struct scheduler_source_type {
+	/** functions for reading source's value */
+
+	/* typed binary read with optional parameters */
+	source_get_value_t *get_value;
+	struct get_value_types get_value_types;
+
+	/* textual read */
+	source_show_value_t *show_value;
+};
+
+/**
+ * Mandatory macro to define a scheduler_source_type. Can be used through
+ * SCHEDULER_SOURCE_TYPE.
+ *
+ * @param _get_value	get_value method of the source
+ * @param _show_value	show_value method of the source
+ * @param value_type	string containing the type name of source's values
+ * @param value_type_size
+ *			size in bytes of a value_type value
+ * @param get_param_type
+ *			string containing the type name of the parameters for
+ *			the get_value method, or NULL
+ * @param get_param_type_size
+ *			size in bytes of a get_param_type parameter
+ */
+#define SCHEDULER_SOURCE_TYPE_INIT(_get_value, _show_value,		\
+				   value_type, value_type_size,		\
+				   get_param_type, get_param_type_size) \
+	{								\
+		.get_value = _get_value,				\
+		.get_value_types = {					\
+			.out_type = value_type,				\
+			.out_type_size = value_type_size,		\
+			.in_type = get_param_type,			\
+			.in_type_size = get_param_type_size		\
+		},							\
+		.show_value = _show_value				\
+	}
+
+/**
+ * Convenience macro to define a scheduler_source_type.
+ *
+ * @param name		name of the variable for the scheduler_source_type
+ * @param get_value	get_value method of the source
+ * @param show_value	show_value method of the source
+ * @param value_type	type of source's values (eg. unsigned int)
+ * @param get_param_type
+ *			type of the parameters for the get_value method
+ */
+#define SCHEDULER_SOURCE_TYPE(name,				   \
+			      get_value, show_value,		   \
+			      value_type, get_param_type)	   \
+	struct scheduler_source_type name =			   \
+		SCHEDULER_SOURCE_TYPE_INIT(get_value, show_value,  \
+					   #value_type,		   \
+					   sizeof(value_type),	   \
+					   #get_param_type,	   \
+					   sizeof(get_param_type))
+
+/*
+ * Internal helpers to define convenience initializing macros for super-classes
+ * of scheduler_source_type
+ */
+
+/**
+ * Initializer for the get_value() method of a scheduler_source_type
+ *
+ * @param prefix	prefix to the scheduler_source_type field in the
+ *			super-class initializer
+ * @param _get_value	function to set as the get_value() method
+ */
+#define __SCHEDULER_SOURCE_GET_VALUE(prefix, _get_value)	\
+	prefix get_value = _get_value
+/**
+ * Initializer for the type of value output by the get_value() method of a
+ * scheduler_source_type
+ *
+ * @param prefix	prefix to the scheduler_source_type field in the
+ *			super-class initializer
+ * @param type		litteral expression of the type to set
+ */
+#define __SCHEDULER_SOURCE_VALUE_TYPE(prefix, type)		\
+	prefix get_value_types.out_type = #type,		\
+	.prefix get_value_types.out_type_size = sizeof(type)
+/**
+ * Initializer for the type of parameter taken by the get_value() method of a
+ * scheduler_source_type
+ *
+ * @param prefix	prefix to the scheduler_source_type field in the
+ *			super-class initializer
+ * @param type		litteral expression of the type to set
+ */
+#define __SCHEDULER_SOURCE_PARAM_TYPE(prefix, type)		\
+	prefix get_value_types.in_type = #type,			\
+	.prefix get_value_types.in_type_size = sizeof(type)
+/**
+ * Initializer for the show_value() method of a scheduler_source_type
+ *
+ * @param prefix	prefix to the scheduler_source_type field in the
+ *			super-class initializer
+ * @param _show_value	function to set as the show_value method
+ */
+#define __SCHEDULER_SOURCE_SHOW_VALUE(prefix, _show_value)	\
+	prefix show_value = _show_value
+
+/*
+ * Structure representing a scheduler source. Must be initiliazed with
+ * scheduler_source_init()
+ */
+struct scheduler_source {
+	struct scheduler_source_type *type;
+	struct list_head pub_sub_head;
+	spinlock_t lock;
+};
+
+/**
+ * Get the scheduler_source_type of a scheduler_source
+ *
+ * @param source	source to get the type of
+ *
+ * @return		pointer to the scheduler_source_type of source
+ */
+static inline
+struct scheduler_source_type *
+scheduler_source_type_of(struct scheduler_source *source)
+{
+	return source->type;
+}
+
+/* Type definitions for scheduler_sink */
+
+struct scheduler_sink;
+
+/**
+ * prototype for the method to notify a scheduler_sink of an update from
+ * its source
+ *
+ * @param sink		sink to be notified
+ * @param source	source notifying sink
+ */
+typedef void sink_update_value_t(struct scheduler_sink *,
+				 struct scheduler_source *);
+
+/* To initialize with SCHEDULER_SINK_TYPE_INIT. */
+struct scheduler_sink_type {
+	sink_update_value_t *update_value;
+	/* types used when calling scheduler_sink_get_value() */
+	struct get_value_types get_value_types;
+};
+
+/**
+ * Mandatory macro to define a scheduler_sink_type.
+ *
+ * @param _update_value	update_value() method of the sink
+ * @param value_type	string containing the type name of source's values
+ * @param value_type_size
+ *			size in bytes of a value_type value
+ * @param get_param_type
+ *			string containing the type name of the parameters for
+ *			the get_value method, or NULL
+ * @param get_param_type_size
+ *			size in bytes of a get_param_type parameter, or 0
+ */
+#define SCHEDULER_SINK_TYPE_INIT(_update_value,			      \
+				 value_type, value_type_size,	      \
+				 get_param_type, get_param_type_size) \
+	{							      \
+		.update_value = _update_value,			      \
+		.get_value_types = {				      \
+			.out_type = value_type,			      \
+			.out_type_size = value_type_size,	      \
+			.in_type = get_param_type,		      \
+			.in_type_size = get_param_type_size	      \
+		}						      \
+	}
+
+/**
+ * Initializer for the update_value() method of a scheduler_sink_type
+ *
+ * @param prefix	prefix to the scheduler_sink_type field in the
+ *			super-class initializer
+ * @param _update_value	function to set as the update_value() method
+ */
+#define __SCHEDULER_SINK_UPDATE_VALUE(prefix, _update_value)	\
+	prefix update_value = _update_value
+
+/**
+ * Initializer for the type of value output by the get_value() methods of
+ * sources connected to sinks of a scheduler_sink_type
+ *
+ * @param prefix	prefix to the scheduler_sink_type field in the
+ *			super-class initializer
+ * @param type		litteral expression of the type to set
+ */
+#define __SCHEDULER_SINK_VALUE_TYPE(prefix, type)		\
+	prefix get_value_types.out_type = #type,		\
+	.prefix get_value_types.out_type_size = sizeof(type)
+
+/**
+ * Initializer for the type of parameter taken by the get_value() methods of
+ * sources connected to sinks of a scheduler_sink_type
+ *
+ * @param prefix	prefix to the scheduler_sink_type field in the
+ *			super-class initializer
+ * @param type		litteral expression of the type to set
+ */
+#define __SCHEDULER_SINK_PARAM_TYPE(prefix, type)		\
+	prefix get_value_types.in_type = #type,			\
+	.prefix get_value_types.in_type_size = sizeof(type)
+
+/*
+ * Structure representing a scheduler sink. Must be initialized with
+ * scheduler_sink_init()
+ */
+struct scheduler_sink {
+	struct scheduler_sink_type *type;
+	struct scheduler_source *source;
+	struct list_head pub_sub_list;
+	int subscribed;
+
+	/* Remote access handling */
+	struct remote_pipe_desc remote_pipe;
+};
+
+/*
+ * To initialize with SCHEDULER_PIPE_TYPE[_INIT]. Initialization must be
+ * completed by scheduler_pipe_type_init() at runtime.
+ */
+struct scheduler_pipe_type {
+	struct config_item_type item_type;
+	struct scheduler_source_type *source_type;
+	struct scheduler_sink_type *sink_type;
+};
+
+/**
+ * Mandatory macro to define a scheduler_pipe_type. Can be used through
+ * SCHEDULER_PIPE_TYPE.
+ *
+ * @param owner		module providing the pipe type
+ * @param item_ops	config_item_operations for the pipe configfs item
+ * @param group_ops	config_group_operations for the pipe configfs group
+ * @param _source_type	type of sources of this pipe type or NULL
+ * @param _sink_type	type of sinks of this pipe type or NULL
+ */
+#define SCHEDULER_PIPE_TYPE_INIT(owner, item_ops, group_ops, \
+				 _source_type, _sink_type)   \
+	{						     \
+		.item_type = {				     \
+			.ct_owner = owner,		     \
+			.ct_item_ops = item_ops,	     \
+			.ct_group_ops = group_ops,	     \
+			.ct_attrs = NULL		     \
+		},					     \
+		.source_type = _source_type,		     \
+		.sink_type = _sink_type,		     \
+	}
+
+/**
+ * Convenience macro to define a scheduler_pipe_type.
+ *
+ * @param name		name of the variable for the scheduler_pipe_type
+ * @param item_ops	config_item_operations for the pipe configfs item
+ * @param group_ops	config_group_operations for the pipe configfs group
+ * @param source_type	pointer to the type of sources of this pipe type or NULL
+ * @param sink_type	pointer tp the type of sinks of this pipe type or NULL
+ */
+#define SCHEDULER_PIPE_TYPE(name, item_ops, group_ops, source_type, sink_type) \
+	struct scheduler_pipe_type name =				       \
+		SCHEDULER_PIPE_TYPE_INIT(THIS_MODULE, item_ops, group_ops,     \
+					 source_type, sink_type)
+
+/**
+ * Initializer for the source_type of sources of a scheduler_pipe_type
+ *
+ * @param prefix	prefix to the scheduler_pipe_type field in the
+ *			super-class initializer
+ * @param _source_type	pointer to the source type to set
+ */
+#define __SCHEDULER_PIPE_SOURCE_TYPE(prefix, _source_type)	\
+	prefix source_type = _source_type
+
+/**
+ * Get the scheduler_pipe_type embedding a config_item_type
+ *
+ * @param type		pointer to the embedded config_item_type
+ *
+ * @return		pointer to the scheduler_pipe_type embedding type
+ */
+static inline
+struct scheduler_pipe_type *
+to_scheduler_pipe_type(struct config_item_type *type)
+{
+	return container_of(type, struct scheduler_pipe_type, item_type);
+}
+
+/*
+ * Structure representing a scheduler sink. Must be initialized with
+ * scheduler_pipe_init()
+ */
+struct scheduler_pipe {
+	struct config_group config;
+	struct scheduler_source *source;
+	struct scheduler_sink *sink;
+};
+
+/**
+ * Get the scheduler_pipe embedding a config_item
+ *
+ * @param item		pointer to the embedded config_item
+ *
+ * @return		pointer to the scheduler_pipe embedding item
+ */
+static inline
+struct scheduler_pipe *to_scheduler_pipe(struct config_item *item)
+{
+	return container_of(item, struct scheduler_pipe, config.cg_item);
+}
+
+/**
+ * Get the scheduler_pipe_type of a scheduler_pipe
+ *
+ * @param pipe		pipe to get the type of
+ *
+ * @return		pointer to the scheduler_pipe_type of pipe
+ */
+static inline
+struct scheduler_pipe_type *scheduler_pipe_type_of(struct scheduler_pipe *pipe)
+{
+	return to_scheduler_pipe_type(pipe->config.cg_item.ci_type);
+}
+
+/**
+ * Initialize a scheduler_source
+ *
+ * @param source	source to initialize
+ * @param type		type of the source
+ */
+void scheduler_source_init(struct scheduler_source *source,
+			   struct scheduler_source_type *type);
+/**
+ * Cleanup a scheduler_source (eg. before freeing the structure)
+ *
+ * @param source	source to cleanup
+ */
+static inline void scheduler_source_cleanup(struct scheduler_source *source)
+{
+}
+
+/**
+ * Initialize a scheduler_sink
+ *
+ * @param sink		sink to initialize
+ * @param type		type of the sink
+ */
+void scheduler_sink_init(struct scheduler_sink *sink,
+			 struct scheduler_sink_type *type);
+/**
+ * Cleanup a scheduler_sink (eg. before freeing the structure)
+ *
+ * @param sink		sink to cleanup
+ */
+void scheduler_sink_cleanup(struct scheduler_sink *sink);
+
+/**
+ * Tells whether a sink can safely call scheduler_source_get_value() on a
+ * source, that is whether the types are compatible.
+ *
+ * @param sink_type	type of the sink
+ * @param source_type	type of the source
+ *
+ * @return		0 if types are not compatible,
+ *			not 0 otherwise
+ */
+int scheduler_types_compatible(const struct scheduler_sink_type *sink_type,
+			       const struct scheduler_source_type *source_type);
+
+/**
+ * Connect a sink to a source
+ *
+ * @param sink		sink to connect
+ * @param source	source to connect the sink to
+ * @param subscribe	subscribe to source's updates if not 0
+ */
+void scheduler_sink_connect(struct scheduler_sink *sink,
+			    struct scheduler_source *source,
+			    int subscribe);
+/**
+ * Disconnect a sink from a source and break any pending request to a remote
+ * pipe
+ * Caller must synchronize_rcu before freeing sink and source, or reconnect sink
+ * to a source
+ * May block
+ *
+ * @param sink		sink to disconnect
+ */
+void scheduler_sink_disconnect(struct scheduler_sink *sink);
+
+/**
+ * Get the source connected to a sink, or return NULL
+ * Caller must take care of races with
+ * scheduler_pipe_sink_{connect,disconnect}(), or hold RCU lock until it stops
+ * using the source.
+ *
+ * @param sink		sink to get the source of
+ *
+ * @return		pointer to the connected source, or NULL
+ */
+struct scheduler_source *
+scheduler_sink_get_peer_source(struct scheduler_sink *sink);
+
+/**
+ * Test whether a source has subscribers
+ * Caller may have to lock the source to be able to rely on the result
+ *
+ * @param source	source to test
+ *
+ * @return		true iff source has at least one subscriber
+ */
+int scheduler_source_has_subscribers(struct scheduler_source *source);
+/**
+ * Test whether a sink has subscribed to a source
+ *
+ * @param sink		sink to test
+ *
+ * @return		true iff sink has subscribed to a source
+ */
+int scheduler_sink_subscribed(struct scheduler_sink *sink);
+
+/**
+ * Lock a source. This will *not* prevent the source value from being queried
+ * nor prevent the scheduler_pipe subsystem to propagate update notifications
+ * from this source. However this will block subscriptions/unsubscriptions.
+ *
+ * @param source	source to lock
+ */
+static inline void scheduler_source_lock(struct scheduler_source *source)
+{
+	spin_lock(&source->lock);
+}
+
+/**
+ * Unlock a source
+ *
+ * @param source	source to unlock
+ */
+static inline void scheduler_source_unlock(struct scheduler_source *source)
+{
+	spin_unlock(&source->lock);
+}
+
+/**
+ * Get the value from a source
+ *
+ * @param source	source to query
+ * @param value_p	array of values to be filled
+ * @param nr		max number of values to fill
+ * @param in_value_p	array of parameters, or NULL
+ * @param in_nr		number of elements in in_value_p
+ *
+ * @return		number of values filled, or
+ *			negative error code
+ */
+int scheduler_source_get_value(struct scheduler_source *source,
+			       void *value_p, unsigned int nr,
+			       const void *in_value_p, unsigned int in_nr);
+/**
+ * Show the value from a source as a string (for instance through configfs)
+ *
+ * @param source	source to query
+ * @param page		buffer to store the value (4 Kbytes size)
+ *
+ * @return		number of bytes written to buffer, or
+ *			negative error code
+ */
+ssize_t scheduler_source_show_value(struct scheduler_source *source,
+				    char *page);
+
+/**
+ * Publish an update to the subscribers of a source
+ *
+ * @param source	source publish an update
+ */
+void scheduler_source_publish(struct scheduler_source *source);
+
+/**
+ * Get the value from the source connected to a sink
+ *
+ * @param sink		sink to query
+ * @param value_p	array of values to be filled
+ * @param nr		max number of values to fill
+ * @param in_value_p	array of parameters, or NULL
+ * @param in_nr		number of elements in in_value_p
+ *
+ * @return		number of values filled, or
+ *			negative error code
+ */
+int scheduler_sink_get_value(struct scheduler_sink *sink,
+			     void *value_p, unsigned int nr,
+			     const void *in_value_p, unsigned int in_nr);
+
+/**
+ * Show the value collected by a sink (from a its connected source) as a string
+ * (for instance through configfs)
+ *
+ * @param sink		sink to query
+ * @param page		buffer to store the value (4 Kbytes size)
+ *
+ * @return		number of bytes written to buffer, or
+ *			negative error code
+ */
+ssize_t scheduler_sink_show_value(struct scheduler_sink *sink, char *page);
+
+
+/**
+ * Complete initialization of a scheduler_pipe_type
+ *
+ * @param type		type to finish initializing
+ * @param attrs		NULL-terminated list of custom configfs-attribute, or
+ *			NULL
+ *
+ * @return		0 if successful,
+ *			-ENOMEM if not sufficient memory could be allocated
+ */
+int scheduler_pipe_type_init(struct scheduler_pipe_type *type,
+			     struct configfs_attribute **attrs);
+/**
+ * Cleanup a scheduler_pipe_type (eg. before freeing the structure)
+ *
+ * @param type		type to cleanup
+ */
+void scheduler_pipe_type_cleanup(struct scheduler_pipe_type *type);
+
+/**
+ * Initialize a scheduler_pipe
+ *
+ * @param pipe		source to initialize
+ * @param name		name of the configfs directory representing the source
+ * @param type		type of the pipe
+ * @param source	source part of the pipe or NULL
+ * @param sink		sink part of the pipe or NULL
+ * @param default_groups
+ *			NULL terminated array of custom config_groups displayed
+ *			as subdirs of the pipe, or NULL
+ *
+ * @return		0 if successful,
+ *			negative error code if error
+ */
+int scheduler_pipe_init(struct scheduler_pipe *pipe,
+			const char *name,
+			struct scheduler_pipe_type *type,
+			struct scheduler_source *source,
+			struct scheduler_sink *sink,
+			struct config_group **default_groups);
+/**
+ * Cleanup a scheduler_pipe (eg. before freeing the structure)
+ *
+ * @param pipe		pipe to cleanup
+ */
+static inline void scheduler_pipe_cleanup(struct scheduler_pipe *pipe)
+{
+}
+
+/* Functions to be used by super classes of scheduler_pipe */
+
+/**
+ * Super classes are responsible for calling this method when their respective
+ * show_attribute() method is called.
+ * Depending on the value returned in *handled, the super class then knows
+ * whether the attribute is a pipe default one, or a super class specific one.
+ *
+ * @param pipe		pipe which attribute is read
+ * @param attr		attribute read
+ * @param page		buffer of at least 4Kbytes length to hold the result
+ * @param handled	points to an integer which on return will be not 0 iff
+ *			the attribute is a pipe default one
+ *
+ * @return		number of bytes written in page, or
+ *			negative error code
+ */
+ssize_t scheduler_pipe_show_attribute(struct scheduler_pipe *pipe,
+				      struct configfs_attribute *attr,
+				      char *page,
+				      int *handled);
+/**
+ * Super classes are responsible for calling this method when their respective
+ * store_attribute() method is called.
+ * Depending on the value returned in *handled, the super class then knows
+ * whether the attribute is a pipe default one, or a super class specific one.
+ *
+ * @param pipe		pipe which attribute is written to
+ * @param attr		attribute written to
+ * @param page		buffer holding the data to write
+ * @param handled	points to an integer which on return will be not 0 iff
+ *			the attribute is a pipe default one
+ * @param count		number of bytes to write to attribute
+ *
+ * @return		number of bytes written to attribute, or
+ *			negative error code
+ */
+ssize_t scheduler_pipe_store_attribute(struct scheduler_pipe *pipe,
+				       struct configfs_attribute *attr,
+				       const char *page, size_t count,
+				       int *handled);
+
+#endif /* __KRG_SCHEDULER_PIPE_H__ */
diff -ruN linux-2.6.29/include/kerrighed/scheduler/placement.h android_cluster/linux-2.6.29/include/kerrighed/scheduler/placement.h
--- linux-2.6.29/include/kerrighed/scheduler/placement.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/scheduler/placement.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,21 @@
+#ifndef __KRG_SCHEDULER_PLACEMENT_H__
+#define __KRG_SCHEDULER_PLACEMENT_H__
+
+#include <kerrighed/sys/types.h>
+
+struct task_struct;
+
+/**
+ * Compute the "best" node on which a new task should be placed.
+ * The node is chosen by asking to each scheduler attached to parent. Ties
+ * are broken as described in placement.c
+ *
+ * @param parent	creator of the new task
+ *
+ * @return		a valid node id (at least when computed), or
+ *			KERRIGHED_NODE_ID_NONE if no scheduler attached to
+ *			parent cares
+ */
+kerrighed_node_t new_task_node(struct task_struct *parent);
+
+#endif /* __KRG_SCHEDULER_PLACEMENT_H__ */
diff -ruN linux-2.6.29/include/kerrighed/scheduler/policy.h android_cluster/linux-2.6.29/include/kerrighed/scheduler/policy.h
--- linux-2.6.29/include/kerrighed/scheduler/policy.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/scheduler/policy.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,228 @@
+#ifndef __KRG_SCHEDULER_POLICY_H__
+#define __KRG_SCHEDULER_POLICY_H__
+
+#include <linux/configfs.h>
+#include <linux/list.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/scheduler/global_config.h>
+
+struct task_struct;
+
+/**
+ * This structure represents pluggable scheduling policies for distributing
+ * load in the cluster based on data measured by probes (e.g. CPU usage, memory
+ * usage, ...). User can implement these scheduling policies as separate Linux
+ * kernel modules and inserts them dynamcally into kernel. By doing this, it
+ * extends set of scheduling algorithms for distributing load across the
+ * cluster.
+ * @author Marko Novak, Louis Rilling
+ */
+struct scheduler_policy {
+	struct config_group group; /** representation of scheduling policy in
+				     ConfigFS. */
+	spinlock_t lock; /** lock for synchronizing scheduling policy access. */
+	struct global_config_item global_item; /** global_config subsystem */
+	struct global_config_attrs global_attrs;
+};
+
+/** struct which contains each policy's operations. */
+struct scheduler_policy_operations {
+	struct scheduler_policy *(*new)(const char *name); /* sched policy
+							     * constructor */
+	void (*destroy)(struct scheduler_policy *policy); /* sched policy
+							   * destructor */
+	/* notifier of node set changes */
+	void (*update_node_set)(struct scheduler_policy *policy,
+				const krgnodemask_t *new_set,
+				const krgnodemask_t *removed_set,
+				const krgnodemask_t *added_set);
+	/* process placement function
+	 * called when a task attached to this policy creates a new task */
+	kerrighed_node_t (*new_task_node)(struct scheduler_policy *policy,
+					  struct task_struct *parent);
+};
+
+/* Same limitation as configfs (see SIMPLE_ATTR_SIZE in fs/configfs/file.c) */
+#define SCHEDULER_POLICY_ATTR_SIZE 4096
+
+/*
+ * This struct is used for representing scheduling policies' attributes.
+ * It contains attribute-specific functions for reading and storing attribute
+ * value.
+ */
+struct scheduler_policy_attribute {
+	struct configfs_attribute attr;
+
+	/** function for reading attribute's value */
+	ssize_t (*show)(struct scheduler_policy *, char *);
+	/** function for storing attribute's value */
+	ssize_t (*store)(struct scheduler_policy *, const char *, size_t);
+};
+
+/*
+ * To be initialized with SCHEDULER_POLICY_TYPE[_INIT]. The sched policy
+ * subsystem will complete init at registration.
+ */
+struct scheduler_policy_type {
+	const char *name;
+	struct config_item_type item_type;
+	struct scheduler_policy_operations *ops;
+	struct scheduler_policy_attribute **attrs;
+	struct list_head list;	/** list of registered sched policy types */
+};
+
+/**
+ * Mandatory macro to define a scheduling policy type. Can be used through the
+ * SCHEDULER_POLICY_TYPE macro.
+ *
+ * @param owner		Module defining the scheduler_policy type
+ * @param _name		Unique name for the scheduler_policy type
+ * @param _ops		scheduler_policy_operations for this type
+ * @param _attrs	NULL-terminated array of scheduler_policy_attribute,
+ *			or NULL
+ */
+#define SCHEDULER_POLICY_TYPE_INIT(owner, _name, _ops, _attrs) \
+	{						       \
+		.name = _name,				       \
+		.item_type = { .ct_owner = owner, },	       \
+		.ops = _ops,				       \
+		.attrs = _attrs,			       \
+	}
+
+/**
+ * Convenience macro to define a scheduling policy type.
+ *
+ * @param var		Name of the variable containing the type
+ * @param name		Unique name of the scheduler_policy type
+ * @param ops		scheduler_policy_operations for this type
+ * @param attrs		NULL-terminated array of scheduler_policy_attribute,
+ *			or NULL
+ */
+#define SCHEDULER_POLICY_TYPE(var, name, ops, attrs)			  \
+	struct scheduler_policy_type var =				  \
+		SCHEDULER_POLICY_TYPE_INIT(THIS_MODULE, name, ops, attrs)
+
+/**
+ * This function initializes a new scheduling policy. Must be called by
+ * scheduler_policy constructors.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param policy	pointer to the scheduler_policy to init
+ * @param name		name of the scheduling policy. This name must be the one
+ *			provided as argument to the constructor.
+ * @param type		type of the scheduler_policy
+ * @param def_groups	NULL-terminated array of subdirs of the scheduler_policy
+ *			directory, or NULL
+ *
+ * @return		0 if successul,
+ *			-ENODEV is module is unloading (should not happen!),
+ *			-ENOMEM if not sufficient memory could be allocated.
+ */
+int scheduler_policy_init(struct scheduler_policy *policy,
+			  const char *name,
+			  struct scheduler_policy_type *type,
+			  struct config_group *def_groups[]);
+
+/**
+ * This function frees all the memory taken by a scheduling policy. Must be
+ * called by the scheduler_policy destructor.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param policy	pointer to scheduler_policy whose memory we want to free
+ */
+void scheduler_policy_cleanup(struct scheduler_policy *policy);
+
+/**
+ * Get a reference on a sched policy
+ *
+ * @param policy	sched policy to get a reference on
+ */
+static inline void scheduler_policy_get(struct scheduler_policy *policy)
+{
+	if (policy)
+		config_group_get(&policy->group);
+}
+
+/**
+ * Put a reference on a sched policy
+ *
+ * @param policy	sched policy which reference to put
+ */
+static inline void scheduler_policy_put(struct scheduler_policy *policy)
+{
+	if (policy)
+		config_group_put(&policy->group);
+}
+
+/**
+ * Notify a policy that its node set was updated
+ * Called with the scheduler's node set mutex locked.
+ *
+ * @param policy	policy to notify
+ * @param new_set	new node set of the policy
+ * @param removed_set	nodes just removed from the set
+ * @param added_set	nodes just added to the set
+ */
+static inline
+void scheduler_policy_update_node_set(struct scheduler_policy *policy,
+				      const krgnodemask_t *new_set,
+				      const krgnodemask_t *removed_set,
+				      const krgnodemask_t *added_set)
+{
+	struct scheduler_policy_type *type =
+		container_of(policy->group.cg_item.ci_type,
+			     struct scheduler_policy_type, item_type);
+	if (type->ops->update_node_set)
+		type->ops->update_node_set(policy,
+					   new_set,
+					   removed_set,
+					   added_set);
+}
+
+/**
+ * Compute the best node to place a new task created by parent according to this
+ * scheduling policy
+ * The new task may not be created on the selected node at all, since another
+ * scheduling policy attached to the same task may decide differently and win.
+ *
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param policy	policy that is consulted
+ * @param parent	parent of the task to be created
+ */
+static inline
+kerrighed_node_t scheduler_policy_new_task_node(struct scheduler_policy *policy,
+						struct task_struct *parent)
+{
+	struct scheduler_policy_type *type =
+		container_of(policy->group.cg_item.ci_type,
+			     struct scheduler_policy_type, item_type);
+	if (type->ops->new_task_node)
+		return type->ops->new_task_node(policy, parent);
+	return KERRIGHED_NODE_ID_NONE;
+}
+
+/**
+ * This function is used for registering newly added scheduling policy types.
+ * Once a type is registered, new scheduling policies of this type can be
+ * created when user does mkdir with the type name.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param type		pointer to the scheduling policy type to register.
+ *
+ * @return		0 if successful,
+ *			-EEXIST if scheduling policy type with the same name
+ *				is already registered.
+ */
+int scheduler_policy_type_register(struct scheduler_policy_type *type);
+
+/**
+ * This function is used for removing scheduling policy registrations.
+ * Must *only* be called at module unloading.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param type		pointer to the scheduling policy type to unregister.
+ */
+void scheduler_policy_type_unregister(struct scheduler_policy_type *type);
+
+#endif /* __KRG_SCHEDULER_POLICY_H__ */
diff -ruN linux-2.6.29/include/kerrighed/scheduler/port.h android_cluster/linux-2.6.29/include/kerrighed/scheduler/port.h
--- linux-2.6.29/include/kerrighed/scheduler/port.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/scheduler/port.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,577 @@
+#ifndef __KRG_SCHEDULER_PORT_H__
+#define __KRG_SCHEDULER_PORT_H__
+
+#include <linux/module.h>
+#include <linux/configfs.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/types.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/scheduler/pipe.h>
+#include <kerrighed/scheduler/global_config.h>
+
+/*
+ * A scheduler_port is a scheduler_pipe having at least a scheduler_sink. The
+ * sink can connect to another scheduler_pipe having a source.
+ *
+ * Doing mkdir in a scheduler_port directory creates a new scheduler_port having
+ * a source, which type is named after the new subdir name. The creator port's
+ * sink is connected to the created port's source.
+ *
+ * Creating a symlink from an entry of port's directory to another
+ * scheduler_pipe directory (currently this is only allowed for probe sources)
+ * connects the port's sink to the symlink target's source.
+ *
+ * In both kind of connections, the connection is publish-subscribe enabled if
+ * the port provides an update_value() method and either has no source or
+ * its source already has subscribers.
+ */
+
+struct scheduler_port;
+struct scheduler_port_type;
+
+/**
+ * prototype for a port callback that gets the value of a remote lower source
+ *
+ * @param port		port for which the callback is called
+ * @param node		node to get the value from
+ * @param value_p	array of values to be filled
+ * @param nr		max number of values to fill
+ * @param in_value_p	array of parameters, or NULL
+ * @param in_nr		number of elements in in_value_p
+ *
+ * @return		number of values filled, or
+ *			-EAGAIN if the request is pending and the caller should
+ *			retry later, or
+ *			other negative error code
+ */
+typedef int port_get_remote_value_t(struct scheduler_port *port,
+				    kerrighed_node_t node,
+				    void *value_p, unsigned int nr,
+				    const void *in_value_p,
+				    unsigned int in_nr);
+
+/**
+ * prototype for a port type callback that creates a new port
+ *
+ * @param name		directory name of the new port
+ *
+ * @return		valid pointer to a new port, or
+ *			NULL if no port could be successfully created
+ */
+typedef struct scheduler_port *port_new_t(const char *name);
+
+/**
+ * prototype for a port type callback that destroys a port (for instance frees
+ * memory)
+ *
+ * @param port		port to destroy
+ */
+typedef void port_destroy_t(struct scheduler_port *);
+
+
+/*
+ * Structure representing a type of port. Must be initialized with
+ * SCHEDULER_PORT_TYPE_INIT (for instance through BEGIN_SCHEDULER_PORT_TYPE and
+ * SCHEDULER_PORT_* helpers).
+ */
+struct scheduler_port_type {
+	const char *name;
+	struct scheduler_sink_type sink_type;
+	struct scheduler_pipe_type pipe_type;
+	/* method to get the value from a remote node */
+	port_get_remote_value_t *get_remote_value;
+	/** constructor of ports of this type */
+	port_new_t *new;
+	/** destructor of ports of this type */
+	port_destroy_t *destroy;
+	struct list_head list;
+};
+
+/* Structure representing a port */
+struct scheduler_port {
+	struct scheduler_sink sink;
+	struct scheduler_pipe pipe;
+	struct scheduler_pipe *peer_pipe;
+	struct global_config_item global_item;	/** global config item referring
+						  * to the connected source */
+	struct global_config_attrs global_attrs;
+};
+
+struct scheduler_port_attribute;
+
+/* Same limitation as configfs (see SIMPLE_ATTR_SIZE in fs/configfs/file.c) */
+#define SCHEDULER_PORT_ATTR_SIZE 4096
+
+/**
+ * prototype for a port_attribute callback that reads a port attribute
+ *
+ * @param port		port which attribute is read
+ * @param attr		structure describing the attribute read
+ * @param page		4Kbytes buffer to fill in
+ *
+ * @return		number of bytes written to page, or
+ *			negative error code
+ */
+typedef ssize_t port_attribute_show_t(struct scheduler_port *port,
+				      struct scheduler_port_attribute *attr,
+				      char *page);
+/**
+ * prototype for a port_attribute callback that modifies a port attribute
+ *
+ * @param port		port which attribute is read
+ * @param attr		structure describing the attribute read
+ * @param buffer	buffer containing the data to modify attr
+ * @param count		size in bytes of the modification data
+ *
+ * @return		number of bytes read from buffer, or
+ *			negative error code
+ */
+typedef ssize_t port_attribute_store_t(struct scheduler_port *port,
+				       struct scheduler_port_attribute *attr,
+				       const char *buffer, size_t count);
+
+/*
+ * Structure representing a scheduler_port attribute. Must be initialized with
+ * SCHEDULER_PORT_ATTRIBUTE_INIT.
+ */
+struct scheduler_port_attribute {
+	struct configfs_attribute config;
+	/** method to read a custom attribute of a port of this type */
+	port_attribute_show_t *show;
+	/** method to modify a custom attribute of a port of this type */
+	port_attribute_store_t *store;
+};
+
+/**
+ * Mandatory macro to define a scheduler_port_type. Can be called through
+ * the BEGIN_SCHEDULER_PORT_TYPE helper.
+ *
+ * @param port_type	variable name of the port type
+ * @param owner		module providing this port type
+ * @param _name		string containing the unique name of this port type
+ * @param snk_update_value
+ *			method to give to the scheduler_sink of a
+ *			port of this type
+ * @param snk_value_type
+ *			string containing the type name of lower source's values
+ * @param snk_value_type_size
+ *			size in bytes of a snk_value_type value
+ * @param snk_get_param_type
+ *			string containing the type name of the parameters for
+ *			the sink's get_value() calls, or NULL
+ * @param snk_get_param_type_size
+ *			size in bytes of a snk_get_param_type parameter
+ * @param source_type	optional source type to attach to the
+ *			scheduler_pipe_type of this port type
+ * @param _get_remote_value
+ *			get_remote_value() method of this port type
+ * @param _new		creator of ports of this type
+ * @param _destroy	destructors of ports of this type
+ */
+#define SCHEDULER_PORT_TYPE_INIT(port_type, owner, _name,		\
+				 snk_update_value,			      \
+				 snk_value_type, snk_value_type_size,	      \
+				 snk_get_param_type, snk_get_param_type_size, \
+				 source_type,				\
+				 _get_remote_value,			\
+				 _new,					\
+				 _destroy)				\
+	{								\
+		.name = _name,						\
+		.sink_type = SCHEDULER_SINK_TYPE_INIT(			\
+			snk_update_value,				\
+			snk_value_type,					\
+			snk_value_type_size,				\
+			snk_get_param_type,				\
+			snk_get_param_type_size),			\
+		.pipe_type = SCHEDULER_PIPE_TYPE_INIT(			\
+			owner,						\
+			NULL, NULL,					\
+			source_type, &port_type.sink_type),		\
+		.get_remote_value = _get_remote_value,		\
+		.new = _new,					\
+		.destroy = _destroy,				\
+	}
+
+/*
+ * Convenience macros to define a scheduler_port_type
+ *
+ * These convenience macros should be used the following way:
+ *
+ * First, implemented methods must be defined using the
+ * DEFINE_SCHEDULER_PORT_<method> macros. Second, the scheduler_port_type must
+ * be filled using {BEGIN,END}_SCHEDULER_PORT_TYPE and SCHEDULER_PORT_* macros:
+ *
+ *	BEGIN_SCHEDULER_PORT_TYPE(name),
+ *		.SCHEDULER_PORT_VALUE_TYPE(name, type),
+ * if needed:
+ *		.SCHEDULER_PORT_<method>(name),
+ *		.SCHEDULER_PORT_PARAM_TYPE(name, type),
+ *		.SCHEDULER_PORT_ATTRS(name, attrs),
+ *		...
+ * and finally:
+ *	END_SCHEDULER_PORT_TYPE(name);
+ */
+
+/**
+ * Convenience macro to start the definition of a scheduler_port_type. The
+ * definition must end with END_SCHEDULER_PORT_TYPE(name). The variable will be
+ * called name_type.
+ *
+ * @param name		name of the scheduler_port type
+ */
+#define BEGIN_SCHEDULER_PORT_TYPE(_name)			   \
+	struct scheduler_port_type _name##_type = {		   \
+		.name = #_name,					   \
+		.sink_type = SCHEDULER_SINK_TYPE_INIT(NULL,	   \
+						      NULL, 0,	   \
+						      NULL, 0),	   \
+		.pipe_type = SCHEDULER_PIPE_TYPE_INIT(		   \
+			THIS_MODULE,				   \
+			NULL, NULL,				   \
+			NULL, &_name##_type.sink_type)
+
+/* Helper to define convenience initializers in super classes */
+#define __SCHEDULER_PORT_UPDATE_VALUE(prefix, name)			\
+	__SCHEDULER_SINK_UPDATE_VALUE(prefix sink_type.,		\
+				      name##_sink_update_value)
+
+/**
+ * Convenience macro to attach a previously defined update_value() method to a
+ * scheduler_port type. The update_value() method must have been defined earlier
+ * with DEFINE_SCHEDULER_PORT_UPDATE_VALUE(name, ...).
+ *
+ * @param name		must match the name used with BEGIN_SCHEDULER_PORT_TYPE
+ */
+#define SCHEDULER_PORT_UPDATE_VALUE(name)	\
+	__SCHEDULER_PORT_UPDATE_VALUE(, name)
+
+/* Helper to define convenience initializers in super classes */
+#define __SCHEDULER_PORT_VALUE_TYPE(prefix, name, type)		\
+	__SCHEDULER_SINK_VALUE_TYPE(prefix sink_type., type)
+
+/**
+ * Convenience macro to declare the value type of a scheduler port. Must be used
+ * within all BEGIN_SCHEDULER_PORT_TYPE sections.
+ *
+ * @param name		must match the name used with BEGIN_SCHEDULER_PORT_TYPE
+ * @param type		litteral expression of the value type read by the sink
+ */
+#define SCHEDULER_PORT_VALUE_TYPE(name, type)		\
+	__SCHEDULER_PORT_VALUE_TYPE(, name, type)
+
+/* Helper to define convenience initializers in super classes */
+#define __SCHEDULER_PORT_PARAM_TYPE(prefix, name, type)		\
+	__SCHEDULER_SINK_PARAM_TYPE(prefix sink_type., type)
+
+/**
+ * Convenience macro to declare the parameter type used when calling the
+ * get_value() method of a connected source.
+ *
+ * @param name		must match the name used with BEGIN_SCHEDULER_PORT_TYPE
+ * @param type		litteral expression of the parameter type
+ */
+#define SCHEDULER_PORT_PARAM_TYPE(name, type)		\
+	__SCHEDULER_PORT_PARAM_TYPE(, name, type)
+
+/* Helper to define convenience initializers in super classes */
+#define __SCHEDULER_PORT_SOURCE_TYPE(prefix, name, source_type)		\
+	__SCHEDULER_PIPE_SOURCE_TYPE(prefix pipe_type., source_type)
+
+/**
+ * Convenience macro to attach a source type to the pipe_type embedded in a
+ * scheduler_port_type
+ *
+ * @param name		must match the name used with BEGIN_SCHEDULER_PORT_TYPE
+ * @param source_type	source_type to attach
+ */
+#define SCHEDULER_PORT_SOURCE_TYPE(name, source_type)		\
+	__SCHEDULER_PORT_SOURCE_TYPE(, name, source_type)
+
+/* Helper to define convenience initializers in super classes */
+#define __SCHEDULER_PORT_GET_REMOTE_VALUE(prefix, name)		\
+	prefix get_remote_value = name##_get_remote_value
+
+/**
+ * Convenience macro to attach a previously defined get_remte_value() method to
+ * a scheduler_port type. The get_remote_value() method must have been defined
+ * earlier with DEFINE_SCHEDULER_PORT_GET_REMOTE_VALUE(name, ...).
+ *
+ * @param name		must match the name used with BEGIN_SCHEDULER_PORT_TYPE
+ */
+#define SCHEDULER_PORT_GET_REMOTE_VALUE(name)		\
+	__SCHEDULER_PORT_GET_REMOTE_VALUE(, name)
+
+/* Helper to define convenience initializers in super classes */
+#define __SCHEDULER_PORT_NEW(prefix, name)	\
+	prefix new = name##_new
+
+/**
+ * Convenience macro to attach a previously defined constructor to a port type.
+ * The constructor must have been defined earlier and must be called name_new.
+ *
+ * @param name		must match the name used with BEGIN_SCHEDULER_PORT_TYPE
+ */
+#define SCHEDULER_PORT_NEW(name)		\
+	__SCHEDULER_PORT_NEW(, name)
+
+/* Helper to define convenience initializers in super classes */
+#define __SCHEDULER_PORT_DESTROY(prefix, name)	\
+	prefix destroy = name##_destroy
+
+/**
+ * Convenience macro to attach a previously defined destructor to a port type.
+ * The destructor must have been defined earlier and must be called
+ * name_destroy.
+ *
+ * @param name		must match the name used with BEGIN_SCHEDULER_PORT_TYPE
+ */
+#define SCHEDULER_PORT_DESTROY(name)		\
+	__SCHEDULER_PORT_DESTROY(, name)
+
+/**
+ * End the definition of a scheduler_port_type. Must close any
+ * BEGIN_SCHEDULER_PORT_TYPE section.
+ *
+ * @param name		must match the name used with BEGIN_SCHEDULER_PORT_TYPE
+ */
+#define END_SCHEDULER_PORT_TYPE(name)		\
+	}
+
+/**
+ * Convenience macro to define an update_value() method for a port.
+ * The method will be called name_update_value.
+ *
+ * @param name		name of the scheduler_port type
+ * @param port		name of the port arg of the method
+ */
+#define DEFINE_SCHEDULER_PORT_UPDATE_VALUE(name, port)			      \
+	static void name##_update_value(struct scheduler_port *);	      \
+	static void name##_sink_update_value(struct scheduler_sink *sink,     \
+					     struct scheduler_source *source) \
+	{								      \
+		name##_update_value(					      \
+			container_of(sink, struct scheduler_port, sink));     \
+	}								      \
+	static void name##_update_value(struct scheduler_port *port)
+
+/**
+ * Convenience macro to define an get_remote_value() method for a port, without
+ * parameters given to the source. The method will be called
+ * name_get_remote_value.
+ *
+ * @param name		name of the scheduler_port type
+ * @param port		name of the port arg of the method
+ * @param value_type	type of the values read by the sink (eg. int)
+ * @param value_p	name of the type *arg of the method
+ * @param nr_value	name of the array length parameter of the method
+ */
+#define DEFINE_SCHEDULER_PORT_GET_REMOTE_VALUE(name, port,		      \
+					       value_type, value_p, nr_value) \
+	static int name##_get_remote_value(struct scheduler_port *,	      \
+					   value_type *, int);		      \
+	static int name##_get_remote_value_untyped(			      \
+		struct scheduler_port *port,				      \
+		void *vp, int nr_v,					      \
+		const void *pp, int nr_p)				      \
+	{								      \
+		return name##_get_remote_value(port, vp, nr_v);		      \
+	}								      \
+	static int name##_get_remote_value(struct scheduler_port *port,       \
+					   value_type *value_p, int nr_value)
+
+/**
+ * Convenience macro to define an get_remote_value() method for a port, with
+ * parameters given to the source. The method will be called
+ * name_get_remote_value.
+ *
+ * @param name		name of the scheduler_port type
+ * @param port		name of the port arg of the method
+ * @param value_type	type of the values read by the sink (eg. int)
+ * @param value_p	name of the type *arg of the method
+ * @param nr_value	name of the array length parameter of the method
+ * @param param_type	type of the parameters given to the source (eg. int)
+ * @param param_p	name of the param_type *arg of the method
+ * @param nr_param	name of the parameters array length arg of the method
+ */
+#define DEFINE_SCHEDULER_PORT_GET_REMOTE_VALUE_WITH_INPUT(		      \
+	name, port,							      \
+	value_type, value_p, nr_value,					      \
+	param_type, param_p, nr_param)					      \
+	static int name##_get_remote_value(struct scheduler_port *,	      \
+					   value_type *, int,		      \
+					   const param_type *, int);	      \
+	static int name##_get_remote_value_untyped(			      \
+		struct scheduler_port *port,				      \
+		void *vp, int nr_v,					      \
+		const void *pp, int nr_p)				      \
+	{								      \
+		return name##_get_remote_value(port, vp, nr_v, pp, nr_p);     \
+	}								      \
+	static int name##_get_remote_value(				      \
+		struct scheduler_port *port,				      \
+		value_type *value_p, int nr_value,			      \
+		const param_type *param_p, int nr_param)
+
+/* End of convenience macros */
+
+/**
+ * Mandatory initializer for a scheduler_port_attribute.
+ *
+ * @param name		name of the attribute entry in the port directory
+ * @param mode		access mode of the attribute entry
+ * @param _show		show callback of the attribute
+ * @param _store	store callback of the attribute
+ */
+#define SCHEDULER_PORT_ATTRIBUTE_INIT(name, mode, _show, _store) \
+	{							 \
+		.config = {					 \
+			.ca_name = name,			 \
+			.ca_owner = THIS_MODULE,		 \
+			.ca_mode = mode				 \
+		},						 \
+		.show = _show,					 \
+		.store = _store					 \
+	}
+
+/* Tool functions for port designers */
+
+/**
+ * Initialize a scheduler port type
+ * Must be called before creating any port of this type. Is called through
+ * scheduler_port_type_register().
+ *
+ * @param type		type to init
+ * @param attrs		NULL-terminated array of pointers to custom attributes,
+ *			or NULL
+ *
+ * @return		0 is successful, or
+ *			-ENOMEM if no sufficient memory could be allocated
+ */
+int scheduler_port_type_init(struct scheduler_port_type *type,
+			     struct configfs_attribute **attrs);
+/**
+ * Free the resources allocated at type initialization
+ *
+ * @param type		type to cleanup
+ */
+void scheduler_port_type_cleanup(struct scheduler_port_type *type);
+
+/**
+ * Initialize and register a new port type
+ *
+ * @param type		type to register
+ * @param attrs		NULL-terminated array of custom configfs_attribute, or
+ *			NULL
+ *
+ * @return		0 if successful,
+ *			-ENOMEM if not sufficient memory could be allocated,
+ *			-EEXIST if a type of this name is already registered.
+ */
+int scheduler_port_type_register(struct scheduler_port_type *type,
+				 struct configfs_attribute **attrs);
+/**
+ * Unregister and cleanup a port type. Must be called at module unload *only*.
+ *
+ * @param type		type to unregister
+ */
+void scheduler_port_type_unregister(struct scheduler_port_type *type);
+
+/**
+ * Initialize a new scheduler_port. Must be called by scheduler_port
+ * constructors.
+ *
+ * @param port		port to initialize
+ * @param name		name of the directory of this port. Must match the name
+ *			given as argument to the constructor.
+ * @param type		type of the new port
+ * @param source	optional source of the object embedding port, or NULL
+ * @param default_groups
+ *			NULL terminated array of custom config_groups displayed
+ *			as subdirs of the port, or NULL
+ *
+ * @return		0 if successful,
+ *			negative error code if error
+ */
+int scheduler_port_init(struct scheduler_port *port,
+			const char *name,
+			struct scheduler_port_type *type,
+			struct scheduler_source *source,
+			struct config_group **default_groups);
+/**
+ * Cleanup a scheduler_port before freeing it. Must be called by the port's
+ * destructor.
+ *
+ * @param port		port to cleanup
+ */
+void scheduler_port_cleanup(struct scheduler_port *port);
+
+/**
+ * Helper to refer to a port through its embedded config_group (for instance
+ * to build the default_groups[] array of a parent config_group)
+ *
+ * @param port		port which config_group address to return
+ *
+ * @return		address of the embedded config_group of the port
+ */
+static inline
+struct config_group *scheduler_port_config_group(struct scheduler_port *port)
+{
+	return &port->pipe.config;
+}
+
+/* Functions to query a port */
+
+/**
+ * Get the value from the source connected to a port
+ *
+ * @param port		port to query
+ * @param value_p	array of values to be filled
+ * @param nr		max number of values to fill
+ * @param in_value_p	array of parameters, or NULL
+ * @param in_nr		number of elements in in_value_p
+ *
+ * @return		number of values filled, or
+ *			negative error code
+ */
+static inline
+int scheduler_port_get_value(struct scheduler_port *port,
+			     void *value_p, unsigned int nr,
+			     const void *in_value_p, unsigned int in_nr)
+{
+	return scheduler_sink_get_value(&port->sink,
+					value_p, nr,
+					in_value_p, in_nr);
+}
+
+/**
+ * Show the value collected by a port (from a its connected source) as a string
+ * (for instance through configfs)
+ *
+ * @param port		port to query
+ * @param page		buffer to store the value (4 Kbytes size)
+ *
+ * @return		number of bytes written to buffer, or
+ *			negative error code
+ */
+static inline
+ssize_t scheduler_port_show_value(struct scheduler_port *port, char *page)
+{
+	return scheduler_sink_show_value(&port->sink, page);
+}
+
+/**
+ * Get the value from the remote peer source of the source connected to a port.
+ * If the connected source is itself a port and defines a get_remote_value()
+ * method, its get_remote_value() method will be called instead. If not
+ * get_remote_value() method is defined, the call will be forwarded down until
+ * either a source being not a port is found or a get_remote_value() method is
+ * defined.
+ *
+ * See the definition of port_get_remote_value_t for the descriptions of
+ * parameters and return value.
+ */
+extern port_get_remote_value_t scheduler_port_get_remote_value;
+
+#endif /* __KRG_SCHEDULER_PORT_H__ */
diff -ruN linux-2.6.29/include/kerrighed/scheduler/probe.h android_cluster/linux-2.6.29/include/kerrighed/scheduler/probe.h
--- linux-2.6.29/include/kerrighed/scheduler/probe.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/scheduler/probe.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,528 @@
+#ifndef __KRG_SCHEDULER_PROBE_H__
+#define __KRG_SCHEDULER_PROBE_H__
+
+#include <linux/configfs.h>
+#include <linux/errno.h>
+#include <linux/module.h>
+#include <kerrighed/scheduler/pipe.h>
+#include <kerrighed/scheduler/global_config.h>
+
+/** default probing period is 1 second (1000 miliseconds). */
+#define SCHEDULER_PROBE_DEFAULT_PERIOD 1000
+
+/*
+ * Structure representing a probe. A probe has top-level attributes and sources.
+ * Top-level attributes are composed of a period refreshement attribute and (in
+ * the future) other custom attributes.
+ * A source is represented by a subdirectory in configfs, and can be linked to
+ * other components collecting measurements.
+ *
+ * A scheduler_probe is described by a scheduler_probe_type.
+ */
+struct scheduler_probe;
+/*
+ * Structure representing a probe source. A probe source is described by a
+ * scheduler_probe_source_type. The current API and implementation only allow
+ * one scheduler_probe_source per scheduler_probe_source_type.
+ */
+struct scheduler_probe_source;
+
+/* Same limitation as configfs (see SIMPLE_ATTR_SIZE in fs/configfs/file.c) */
+#define SCHEDULER_PROBE_ATTR_SIZE 4096
+
+/*
+ * To define with SCHEDULER_PROBE_ATTRIBUTE
+ * Structure representing probe attribute. Used to implement custom
+ * probe attributes.
+ */
+struct scheduler_probe_attribute {
+	struct configfs_attribute config;
+
+	/** function for reading attribute's value. */
+	ssize_t (*show)(struct scheduler_probe *, char *);
+	/** function for storing attribute's value. */
+	ssize_t (*store)(struct scheduler_probe *, const char *, size_t);
+};
+
+/**
+ * Mandatory macro to define a scheduler_probe_attribute.
+ *
+ * @param var		variable name of the scheduler_probe_attribute
+ * @param name		entry name of the attribute in the probe directory
+ * @param mode		access mode of the attribute entry
+ * @param _show		show callback of the attribute
+ * @param _store	store callback of the attribute
+ */
+#define SCHEDULER_PROBE_ATTRIBUTE(var, name, mode, _show, _store) \
+	struct scheduler_probe_attribute var = {		  \
+		.config = {					  \
+			.ca_name = name,			  \
+			.ca_owner = THIS_MODULE,		  \
+			.ca_mode = mode				  \
+		},						  \
+		.show = _show,					  \
+		.store = _store					  \
+	}
+
+/** struct which contains each probe's operations. */
+/*
+ * To initialize with SCHEDULER_PROBE_TYPE. The probe subsystem completes this
+ * init.
+ */
+struct scheduler_probe_type {
+	struct config_item_type item_type;
+	void (*perform_measurement)(void); /** function for performing resource
+					     * measurement only periodically.
+					     * This function is suitable for
+					     * measuring dynamic resource
+					     * properties. */
+	struct scheduler_probe_attribute **attrs; /** NULL-terminated array of
+						    *  custom attributes */
+};
+
+/**
+ * Mandatory macro to define a scheduler_probe_type.
+ *
+ * @param name		Name of the variable containing the probe type.
+ * @param _attrs	NULL-terminated array of custom attributes, or
+ *			NULL
+ * @param _perform_measurement
+ *			Function to use for periodic measurement and subscribers
+ *			refreshment, or NULL
+ */
+#define SCHEDULER_PROBE_TYPE(name, _attrs, _perform_measurement) \
+	struct scheduler_probe_type name = {			 \
+		.item_type = {					 \
+			.ct_owner = THIS_MODULE,		 \
+			.ct_item_ops = NULL,			 \
+			.ct_group_ops = NULL,			 \
+			.ct_attrs = NULL			 \
+		},						 \
+		.perform_measurement = _perform_measurement,	 \
+		.attrs = _attrs					 \
+	}
+
+#define SCHEDULER_PROBE_SOURCE_ATTR_SIZE 4096
+
+/*
+ * To define with SCHEDULER_PROBE_SOURCE_ATTRIBUTE
+ * Structure representing a custom probe source attribute
+ */
+struct scheduler_probe_source_attribute {
+	struct configfs_attribute config; /** representation of attribute in
+					    * configfs. */
+	ssize_t (*show)(char *);	/** Method to read the attribute in
+					  * configfs */
+	ssize_t (*store)(const char *, size_t); /** Method to modify the
+						  * attribute with configfs */
+};
+
+/**
+ * Mandatory macro to define a scheduler_probe_source_attribute.
+ *
+ * @param var		variable name of the scheduler_probe_source_attribute.
+ * @param name		entry name of the attribute in the probe source
+ *			directory
+ * @param mode		access mode of the attribute entry
+ * @param _show		show callback of the attribute
+ * @param _store	store callback of the attribute
+ */
+#define SCHEDULER_PROBE_SOURCE_ATTRIBUTE(var, name, mode, _show, _store) \
+	struct scheduler_probe_source_attribute var = {			 \
+		.config = {						 \
+			.ca_name = name,				 \
+			.ca_owner = THIS_MODULE,			 \
+			.ca_mode = mode					 \
+		},							 \
+		.show = _show,						 \
+		.store = _store						 \
+	}
+
+/*
+ * To initialize with SCHEDULER_PROBE_SOURCE_TYPE_INIT (possibly through the
+ * BEGIN_SCHEDULER_PROBE_SOURCE_TYPE helper)
+ * The probe subsystem completes this init.
+ */
+struct scheduler_probe_source_type {
+	struct scheduler_source_type source_type;
+	struct scheduler_pipe_type pipe_type;
+	int (*has_changed)(void);  /** returns 1, if attribute value has
+				    *  changed since last measurement,
+				    *  otherwise returns 0. You also have
+				    *  to update previous value here.*/
+	struct scheduler_probe_source_attribute **attrs;
+};
+
+/**
+ * Mandatory macro to define a scheduler_probe_source_type
+ *
+ * @param var		variable containing the scheduler_probe_source_type
+ * @param owner		module owning the scheduler_probe_source_type
+ * @param attrs		not used yet
+ * @param get_value	get_value() method of the scheduler_probe_source, or
+ *			NULL
+ * @param show_value	show_value() method of the scheduler_probe_source, or
+ *			NULL
+ * @param value_type	string containing the type name of
+ *			scheduler_probe_source's values
+ * @param value_type_size
+ *			size in bytes of a value_type value
+ * @param get_param_type
+ *			string containing the type name of the parameters for
+ *			the get() method, or NULL
+ * @param get_param_type_size
+ *			size in bytes of a get_param_type parameter
+ * @param _has_changed	has_changed() method of the scheduler_probe_source, or
+ *			NULL
+ */
+#define SCHEDULER_PROBE_SOURCE_TYPE_INIT(var, owner, attrs,		      \
+					 get_value, show_value,		      \
+					 value_type, value_type_size,	      \
+					 get_param_type, get_param_type_size, \
+					 _has_changed)			      \
+	{								      \
+		.source_type =						      \
+			SCHEDULER_SOURCE_TYPE_INIT(get_value, show_value,     \
+						   value_type,		      \
+						   value_type_size,	      \
+						   get_param_type,	      \
+						   get_param_type_size),      \
+		.pipe_type =						      \
+			SCHEDULER_PIPE_TYPE_INIT(owner,			      \
+						 NULL, NULL,		      \
+						 &var.source_type, NULL),     \
+		.has_changed = _has_changed,				      \
+	}
+
+/*
+ * Structure representing a probe source. As a source, a probe source implements
+ * scheduler_source. As a directory, a probe source implements
+ * scheduler_pipe.
+ */
+struct scheduler_probe_source {
+	struct scheduler_source source;
+	struct scheduler_pipe pipe;
+	struct global_config_attrs global_attrs;
+	struct scheduler_probe *parent; /** pointer to a scheduler_probe which
+					 *  contains this scheduler_probe_source
+					 */
+	struct work_struct notify_update_work; /** aperiodic refreshment of
+						*  publish-subscribe ports
+						*  linked to the attribute */
+};
+
+/*
+ * Convenience macros to define a scheduler_probe_source_type
+ *
+ * These convenience macros should be used the following way:
+ *
+ * First, implemented methods must be defined using the
+ * DEFINE_SCHEDULER_PROBE_SOURCE_<method> macros. Second, the
+ * scheduler_probe_source_type must be filled using
+ * {BEGIN,END}_SCHEDULER_PROBE_SOURCE_TYPE and SCHEDULER_PROBE_SOURCE_* macros:
+ *	BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(name),
+ *		.SCHEDULER_PROBE_SOURCE_VALUE_TYPE(name, type),
+ * if needed:
+ *		.SCHEDULER_PROBE_SOURCE_<method>(name),
+ *		.SCHEDULER_PROBE_SOURCE_PARAM_TYPE(name, type),
+ *		.SCHEDULER_PROBE_SOURCE_ATTRS(name, attrs),
+ * and finally:
+ *	END_SCHEDULER_PROBE_SOURCE_TYPE(name);
+ */
+
+#define scheduler_probe_source_lock_wrapper(source, call)		      \
+	({								      \
+		struct scheduler_probe_source *____ps;			      \
+		typeof(call) ____ret;					      \
+		____ps = container_of(source,				      \
+				      struct scheduler_probe_source, source); \
+		scheduler_probe_source_lock(____ps);			      \
+		____ret = call;						      \
+		scheduler_probe_source_unlock(____ps);			      \
+		____ret;						      \
+	})
+
+/**
+ * Convenience macro to define a typed get() method without a source argument
+ * and with parameters. The typed method will be called name_get. When called
+ * from the framework, the probe source lock is held.
+ *
+ * @param name		name of the probe_source type
+ * @param type		type of the values output by the probe source (eg. int)
+ * @param ptr		name of the type *arg of the method
+ * @param nr		name of the array length parameter of the method
+ * @param in_type	type of the parameters of the method (eg. int)
+ * @param in_ptr	name of the in_type *arg of the method
+ * @param in_nr		name of the parameters array length arg of the method
+ */
+#define DEFINE_SCHEDULER_PROBE_SOURCE_GET_WITH_INPUT(name, type, ptr, nr,   \
+					   in_type, in_ptr, in_nr)	    \
+	static int name##_get(type *, unsigned int,			    \
+			      const in_type *, unsigned int);		    \
+	static int name##_source_get_value(struct scheduler_source *source, \
+					   void *__ptr, unsigned int __nr,  \
+					   const void *__in_ptr,	    \
+					   unsigned int __in_nr)	    \
+	{								    \
+		return scheduler_probe_source_lock_wrapper(		    \
+			source,						    \
+			name##_get(__ptr, __nr, __in_ptr, __in_nr));	    \
+	}								    \
+	static int name##_get(type *ptr, unsigned int nr,		    \
+			      const in_type *in_ptr, unsigned int in_nr)
+
+/**
+ * Convenience macro to define a typed get() method without a source argument
+ * and with no parameters. The typed method will be called name_get. When called
+ * from the framework, the probe source lock is held.
+ *
+ * @param name		name of the scheduler_probe_source type
+ * @param type		type of the values output by the probe source (eg. int)
+ * @param ptr		name of the type *arg of the method
+ * @param nr		name of the array length parameter of the method
+ */
+#define DEFINE_SCHEDULER_PROBE_SOURCE_GET(name, type, ptr, nr)		    \
+	static int name##_get(type *, unsigned int);			    \
+	static int name##_source_get_value(struct scheduler_source *source, \
+					   void *__ptr, unsigned int __nr,  \
+					   const void *input_ptr,	    \
+					   unsigned int input_nr)	    \
+	{								    \
+		if (!__nr)						    \
+			return 0;					    \
+		return scheduler_probe_source_lock_wrapper(source,	    \
+						 name##_get(__ptr, __nr));  \
+	}								    \
+	static int name##_get(type *ptr, unsigned int nr)
+
+/**
+ * Convenience macro to define a show() method without a source argument. The
+ * method will be called name_show. When called from the framework, the probe
+ * source lock is held.
+ *
+ * @param name		name of the scheduler_probe_source type
+ * @param page		name of the buffer arg of the method
+ */
+#define DEFINE_SCHEDULER_PROBE_SOURCE_SHOW(name, page)			\
+	static ssize_t name##_show(char *page);				\
+	static ssize_t name##_source_show_value(			\
+		struct scheduler_source *source,			\
+		char *__page)						\
+	{								\
+		return scheduler_probe_source_lock_wrapper(source,	\
+						 name##_show(__page));	\
+	}								\
+	static ssize_t name##_show(char *page)
+
+/**
+ * Convenience macro to define a has_changed() method.
+ * The method will be called name_has_changed.
+ *
+ * @param name		name of the scheduler_probe_source type
+ */
+#define DEFINE_SCHEDULER_PROBE_SOURCE_HAS_CHANGED(name)	\
+	static int name##_has_changed(void)
+
+/**
+ * Convenience macro to start the definition of a scheduler_probe_source_type.
+ * The definition must end with END_SCHEDULER_PROBE_SOURCE_TYPE(name). The
+ * variable will be called name_type.
+ *
+ * @param name		name of the scheduler_probe_source type
+ */
+#define BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(name)				     \
+	struct scheduler_probe_source_type name##_type = {		     \
+		.source_type = SCHEDULER_SOURCE_TYPE_INIT(NULL, NULL,	     \
+							  NULL, 0, NULL, 0), \
+		.pipe_type =						     \
+			SCHEDULER_PIPE_TYPE_INIT(THIS_MODULE,		     \
+						 NULL, NULL,		     \
+						 &name##_type.source_type,   \
+						 NULL)
+
+/**
+ * Convenience macro to attach custom probe source attributes to a probe source
+ * type.
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_PROBE_SOURCE_TYPE
+ * @param _attrs	NULL terminated array of pointers to custom probe source
+ *			attributes (struct scheduler_probe_source_attribute)
+ */
+#define SCHEDULER_PROBE_SOURCE_ATTRS(name, _attrs) \
+	attrs = _attrs
+
+/**
+ * Convenience macro to attach a previously defined get() method to a probe
+ * source type. The get() method must have been defined earlier with
+ * DEFINE_SCHEDULER_PROBE_SOURCE_GET[_WITH_INPUT](name, ...). The value type
+ * (and parameter type if used) must be attached with
+ * SCHEDULER_PROBE_SOURCE_VALUE_TYPE(name, ...) (resp,
+ * SCHEDULER_PROBE_SOURCE_PARAM_TYPE(name, ...)).
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_PROBE_SOURCE_TYPE
+ */
+#define SCHEDULER_PROBE_SOURCE_GET(name)				    \
+	__SCHEDULER_SOURCE_GET_VALUE(source_type., name##_source_get_value)
+
+/**
+ * Convenience macro to attach a previously defined show() method to a probe
+ * source type. The show() method must have been defined earlier with
+ * DEFINE_SCHEDULER_PROBE_SOURCE_SHOW(name, ...).
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_PROBE_SOURCE_TYPE
+ */
+#define SCHEDULER_PROBE_SOURCE_SHOW(name)				      \
+	__SCHEDULER_SOURCE_SHOW_VALUE(source_type., name##_source_show_value)
+
+/**
+ * Convenience macro to declare the value type of a probe source. Must be used
+ * within all BEGIN_SCHEDULER_PROBE_SOURCE_TYPE sections.
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_PROBE_SOURCE_TYPE
+ * @param type		litteral expression of the value type output
+ */
+#define SCHEDULER_PROBE_SOURCE_VALUE_TYPE(name, type)		\
+	__SCHEDULER_SOURCE_VALUE_TYPE(source_type., type)
+
+/**
+ * Convenience macro to declare the parameter type of the get() method of a
+ * probe source.
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_PROBE_SOURCE_TYPE
+ * @param type		litteral expression of the parameter type
+ */
+#define SCHEDULER_PROBE_SOURCE_PARAM_TYPE(name, type)		\
+	__SCHEDULER_SOURCE_PARAM_TYPE(source_type., type)
+
+/**
+ * Convenience macro to attach a previously defined has_changed() method to a
+ * probe source type. The has_changed() method must have been defined earlier
+ * with DEFINE_SCHEDULER_PROBE_SOURCE_HAS_CHANGED(name).
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_PROBE_SOURCE_TYPE
+ */
+#define SCHEDULER_PROBE_SOURCE_HAS_CHANGED(name)	\
+	has_changed = name##_has_changed
+
+/**
+ * End the definition of a scheduler_probe_source_type. Must close any
+ * BEGIN_SCHEDULER_PROBE_SOURCE_TYPE section.
+ *
+ * @param name		must match the name used with
+ *			BEGIN_SCHEDULER_PROBE_SOURCE_TYPE
+ */
+#define END_SCHEDULER_PROBE_SOURCE_TYPE(name)	\
+	}
+
+/* End of convenience macros */
+
+/**
+ * This function allocates memory for new probe and initializes it.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param type		Type of the probe, defined with SCHEDULER_PROBE_TYPE
+ * @param name		Name of the probe. This name must be unique for each
+ *			probe and must match the module file name.
+ * @param sources	NULL-terminated array of probe's sources created with
+ *			scheduler_probe_source_create().
+ * @param def_groups	NULL-terminated array of subdirs of the probe
+ *			directory, or NULL
+ *
+ * @return		pointer to newly created probe or NULL if probe
+ *			creation failed.
+ */
+struct scheduler_probe *
+scheduler_probe_create(struct scheduler_probe_type *type,
+		       const char *name,
+		       struct scheduler_probe_source **sources,
+		       struct config_group *def_groups[]);
+/**
+ * This function frees all the memory taken by a probe.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param probe		pointer to probe whose memory we want to free.
+ */
+void scheduler_probe_free(struct scheduler_probe *probe);
+
+/**
+ * This function allocates memory and initializes a probe source.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param type		Type describing the probe source, defined with
+ *			SCHEDULER_PROBE_SOURCE_TYPE
+ * @param name		Name of the source's subdirectory in the probe's
+ *			directory. Must be unique for a given a probe.
+ *
+ * @return		Pointer to the created scheduler_probe_source, or
+ *			NULL if error
+ */
+struct scheduler_probe_source *
+scheduler_probe_source_create(struct scheduler_probe_source_type *type,
+			      const char *name);
+void scheduler_probe_source_free(struct scheduler_probe_source *source);
+
+/**
+ * Lock a probe source. No sleep is allowed while a probe source is locked.
+ * This actually locks the probe containing this source.
+ *
+ * @param probe_source	probe source to lock
+ */
+void scheduler_probe_source_lock(struct scheduler_probe_source *probe_source);
+/**
+ * Unlock a probe source.
+ *
+ * @param probe_source	probe source to unlock
+ */
+void scheduler_probe_source_unlock(struct scheduler_probe_source *probe_source);
+
+/**
+ * Function that a probe source should call when the value changes and the probe
+ * does not have a perform_measurement() method.
+ * Does nothing if the probe provides a perform_measurement() method.
+ *
+ * @param source		Source having been updated
+ */
+void
+scheduler_probe_source_notify_update(struct scheduler_probe_source *source);
+
+/**
+ * This function is used for registering probe. This function has to
+ * be called at the end of "init_module" function for each probe's module.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param probe		pointer to the probe we wish to register.
+ *
+ * @return		0, if probe was successfully registered.
+ *			-EEXIST, if probe with same name is already registered.
+ */
+int scheduler_probe_register(struct scheduler_probe *probe);
+
+/**
+ * This function is used for removing probe registration. This function must
+ * *only* be called at module unloading (from "cleanup_module" function).
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param probe		pointer to the probe we wish to unregister.
+ */
+void scheduler_probe_unregister(struct scheduler_probe *probe);
+
+/**
+ * Lock a probe. No sleep is allowed while a probe is locked.
+ *
+ * @param probe		probe to lock
+ */
+void scheduler_probe_lock(struct scheduler_probe *probe);
+/**
+ * Unlock a probe.
+ *
+ * @param probe		probe to unlock
+ */
+void scheduler_probe_unlock(struct scheduler_probe *probe);
+
+#endif /* __KRG_SCHEDULER_PROBE_H__ */
diff -ruN linux-2.6.29/include/kerrighed/scheduler/process_set.h android_cluster/linux-2.6.29/include/kerrighed/scheduler/process_set.h
--- linux-2.6.29/include/kerrighed/scheduler/process_set.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/scheduler/process_set.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,305 @@
+#ifndef __KRG_SCHEDULER_PROCESS_SET__
+#define __KRG_SCHEDULER_PROCESS_SET__
+
+#include <linux/configfs.h>
+#include <linux/pid.h>
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/sched.h>
+#include <linux/nsproxy.h>
+#include <linux/rcupdate.h>
+#include <linux/spinlock.h>
+#include <kerrighed/scheduler/global_config.h>
+
+/**
+ * Representation of particual element in process set. This is used both for
+ * single processes and process groups.
+ */
+struct process_set_element {
+	struct config_item item;
+	pid_t id; /** PID of single process, or PGID of process group,
+		    * or SID of process session. */
+	struct pid *pid; /** link to locally attached processes */
+	struct hlist_node pid_node; /** entry in pid's list of attached
+				      * elements  */
+	int in_subset;		/** true if still in a subset */
+	struct list_head list; /** entry in process_subset */
+	struct global_config_item global_item;
+};
+
+/**
+ * Generic representation of a subset of IDs handled the same way (all PID, or
+ * all PGID, etc.)
+ */
+struct process_subset {
+	struct config_group group;
+	struct list_head elements_head;
+};
+
+/**
+ * process_set represents a set of processes that are taken care of by
+ * particular scheduler. This set can contain processes or process groups.
+ * A particual process_set can be marked that it contains all processes.
+ */
+struct process_set {
+	struct config_group group; /** representation of process set in
+				     * configfs. */
+	struct global_config_attrs global_attrs;
+
+	int handle_all;		/** if true, process set contains all processes. */
+	struct list_head handle_all_list;
+
+	/** subsets of processes that the set contains, separated by PID
+	  * classes. */
+	struct process_subset subsets[PIDTYPE_MAX];
+	/** list of default configfs subdirs */
+	struct config_group *def_groups[PIDTYPE_MAX + 1];
+
+	spinlock_t lock; /** lock for synchronizing process set accesses. */
+
+	struct rcu_head rcu;	/** delayed kfree to allow RCU traversals */
+};
+
+/** List head of process sets handling all processes */
+extern struct list_head process_set_handle_all_head;
+/*
+ * Lock protecting the list of process sets handling all processes as well as
+ * linking between process_set_elements and pids
+ */
+extern spinlock_t process_set_link_lock;
+
+/**
+ * This function allocates memory for new process set and initializes it.
+ * Note: at the beginning the process set doesn't contain any processes nor
+ * process groups.
+ * Note: to free a process set use process_set_put()
+ * @author Marko Novak, Louis Rilling
+ *
+ * @return		pointer to newly created process set or NULL if
+ *			creation failed.
+ */
+struct process_set *process_set_create(void);
+
+/**
+ * Mark a process set as deactivated from now, and drop the reference count
+ * Assumes that the process set is already removed from configfs
+ *
+ * @param pset		process set to stop using
+ */
+void process_set_drop(struct process_set *pset);
+
+/**
+ * Get a reference on a process set
+ *
+ * @param pset		process set to get a reference on
+ */
+static inline void process_set_get(struct process_set *pset)
+{
+	if (pset)
+		config_group_get(&pset->group);
+}
+
+/**
+ * Drop a refrence on a process set
+ * Dropping last reference frees the process set
+ *
+ * @param pset		process set which reference to drop
+ */
+static inline void process_set_put(struct process_set *pset)
+{
+	if (pset)
+		config_group_put(&pset->group);
+}
+
+/**
+ * Lock a process set
+ * This will prevent any addition and removal of elements in the set as well as
+ * changing the handle_all flag.
+ *
+ * @param pset		process set to lock
+ */
+static inline void process_set_lock(struct process_set *pset)
+{
+	spin_lock(&pset->lock);
+}
+
+/**
+ * Unlock a process set
+ *
+ * @param pset		process set to unlock
+ */
+static inline void process_set_unlock(struct process_set *pset)
+{
+	spin_unlock(&pset->lock);
+}
+
+/**
+ * Tells whether a process set contains all processes
+ * Caller must hold process set lock to be sure to obtain a correct result
+ * RCU traversals only get a hint without taking process set lock
+ *
+ * @param pset		process set to test
+ *
+ * @return		non 0 if pset contains all processes,
+ *			0 otherwise
+ */
+static inline int process_set_contains_all(struct process_set *pset)
+{
+	return pset->handle_all;
+}
+
+/**
+ * Prepare a process set for iterations over its elements
+ * Must be called before doing any iteration over a process set
+ *
+ * @param pset		process set to prepare for iteration
+ */
+static inline void process_set_prepare_do_each_process(struct process_set *pset)
+{
+	rcu_read_lock();
+}
+
+/**
+ * do {} while () style macro to begin an iterating loop over the local
+ * processes of a process set
+ *
+ * Note that it's composed of nested loops, so that break will not exit from
+ * the loop. Use goto instead.
+ * One can use continue to skip the current element however.
+ *
+ * @param p		task_struct pointer to hold the successive local tasks
+ * @param pset		process set to iterate over
+ */
+#define process_set_do_each_process(p, pset)				   \
+	do {								   \
+		__label__ __common_begin, __all_end_of_loop;		   \
+		struct process_subset *__psubset = NULL;		   \
+		struct process_set_element *__pset_el = NULL;		   \
+		struct task_struct *__p = NULL;				   \
+		struct pid *__pid;					   \
+		enum pid_type __type;					   \
+		int __all = process_set_contains_all(pset);		   \
+		if (__all) {						   \
+			__type = PIDTYPE_PID;				   \
+			for_each_process(__p) {				   \
+				struct nsproxy *__nsp;			   \
+				__nsp = rcu_dereference(__p->nsproxy);	   \
+				if (__nsp && __nsp->krg_ns) {		   \
+					__pid = __p->pids[PIDTYPE_PID].pid;\
+					goto __common_begin;		   \
+				}					   \
+			__all_end_of_loop:				   \
+				continue;				   \
+			}						   \
+			break;						   \
+		}							   \
+		for (__type = 0; __type < PIDTYPE_MAX; __type++) {	   \
+			__psubset = &(pset)->subsets[__type];		   \
+			list_for_each_entry_rcu(__pset_el,		   \
+						&__psubset->elements_head, \
+						list) {			   \
+				__pid = __pset_el->pid;			   \
+			__common_begin:					   \
+				do_each_pid_task(__pid, __type, p) {
+
+/**
+ * do {} while () style macro to end an iterating loop over the elements of a
+ * process set
+ * Arguments must be the same as for process_set_do_each_process()
+ */
+#define process_set_while_each_process(p, pset)				 \
+				} while_each_pid_task(__pid, __type, p); \
+				if (__all)				 \
+					goto __all_end_of_loop;		 \
+			}						 \
+		}							 \
+	} while (0)
+
+/**
+ * Cleanup all preparations done for process set iterations
+ * Must be called after having finished iterating over a process set
+ *
+ * @param pset		process set to cleanup
+ */
+static inline void process_set_cleanup_do_each_process(struct process_set *pset)
+{
+	rcu_read_unlock();
+}
+
+/**
+ * for (;;) like macro to iterate over all the process sets containing all
+ * processes
+ * caller must hold process_set_link_lock or RCU read lock
+ *
+ * @param pset		the process_set * to use as a loop cursor
+ */
+#define for_each_process_set_full(pset)			      \
+	list_for_each_entry_rcu(pset,			      \
+				&process_set_handle_all_head, \
+				handle_all_list)
+
+/**
+ * do {} while () style macro to begin an iteration over the process sets
+ * attached to a pid for a given pid_type
+ * process sets attached to all processes must be parsed separately with
+ * for_each_process_set_full().
+ * caller must hold process_set_link_lock or RCU read lock
+ *
+ * @param pset		the process_set * to use as loop cursor
+ * @param pid		the pid which process sets to iterate over
+ * @param type	        pid_type for which process sets are attached to the pid
+ */
+#define do_each_process_set_pid(pset, pid, type)			      \
+	do {								      \
+		struct process_set_element *__pset_el;			      \
+		struct hlist_node *__pos;				      \
+		if (pid)						      \
+			hlist_for_each_entry_rcu(__pset_el, __pos,	      \
+						 &(pid)->process_sets[type],  \
+						 pid_node) {		      \
+				pset = container_of(			      \
+					__pset_el->item.ci_parent->ci_parent, \
+					struct process_set, group.cg_item);   \
+				{
+
+/**
+ * do {} while () style macro to end an iteration over the process sets
+ * attached to a pid for a given pid_type
+ * Arguments must be the same as for do_each_process_set_pid()
+ */
+#define while_each_process_set_pid(pset, pid, type)			      \
+				}					      \
+			}						      \
+	} while (0)
+
+/**
+ * do {} while () style macro to begin an iteration over the process sets
+ * attached to a task
+ * caller must either hold process_set_link_lock, and at least one of RCU and
+ * tasklist_lock, or hold RCU read lock
+ * process sets attached to all processes must be parsed separately with
+ * for_each_process_set_full().
+ * Note: process sets attached by several pid types will appear as many times.
+ *
+ * @param pset		the process_set * to use as loop cursor
+ * @param task		task which attached process sets to iterate on
+ */
+#define do_each_process_set_task(pset, task)				     \
+	do {								     \
+		enum pid_type __type;					     \
+		struct pid *__pid;					     \
+		for (__type = PIDTYPE_PID; __type < PIDTYPE_MAX; __type++) { \
+			__pid = rcu_dereference((task)->pids[__type].pid);   \
+			do_each_process_set_pid(pset, __pid, __type) {
+
+/**
+ * do {} while () style macro to end an iteration over the process sets
+ * attached to a task
+ * Arguments must be the same as for do_each_process_set_task()
+ */
+#define while_each_process_set_task(pset, task)				     \
+			} while_each_process_set_pid(pset, __pid, __type);   \
+		}							     \
+	} while(0)
+
+#endif /* __KRG_SCHEDULER_PROCESS_SET__ */
diff -ruN linux-2.6.29/include/kerrighed/scheduler/remote_pipe.h android_cluster/linux-2.6.29/include/kerrighed/scheduler/remote_pipe.h
--- linux-2.6.29/include/kerrighed/scheduler/remote_pipe.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/scheduler/remote_pipe.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,85 @@
+#ifndef __KRG_SCHEDULER_REMOTE_PIPE_H__
+#define __KRG_SCHEDULER_REMOTE_PIPE_H__
+
+#include <linux/workqueue.h>
+#include <kerrighed/sys/types.h>
+
+struct rpc_desc;
+
+/*
+ * Internal descriptor to hold the necessary state during an asynchronous
+ * get_remote_value
+ */
+struct remote_pipe_desc {
+	int pending;
+	struct work_struct work;
+	struct rpc_desc *desc;
+	kerrighed_node_t node;
+	void *value_p;
+	int ret;
+	spinlock_t lock;
+};
+
+struct scheduler_sink;
+struct scheduler_pipe;
+
+/**
+ * Get the value of a remote globalized pipe source
+ *
+ * The request is asynchronous and non-blocking, which means that the call will
+ * first return -EGAIN to notice the caller that the request is being
+ * processed. The caller must retry later with the *same* parameters, until it
+ * gets a result differing from -EAGAIN. If the calling sink provides an
+ * update_value callback, it will be called once the result is available.
+ *
+ * Once a request is started, subsequent calls with different parameters will
+ * return -EINVAL as long as the result is not retrieved by a call with the
+ * parameters that started the request.
+ *
+ * The caller's sink must ensure that the array to get the values remains
+ * available until the request finishes (result != -EAGAIN).
+ *
+ * @param sink		sink of the caller
+ * @param local_pipe	local peer pipe of the remote pipe queried
+ * @param node		node to get values from
+ * @param value_p	array of values to be filled
+ * @param nr		max number of values to fill
+ * @param in_value_p	array of parameters, or NULL
+ * @param in_nr		number of elements in in_value_p
+ *
+ * @return		number of values filled, or
+ *			-EAGAIN if the results are not available yet, or
+ *			other negative error code
+ */
+int scheduler_pipe_get_remote_value(
+	struct scheduler_sink *sink,
+	struct scheduler_pipe *local_pipe,
+	kerrighed_node_t node,
+	void *value_p, unsigned int nr,
+	const void *in_value_p, unsigned int in_nr);
+
+/**
+ * Initialize the remote_pipe_desc embedded in a scheduler_sink
+ *
+ * @param sink		sink containing the remote_pipe_desc
+ */
+void scheduler_sink_remote_pipe_init(struct scheduler_sink *sink);
+/**
+ * Cleanup the remote_pipe_desc embedded in a scheduler_sink
+ *
+ * @param sink		sink containing the remote_pipe_desc
+ */
+static inline
+void scheduler_sink_remote_pipe_cleanup(struct scheduler_sink *sink)
+{
+}
+
+/**
+ * Break any pending request to a remote scheduler_pipe
+ * May block
+ *
+ * @param sink		sink containing the remote_pipe_desc
+ */
+void scheduler_sink_remote_pipe_disconnect(struct scheduler_sink *sink);
+
+#endif /* __KRG_SCHEDULER_REMOTE_PIPE_H__ */
diff -ruN linux-2.6.29/include/kerrighed/scheduler/scheduler.h android_cluster/linux-2.6.29/include/kerrighed/scheduler/scheduler.h
--- linux-2.6.29/include/kerrighed/scheduler/scheduler.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/scheduler/scheduler.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,131 @@
+#ifndef __KRG_SCHEDULER_SCHEDULER_H__
+#define __KRG_SCHEDULER_SCHEDULER_H__
+
+#include <kerrighed/scheduler/process_set.h>
+
+struct scheduler_policy;
+struct scheduler;
+
+/**
+ * Get a reference on a scheduler
+ *
+ * @param scheduler	scheduler to get a reference on
+ */
+void scheduler_get(struct scheduler *scheduler);
+/**
+ * Put a reference on a scheduler
+ *
+ * @param scheduler	scheduler to put a reference on
+ */
+void scheduler_put(struct scheduler *scheduler);
+
+/**
+ * Get a reference on the scheduler owning a scheduler_policy
+ * The reference must be put with scheduler_put()
+ *
+ * @param policy	scheduling policy of the searched scheduler
+ *
+ * @return		scheduler owning the scheduler_policy, or
+ *			NULL if the scheduler_policy is not used anymore
+ */
+struct scheduler *
+scheduler_policy_get_scheduler(struct scheduler_policy *policy);
+
+/**
+ * Get a reference on the scheduler owning a process set
+ * The reference must be put with scheduler_put()
+ *
+ * @param pset		process set of the searched scheduler
+ *
+ * @return		scheduler owning the process set
+ */
+struct scheduler *process_set_get_scheduler(struct process_set *pset);
+
+/**
+ * Get a reference on the sched policy of a scheduler
+ * The reference must be put with scheduler_policy_put()
+ *
+ * @param scheduler	scheduler which sched policy to get
+ *
+ * @return		sched policy of the scheduler
+ */
+struct scheduler_policy *
+scheduler_get_scheduler_policy(struct scheduler *scheduler);
+
+/**
+ * Get a reference on the process set managed by a scheduler
+ * The reference must be put with process_set_put()
+ *
+ * @param scheduler	scheduler to get the process set of
+ *
+ * @return		process set of the scheduler, or
+ *			NULL if the scheduler is not active anymore
+ */
+struct process_set *scheduler_get_process_set(struct scheduler *scheduler);
+
+/**
+ * Get the current node set of the scheduler
+ *
+ * @param scheduler	scheduler which node set to get
+ * @param node_set	node_set to copy the scheduler's node set in
+ */
+void scheduler_get_node_set(struct scheduler *scheduler,
+			    krgnodemask_t *node_set);
+
+/**
+ * do {} while () style macro to begin an iteration over all universal
+ * schedulers (that is set to handle all processes)
+ *
+ * @param scheduler	the scheduler * to use as a loop cursor
+ */
+#define do_each_scheduler_universal(scheduler)			       \
+	do {							       \
+		struct process_set *__pset;			       \
+		for_each_process_set_full(__pset) {		       \
+			scheduler = process_set_get_scheduler(__pset); \
+			if (scheduler) {			       \
+				do {
+
+/**
+ * do {} while () style macro to end an iteration over all universal
+ * schedulers (that is set to handle all processes)
+ * Arguments must be the same as for do_each_scheduler_universal()
+ */
+#define while_each_scheduler_universal(scheduler)		       \
+				} while (0);			       \
+				scheduler_put(scheduler);	       \
+			}					       \
+		}						       \
+	} while (0)
+
+/**
+ * do {} while () style macro to begin an iteration over the schedulers managing
+ * a task
+ * Schedulers attached to all tasks have to be separately parsed with
+ * do_each_scheduler_universal()
+ * caller must hold either RCU lock or tasklist_lock
+ *
+ * @param scheduler	the scheduler * to use a loop cursor
+ * @param task		task which schedulers to iterate over
+ */
+#define do_each_scheduler_task(scheduler, task)			       \
+	do {							       \
+		struct process_set *__pset;			       \
+		do_each_process_set_task(__pset, task) {	       \
+			scheduler = process_set_get_scheduler(__pset); \
+			if (scheduler) {			       \
+				do {
+
+/**
+ * do {} while () style macro to end an iteration over the schedulers managing
+ * a task
+ * Arguments must be the same as for do_each_scheduler_task()
+ */
+#define while_each_scheduler_task(scheduler, task)		       \
+				} while (0);			       \
+				scheduler_put(scheduler);	       \
+			}					       \
+		} while_each_process_set_task(__pset, task);	       \
+	} while (0)
+
+#endif /* __KRG_SCHEDULER_SCHEDULER_H__ */
diff -ruN linux-2.6.29/include/kerrighed/signal.h android_cluster/linux-2.6.29/include/kerrighed/signal.h
--- linux-2.6.29/include/kerrighed/signal.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/signal.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,46 @@
+#ifndef __KERRIGHED_SIGNAL_H__
+#define __KERRIGHED_SIGNAL_H__
+
+#ifdef CONFIG_KRG_EPM
+
+#include <kddm/kddm_types.h>
+
+/* signal_struct sharing */
+
+struct signal_struct;
+struct task_struct;
+struct pid;
+
+void krg_signal_alloc(struct task_struct *task, struct pid *pid,
+		      unsigned long clone_flags);
+void krg_signal_share(struct signal_struct *sig);
+struct signal_struct *krg_signal_exit(struct signal_struct *sig);
+struct signal_struct *krg_signal_readlock(struct signal_struct *sig);
+struct signal_struct *krg_signal_writelock(struct signal_struct *sig);
+void krg_signal_unlock(struct signal_struct *sig);
+void krg_signal_pin(struct signal_struct *sig);
+void krg_signal_unpin(struct signal_struct *sig);
+
+/* sighand_struct sharing */
+
+struct sighand_struct;
+
+void krg_sighand_alloc(struct task_struct *task, unsigned long clone_flags);
+void krg_sighand_alloc_unshared(struct task_struct *task,
+				struct sighand_struct *newsig);
+void krg_sighand_share(struct task_struct *task);
+objid_t krg_sighand_exit(struct sighand_struct *sig);
+void krg_sighand_cleanup(struct sighand_struct *sig);
+struct sighand_struct *krg_sighand_readlock(objid_t id);
+struct sighand_struct *krg_sighand_writelock(objid_t id);
+void krg_sighand_unlock(objid_t id);
+void krg_sighand_pin(struct sighand_struct *sig);
+void krg_sighand_unpin(struct sighand_struct *sig);
+
+/* Used by restart */
+struct sighand_struct *cr_sighand_alloc(void);
+void cr_sighand_free(objid_t id);
+
+#endif /* CONFIG_KRG_EPM */
+
+#endif /* __KERRIGHED_SIGNAL_H__ */
diff -ruN linux-2.6.29/include/kerrighed/sys/capabilities.h android_cluster/linux-2.6.29/include/kerrighed/sys/capabilities.h
--- linux-2.6.29/include/kerrighed/sys/capabilities.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/sys/capabilities.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,39 @@
+/**
+ * Define Kerrighed Capabilities
+ * @author David Margery (c) Inria 2004
+ */
+
+#ifndef _KERRIGHED_CAPABILITIES_H
+#define _KERRIGHED_CAPABILITIES_H
+
+enum {
+       CAP_CHANGE_KERRIGHED_CAP = 0,
+       CAP_CAN_MIGRATE,
+       CAP_DISTANT_FORK,
+       CAP_FORK_DELAY,
+       CAP_CHECKPOINTABLE,
+       CAP_USE_REMOTE_MEMORY,
+       CAP_USE_INTRA_CLUSTER_KERSTREAMS,
+       CAP_USE_INTER_CLUSTER_KERSTREAMS,
+       CAP_USE_WORLD_VISIBLE_KERSTREAMS,
+       CAP_SEE_LOCAL_PROC_STAT,
+       CAP_DEBUG,
+       CAP_SYSCALL_EXIT_HOOK,
+       CAP_SIZE /* keep as last capability */
+};
+
+typedef struct krg_cap_struct
+{
+	int krg_cap_effective;
+	int krg_cap_permitted;
+	int krg_cap_inheritable_permitted;
+	int krg_cap_inheritable_effective;
+} krg_cap_t;
+
+typedef struct krg_cap_pid_desc
+{
+	pid_t pid;
+	krg_cap_t *caps;
+} krg_cap_pid_t;
+
+#endif /* _KERRIGHED_CAPABILITIES_H */
diff -ruN linux-2.6.29/include/kerrighed/sys/checkpoint.h android_cluster/linux-2.6.29/include/kerrighed/sys/checkpoint.h
--- linux-2.6.29/include/kerrighed/sys/checkpoint.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/sys/checkpoint.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,66 @@
+#include <linux/types.h>
+
+#ifndef CHECKPOINT_TYPES_H
+#define CHECKPOINT_TYPES_H
+
+#define E_CR_APPBUSY     1000
+#define E_CR_PIDBUSY     1001
+#define E_CR_TASKDEAD    1002
+#define E_CR_BADDATA     1003
+
+#define APP_FROM_PID		1
+#define CKPT_W_UNSUPPORTED_FILE	2
+
+#define APP_REPLACE_PGRP_SID	1
+
+struct checkpoint_info
+{
+	long app_id;
+
+	int flags;
+
+	int chkpt_sn;
+	int result;
+
+	int signal;
+};
+
+struct cr_subst_file
+{
+	int fd;
+	char *file_id;
+};
+
+struct cr_subst_files_array
+{
+	unsigned int nr;
+	struct cr_subst_file *files;
+};
+
+struct restart_request
+{
+	long app_id;
+	int chkpt_sn;
+	int flags;
+	pid_t root_pid;
+
+	struct cr_subst_files_array substitution;
+};
+
+struct app_userdata_request
+{
+	long app_id;
+	int flags;
+	__u64 user_data;
+};
+
+struct cr_mm_region
+{
+	pid_t pid;
+	unsigned long addr;
+	size_t size;
+
+	struct cr_mm_region *next;
+};
+
+#endif
diff -ruN linux-2.6.29/include/kerrighed/sys/types.h android_cluster/linux-2.6.29/include/kerrighed/sys/types.h
--- linux-2.6.29/include/kerrighed/sys/types.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/sys/types.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,66 @@
+/** Main kerrighed types.
+ *  @file gtypes.h
+ *
+ *  Definition of the main types and structures.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __KERRIGHED_TYPES__
+#define __KERRIGHED_TYPES__
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                 MACROS                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+#ifdef CONFIG_KRG_AUTONODEID
+#define NR_BITS_IN_MAX_NODE_ID     8
+#else
+#define NR_BITS_IN_MAX_NODE_ID     7
+#endif
+
+#define KERRIGHED_MAX_NODES      (1<<NR_BITS_IN_MAX_NODE_ID)        /* Real limit 32766 */
+#define KERRIGHED_HARD_MAX_NODES 256
+
+#define KERRIGHED_MAX_CLUSTERS   256
+#define KERRIGHED_NODE_ID_NONE    -1        /* Invalid node id */
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+#ifndef __ASSEMBLER__
+
+/** Type for node id           */
+typedef short kerrighed_node_t;
+
+/** Event counter type */
+typedef unsigned long event_counter_t;
+
+/** Physical address type */
+typedef unsigned long physaddr_t;
+
+/** Network id */
+typedef unsigned int kerrighed_network_t;
+
+enum kerrighed_status {
+	KRG_FIRST_START,
+	KRG_FINAL_STOP,
+	KRG_NODE_STARTING,
+	KRG_NODE_STOPING,
+	KRG_RUNNING_CLUSTER,
+};
+typedef enum kerrighed_status kerrighed_status_t;
+
+#endif /* __ASSEMBLER__ */
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                             EXTERN VARIABLES                             *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+#endif /* __KERRIGHED_TYPES__ */
diff -ruN linux-2.6.29/include/kerrighed/task.h android_cluster/linux-2.6.29/include/kerrighed/task.h
--- linux-2.6.29/include/kerrighed/task.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/task.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,98 @@
+#ifndef __KERRIGHED_TASK_H__
+#define __KERRIGHED_TASK_H__
+
+#ifdef CONFIG_KRG_PROC
+
+#include <linux/types.h>
+#include <linux/rwsem.h>
+#include <linux/kref.h>
+#include <linux/rcupdate.h>
+#include <kerrighed/sys/types.h>
+#include <asm/cputime.h>
+
+/** management of process than can or have migrated
+ *  @author Geoffroy Vallee, David Margery, Pascal Gallard and Louis Rilling
+ */
+
+/* task kddm object */
+
+struct task_struct;
+struct pid;
+#ifdef CONFIG_KRG_EPM
+struct pid_kddm_object;
+#endif
+
+struct task_kddm_object {
+	volatile long state;
+	unsigned int flags;
+	unsigned int ptrace;
+	int exit_state;
+	int exit_code, exit_signal;
+
+	kerrighed_node_t node;
+	u32 self_exec_id;
+	int thread_group_empty;
+
+	pid_t pid;
+	pid_t parent;
+	kerrighed_node_t parent_node;
+	pid_t real_parent;
+	pid_t real_parent_tgid;
+	pid_t group_leader;
+
+	uid_t uid;
+	uid_t euid;
+	gid_t egid;
+
+	cputime_t utime, stime;
+
+	unsigned int dumpable;
+
+	/* The remaining fields are not shared */
+#ifdef CONFIG_KRG_EPM
+	struct pid_kddm_object *pid_obj;
+#endif
+	struct task_struct *task;
+
+	struct rw_semaphore sem;
+	int write_locked;
+
+	int alive;
+	struct kref kref;
+
+	struct rcu_head rcu;
+};
+
+void krg_task_get(struct task_kddm_object *obj);
+void krg_task_put(struct task_kddm_object *obj);
+int krg_task_alive(struct task_kddm_object *obj);
+struct task_kddm_object *krg_task_readlock(pid_t pid);
+struct task_kddm_object *__krg_task_readlock(struct task_struct *task);
+struct task_kddm_object *krg_task_create_writelock(pid_t pid);
+struct task_kddm_object *krg_task_writelock(pid_t pid);
+struct task_kddm_object *__krg_task_writelock(struct task_struct *task);
+struct task_kddm_object *krg_task_writelock_nested(pid_t pid);
+struct task_kddm_object *__krg_task_writelock_nested(struct task_struct *task);
+void krg_task_unlock(pid_t pid);
+void __krg_task_unlock(struct task_struct *task);
+int krg_task_alloc(struct task_struct *task, struct pid *pid);
+void krg_task_fill(struct task_struct *task, unsigned long clone_flags);
+void krg_task_commit(struct task_struct *task);
+void krg_task_abort(struct task_struct *task);
+#ifdef CONFIG_KRG_EPM
+void __krg_task_free(struct task_struct *task);
+#endif
+void krg_task_free(struct task_struct *task);
+
+/* exit */
+#ifdef CONFIG_KRG_EPM
+int krg_delay_release_task(struct task_struct *task);
+#endif
+void krg_release_task(struct task_struct *task);
+
+void __krg_task_unlink(struct task_kddm_object *obj, int need_update);
+void krg_task_unlink(struct task_kddm_object *obj, int need_update);
+
+#endif /* CONFIG_KRG_PROC */
+
+#endif /* __KERRIGHED_TASK_H__ */
diff -ruN linux-2.6.29/include/kerrighed/types.h android_cluster/linux-2.6.29/include/kerrighed/types.h
--- linux-2.6.29/include/kerrighed/types.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/types.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,33 @@
+#ifndef __KRG_TYPES_INTERNAL__
+#define __KRG_TYPES_INTERNAL__
+
+#include <kerrighed/sys/types.h>
+
+#ifdef __KERNEL__
+#include <kerrighed/krgnodemask.h>
+#endif
+
+#define KRGFCT(p) if(p!=NULL) p
+
+#if defined(CONFIG_KERRIGHED) || defined(CONFIG_KRGRPC)
+
+typedef unsigned char kerrighed_session_t;
+typedef int kerrighed_subsession_t;
+typedef unsigned long unique_id_t;   /**< Unique id type */
+
+#endif /* CONFIG_KERRIGHED */
+
+#ifdef __KERNEL__
+
+#ifdef CONFIG_KRG_STREAM
+struct dstream_socket { // shared node-wide
+	unique_id_t id_socket;
+	unique_id_t id_container;
+	struct dstream_interface_ctnr *interface_ctnr;
+	struct stream_socket *krg_socket;
+};
+#endif
+
+#endif /* __KERNEL__ */
+
+#endif /* __KRG_TYPES_INTERNAL__ */
diff -ruN linux-2.6.29/include/kerrighed/version.h android_cluster/linux-2.6.29/include/kerrighed/version.h
--- linux-2.6.29/include/kerrighed/version.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/version.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,6 @@
+#ifndef __KRGVERSION__
+#define __KRGVERSION__
+
+#define KERRIGHED_ABI "1"
+
+#endif
diff -ruN linux-2.6.29/include/kerrighed/workqueue.h android_cluster/linux-2.6.29/include/kerrighed/workqueue.h
--- linux-2.6.29/include/kerrighed/workqueue.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/kerrighed/workqueue.h	2014-05-27 23:04:10.250031581 -0700
@@ -0,0 +1,9 @@
+#ifndef __TOOLS_WORKQUEUE_H__
+#define __TOOLS_WORKQUEUE_H__
+
+struct workqueue_struct;
+
+extern struct workqueue_struct *krg_wq;
+extern struct workqueue_struct *krg_nb_wq;
+
+#endif /* __TOOLS_WORKQUEUE_H__ */
diff -ruN linux-2.6.29/include/linux/binfmts.h android_cluster/linux-2.6.29/include/linux/binfmts.h
--- linux-2.6.29/include/linux/binfmts.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/binfmts.h	2014-05-27 23:04:10.258031414 -0700
@@ -49,6 +49,12 @@
 	struct file * file;
 	struct cred *cred;	/* new credentials */
 	int unsafe;		/* how unsafe this exec is (mask of LSM_UNSAFE_*) */
+#ifdef CONFIG_KRG_CAP
+	/* The model needs changes with filesystem support ... */
+#if 0
+	kernel_cap_t krg_cap_permitted, krg_cap_forced, krg_cap_effective;
+#endif /* 0 */
+#endif /* CONFIG_KRG_CAP */
 	unsigned int per_clear;	/* bits to clear in current->personality */
 	int argc, envc;
 	char * filename;	/* Name of binary as seen by procps */
diff -ruN linux-2.6.29/include/linux/cluster_barrier.h android_cluster/linux-2.6.29/include/linux/cluster_barrier.h
--- linux-2.6.29/include/linux/cluster_barrier.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/linux/cluster_barrier.h	2014-05-27 23:04:10.262031331 -0700
@@ -0,0 +1,42 @@
+/** Cluster wide barrier
+ *  @file cluster_barrier.h
+ *
+ *  @author Renaud Lottiaux
+ */
+#include <linux/wait.h>
+#include <linux/spinlock_types.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/types.h>
+
+enum static_cluster_barrier_id {
+	CLUSTER_BARRIER_NONE = 0,
+	KDDM_HOTPLUG_BARRIER,
+	SCHED_HOTPLUG_BARRIER,
+	CLUSTER_BARRIER_MAX,
+};
+
+struct cluster_barrier_core {
+	krgnodemask_t nodes_in_barrier;
+	krgnodemask_t nodes_to_wait;
+	wait_queue_head_t waiting_tsk;
+	int in_barrier;
+};
+
+struct cluster_barrier_id {
+	unique_id_t key;
+	int toggle;
+};
+
+struct cluster_barrier {
+	spinlock_t lock;
+	struct cluster_barrier_id id;
+	struct cluster_barrier_core core[2];
+};
+
+
+struct cluster_barrier *alloc_cluster_barrier(unique_id_t key);
+void free_cluster_barrier(struct cluster_barrier *barrier);
+int cluster_barrier(struct cluster_barrier *barrier, krgnodemask_t *nodes,
+		    kerrighed_node_t master);
+void init_cluster_barrier(void);
+
diff -ruN linux-2.6.29/include/linux/configfs.h android_cluster/linux-2.6.29/include/linux/configfs.h
--- linux-2.6.29/include/linux/configfs.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/configfs.h	2014-05-27 23:04:10.262031331 -0700
@@ -225,7 +225,12 @@
 	void (*release)(struct config_item *);
 	ssize_t	(*show_attribute)(struct config_item *, struct configfs_attribute *,char *);
 	ssize_t	(*store_attribute)(struct config_item *,struct configfs_attribute *,const char *, size_t);
+#ifdef CONFIG_KRG_SCHED
+	int (*allow_link)(struct config_item *src, struct config_item *target, const char *);
+	int (*allow_drop_link)(struct config_item *src, struct config_item *target);
+#else
 	int (*allow_link)(struct config_item *src, struct config_item *target);
+#endif
 	int (*drop_link)(struct config_item *src, struct config_item *target);
 };
 
@@ -234,6 +239,9 @@
 	struct config_group *(*make_group)(struct config_group *group, const char *name);
 	int (*commit_item)(struct config_item *item);
 	void (*disconnect_notify)(struct config_group *group, struct config_item *item);
+#ifdef CONFIG_KRG_SCHED
+	int (*allow_drop_item)(struct config_group *group, struct config_item *item);
+#endif
 	void (*drop_item)(struct config_group *group, struct config_item *item);
 };
 
diff -ruN linux-2.6.29/include/linux/dcache.h android_cluster/linux-2.6.29/include/linux/dcache.h
--- linux-2.6.29/include/linux/dcache.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/dcache.h	2014-05-27 23:04:10.266031249 -0700
@@ -309,6 +309,11 @@
  */
 extern char *dynamic_dname(struct dentry *, char *, int, const char *, ...);
 
+#ifdef CONFIG_KRG_DVFS
+char *____d_path(const struct path *path, struct path *root,
+		 char *buffer, int buflen, bool *deleted);
+char *d_path_check(const struct path *, char *, int buflen, bool *deleted);
+#endif
 extern char *__d_path(const struct path *path, struct path *root, char *, int);
 extern char *d_path(const struct path *, char *, int);
 extern char *dentry_path(struct dentry *, char *, int);
diff -ruN linux-2.6.29/include/linux/fcntl.h android_cluster/linux-2.6.29/include/linux/fcntl.h
--- linux-2.6.29/include/linux/fcntl.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/fcntl.h	2014-05-27 23:04:10.274031082 -0700
@@ -68,4 +68,8 @@
 
 #endif /* __KERNEL__ */
 
+#ifdef CONFIG_KRG_DVFS
+#include <kerrighed/fcntl.h>
+#endif
+
 #endif
diff -ruN linux-2.6.29/include/linux/file.h android_cluster/linux-2.6.29/include/linux/file.h
--- linux-2.6.29/include/linux/file.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/file.h	2014-06-09 18:19:41.232402053 -0700
@@ -10,7 +10,10 @@
 #include <linux/posix_types.h>
 
 struct file;
-
+#ifdef CONFIG_KRG_FAF
+struct files_struct;
+struct task_struct;
+#endif
 extern void __fput(struct file *);
 extern void fput(struct file *);
 extern void drop_file_write_access(struct file *file);
@@ -36,9 +39,25 @@
 extern void put_filp(struct file *);
 extern int alloc_fd(unsigned start, unsigned flags);
 extern int get_unused_fd(void);
+#ifdef CONFIG_KRG_FAF
+int __get_unused_fd(struct task_struct *task);
+#endif
 #define get_unused_fd_flags(flags) alloc_fd(0, (flags))
+#ifdef CONFIG_KRG_FAF
+extern void __put_unused_fd(struct files_struct *files, unsigned int fd);
+#endif
 extern void put_unused_fd(unsigned int fd);
-
+#ifdef CONFIG_KRG_FAF
+extern void __fd_install(struct files_struct *files,
+			 unsigned int fd, struct file *file);
+#endif
 extern void fd_install(unsigned int fd, struct file *file);
 
+#ifdef CONFIG_KRG_DVFS
+struct fdtable;
+int count_open_files(struct fdtable *fdt);
+
+struct fdtable * alloc_fdtable(unsigned int nr);
+#endif
+
 #endif /* __LINUX_FILE_H */
diff -ruN linux-2.6.29/include/linux/fs.h android_cluster/linux-2.6.29/include/linux/fs.h
--- linux-2.6.29/include/linux/fs.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/fs.h	2014-05-27 23:04:10.278030999 -0700
@@ -167,6 +167,9 @@
 #define S_NOCMTIME	128	/* Do not update file c/mtime */
 #define S_SWAPFILE	256	/* Do not truncate: swapon got its bmaps */
 #define S_PRIVATE	512	/* Inode is fs-internal */
+#ifdef CONFIG_KRG_FAF
+#define S_IFAF          1024
+#endif
 
 /*
  * Note that nosuid etc flags are inode-specific: setting some file-system
@@ -563,6 +566,9 @@
 	spinlock_t		private_lock;	/* for use by the address_space */
 	struct list_head	private_list;	/* ditto */
 	struct address_space	*assoc_mapping;	/* ditto */
+#ifdef CONFIG_KRG_DVFS
+	struct kddm_set         *kddm_set;
+#endif
 } __attribute__((aligned(sizeof(long))));
 	/*
 	 * On most architectures that alignment is already the case; but
@@ -691,6 +697,9 @@
 
 	__u32			i_generation;
 
+#ifdef CONFIG_KRG_DVFS
+	unsigned long           i_objid;
+#endif
 #ifdef CONFIG_DNOTIFY
 	unsigned long		i_dnotify_mask; /* Directory notify events */
 	struct dnotify_struct	*i_dnotify; /* for directory notifications */
@@ -849,7 +858,11 @@
 #define f_vfsmnt	f_path.mnt
 	const struct file_operations	*f_op;
 	atomic_long_t		f_count;
+#ifdef CONFIG_KRG_FAF
+	unsigned long           f_flags;
+#else
 	unsigned int 		f_flags;
+#endif
 	fmode_t			f_mode;
 	loff_t			f_pos;
 	struct fown_struct	f_owner;
@@ -860,6 +873,13 @@
 #ifdef CONFIG_SECURITY
 	void			*f_security;
 #endif
+#ifdef CONFIG_KRG_DVFS
+	unsigned long           f_objid;
+#endif
+#ifdef CONFIG_KRG_FAF
+	unsigned long           f_faf_srv_index;
+#endif
+
 	/* needed for tty driver, and maybe others */
 	void			*private_data;
 
diff -ruN linux-2.6.29/include/linux/hashtable.h android_cluster/linux-2.6.29/include/linux/hashtable.h
--- linux-2.6.29/include/linux/hashtable.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/linux/hashtable.h	2014-05-27 23:04:10.278030999 -0700
@@ -0,0 +1,279 @@
+/** Hashtable management interface
+ *  @file hashtable.h
+ *
+ *  Definition of hashtable management functions.
+ *  @author Viet Hoa Dinh, Renaud Lottiaux
+ */
+
+#ifndef __HASHTABLE_H__
+#define __HASHTABLE_H__
+
+#include <linux/stddef.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <asm/system.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                 MACROS                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+#define HASHTABLE_SIZE 1024
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+/** Hashtable linked list element */
+struct hash_list {
+	unsigned long hash;        /* Key of stored data */
+	void * data;               /* Data stored in the table */
+	struct hash_list * next;   /* Next stored data */
+};
+
+/** Hashtable Structure */
+typedef struct hashtable_t {
+	spinlock_t lock;                /** Structure lock */
+	struct hash_list * table;       /** Hash table */
+	unsigned long hashtable_size;   /** Size of the hash table */
+	unsigned long flags[NR_CPUS];
+} hashtable_t;
+
+#define hashtable_lock(table) spin_lock (&table->lock)
+#define hashtable_unlock(table) spin_unlock (&table->lock)
+
+#define hashtable_lock_bh(table) spin_lock_bh (&table->lock)
+#define hashtable_unlock_bh(table) spin_unlock_bh (&table->lock)
+
+#define hashtable_lock_irq(table) spin_lock_irq (&table->lock)
+#define hashtable_unlock_irq(table) spin_unlock_irq (&table->lock)
+
+#define hashtable_lock_irqsave(table) spin_lock_irqsave (&table->lock, table->flags[smp_processor_id()])
+#define hashtable_unlock_irqrestore(table) spin_unlock_irqrestore (&table->lock, table->flags[smp_processor_id()])
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Create a new hash table
+ *  @author Viet Hoa Dinh
+ * 
+ *  @param hashtable_size  Size of the hashtable to create.
+ *
+ *  @return  A pointer to the newly created hash table.
+ */
+hashtable_t *_hashtable_new(unsigned long hashtable_size);
+static inline hashtable_t * hashtable_new(unsigned long hashtable_size)
+{
+	hashtable_t *ht;
+
+	ht = _hashtable_new(hashtable_size);
+	if (ht)
+		spin_lock_init(&ht->lock);
+
+	return ht;
+}
+
+
+
+/** Free a hash table
+ *  @author Viet Hoa Dinh
+ * 
+ *  @param table  The table to free
+ */
+void hashtable_free(hashtable_t * table);
+
+
+
+/** Add an element to a hash table
+ *  @author Viet Hoa Dinh
+ * 
+ *  @param table  The table to add the element in.
+ *  @param hash   The element key.
+ *  @param data   The element to add in the table
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int __hashtable_add(hashtable_t * table, unsigned long hash, void * data);
+
+static inline int hashtable_add(hashtable_t * table, unsigned long hash,
+			       void * data)
+{
+	int r;
+
+	hashtable_lock_irqsave (table);
+
+	r = __hashtable_add (table, hash, data);
+
+	hashtable_unlock_irqrestore (table);
+
+	return r;
+}
+
+/** Add an element to a hash table
+ *  It fails with EEXIST if there is already an element with the same hash.
+ *
+ *  @author Matthieu Fertré
+ *
+ *  @param table  The table to add the element in.
+ *  @param hash   The element key.
+ *  @param data   The element to add in the table
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int __hashtable_add_unique(hashtable_t *table, unsigned long hash, void *data);
+
+static inline int hashtable_add_unique(hashtable_t *table, unsigned long hash,
+				       void *data)
+{
+	int r;
+
+	hashtable_lock_irqsave(table);
+
+	r = __hashtable_add_unique(table, hash, data);
+
+	hashtable_unlock_irqrestore(table);
+
+	return r;
+}
+
+/** Remove an element from a hash table
+ *  @author Viet Hoa Dinh
+ * 
+ *  @param table  The table to remove the element from.
+ *  @param hash   The element key.
+ *
+ *  @return  The data removed.
+ *           NULL value otherwise.
+ */
+void *__hashtable_remove(hashtable_t * table, unsigned long hash);
+
+static inline void * hashtable_remove(hashtable_t * table, unsigned long hash)
+{
+	void *data;
+
+	hashtable_lock_irqsave (table);
+
+	data = __hashtable_remove (table, hash);
+
+	hashtable_unlock_irqrestore (table);
+
+	return data;
+}
+
+
+
+/** Find an element in a hash table
+ *  @author Viet Hoa Dinh
+ * 
+ *  @param table  The table to search the element in.
+ *  @param hash   The element key.
+ *
+ *  @return  A pointer to the data, if in the table.
+ *           NULL if data not found.
+ */
+void * __hashtable_find(hashtable_t * table, unsigned long hash);
+
+static inline void * hashtable_find(hashtable_t * table, unsigned long hash)
+{
+	void * r;
+
+	hashtable_lock_irqsave (table);
+
+	r = __hashtable_find (table, hash);
+
+	hashtable_unlock_irqrestore (table);
+
+	return r;
+}
+
+
+
+/** Find the element just following the given hash in hash order.
+ *  @author Renaud Lottiaux
+ *
+ *  @param table  The table to search the element in.
+ *  @param hash   The element key.
+ *
+ *  @return  A pointer to the data, if in the table.
+ *           NULL if data not found.
+ *           Hash of the found element is returned in hash_found.
+ */
+void * __hashtable_find_next(hashtable_t * table, unsigned long hash,
+			     unsigned long *hash_found);
+static inline void * hashtable_find_next(hashtable_t * table,
+					 unsigned long hash,
+					 unsigned long *hash_found)
+{
+	void * r;
+
+	hashtable_lock_irqsave (table);
+
+	r = __hashtable_find_next (table, hash, hash_found);
+
+	hashtable_unlock_irqrestore (table);
+
+	return r;
+}
+
+
+/** Apply a function on each hash table key.
+ *  @author Viet Hoa Dinh, Pascal Gallard
+ * 
+ *  @param table  The table to work with.
+ *  @param func   The function to apply.
+ *  @param data   Data to send to the given function.
+ */
+void __hashtable_foreach_key(hashtable_t * table,
+			     void (* func)(unsigned long, void *),
+			     void * data);
+
+
+
+/** Apply a function on each hash table element.
+ *  @author Viet Hoa Dinh, Pascal Gallard
+ * 
+ *  @param table  The table to work with.
+ *  @param func   The function to apply.
+ *  @param data   Data to send to the given function.
+ */
+void __hashtable_foreach_data(hashtable_t * table,
+			      void (* fun)(void *, void *),
+			      void * data);
+
+/** Apply a function on each hash table pair (key,element).
+ *  @author Louis Rilling
+ *
+ *  @param table  The table to work with.
+ *  @param func   The function to apply.
+ *  @param data   Data to send to the given function.
+ */
+void __hashtable_foreach_key_data(hashtable_t * table,
+				  void (* func)(unsigned long, void *, void *),
+				  void * data);
+
+/** Find an element of the hashtable that staifies a criteria
+ *  @author David Margery
+ * 
+ *  @param table  The table to work with.
+ *  @param func   The function to apply.
+ *  @param data   Data to send to the given function.
+ */
+void * hashtable_find_data(hashtable_t * table,
+			   int (* fun)(void *, void *),
+			   void * data);
+
+
+#endif // __HASHTABLE_H__
diff -ruN linux-2.6.29/include/linux/highmem.h android_cluster/linux-2.6.29/include/linux/highmem.h
--- linux-2.6.29/include/linux/highmem.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/highmem.h	2014-05-27 23:04:10.282030916 -0700
@@ -187,4 +187,22 @@
 	kunmap_atomic(vto, KM_USER1);
 }
 
+#ifdef CONFIG_KRG_MM
+static inline void copy_buff_to_highpage(struct page * page, char *buffer)
+{
+	char *to;
+	to = (char *)kmap_atomic(page, KM_USER0);
+	copy_page(to, buffer);
+	kunmap_atomic(to, KM_USER0);
+}
+
+static inline void copy_highpage_to_buff(char *buffer, struct page * page)
+{
+	char *from;
+	from = (char *)kmap_atomic(page, KM_USER0);
+	copy_page(buffer, from);
+	kunmap_atomic(from, KM_USER0);
+}
+#endif
+
 #endif /* _LINUX_HIGHMEM_H */
diff -ruN linux-2.6.29/include/linux/init_task.h android_cluster/linux-2.6.29/include/linux/init_task.h
--- linux-2.6.29/include/linux/init_task.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/init_task.h	2014-05-27 23:04:10.290030750 -0700
@@ -10,10 +10,27 @@
 #include <linux/user_namespace.h>
 #include <linux/securebits.h>
 #include <net/net_namespace.h>
+#ifdef CONFIG_KRG_CAP
+#include <kerrighed/capabilities.h>
+#endif
 
 extern struct files_struct init_files;
 extern struct fs_struct init_fs;
 
+#ifdef CONFIG_KRG_EPM
+#define INIT_MM_EPM						\
+	.mm_ltasks      = ATOMIC_INIT(1),
+#else
+#define INIT_MM_EPM
+#endif
+
+#ifdef CONFIG_KRG_MM
+#define INIT_MM_MM                                              \
+        .mm_tasks       = ATOMIC_INIT(1),
+#else
+#define INIT_MM_MM
+#endif
+
 #define INIT_KIOCTX(name, which_mm) \
 {							\
 	.users		= ATOMIC_INIT(1),		\
@@ -31,6 +48,8 @@
 {			 					\
 	.mm_rb		= RB_ROOT,				\
 	.pgd		= swapper_pg_dir, 			\
+	INIT_MM_MM						\
+	INIT_MM_EPM						\
 	.mm_users	= ATOMIC_INIT(2), 			\
 	.mm_count	= ATOMIC_INIT(1), 			\
 	.mmap_sem	= __RWSEM_INITIALIZER(name.mmap_sem),	\
@@ -120,6 +139,23 @@
 
 extern struct cred init_cred;
 
+#ifdef CONFIG_KRG_CAP
+#define INIT_KRG_CAP .krg_caps = {			    \
+	.permitted = KRG_CAP_INIT_PERM_SET,		    \
+	.effective = KRG_CAP_INIT_EFF_SET,		    \
+	.inheritable_permitted = KRG_CAP_INIT_INH_PERM_SET, \
+	.inheritable_effective = KRG_CAP_INIT_INH_EFF_SET   \
+},
+#else
+#define INIT_KRG_CAP
+#endif
+
+#ifdef CONFIG_KRG_KDDM
+#define INIT_KDDM .kddm_info = NULL,
+#else
+#define INIT_KDDM
+#endif
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -184,6 +220,8 @@
 	INIT_IDS							\
 	INIT_TRACE_IRQFLAGS						\
 	INIT_LOCKDEP							\
+	INIT_KRG_CAP							\
+	INIT_KDDM							\
 }
 
 
diff -ruN linux-2.6.29/include/linux/ipc.h android_cluster/linux-2.6.29/include/linux/ipc.h
--- linux-2.6.29/include/linux/ipc.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/ipc.h	2014-05-27 23:04:10.290030750 -0700
@@ -2,6 +2,7 @@
 #define _LINUX_IPC_H
 
 #include <linux/types.h>
+#include <linux/mutex.h>
 
 #define IPC_PRIVATE ((__kernel_key_t) 0)  
 
@@ -84,10 +85,18 @@
 
 #define IPCMNI 32768  /* <= MAX_INT limit for ipc arrays (including sysctl changes) */
 
+#ifdef CONFIG_KRG_IPC
+struct krgipc_ops;
+#endif
+
 /* used by in-kernel data structures */
 struct kern_ipc_perm
 {
+#ifdef CONFIG_KRG_IPC
+	struct mutex    mutex;
+#else
 	spinlock_t	lock;
+#endif
 	int		deleted;
 	int		id;
 	key_t		key;
@@ -98,8 +107,19 @@
 	mode_t		mode; 
 	unsigned long	seq;
 	void		*security;
+#ifdef CONFIG_KRG_IPC
+	struct krgipc_ops *krgops;
+#endif
 };
 
+#ifdef CONFIG_KRG_IPC
+struct ipc_namespace;
+
+bool ipc_used(struct ipc_namespace *ns);
+#endif
+
+void cleanup_ipc_objects (void);
+
 #endif /* __KERNEL__ */
 
 #endif /* _LINUX_IPC_H */
diff -ruN linux-2.6.29/include/linux/ipc_namespace.h android_cluster/linux-2.6.29/include/linux/ipc_namespace.h
--- linux-2.6.29/include/linux/ipc_namespace.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/ipc_namespace.h	2014-06-09 18:19:41.284396853 -0700
@@ -15,6 +15,9 @@
 
 #define IPCNS_CALLBACK_PRI 0
 
+#ifdef CONFIG_KRG_IPC
+struct krgipc_ops;
+#endif
 
 struct ipc_ids {
 	int in_use;
@@ -22,6 +25,9 @@
 	unsigned short seq_max;
 	struct rw_semaphore rw_mutex;
 	struct idr ipcs_idr;
+#ifdef CONFIG_KRG_IPC
+	struct krgipc_ops *krgops;
+#endif
 };
 
 struct ipc_namespace {
diff -ruN linux-2.6.29/include/linux/kernel.h android_cluster/linux-2.6.29/include/linux/kernel.h
--- linux-2.6.29/include/linux/kernel.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/kernel.h	2014-05-27 23:04:10.298030584 -0700
@@ -165,6 +165,10 @@
 extern int oops_may_print(void);
 NORET_TYPE void do_exit(long error_code)
 	ATTRIB_NORET;
+#ifdef CONFIG_KRG_EPM
+NORET_TYPE void do_exit_wo_notify(long code)
+	ATTRIB_NORET;
+#endif
 NORET_TYPE void complete_and_exit(struct completion *, long)
 	ATTRIB_NORET;
 extern unsigned long simple_strtoul(const char *,char **,unsigned int);
diff -ruN linux-2.6.29/include/linux/magic.h android_cluster/linux-2.6.29/include/linux/magic.h
--- linux-2.6.29/include/linux/magic.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/magic.h	2014-05-27 23:04:10.302030500 -0700
@@ -49,4 +49,7 @@
 #define FUTEXFS_SUPER_MAGIC	0xBAD1DEA
 #define INOTIFYFS_SUPER_MAGIC	0x2BAD1DEA
 
+#ifdef CONFIG_KRG_DVFS
+#define OCFS2_SUPER_MAGIC		0x7461636f
+#endif
 #endif /* __LINUX_MAGIC_H__ */
diff -ruN linux-2.6.29/include/linux/memcontrol.h android_cluster/linux-2.6.29/include/linux/memcontrol.h
--- linux-2.6.29/include/linux/memcontrol.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/memcontrol.h	2014-05-27 23:04:10.302030500 -0700
@@ -64,7 +64,11 @@
 					unsigned long *scanned, int order,
 					int mode, struct zone *z,
 					struct mem_cgroup *mem_cont,
+#ifdef CONFIG_KRG_MM
+					int active, int file, int kddm);
+#else
 					int active, int file);
+#endif
 extern void mem_cgroup_out_of_memory(struct mem_cgroup *mem, gfp_t gfp_mask);
 int task_in_mem_cgroup(struct task_struct *task, const struct mem_cgroup *mem);
 
diff -ruN linux-2.6.29/include/linux/mm.h android_cluster/linux-2.6.29/include/linux/mm.h
--- linux-2.6.29/include/linux/mm.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/mm.h	2014-06-09 18:19:41.284396853 -0700
@@ -105,6 +105,10 @@
 #define VM_MIXEDMAP	0x10000000	/* Can contain "struct page" and pure PFN pages */
 #define VM_SAO		0x20000000	/* Strong Access Ordering (powerpc) */
 
+#ifdef CONFIG_KRG_MM
+#define VM_KDDM		0x80000000	/* The vma is stored inside KDDM */
+#endif
+
 #ifndef VM_STACK_DEFAULT_FLAGS		/* arch can override this */
 #define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS
 #endif
@@ -167,6 +171,9 @@
 	pgoff_t pgoff;			/* Logical page offset based on vma */
 	void __user *virtual_address;	/* Faulting virtual address */
 
+#ifdef CONFIG_KRG_MM
+	pte_t pte;
+#endif
 	struct page *page;		/* ->fault handlers should return a
 					 * page here, unless VM_FAULT_NOPAGE
 					 * is set (which is also implied by
@@ -183,6 +190,12 @@
 	void (*open)(struct vm_area_struct * area);
 	void (*close)(struct vm_area_struct * area);
 	int (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);
+#ifdef CONFIG_KRG_MM
+	struct page * (*wppage)(struct vm_area_struct *area,
+				unsigned long address,
+				struct page *old_page);
+	void (*unlink)(struct vm_area_struct *area);
+#endif // CONFIG_KRG_MM
 
 	/* notification that a previously read-only page is about to become
 	 * writable, if an error is returned it will cause a SIGBUS */
@@ -291,6 +304,13 @@
 	return page;
 }
 
+#ifdef CONFIG_KRG_MM
+static inline int page_kddm_count(struct page *page)
+{
+	return atomic_read(&page->_kddm_count);
+}
+#endif
+
 static inline int page_count(struct page *page)
 {
 	return atomic_read(&compound_head(page)->_count);
@@ -788,8 +808,13 @@
 		struct mm_walk *walk);
 void free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
 		unsigned long end, unsigned long floor, unsigned long ceiling);
+#ifdef CONFIG_KRG_MM
+int copy_page_range(struct mm_struct *dst, struct mm_struct *src,
+			struct vm_area_struct *vma, int anon_only);
+#else
 int copy_page_range(struct mm_struct *dst, struct mm_struct *src,
 			struct vm_area_struct *vma);
+#endif
 void unmap_mapping_range(struct address_space *mapping,
 		loff_t const holebegin, loff_t const holelen, int even_cows);
 int follow_phys(struct vm_area_struct *vma, unsigned long address,
@@ -820,6 +845,12 @@
 }
 #endif
 
+#ifdef CONFIG_KRG_MM
+int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,
+		 unsigned long address, pte_t *page_table, pmd_t *pmd,
+		 int write_access, pte_t orig_pte);
+#endif
+
 extern int make_pages_present(unsigned long addr, unsigned long end);
 extern int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len, int write);
 
@@ -840,6 +871,14 @@
 extern unsigned long move_page_tables(struct vm_area_struct *vma,
 		unsigned long old_addr, struct vm_area_struct *new_vma,
 		unsigned long new_addr, unsigned long len);
+#ifdef CONFIG_KRG_MM
+int do_mprotect(struct mm_struct *mm, unsigned long start, size_t len,
+		unsigned long prot, int personality);
+unsigned long __do_mremap(struct mm_struct *mm, unsigned long addr,
+			  unsigned long old_len, unsigned long new_len,
+			  unsigned long flags, unsigned long new_addr,
+			  unsigned long *_new_addr, unsigned long _lock_limit);
+#endif
 extern unsigned long do_mremap(unsigned long addr,
 			       unsigned long old_len, unsigned long new_len,
 			       unsigned long flags, unsigned long new_addr);
@@ -1137,13 +1176,22 @@
 
 extern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);
 
+#ifdef CONFIG_KRG_MM
+extern unsigned long __get_unmapped_area(struct mm_struct *, struct file *, unsigned long , unsigned long , unsigned long , unsigned long );
+#endif
+
 extern unsigned long do_mmap_pgoff(struct file *file, unsigned long addr,
 	unsigned long len, unsigned long prot,
 	unsigned long flag, unsigned long pgoff);
 extern unsigned long mmap_region(struct file *file, unsigned long addr,
 	unsigned long len, unsigned long flags,
 	unsigned int vm_flags, unsigned long pgoff);
-
+#ifdef CONFIG_KRG_MM
+extern unsigned long __mmap_region(struct mm_struct *mm, struct file *file,
+				   unsigned long addr, unsigned long len,
+				   unsigned long flags, unsigned int vm_flags,
+				   unsigned long pgoff, int handler_call);
+#endif
 static inline unsigned long do_mmap(struct file *file, unsigned long addr,
 	unsigned long len, unsigned long prot,
 	unsigned long flag, unsigned long offset)
@@ -1159,6 +1207,12 @@
 
 extern int do_munmap(struct mm_struct *, unsigned long, size_t);
 
+#ifdef CONFIG_KRG_MM
+extern unsigned long __do_brk(struct mm_struct * mm, unsigned long addr,
+			      unsigned long len, unsigned long _lock_limit);
+unsigned long __sys_brk(struct mm_struct *mm, unsigned long brk,
+			unsigned long lock_limit, unsigned long data_limit);
+#endif
 extern unsigned long do_brk(unsigned long, unsigned long);
 
 /* filemap.c */
@@ -1199,7 +1253,23 @@
 unsigned long max_sane_readahead(unsigned long nr);
 
 /* Do stack extension */
+#ifdef CONFIG_KRG_MM
+extern int krg_expand_stack(struct vm_area_struct *vma, unsigned long address);
+extern int __expand_stack(struct vm_area_struct *vma, unsigned long address);
+static inline int expand_stack(struct vm_area_struct *vma,
+			       unsigned long address)
+{
+	int err;
+
+	err = __expand_stack(vma, address);
+	if (!err && vma->vm_mm->anon_vma_kddm_set)
+		krg_expand_stack(vma, address);
+
+	return err;
+}
+#else
 extern int expand_stack(struct vm_area_struct *vma, unsigned long address);
+#endif
 #ifdef CONFIG_IA64
 extern int expand_upwards(struct vm_area_struct *vma, unsigned long address);
 #endif
@@ -1319,5 +1389,10 @@
 extern void *alloc_locked_buffer(size_t size);
 extern void free_locked_buffer(void *buffer, size_t size);
 extern void release_locked_buffer(void *buffer, size_t size);
+
+#ifdef CONFIG_KRG_MM
+#include <kerrighed/mm.h>
+#endif
+
 #endif /* __KERNEL__ */
 #endif /* _LINUX_MM_H */
diff -ruN linux-2.6.29/include/linux/mm_inline.h android_cluster/linux-2.6.29/include/linux/mm_inline.h
--- linux-2.6.29/include/linux/mm_inline.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/mm_inline.h	2014-05-27 23:04:10.310030334 -0700
@@ -23,6 +23,16 @@
 	return LRU_FILE;
 }
 
+#ifdef CONFIG_KRG_MM
+static inline int page_is_migratable(struct page *page)
+{
+	if (PageMigratable(page))
+		return LRU_MIGR;
+
+	return 0;
+}
+#endif
+
 static inline void
 add_page_to_lru_list(struct zone *zone, struct page *page, enum lru_list l)
 {
@@ -53,6 +63,11 @@
 			__ClearPageActive(page);
 			l += LRU_ACTIVE;
 		}
+#ifdef CONFIG_KRG_MM
+		if (PageMigratable(page))
+			l += LRU_MIGR;
+		else
+#endif
 		l += page_is_file_cache(page);
 	}
 	__dec_zone_state(zone, NR_LRU_BASE + l);
@@ -75,10 +90,26 @@
 	else {
 		if (PageActive(page))
 			lru += LRU_ACTIVE;
+#ifdef CONFIG_KRG_MM
+		if (PageMigratable(page))
+			lru += LRU_MIGR;
+		else
+#endif
 		lru += page_is_file_cache(page);
 	}
 
 	return lru;
 }
 
+#define BUILD_LRU_ID(active,file,kddm) (LRU_BASE + LRU_MIGR * kddm + LRU_FILE * file + active)
+
+#ifdef CONFIG_KRG_MM
+#define RECLAIM_STAT_INDEX(file,kddm) ((!!file) + 2 * (!!kddm))
+static inline int reclaim_stat_index(struct page *page)
+{
+	return RECLAIM_STAT_INDEX(page_is_file_cache(page),
+				  page_is_migratable(page));
+}
+#endif
+
 #endif
diff -ruN linux-2.6.29/include/linux/mm_types.h android_cluster/linux-2.6.29/include/linux/mm_types.h
--- linux-2.6.29/include/linux/mm_types.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/mm_types.h	2014-05-27 23:04:10.310030334 -0700
@@ -14,6 +14,10 @@
 #include <asm/page.h>
 #include <asm/mmu.h>
 
+#ifdef CONFIG_KRG_MM
+#include <kerrighed/types.h>
+#endif
+
 #ifndef AT_VECTOR_SIZE_ARCH
 #define AT_VECTOR_SIZE_ARCH 0
 #endif
@@ -50,6 +54,11 @@
 			u16 objects;
 		};
 	};
+#ifdef CONFIG_KRG_MM
+	atomic_t _kddm_count;		/* Count number of KDDM set sharing
+					 * the page */
+	void *obj_entry;
+#endif
 	union {
 	    struct {
 		unsigned long private;		/* Mapping-private opaque data:
@@ -130,6 +139,9 @@
 
 	pgprot_t vm_page_prot;		/* Access permissions of this VMA. */
 	unsigned long vm_flags;		/* Flags, see mm.h. */
+#ifdef CONFIG_KRG_MM
+	struct vm_operations_struct * initial_vm_ops;
+#endif
 
 	struct rb_node vm_rb;
 
@@ -200,6 +212,13 @@
 	unsigned long cached_hole_size; 	/* if non-zero, the largest hole below free_area_cache */
 	unsigned long free_area_cache;		/* first hole of size cached_hole_size or larger */
 	pgd_t * pgd;
+#ifdef CONFIG_KRG_MM
+	atomic_t mm_tasks;			/* How many tasks sharing this mm_struct cluster wide */
+	struct rw_semaphore remove_sem;         /* Protect struct remove during a migration */
+#endif
+#ifdef CONFIG_KRG_EPM
+	atomic_t mm_ltasks;			/* How many tasks sharing this mm_struct locally */
+#endif
 	atomic_t mm_users;			/* How many users with user space? */
 	atomic_t mm_count;			/* How many references to "struct mm_struct" (users count as 1) */
 	int map_count;				/* number of VMAs */
@@ -248,6 +267,13 @@
 
 	struct core_state *core_state; /* coredumping support */
 
+#ifdef CONFIG_KRG_MM
+	struct kddm_set * anon_vma_kddm_set;
+	unique_id_t anon_vma_kddm_id;
+	krgnodemask_t copyset;		/* Nodes owning a copy of the struct */
+	unique_id_t mm_id;
+#endif
+
 	/* aio bits */
 	spinlock_t		ioctx_lock;
 	struct hlist_head	ioctx_list;
diff -ruN linux-2.6.29/include/linux/mmzone.h android_cluster/linux-2.6.29/include/linux/mmzone.h
--- linux-2.6.29/include/linux/mmzone.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/mmzone.h	2014-05-27 23:04:10.310030334 -0700
@@ -86,6 +86,10 @@
 	NR_ACTIVE_ANON,		/*  "     "     "   "       "         */
 	NR_INACTIVE_FILE,	/*  "     "     "   "       "         */
 	NR_ACTIVE_FILE,		/*  "     "     "   "       "         */
+#ifdef CONFIG_KRG_MM
+	NR_INACTIVE_MIGR,
+	NR_ACTIVE_MIGR,
+#endif
 #ifdef CONFIG_UNEVICTABLE_LRU
 	NR_UNEVICTABLE,		/*  "     "     "   "       "         */
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
@@ -129,12 +133,17 @@
 #define LRU_BASE 0
 #define LRU_ACTIVE 1
 #define LRU_FILE 2
+#define LRU_MIGR 4
 
 enum lru_list {
 	LRU_INACTIVE_ANON = LRU_BASE,
 	LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,
 	LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,
 	LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,
+#ifdef CONFIG_KRG_MM
+	LRU_INACTIVE_MIGR = LRU_BASE + LRU_MIGR,
+	LRU_ACTIVE_MIGR = LRU_BASE + LRU_MIGR + LRU_ACTIVE,
+#endif
 #ifdef CONFIG_UNEVICTABLE_LRU
 	LRU_UNEVICTABLE,
 #else
@@ -145,16 +154,32 @@
 
 #define for_each_lru(l) for (l = 0; l < NR_LRU_LISTS; l++)
 
+#ifdef CONFIG_KRG_MM
+#define for_each_evictable_lru(l) for (l = 0; l <= LRU_ACTIVE_MIGR; l++)
+#else
 #define for_each_evictable_lru(l) for (l = 0; l <= LRU_ACTIVE_FILE; l++)
+#endif
 
 static inline int is_file_lru(enum lru_list l)
 {
 	return (l == LRU_INACTIVE_FILE || l == LRU_ACTIVE_FILE);
 }
 
+#ifdef CONFIG_KRG_MM
+static inline int is_kddm_lru(enum lru_list l)
+{
+	return (l == LRU_INACTIVE_MIGR || l == LRU_ACTIVE_MIGR);
+}
+#endif
+
 static inline int is_active_lru(enum lru_list l)
 {
+#ifdef CONFIG_KRG_MM
+	return (l == LRU_ACTIVE_ANON || l == LRU_ACTIVE_FILE ||
+		l == LRU_ACTIVE_MIGR);
+#else
 	return (l == LRU_ACTIVE_ANON || l == LRU_ACTIVE_FILE);
+#endif
 }
 
 static inline int is_unevictable_lru(enum lru_list l)
@@ -272,8 +297,14 @@
 	 *
 	 * The anon LRU stats live in [0], file LRU stats in [1]
 	 */
+#ifdef CONFIG_KRG_MM
+	/* The KDDM migratable LRU stats live in [2] */
+	unsigned long		recent_rotated[3];
+	unsigned long		recent_scanned[3];
+#else
 	unsigned long		recent_rotated[2];
 	unsigned long		recent_scanned[2];
+#endif
 };
 
 struct zone {
diff -ruN linux-2.6.29/include/linux/module_hook.h android_cluster/linux-2.6.29/include/linux/module_hook.h
--- linux-2.6.29/include/linux/module_hook.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/linux/module_hook.h	2014-05-27 23:04:10.310030334 -0700
@@ -0,0 +1,22 @@
+#ifndef __MODULE_HOOK_H__
+#define __MODULE_HOOK_H__
+
+#ifdef CONFIG_MODULE_HOOK
+
+typedef void module_hook_cb_t(unsigned long arg);
+struct module;
+
+struct module_hook_desc {
+	module_hook_cb_t *callback;
+	struct module *owner;
+};
+
+int module_hook_register(struct module_hook_desc *desc,
+			 module_hook_cb_t *callback, struct module *owner);
+void module_hook_unregister(struct module_hook_desc *desc,
+			    module_hook_cb_t *callback);
+void module_hook_call(struct module_hook_desc *desc, unsigned long arg);
+
+#endif /* CONFIG_MODULE_HOOK */
+
+#endif /* __MODULE_HOOK_H__ */
diff -ruN linux-2.6.29/include/linux/msg.h android_cluster/linux-2.6.29/include/linux/msg.h
--- linux-2.6.29/include/linux/msg.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/msg.h	2014-05-27 23:04:10.314030251 -0700
@@ -99,6 +99,10 @@
 	struct list_head q_messages;
 	struct list_head q_receivers;
 	struct list_head q_senders;
+
+#ifdef CONFIG_KRG_IPC
+	int is_master;
+#endif
 };
 
 /* Helper routines for sys_msgsnd and sys_msgrcv */
diff -ruN linux-2.6.29/include/linux/nsproxy.h android_cluster/linux-2.6.29/include/linux/nsproxy.h
--- linux-2.6.29/include/linux/nsproxy.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/nsproxy.h	2014-05-27 23:04:10.330029919 -0700
@@ -9,6 +9,12 @@
 struct ipc_namespace;
 struct pid_namespace;
 
+#ifdef CONFIG_KRG_EPM
+struct kmem_cache;
+
+extern struct kmem_cache *nsproxy_cachep;
+#endif
+
 /*
  * A structure to contain pointers to all per-process
  * namespaces - fs (mount), uts, network, sysvipc, etc.
@@ -28,6 +34,9 @@
 	struct mnt_namespace *mnt_ns;
 	struct pid_namespace *pid_ns;
 	struct net 	     *net_ns;
+#ifdef CONFIG_KRG_HOTPLUG
+	struct krg_namespace *krg_ns;
+#endif
 };
 extern struct nsproxy init_nsproxy;
 
diff -ruN linux-2.6.29/include/linux/page-flags.h android_cluster/linux-2.6.29/include/linux/page-flags.h
--- linux-2.6.29/include/linux/page-flags.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/page-flags.h	2014-05-27 23:04:10.330029919 -0700
@@ -93,6 +93,15 @@
 	PG_mappedtodisk,	/* Has blocks allocated on-disk */
 	PG_reclaim,		/* To be reclaimed asap */
 	PG_buddy,		/* Page is free, on buddy lists */
+#ifdef CONFIG_KRG_MM
+	PG_to_invalidate,
+	PG_to_set_read_only,
+	PG_locked_kddm,           /* Page is locked in kddm layer */
+	PG_migratable,
+#ifdef CONFIG_DEBUG_PAGEALLOC
+	PG_in_vec,
+#endif
+#endif
 	PG_swapbacked,		/* Page is backed by RAM/swap */
 #ifdef CONFIG_UNEVICTABLE_LRU
 	PG_unevictable,		/* Page is "unevictable"  */
@@ -256,6 +265,17 @@
 PAGEFLAG_FALSE(Uncached)
 #endif
 
+#ifdef CONFIG_KRG_MM
+PAGEFLAG(ToInvalidate, to_invalidate)
+PAGEFLAG(ToSetReadOnly, to_set_read_only)
+PAGEFLAG(LockedKDDM, locked_kddm)
+TESTSCFLAG(LockedKDDM, locked_kddm)
+PAGEFLAG(Migratable, migratable)
+#ifdef CONFIG_DEBUG_PAGEALLOC
+PAGEFLAG(InVec, in_vec)
+#endif
+#endif
+
 static inline int PageUptodate(struct page *page)
 {
 	int ret = test_bit(PG_uptodate, &(page)->flags);
diff -ruN linux-2.6.29/include/linux/pagevec.h android_cluster/linux-2.6.29/include/linux/pagevec.h
--- linux-2.6.29/include/linux/pagevec.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/pagevec.h	2014-05-27 23:04:10.334029836 -0700
@@ -8,6 +8,10 @@
 #ifndef _LINUX_PAGEVEC_H
 #define _LINUX_PAGEVEC_H
 
+#if defined(CONFIG_KRG_MM) && defined(CONFIG_DEBUG_PAGEALLOC)
+#include <linux/page-flags.h>
+#endif
+
 /* 14 pointers + two long's align the pagevec structure to a power of two */
 #define PAGEVEC_SIZE	14
 
@@ -57,6 +61,9 @@
  */
 static inline unsigned pagevec_add(struct pagevec *pvec, struct page *page)
 {
+#if defined(CONFIG_KRG_MM) && defined(CONFIG_DEBUG_PAGEALLOC)
+	SetPageInVec(page);
+#endif
 	pvec->pages[pvec->nr++] = page;
 	return pagevec_space(pvec);
 }
diff -ruN linux-2.6.29/include/linux/pid.h android_cluster/linux-2.6.29/include/linux/pid.h
--- linux-2.6.29/include/linux/pid.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/pid.h	2014-05-27 23:04:10.334029836 -0700
@@ -61,6 +61,12 @@
 	/* lists of tasks that use this pid */
 	struct hlist_head tasks[PIDTYPE_MAX];
 	struct rcu_head rcu;
+#ifdef CONFIG_KRG_EPM
+	struct pid_kddm_object *kddm_obj;
+#endif
+#ifdef CONFIG_KRG_SCHED
+	struct hlist_head process_sets[PIDTYPE_MAX];
+#endif
 	struct upid numbers[1];
 };
 
@@ -119,8 +125,22 @@
 extern struct pid *find_ge_pid(int nr, struct pid_namespace *);
 int next_pidmap(struct pid_namespace *pid_ns, int last);
 
+#ifndef CONFIG_KRG_EPM
 extern struct pid *alloc_pid(struct pid_namespace *ns);
+#endif
 extern void free_pid(struct pid *pid);
+#ifdef CONFIG_KRG_EPM
+extern struct pid *__alloc_pid(struct pid_namespace *ns, const int *req_nr);
+extern int reserve_pidmap(struct pid_namespace *ns, int nr);
+extern void __free_pidmap(struct upid *upid);
+struct pidmap;
+extern int alloc_pidmap_page(struct pidmap *map);
+
+static inline struct pid *alloc_pid(struct pid_namespace *ns)
+{
+	return __alloc_pid(ns, NULL);
+}
+#endif /* CONFIG_KRG_EPM */
 
 /*
  * ns_of_pid() returns the pid namespace in which the specified pid was
diff -ruN linux-2.6.29/include/linux/pid_namespace.h android_cluster/linux-2.6.29/include/linux/pid_namespace.h
--- linux-2.6.29/include/linux/pid_namespace.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/pid_namespace.h	2014-05-27 23:04:10.334029836 -0700
@@ -30,6 +30,10 @@
 #ifdef CONFIG_BSD_PROCESS_ACCT
 	struct bsd_acct_struct *bacct;
 #endif
+#ifdef CONFIG_KRG_PROC
+	struct pid_namespace *krg_ns_root;
+	unsigned global:1;
+#endif
 };
 
 extern struct pid_namespace init_pid_ns;
@@ -52,6 +56,19 @@
 		kref_put(&ns->kref, free_pid_ns);
 }
 
+#ifdef CONFIG_KRG_PROC
+static inline bool is_krg_pid_ns_root(struct pid_namespace *ns)
+{
+	return ns == ns->krg_ns_root;
+}
+
+struct pid_namespace *find_get_krg_pid_ns(void);
+#endif
+
+#ifdef CONFIG_KRG_EPM
+struct pid_namespace *create_pid_namespace(unsigned int level);
+#endif
+
 #else /* !CONFIG_PID_NS */
 #include <linux/err.h>
 
@@ -77,6 +94,18 @@
 {
 	BUG();
 }
+
+#ifdef CONFIG_KRG_PROC
+static inline bool is_krg_pid_ns_root(struct pid_namespace *ns)
+{
+	return true;
+}
+
+static inline struct pid_namespace *find_get_krg_pid_ns(void)
+{
+	return &init_pid_ns;
+}
+#endif
 #endif /* CONFIG_PID_NS */
 
 extern struct pid_namespace *task_active_pid_ns(struct task_struct *tsk);
diff -ruN linux-2.6.29/include/linux/pipe_fs_i.h android_cluster/linux-2.6.29/include/linux/pipe_fs_i.h
--- linux-2.6.29/include/linux/pipe_fs_i.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/pipe_fs_i.h	2014-05-27 23:04:10.334029836 -0700
@@ -55,6 +55,10 @@
 	struct fasync_struct *fasync_writers;
 	struct inode *inode;
 	struct pipe_buffer bufs[PIPE_BUFFERS];
+#ifdef CONFIG_KRG_EPM
+	struct file *fread;
+	struct file *fwrite;
+#endif
 };
 
 /*
diff -ruN linux-2.6.29/include/linux/posix-timers.h android_cluster/linux-2.6.29/include/linux/posix-timers.h
--- linux-2.6.29/include/linux/posix-timers.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/posix-timers.h	2014-05-27 23:04:10.338029752 -0700
@@ -109,6 +109,9 @@
 void posix_cpu_timer_schedule(struct k_itimer *timer);
 
 void run_posix_cpu_timers(struct task_struct *task);
+#ifdef CONFIG_KRG_EPM
+void posix_cpu_timers_init_group(struct signal_struct *sig);
+#endif
 void posix_cpu_timers_exit(struct task_struct *task);
 void posix_cpu_timers_exit_group(struct task_struct *task);
 
diff -ruN linux-2.6.29/include/linux/proc_fs.h android_cluster/linux-2.6.29/include/linux/proc_fs.h
--- linux-2.6.29/include/linux/proc_fs.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/proc_fs.h	2014-05-27 23:04:10.338029752 -0700
@@ -276,6 +276,32 @@
 		struct task_struct *task);
 };
 
+#if defined(CONFIG_KRG_PROCFS) && defined(CONFIG_KRG_PROC)
+#include <linux/types.h>
+#include <kerrighed/sys/types.h>
+
+struct proc_distant_pid_info;
+
+union proc_distant_op {
+	int (*proc_get_link)(struct inode *, struct path *);
+	int (*proc_read)(struct proc_distant_pid_info *task, char *page);
+	int (*proc_show)(struct file *file, struct proc_distant_pid_info *task,
+			 char *buf, size_t count);
+};
+
+struct task_kddm_object;
+
+struct proc_distant_pid_info {
+	struct task_kddm_object *task_obj;
+	pid_t pid;
+	kerrighed_node_t prob_node;
+	int dumpable;
+	uid_t euid;
+	gid_t egid;
+	union proc_distant_op op;
+};
+#endif
+
 struct ctl_table_header;
 struct ctl_table;
 
@@ -284,6 +310,12 @@
 	int fd;
 	union proc_op op;
 	struct proc_dir_entry *pde;
+#ifdef CONFIG_KRG_PROCFS
+	void *krg_procfs_private;
+#ifdef CONFIG_KRG_PROC
+	struct proc_distant_pid_info distant_proc;
+#endif
+#endif
 	struct ctl_table_header *sysctl;
 	struct ctl_table *sysctl_entry;
 	struct inode vfs_inode;
diff -ruN linux-2.6.29/include/linux/procfs_internal.h android_cluster/linux-2.6.29/include/linux/procfs_internal.h
--- linux-2.6.29/include/linux/procfs_internal.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/linux/procfs_internal.h	2014-05-27 23:04:10.338029752 -0700
@@ -0,0 +1,76 @@
+#ifndef __PROCFS_INTERNAL_H__
+#define __PROCFS_INTERNAL_H__
+
+/* All definitions below are moved from fs/proc/internal.h */
+#ifdef CONFIG_KRG_PROCFS
+
+struct vmalloc_info {
+	unsigned long	used;
+	unsigned long	largest_chunk;
+};
+
+#ifdef CONFIG_MMU
+#define VMALLOC_TOTAL (VMALLOC_END - VMALLOC_START)
+extern void get_vmalloc_info(struct vmalloc_info *vmi);
+#else
+
+#define VMALLOC_TOTAL 0UL
+#define get_vmalloc_info(vmi)			\
+do {						\
+	(vmi)->used = 0;			\
+	(vmi)->largest_chunk = 0;		\
+} while(0)
+#endif
+
+struct seq_file;
+int meminfo_proc_show(struct seq_file *m, void *v);
+int show_stat(struct seq_file *p, void *v);
+int loadavg_proc_show(struct seq_file *m, void *v);
+int uptime_proc_show(struct seq_file *m, void *v);
+
+#ifdef CONFIG_KRG_PROC
+/* From fs/proc/base.c */
+struct tgid_iter {
+	unsigned int tgid;
+	struct task_struct *task;
+};
+
+extern const struct inode_operations proc_def_inode_operations;
+
+int proc_setattr(struct dentry *dentry, struct iattr *attr);
+int proc_pid_fill_cache(struct file *filp, void *dirent, filldir_t filldir,
+			struct tgid_iter iter);
+int do_proc_readlink(struct path *path, char __user *buffer, int buflen);
+
+int proc_pid_cmdline(struct task_struct *task, char * buffer);
+int proc_pid_auxv(struct task_struct *task, char *buffer);
+int proc_pid_limits(struct task_struct *task, char *buffer);
+int proc_pid_syscall(struct task_struct *task, char *buffer);
+int proc_pid_wchan(struct task_struct *task, char *buffer);
+int proc_pid_schedstat(struct task_struct *task, char *buffer);
+int proc_oom_score(struct task_struct *task, char *buffer);
+
+int proc_pid_personality(struct seq_file *m, struct pid_namespace *ns,
+			 struct pid *pid, struct task_struct *task);
+int proc_pid_stack(struct seq_file *m, struct pid_namespace *ns,
+		   struct pid *pid, struct task_struct *task);
+int proc_tgid_io_accounting(struct task_struct *task, char *buffer);
+
+/* From fs/proc/internal.h */
+extern int proc_tgid_stat(struct seq_file *m, struct pid_namespace *ns,
+				struct pid *pid, struct task_struct *task);
+extern int proc_pid_status(struct seq_file *m, struct pid_namespace *ns,
+				struct pid *pid, struct task_struct *task);
+extern int proc_pid_statm(struct seq_file *m, struct pid_namespace *ns,
+				struct pid *pid, struct task_struct *task);
+
+extern struct dentry *krg_proc_pid_lookup(struct inode *dir,
+					  struct dentry *dentry, pid_t pid);
+extern int krg_proc_pid_readdir(struct file *filp,
+				void *dirent, filldir_t filldir,
+				loff_t offset);
+#endif /* CONFIG_KRG_PROC */
+
+#endif /* CONFIG_KRG_PROCFS */
+
+#endif /* __PROCFS_INTERNAL_H__ */
diff -ruN linux-2.6.29/include/linux/ptrace.h android_cluster/linux-2.6.29/include/linux/ptrace.h
--- linux-2.6.29/include/linux/ptrace.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/ptrace.h	2014-05-27 23:04:10.338029752 -0700
@@ -78,6 +78,9 @@
 
 #include <linux/compiler.h>		/* For unlikely.  */
 #include <linux/sched.h>		/* For struct task_struct.  */
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/children.h>
+#endif
 
 
 extern long arch_ptrace(struct task_struct *child, long request, long addr, long data);
@@ -101,16 +104,40 @@
 extern int __ptrace_may_access(struct task_struct *task, unsigned int mode);
 /* Returns true on success, false on denial. */
 extern bool ptrace_may_access(struct task_struct *task, unsigned int mode);
+#ifdef CONFIG_KRG_EPM
+extern
+int krg_ptrace_link(struct task_struct *task, struct task_struct *tracer);
+extern void krg_ptrace_unlink(struct task_struct *task);
+extern void krg_ptrace_reparent_ptraced(struct task_struct *real_parent,
+					struct task_struct *task);
+#endif /* CONFIG_KRG_EPM */
 
 static inline int ptrace_reparented(struct task_struct *child)
 {
+#ifdef CONFIG_KRG_EPM
+/*
+ * if (child->parent == baby_sitter || child->real_parent == baby_sitter)
+ *		return child->task_obj->parent != child->task_obj->real_parent;
+ */
+#endif
 	return child->real_parent != child->parent;
 }
 static inline void ptrace_link(struct task_struct *child,
 			       struct task_struct *new_parent)
 {
+#ifdef CONFIG_KRG_EPM
+	if (unlikely(child->ptrace)) {
+		int ret = krg_ptrace_link(child, new_parent);
+		BUG_ON(ret);
+		ret = krg_set_child_ptraced(child->parent_children_obj,
+					    child, 1);
+		BUG_ON(ret);
+		__ptrace_link(child, new_parent);
+	}
+#else
 	if (unlikely(child->ptrace))
 		__ptrace_link(child, new_parent);
+#endif
 }
 static inline void ptrace_unlink(struct task_struct *child)
 {
diff -ruN linux-2.6.29/include/linux/remote_sleep.h android_cluster/linux-2.6.29/include/linux/remote_sleep.h
--- linux-2.6.29/include/linux/remote_sleep.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/linux/remote_sleep.h	2014-05-27 23:04:10.342029669 -0700
@@ -0,0 +1,17 @@
+#ifndef __REMOTE_SLEEP_H__
+#define __REMOTE_SLEEP_H__
+
+#include <linux/types.h>
+
+struct rpc_desc;
+
+int remote_sleep_prepare(struct rpc_desc *desc);
+void remote_sleep_finish(void);
+
+int unpack_remote_sleep_res_prepare(struct rpc_desc *desc);
+int unpack_remote_sleep_res(struct rpc_desc *desc, void *res, size_t size);
+
+#define unpack_remote_sleep_res_type(desc, v) \
+	unpack_remote_sleep_res(desc, &v, sizeof(v))
+
+#endif /* __REMOTE_SLEEP_H__ */
diff -ruN linux-2.6.29/include/linux/rmap.h android_cluster/linux-2.6.29/include/linux/rmap.h
--- linux-2.6.29/include/linux/rmap.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/rmap.h	2014-05-27 23:04:10.346029586 -0700
@@ -85,6 +85,12 @@
  */
 int page_referenced(struct page *, int is_locked, struct mem_cgroup *cnt);
 int try_to_unmap(struct page *, int ignore_refs);
+#ifdef CONFIG_KRG_MM
+int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+		     int migration);
+struct anon_vma *page_lock_anon_vma(struct page *page);
+void page_unlock_anon_vma(struct anon_vma *anon_vma);
+#endif
 
 /*
  * Called from mm/filemap_xip.c to unmap empty zero page
@@ -142,5 +148,12 @@
 #define SWAP_AGAIN	1
 #define SWAP_FAIL	2
 #define SWAP_MLOCK	3
+#ifdef CONFIG_KRG_MM
+#define SWAP_FLUSH_FAIL	4
+#endif
+
+#ifdef CONFIG_KRG_MM
+extern struct kmem_cache *anon_vma_cachep;
+#endif
 
 #endif	/* _LINUX_RMAP_H */
diff -ruN linux-2.6.29/include/linux/sched.h android_cluster/linux-2.6.29/include/linux/sched.h
--- linux-2.6.29/include/linux/sched.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/sched.h	2014-05-27 23:04:10.346029586 -0700
@@ -88,6 +88,12 @@
 #include <linux/kobject.h>
 #include <linux/latencytop.h>
 #include <linux/cred.h>
+#ifdef CONFIG_KRG_CAP
+#include <kerrighed/capabilities.h>
+#endif
+#ifdef CONFIG_KRG_EPM
+#include <kddm/kddm_types.h>
+#endif
 
 #include <asm/processor.h>
 
@@ -161,6 +167,7 @@
 
 extern unsigned long long time_sync_thresh;
 
+
 /*
  * Task state bitmask. NOTE! These bits are also
  * encoded in fs/proc/array.c: get_task_state().
@@ -182,6 +189,10 @@
 /* in tsk->state again */
 #define TASK_DEAD		64
 #define TASK_WAKEKILL		128
+#ifdef CONFIG_KRG_EPM
+/* in tsk->exit_state */
+#define EXIT_MIGRATION		256
+#endif
 
 /* Convenience macros for the sake of set_task_state */
 #define TASK_KILLABLE		(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
@@ -428,6 +439,10 @@
 	struct k_sigaction	action[_NSIG];
 	spinlock_t		siglock;
 	wait_queue_head_t	signalfd_wqh;
+#ifdef CONFIG_KRG_EPM
+	objid_t			krg_objid;
+	struct sighand_struct_kddm_object *kddm_obj;
+#endif
 };
 
 struct pacct_struct {
@@ -605,6 +620,10 @@
 	unsigned audit_tty;
 	struct tty_audit_buf *tty_audit_buf;
 #endif
+#ifdef CONFIG_KRG_EPM
+	objid_t krg_objid;
+	struct signal_struct_kddm_object *kddm_obj;
+#endif
 };
 
 /* Context switch must be unlocked if interrupts are to be enabled */
@@ -1175,6 +1194,12 @@
 	/* ??? */
 	unsigned int personality;
 	unsigned did_exec:1;
+#ifdef CONFIG_KRG_HOTPLUG
+	unsigned create_krg_ns:1;
+#endif
+#ifdef CONFIG_KRG_EPM
+	unsigned remote_vfork_done:1;
+#endif
 	pid_t pid;
 	pid_t tgid;
 
@@ -1417,6 +1442,28 @@
 	/* state flags for use by tracers */
 	unsigned long trace;
 #endif
+#ifdef CONFIG_KRG_CAP
+	kernel_krg_cap_t krg_caps;
+	atomic_t krg_cap_used[CAP_SIZE];
+	atomic_t krg_cap_unavailable[CAP_SIZE];
+	atomic_t krg_cap_unavailable_private[CAP_SIZE];
+#endif
+#ifdef CONFIG_KRG_KDDM
+	struct kddm_info_struct *kddm_info;
+#endif
+#ifdef CONFIG_KRG_PROC
+	struct task_kddm_object *task_obj;
+#endif
+#ifdef CONFIG_KRG_EPM
+	int krg_action_flags;
+	struct task_struct *effective_current;
+	struct children_kddm_object *parent_children_obj;
+	struct children_kddm_object *children_obj;
+	struct app_struct *application;
+#endif
+#ifdef CONFIG_KRG_SCHED
+	struct krg_sched_info *krg_sched;
+#endif
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
@@ -1601,12 +1648,22 @@
 /*
  * Per process flags
  */
+#ifdef CONFIG_KRG_EPM
+/* PF_ALIGNWARN is unused */
+#define PF_DELAY_NOTIFY	0x00000001	/* must do_notify_parent() before can be */
+					/* reaped */
+#else
 #define PF_ALIGNWARN	0x00000001	/* Print alignment warning msgs */
 					/* Not implemented yet, only for 486*/
+#endif
 #define PF_STARTING	0x00000002	/* being created */
 #define PF_EXITING	0x00000004	/* getting shut down */
 #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */
 #define PF_VCPU		0x00000010	/* I'm a virtual CPU */
+#ifdef CONFIG_KRG_EPM
+#define PF_AWAY		0x00000020	/* I don't want to be considered as local */
+					/* by my relatives */
+#endif
 #define PF_FORKNOEXEC	0x00000040	/* forked but didn't exec */
 #define PF_SUPERPRIV	0x00000100	/* used super-user privileges */
 #define PF_DUMPCORE	0x00000200	/* dumped core */
@@ -1808,6 +1865,10 @@
 
 extern struct pid_namespace init_pid_ns;
 
+#ifdef CONFIG_KRG_EPM
+extern struct task_struct *baby_sitter;
+#endif
+
 /*
  * find a task by one of its numerical ids
  *
@@ -1893,6 +1954,11 @@
 extern void force_sig_specific(int, struct task_struct *);
 extern int send_sig(int, struct task_struct *, int);
 extern void zap_other_threads(struct task_struct *p);
+#ifdef CONFIG_KRG_EPM
+extern struct sigqueue *__sigqueue_alloc(struct task_struct *t, gfp_t flags,
+					 int override_rlimit);
+extern void __sigqueue_free(struct sigqueue *q);
+#endif
 extern struct sigqueue *sigqueue_alloc(void);
 extern void sigqueue_free(struct sigqueue *);
 extern int send_sigqueue(struct sigqueue *,  struct task_struct *, int group);
@@ -1960,6 +2026,11 @@
 extern void exit_itimers(struct signal_struct *);
 extern void flush_itimer_signals(void);
 
+#ifdef CONFIG_KRG_EPM
+int wait_task_zombie(struct task_struct *p, int options,
+		     struct siginfo __user *infop,
+		     int __user *stat_addr, struct rusage __user *ru);
+#endif
 extern NORET_TYPE void do_group_exit(int);
 
 extern void daemonize(const char *, ...);
@@ -1969,6 +2040,26 @@
 extern int do_execve(char *, char __user * __user *, char __user * __user *, struct pt_regs *);
 extern long do_fork(unsigned long, unsigned long, struct pt_regs *, unsigned long, int __user *, int __user *);
 struct task_struct *fork_idle(int);
+#ifdef CONFIG_KRG_EPM
+struct task_struct *copy_process(unsigned long clone_flags,
+				 unsigned long stack_start,
+				 struct pt_regs *regs,
+				 unsigned long stack_size,
+				 int __user *child_tidptr,
+				 struct pid *pid,
+				 int trace);
+/* remote clone */
+int krg_do_fork(unsigned long clone_flags,
+		unsigned long stack_start,
+		struct pt_regs *regs,
+		unsigned long stack_size,
+		int *parent_tidptr,
+		int *child_tidptr,
+		int trace);
+bool in_krg_do_fork(void);
+/* vfork with remote child */
+void krg_vfork_done(struct completion *vfork_done);
+#endif /* CONFIG_KRG_EPM */
 
 extern void set_task_comm(struct task_struct *tsk, char *from);
 extern char *get_task_comm(char *to, struct task_struct *tsk);
@@ -2034,6 +2125,11 @@
 #define delay_group_leader(p) \
 		(thread_group_leader(p) && !thread_group_empty(p))
 
+static inline int task_detached(struct task_struct *p)
+{
+	return p->exit_signal == -1;
+}
+
 /*
  * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring
  * subscriptions and synchronises with wait4().  Also used in procfs.  Also
@@ -2224,6 +2320,9 @@
  * callers must hold sighand->siglock.
  */
 extern void recalc_sigpending_and_wake(struct task_struct *t);
+#ifdef CONFIG_KRG_EPM
+extern int recalc_sigpending_tsk(struct task_struct *t);
+#endif
 extern void recalc_sigpending(void);
 
 extern void signal_wake_up(struct task_struct *t, int resume_stopped);
diff -ruN linux-2.6.29/include/linux/sem.h android_cluster/linux-2.6.29/include/linux/sem.h
--- linux-2.6.29/include/linux/sem.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/sem.h	2014-05-27 23:04:10.350029503 -0700
@@ -2,6 +2,11 @@
 #define _LINUX_SEM_H
 
 #include <linux/ipc.h>
+#ifdef CONFIG_KRG_IPC
+#include <linux/unique_id.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/types.h>
+#endif
 
 /* semop flags */
 #define SEM_UNDO        0x1000  /* undo the operation on exit */
@@ -97,6 +102,9 @@
 	struct list_head	sem_pending;	/* pending operations to be processed */
 	struct list_head	list_id;	/* undo requests on this array */
 	unsigned long		sem_nsems;	/* no. of semaphores in array */
+#ifdef CONFIG_KRG_IPC
+	struct list_head        remote_sem_pending;
+#endif
 };
 
 /* One queue for each sleeping process in the system. */
@@ -109,12 +117,20 @@
 	struct sembuf		*sops;	 /* array of pending operations */
 	int			nsops;	 /* number of operations */
 	int			alter;   /* does the operation alter the array? */
+#ifdef CONFIG_KRG_IPC
+	int                     semid;
+	kerrighed_node_t        node;
+#endif
 };
 
 /* Each task has a list of undo requests. They are executed automatically
  * when the process exits.
  */
 struct sem_undo {
+#ifdef CONFIG_KRG_IPC
+	unique_id_t             proc_list_id;
+	/* list_proc is useless in KRG code */
+#endif
 	struct list_head	list_proc;	/* per-process list: all undos from one process. */
 						/* rcu protected */
 	struct rcu_head		rcu;		/* rcu struct for sem_undo() */
@@ -134,6 +150,10 @@
 };
 
 struct sysv_sem {
+#ifdef CONFIG_KRG_IPC
+	unique_id_t undo_list_id;
+	/* pointer to undo_list is useless in KRG code */
+#endif
 	struct sem_undo_list *undo_list;
 };
 
diff -ruN linux-2.6.29/include/linux/signal.h android_cluster/linux-2.6.29/include/linux/signal.h
--- linux-2.6.29/include/linux/signal.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/signal.h	2014-05-27 23:04:10.350029503 -0700
@@ -235,6 +235,12 @@
 extern int next_signal(struct sigpending *pending, sigset_t *mask);
 extern int group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p);
 extern int __group_send_sig_info(int, struct siginfo *, struct task_struct *);
+#ifdef CONFIG_KRG_PROC
+extern
+int krg_group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p,
+			    pid_t session);
+extern int __krg_group_send_sig_info(int, struct siginfo *, struct task_struct *);
+#endif
 extern long do_sigpending(void __user *, unsigned long);
 extern int sigprocmask(int, sigset_t *, sigset_t *);
 extern int show_unhandled_signals;
@@ -243,6 +249,9 @@
 extern int get_signal_to_deliver(siginfo_t *info, struct k_sigaction *return_ka, struct pt_regs *regs, void *cookie);
 extern void exit_signals(struct task_struct *tsk);
 
+#ifdef CONFIG_KRG_EPM
+extern struct kmem_cache *signal_cachep;
+#endif
 extern struct kmem_cache *sighand_cachep;
 
 int unhandled_signal(struct task_struct *tsk, int sig);
diff -ruN linux-2.6.29/include/linux/splice.h android_cluster/linux-2.6.29/include/linux/splice.h
--- linux-2.6.29/include/linux/splice.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/splice.h	2014-05-27 23:04:10.354029420 -0700
@@ -71,4 +71,17 @@
 extern ssize_t splice_direct_to_actor(struct file *, struct splice_desc *,
 				      splice_direct_actor *);
 
+#ifdef CONFIG_KRG_EPM
+extern long do_splice_from(struct pipe_inode_info *pipe, struct file *out,
+			   loff_t *ppos, size_t len, unsigned int flags);
+
+extern long do_splice_to(struct file *in, loff_t *ppos,
+			 struct pipe_inode_info *pipe, size_t len,
+			 unsigned int flags);
+
+extern int link_pipe(struct pipe_inode_info *ipipe,
+		     struct pipe_inode_info *opipe,
+		     size_t len, unsigned int flags);
+#endif
+
 #endif
diff -ruN linux-2.6.29/include/linux/swap.h android_cluster/linux-2.6.29/include/linux/swap.h
--- linux-2.6.29/include/linux/swap.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/swap.h	2014-05-27 23:04:10.358029337 -0700
@@ -8,6 +8,9 @@
 #include <linux/memcontrol.h>
 #include <linux/sched.h>
 #include <linux/node.h>
+#ifdef CONFIG_KRG_MM
+#include <linux/page-flags.h>
+#endif
 
 #include <asm/atomic.h>
 #include <asm/page.h>
@@ -173,6 +176,9 @@
 /* Definition of global_page_state not available yet */
 #define nr_free_pages() global_page_state(NR_FREE_PAGES)
 
+#ifdef CONFIG_KRG_MM
+int page_swapcount(struct page *page);
+#endif
 
 /* linux/mm/swap.c */
 extern void __lru_cache_add(struct page *, enum lru_list lru);
@@ -192,11 +198,21 @@
  */
 static inline void lru_cache_add_anon(struct page *page)
 {
+#ifdef CONFIG_KRG_MM
+	if (PageMigratable(page))
+		__lru_cache_add(page, LRU_INACTIVE_MIGR);
+	else
+#endif
 	__lru_cache_add(page, LRU_INACTIVE_ANON);
 }
 
 static inline void lru_cache_add_active_anon(struct page *page)
 {
+#ifdef CONFIG_KRG_MM
+	if (PageMigratable(page))
+		__lru_cache_add(page, LRU_ACTIVE_MIGR);
+	else
+#endif
 	__lru_cache_add(page, LRU_ACTIVE_ANON);
 }
 
diff -ruN linux-2.6.29/include/linux/threads.h android_cluster/linux-2.6.29/include/linux/threads.h
--- linux-2.6.29/include/linux/threads.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/threads.h	2014-05-27 23:04:10.362029254 -0700
@@ -21,6 +21,7 @@
 
 #define MIN_THREADS_LEFT_FOR_ROOT 4
 
+#ifndef CONFIG_KRG_PROC
 /*
  * This controls the default maximum pid allocated to a process
  */
@@ -33,4 +34,24 @@
 #define PID_MAX_LIMIT (CONFIG_BASE_SMALL ? PAGE_SIZE * 8 : \
 	(sizeof(long) > 4 ? 4 * 1024 * 1024 : PID_MAX_DEFAULT))
 
+#else /* CONFIG_KRG_PROC */
+
+#include <kerrighed/sys/types.h>
+
+/* We need the number of bits for Kerrighed PIDs definitions. */
+#define NR_BITS_PID_MAX_DEFAULT (CONFIG_BASE_SMALL ? 12 : 15)
+#define PID_MAX_DEFAULT (1 << NR_BITS_PID_MAX_DEFAULT)
+
+/*
+ * Maximise number of PID bits:
+ * - 29 bits are the limitation in futex.h,
+ * - node bits are defined in include/kerrighed/sys/types.h
+ */
+#define NR_BITS_PID_MAX_LIMIT (CONFIG_BASE_SMALL ? PAGE_SHIFT + 3 : \
+	(sizeof(long) > 4 ? (29 - NR_BITS_IN_MAX_NODE_ID) : \
+	NR_BITS_PID_MAX_DEFAULT))
+#define PID_MAX_LIMIT (1 << NR_BITS_PID_MAX_LIMIT)
+
+#endif /* CONFIG_KRG_PROC */
+
 #endif
diff -ruN linux-2.6.29/include/linux/tipc_config.h android_cluster/linux-2.6.29/include/linux/tipc_config.h
--- linux-2.6.29/include/linux/tipc_config.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/tipc_config.h	2014-05-27 23:04:10.362029254 -0700
@@ -38,7 +38,11 @@
 #define _LINUX_TIPC_CONFIG_H_
 
 #include <linux/types.h>
+#ifdef __KERNEL__
 #include <linux/string.h>
+#else
+#include <string.h>
+#endif
 #include <asm/byteorder.h>
 
 /*
@@ -73,12 +77,13 @@
 #define  TIPC_CMD_GET_LINKS         0x0004    /* tx net_addr, rx link_info(s) */
 #define  TIPC_CMD_SHOW_NAME_TABLE   0x0005    /* tx name_tbl_query, rx ultra_string */
 #define  TIPC_CMD_SHOW_PORTS        0x0006    /* tx none, rx ultra_string */
+#define  TIPC_CMD_GET_ROUTES        0x000A    /* tx net_addr, rx route_info(s) */
 #define  TIPC_CMD_SHOW_LINK_STATS   0x000B    /* tx link_name, rx ultra_string */
+#define  TIPC_CMD_SHOW_STATS        0x000F    /* tx unsigned, rx ultra_string */
 
 #if 0
 #define  TIPC_CMD_SHOW_PORT_STATS   0x0008    /* tx port_ref, rx ultra_string */
 #define  TIPC_CMD_RESET_PORT_STATS  0x0009    /* tx port_ref, rx none */
-#define  TIPC_CMD_GET_ROUTES        0x000A    /* tx ?, rx ? */
 #define  TIPC_CMD_GET_LINK_PEER     0x000D    /* tx link_name, rx ? */
 #endif
 
@@ -96,8 +101,11 @@
 #define  TIPC_CMD_GET_MAX_ZONES     0x4007    /* tx none, rx unsigned */
 #define  TIPC_CMD_GET_MAX_CLUSTERS  0x4008    /* tx none, rx unsigned */
 #define  TIPC_CMD_GET_MAX_NODES     0x4009    /* tx none, rx unsigned */
+#if 0
 #define  TIPC_CMD_GET_MAX_SLAVES    0x400A    /* tx none, rx unsigned */
+#endif
 #define  TIPC_CMD_GET_NETID         0x400B    /* tx none, rx unsigned */
+#define  TIPC_CMD_GET_MAX_REMOTES   0x400C    /* tx none, rx unsigned */
 
 #define  TIPC_CMD_ENABLE_BEARER     0x4101    /* tx bearer_config, rx none */
 #define  TIPC_CMD_DISABLE_BEARER    0x4102    /* tx bearer_name, rx none */
@@ -107,10 +115,9 @@
 #define  TIPC_CMD_SET_LOG_SIZE      0x410A    /* tx unsigned, rx none */
 #define  TIPC_CMD_DUMP_LOG          0x410B    /* tx none, rx ultra_string */
 #define  TIPC_CMD_RESET_LINK_STATS  0x410C    /* tx link_name, rx none */
-
-#if 0
 #define  TIPC_CMD_CREATE_LINK       0x4103    /* tx link_create, rx none */
-#define  TIPC_CMD_REMOVE_LINK       0x4104    /* tx link_name, rx none */
+#define  TIPC_CMD_DELETE_LINK       0x4104    /* tx link_name, rx none */
+#if 0
 #define  TIPC_CMD_BLOCK_LINK        0x4105    /* tx link_name, rx none */
 #define  TIPC_CMD_UNBLOCK_LINK      0x4106    /* tx link_name, rx none */
 #endif
@@ -132,8 +139,11 @@
 #define  TIPC_CMD_SET_MAX_ZONES     0x8007    /* tx unsigned, rx none */
 #define  TIPC_CMD_SET_MAX_CLUSTERS  0x8008    /* tx unsigned, rx none */
 #define  TIPC_CMD_SET_MAX_NODES     0x8009    /* tx unsigned, rx none */
+#if 0
 #define  TIPC_CMD_SET_MAX_SLAVES    0x800A    /* tx unsigned, rx none */
+#endif
 #define  TIPC_CMD_SET_NETID         0x800B    /* tx unsigned, rx none */
+#define  TIPC_CMD_SET_MAX_REMOTES   0x800C    /* tx unsigned, rx none */
 
 /*
  * Reserved commands:
@@ -165,6 +175,8 @@
 #define TIPC_TLV_LINK_CONFIG    24	/* struct tipc_link_config */
 #define TIPC_TLV_NAME_TBL_QUERY	25	/* struct tipc_name_table_query */
 #define TIPC_TLV_PORT_REF   	26	/* 32-bit port reference */
+#define TIPC_TLV_CREATE_LINK   	27	/* char[TIPC_MAX_BEARER_NAME + TIPC_MAX_MEDIA_ADDR] */
+#define TIPC_TLV_ROUTE_INFO	28	/* struct tipc_route_info */
 
 /*
  * Maximum sizes of TIPC bearer-related names (including terminating NUL)
@@ -174,6 +186,8 @@
 #define TIPC_MAX_IF_NAME	16	/* format = interface */
 #define TIPC_MAX_BEARER_NAME	32	/* format = media:interface */
 #define TIPC_MAX_LINK_NAME	60	/* format = Z.C.N:interface-Z.C.N:interface */
+#define TIPC_MAX_MEDIA_ADDR	64	/* format = <media-dependent> */
+#define TIPC_MAX_ADDR	        16	/* format = Z.C.N */
 
 /*
  * Link priority limits (min, default, max, media default)
@@ -197,7 +211,7 @@
  */
 
 #define TIPC_MIN_LINK_WIN 16
-#define TIPC_DEF_LINK_WIN 50
+#define TIPC_DEF_LINK_WIN 100
 #define TIPC_MAX_LINK_WIN 150
 
 
@@ -206,6 +220,12 @@
 	__be32 up;			/* 0=down, 1= up */
 };
 
+struct tipc_route_info {
+	__be32 remote_addr;		/* <Z.C.N> of remote cluster/zone */
+	__be32 local_router;    	/* <Z.C.N> of local router */
+	__be32 remote_router;    	/* <Z.C.N> of remote router */
+};
+
 struct tipc_link_info {
 	__be32 dest;			/* network address of peer node */
 	__be32 up;			/* 0=down, 1=up */
@@ -214,7 +234,7 @@
 
 struct tipc_bearer_config {
 	__be32 priority;		/* Range [1,31]. Override per link  */
-	__be32 detect_scope;
+	__be32 disc_domain;		/* <Z.C.N> containing desired nodes */     
 	char name[TIPC_MAX_BEARER_NAME];
 };
 
@@ -249,15 +269,10 @@
 #if 0
 /* prototypes TLV structures for proposed commands */
 struct tipc_link_create {
-	__u32   domain;
+	__be32   domain;
 	struct tipc_media_addr peer_addr;
 	char bearer_name[TIPC_MAX_BEARER_NAME];
 };
-
-struct tipc_route_info {
-	__u32 dest;
-	__u32 router;
-};
 #endif
 
 /*
@@ -271,7 +286,7 @@
 
 struct tlv_desc {
 	__be16 tlv_len;		/* TLV length (descriptor + value) */
-	__be16 tlv_type;		/* TLV identifier */
+	__be16 tlv_type;	/* TLV identifier */
 };
 
 #define TLV_ALIGNTO 4
diff -ruN linux-2.6.29/include/linux/tipc.h android_cluster/linux-2.6.29/include/linux/tipc.h
--- linux-2.6.29/include/linux/tipc.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/tipc.h	2014-05-27 23:04:10.362029254 -0700
@@ -2,7 +2,7 @@
  * include/linux/tipc.h: Include file for TIPC socket interface
  * 
  * Copyright (c) 2003-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
+ * Copyright (c) 2005-2008, Wind River Systems
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -96,6 +96,7 @@
 #define TIPC_ZONE_SCOPE		1
 #define TIPC_CLUSTER_SCOPE	2
 #define TIPC_NODE_SCOPE		3
+#define TIPC_NETWORK_SCOPE	4	/* NOT AVAILABLE YET */
 
 /*
  * Limiting values for messages
@@ -107,7 +108,7 @@
  * Message importance levels
  */
 
-#define TIPC_LOW_IMPORTANCE		0  /* default */
+#define TIPC_LOW_IMPORTANCE		0
 #define TIPC_MEDIUM_IMPORTANCE		1
 #define TIPC_HIGH_IMPORTANCE		2
 #define TIPC_CRITICAL_IMPORTANCE	3
@@ -188,7 +189,7 @@
 		struct tipc_name_seq nameseq;
 		struct {
 			struct tipc_name name;
-			__u32 domain; /* 0: own zone */
+			__u32 domain;
 		} name;
 	} addr;
 };
@@ -206,7 +207,7 @@
  */
 
 #define TIPC_IMPORTANCE		127	/* Default: TIPC_LOW_IMPORTANCE */
-#define TIPC_SRC_DROPPABLE	128	/* Default: 0 (resend congested msg) */
+#define TIPC_SRC_DROPPABLE	128	/* Default: based on socket type */
 #define TIPC_DEST_DROPPABLE	129	/* Default: based on socket type */
 #define TIPC_CONN_TIMEOUT	130	/* Default: 8000 (ms)  */
 
diff -ruN linux-2.6.29/include/linux/tracehook.h android_cluster/linux-2.6.29/include/linux/tracehook.h
--- linux-2.6.29/include/linux/tracehook.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/tracehook.h	2014-05-27 23:04:10.362029254 -0700
@@ -507,6 +507,11 @@
 static inline int tracehook_notify_death(struct task_struct *task,
 					 void **death_cookie, int group_dead)
 {
+#ifdef CONFIG_KRG_EPM
+	/* Remote ptracers are not supported yet. */
+	BUG_ON(task->ptrace && (task->parent == baby_sitter
+				|| task->real_parent == baby_sitter));
+#endif
 	if (task->exit_signal == -1)
 		return task->ptrace ? SIGCHLD : DEATH_REAP;
 
diff -ruN linux-2.6.29/include/linux/unique_id.h android_cluster/linux-2.6.29/include/linux/unique_id.h
--- linux-2.6.29/include/linux/unique_id.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/linux/unique_id.h	2014-05-27 23:04:10.366029171 -0700
@@ -0,0 +1,111 @@
+/** Unique id generator interface
+ *  @file unique_id.h
+ *
+ *  Definition of unique id generator interface. This mechanism generates
+ *  locally, an indentifier which is unique in the cluster.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __UNIQUE_ID_H__
+#define __UNIQUE_ID_H__
+
+
+#include <linux/spinlock.h>
+#include <kerrighed/types.h>
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                 MACROS                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+#define UNIQUE_ID_NONE 0UL          /* Invalid unique id */
+
+#if BITS_PER_LONG == 64
+#define UNIQUE_ID_LOCAL_BITS 56
+#else
+#define UNIQUE_ID_LOCAL_BITS 24
+#endif
+
+#define UNIQUE_ID_NODE_BITS 8     /* Number of bits used for nodeid part of
+				       the unique id. */
+
+#define UNIQUE_ID_NODE_SHIFT (UNIQUE_ID_LOCAL_BITS)
+#define UNIQUE_ID_LOCAL_MASK ((1UL << UNIQUE_ID_LOCAL_BITS) - 1)
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Unique id root type
+ */
+typedef struct unique_id_root {
+	atomic_long_t local_unique_id;   /**< Local unique id */
+} unique_id_root_t;
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN VARIABLES                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+extern unique_id_root_t mm_unique_id_root;
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Initialize a unique id root.
+ *  @author Renaud Lottiaux
+ *
+ *  @param root   The root to initialize
+ *  @return       0 if everything ok.
+ *                Negative value otherwise.
+ */
+int init_unique_id_root(unique_id_root_t *root);
+
+
+
+/** Initialize a unique id root with a given init value.
+ *  @author Renaud Lottiaux
+ *
+ *  @param root   The root to initialize
+ *  @param base   Base value for the key generator.
+ *  @return       0 if everything ok.
+ *                Negative value otherwise.
+ */
+int init_and_set_unique_id_root(unique_id_root_t *root, int base);
+
+
+
+/** Generate a unique id from a given root.
+ *  @author Renaud Lottiaux
+ *
+ *  @param root   The root of the unique id to generate.
+ *  @return       A unique id !
+ */
+unique_id_t get_unique_id(unique_id_root_t *unique_id_root);
+
+
+void init_unique_ids(void);
+
+#endif // __UNIQUE_ID_H__
diff -ruN linux-2.6.29/include/linux/vmstat.h android_cluster/linux-2.6.29/include/linux/vmstat.h
--- linux-2.6.29/include/linux/vmstat.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/linux/vmstat.h	2014-05-27 23:04:10.370029087 -0700
@@ -171,6 +171,10 @@
 {
 	return (zone_page_state(zone, NR_ACTIVE_ANON)
 		+ zone_page_state(zone, NR_ACTIVE_FILE)
+#ifdef CONFIG_KRG_MM
+		+ zone_page_state(zone, NR_ACTIVE_MIGR)
+		+ zone_page_state(zone, NR_INACTIVE_MIGR)
+#endif
 		+ zone_page_state(zone, NR_INACTIVE_ANON)
 		+ zone_page_state(zone, NR_INACTIVE_FILE));
 }
diff -ruN linux-2.6.29/include/net/krgrpc/rpc.h android_cluster/linux-2.6.29/include/net/krgrpc/rpc.h
--- linux-2.6.29/include/net/krgrpc/rpc.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/net/krgrpc/rpc.h	2014-05-27 23:04:10.386028755 -0700
@@ -0,0 +1,315 @@
+#ifndef __KRG_RPC__
+#define __KRG_RPC__
+
+#include <net/krgrpc/rpcid.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/sys/types.h>
+
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/errno.h>
+
+enum rpc_target {
+	RPC_TARGET_NODE,
+	RPC_TARGET_PIDTYPE,
+};
+
+enum rpc_handler {
+	RPC_HANDLER_KTHREAD,
+	RPC_HANDLER_KTHREAD_VOID,
+	RPC_HANDLER_KTHREAD_INT,
+	RPC_HANDLER_MAX
+};
+
+enum rpc_error {
+	RPC_EOK = 0,
+	RPC_EINTR,
+	RPC_ESIGACK,
+	RPC_EPIPE,
+	RPC_ECLOSE,
+	RPC_EVAL,
+};
+
+enum {
+	__RPC_FLAGS_NOBLOCK, /* request async operation */
+	__RPC_FLAGS_EARLIER, /* do the action as soon as possible */
+	__RPC_FLAGS_LATER,   /* do the action during the rpc_end_xxx */
+	__RPC_FLAGS_SECURE,  /* force a copy of the sent buffer */
+	__RPC_FLAGS_NOCOPY,  /* request the network buffer */
+	__RPC_FLAGS_INTR,    /* sleep in INTERRUPTIBLE state */
+	__RPC_FLAGS_REPOST,  /* post a send/recv without update seqid */
+	__RPC_FLAGS_SIGACK,  /* unpack() should return SIGACKs */
+	__RPC_FLAGS_MAX      /* Must be last */
+};
+
+#define RPC_FLAGS_NOBLOCK (1<<__RPC_FLAGS_NOBLOCK)
+#define RPC_FLAGS_EARLIER (1<<__RPC_FLAGS_EARLIER)
+#define RPC_FLAGS_LATER   (1<<__RPC_FLAGS_LATER)
+#define RPC_FLAGS_SECURE  (1<<__RPC_FLAGS_SECURE)
+#define RPC_FLAGS_NOCOPY  (1<<__RPC_FLAGS_NOCOPY)
+#define RPC_FLAGS_INTR    (1<<__RPC_FLAGS_INTR)
+#define RPC_FLAGS_REPOST  (1<<__RPC_FLAGS_REPOST)
+#define RPC_FLAGS_SIGACK  (1<<__RPC_FLAGS_SIGACK)
+
+enum rpc_rq_type {
+	RPC_RQ_UNDEF,
+	RPC_RQ_CLT,
+	RPC_RQ_SRV,
+	RPC_RQ_FWD,
+};
+
+enum rpc_rq_state {
+	__RPC_STATE_NEW,
+	__RPC_STATE_HANDLE,
+	__RPC_STATE_RUN,
+	__RPC_STATE_CANCEL,
+	__RPC_STATE_END,
+	__RPC_STATE_WAIT,
+	__RPC_STATE_WAIT1,
+};
+
+#define RPC_STATE_NEW    (1<<__RPC_STATE_NEW)
+#define RPC_STATE_HANDLE (1<<__RPC_STATE_HANDLE)
+#define RPC_STATE_RUN    (1<<__RPC_STATE_RUN)
+#define RPC_STATE_CANCEL (1<<__RPC_STATE_CANCEL)
+#define RPC_STATE_END    (1<<__RPC_STATE_END)
+#define RPC_STATE_WAIT   (1<<__RPC_STATE_WAIT)
+#define RPC_STATE_WAIT1  (1<<__RPC_STATE_WAIT1)
+
+#define RPC_STATE_MASK_VALID (RPC_STATE_RUN\
+ | RPC_STATE_HANDLE \
+ | RPC_STATE_NEW \
+ | RPC_STATE_WAIT \
+ | RPC_STATE_WAIT1)
+
+struct rpc_service;
+
+struct rpc_desc {
+	struct rpc_desc_send* desc_send;
+	struct rpc_desc_recv* desc_recv[KERRIGHED_MAX_NODES];
+	struct rpc_service* service;
+	krgnodemask_t nodes;
+	enum rpc_rq_type type;
+	struct list_head list;
+	int in_interrupt;
+	unsigned long desc_id;
+	spinlock_t desc_lock;
+	enum rpcid rpcid;
+	kerrighed_node_t client;
+	enum rpc_rq_state state;
+	struct task_struct *thread;
+	kerrighed_node_t wait_from;
+	atomic_t usage;
+	struct __rpc_synchro *__synchro;
+};
+
+struct rpc_data {
+	void *raw;
+	void *data;
+	size_t size;
+};
+
+typedef void (*rpc_handler_t) (struct rpc_desc* rpc_desc);
+
+typedef void (*rpc_handler_void_t)(struct rpc_desc* rpc_desc,
+				   void* data, size_t size);
+
+typedef int (*rpc_handler_int_t) (struct rpc_desc* rpc_desc,
+				  void* data, size_t size);
+
+/*
+ * RPC synchro
+ */
+
+struct rpc_synchro* rpc_synchro_new(int max,
+				    char *label,
+				    int order);
+
+/*
+ * RPC management
+ */
+int __rpc_register(enum rpcid rpcid,
+		   enum rpc_target rpc_target,
+		   enum rpc_handler rpc_handler,
+		   struct rpc_synchro *rpc_synchro,
+		   void* _h,
+		   unsigned long flags);
+
+struct rpc_desc* rpc_begin_m(enum rpcid rpcid,
+			     krgnodemask_t* nodes);
+
+int rpc_cancel(struct rpc_desc* desc);
+
+int rpc_pack(struct rpc_desc* desc, int flags, const void* data, size_t size);
+int rpc_wait_pack(struct rpc_desc* desc, int seq_id);
+int rpc_cancel_pack(struct rpc_desc* desc);
+
+int rpc_forward(struct rpc_desc* desc, kerrighed_node_t node);
+
+enum rpc_error rpc_unpack(struct rpc_desc* desc, int flags, void* data, size_t size);
+enum rpc_error rpc_unpack_from(struct rpc_desc* desc, kerrighed_node_t node,
+			       int flags, void* data, size_t size);
+void rpc_cancel_unpack(struct rpc_desc* desc);
+
+kerrighed_node_t rpc_wait_return(struct rpc_desc* desc, int* value);
+int rpc_wait_return_from(struct rpc_desc* desc, kerrighed_node_t node);
+int rpc_wait_all(struct rpc_desc *desc);
+
+int rpc_signal(struct rpc_desc* desc, int sigid);
+
+int rpc_end(struct rpc_desc *rpc_desc, int flags);
+
+void rpc_free_buffer(struct rpc_data *buf);
+
+s64 rpc_consumed_bytes(void);
+
+void rpc_enable_lowmem_mode(kerrighed_node_t nodeid);
+void rpc_disable_lowmem_mode(kerrighed_node_t nodeid);
+void rpc_enable_local_lowmem_mode(void);
+void rpc_disable_local_lowmem_mode(void);
+
+/*
+ * Convenient define
+ */
+
+#define rpc_pack_type(desc, v) rpc_pack(desc, 0, &v, sizeof(v))
+#define rpc_unpack_type(desc, v) rpc_unpack(desc, 0, &v, sizeof(v))
+#define rpc_unpack_type_from(desc, n, v) rpc_unpack_from(desc, n, 0, &v, sizeof(v))
+
+/*
+ * Convenient functions
+ */
+
+static inline
+int rpc_register_void(enum rpcid rpcid,
+		      rpc_handler_void_t h,
+		      unsigned long flags){
+	return __rpc_register(rpcid, RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+			      NULL, (rpc_handler_t)h, flags);
+};
+
+static inline
+int rpc_register_int(enum rpcid rpcid,
+		     rpc_handler_int_t h,
+		     unsigned long flags){
+	return __rpc_register(rpcid, RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_INT,
+			      NULL, (rpc_handler_t)h, flags);
+};
+
+static inline
+int rpc_register(enum rpcid rpcid,
+		 rpc_handler_t h,
+		 unsigned long flags){
+	return __rpc_register(rpcid, RPC_TARGET_NODE, RPC_HANDLER_KTHREAD,
+			      NULL, h, flags);
+};
+
+static inline
+struct rpc_desc* rpc_begin(enum rpcid rpcid,
+			   kerrighed_node_t node){
+	krgnodemask_t nodes;
+
+	krgnodes_clear(nodes);
+	krgnode_set(node, nodes);
+
+	return rpc_begin_m(rpcid, &nodes);
+};
+
+static inline
+int rpc_async_m(enum rpcid rpcid,
+		krgnodemask_t* nodes,
+		const void* data, size_t size){
+	struct rpc_desc* desc;
+	int err = -ENOMEM;
+
+	desc = rpc_begin_m(rpcid, nodes);
+	if (!desc)
+		goto out;
+
+	err = rpc_pack(desc, 0, data, size);
+
+	/* rpc_end() always succeeds without delayed rpc_pack() */
+	rpc_end(desc, 0);
+
+out:
+	return err;
+};
+
+static inline
+int rpc_async(enum rpcid rpcid,
+	      kerrighed_node_t node,
+	      const void* data, size_t size){
+	krgnodemask_t nodes;
+
+	krgnodes_clear(nodes);
+	krgnode_set(node, nodes);
+	
+	return rpc_async_m(rpcid, &nodes, data, size);
+};
+
+static inline
+int rpc_sync_m(enum rpcid rpcid,
+	       krgnodemask_t* nodes,
+	       const void* data, size_t size){
+	struct rpc_desc *desc;
+	int rold, r, first, error;
+	int i;
+
+	r = -ENOMEM;
+	desc = rpc_begin_m(rpcid, nodes);
+	if (!desc)
+		goto out;
+
+	r = rpc_pack(desc, 0, data, size);
+	if (r)
+		goto end;
+
+	i = 0;
+	first = 1;
+	error = 0;
+	r = 0;
+
+	__for_each_krgnode_mask(i, nodes){
+		rpc_unpack_type_from(desc, i, rold);
+		if(first){
+			r = rold;
+			first = 0;
+		}else
+			error = error || (r != rold);
+		i++;
+	};
+
+end:
+	/* rpc_end() always succeeds without delayed rpc_pack() */
+	rpc_end(desc, 0);
+
+out:
+	return r;
+};
+
+static inline
+int rpc_sync(enum rpcid rpcid,
+	     kerrighed_node_t node,
+	     const void* data, size_t size){
+	krgnodemask_t nodes;
+
+	krgnodes_clear(nodes);
+	krgnode_set(node, nodes);
+	
+	return rpc_sync_m(rpcid, &nodes, data, size);
+};
+
+void rpc_enable(enum rpcid rpcid);
+void rpc_enable_all(void);
+void rpc_disable(enum rpcid rpcid);
+
+void rpc_enable_alldev(void);
+int rpc_enable_dev(const char *name);
+void rpc_disable_alldev(void);
+int rpc_disable_dev(const char *name);
+
+kerrighed_node_t rpc_desc_get_client(struct rpc_desc *desc);
+
+extern struct task_struct *first_krgrpc;
+
+#endif
diff -ruN linux-2.6.29/include/net/krgrpc/rpcid.h android_cluster/linux-2.6.29/include/net/krgrpc/rpcid.h
--- linux-2.6.29/include/net/krgrpc/rpcid.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/net/krgrpc/rpcid.h	2014-05-27 23:04:10.386028755 -0700
@@ -0,0 +1,232 @@
+#ifndef __RPC_ID__
+#define __RPC_ID__
+
+enum rpcid {
+	RPC_UNDEF,
+	RPC_ACK,
+	RPC_FORWARD_DATA,
+	RPC_ADVERT_FORWARD,
+	RPC_TEST,
+	RPC_PINGPONG,
+
+	RPC_FAF_INIT_SERVER,
+	RPC_FAF_READ,
+	RPC_FAF_WRITE,
+	RPC_FAF_READV,
+	RPC_FAF_WRITEV,
+	RPC_FAF_LSEEK,
+	RPC_FAF_LLSEEK,
+	RPC_FAF_IOCTL,
+	RPC_FAF_FCNTL,
+	RPC_FAF_FCNTL64,
+	RPC_FAF_FSTAT,
+	RPC_FAF_FSTATFS,
+	RPC_FAF_FSYNC,
+	RPC_FAF_FLOCK,
+	RPC_FAF_POLL_WAIT,
+	RPC_FAF_POLL_DEQUEUE,
+	RPC_FAF_POLL_NOTIFY,
+	RPC_FAF_D_PATH,
+	RPC_FAF_BIND,
+	RPC_FAF_CONNECT,
+	RPC_FAF_LISTEN,
+	RPC_FAF_ACCEPT,
+	RPC_FAF_GETSOCKNAME,
+	RPC_FAF_GETPEERNAME,
+	RPC_FAF_SHUTDOWN,
+	RPC_FAF_SETSOCKOPT,
+	RPC_FAF_GETSOCKOPT,
+	RPC_FAF_SENDMSG,
+	RPC_FAF_RECVMSG,
+	RPC_FAF_NOTIFY_CLOSE,
+
+/* Ctnr Object Server Types */
+	REQ_OBJECT_COPY,
+	REQ_OBJECT_INVALID,
+	REQ_OBJECT_REMOVE,
+	REQ_OBJECT_REMOVE_TO_MGR,
+
+	SEND_BACK_FIRST_TOUCH,
+	OBJECT_SEND,
+	NO_OBJECT_SEND,
+	SEND_WRITE_ACCESS,
+	INVALIDATION_ACK,
+	REMOVE_ACK,
+	REMOVE_ACK2,
+	REMOVE_DONE,
+	SEND_OWNERSHIP,
+	CHANGE_OWNERSHIP_ACK,
+
+/* Ctnr Container Manager Types */
+	REQ_KDDM_SET_LOOKUP,
+	REQ_KDDM_SET_DESTROY,
+	SEND_KDDM_SET_CREATE_DATA,
+	REQ_INC_KDDM_SET_USE,
+	REQ_DEC_KDDM_SET_USE,
+
+	GENERIC_ACK_REQUEST,
+	REQ_KILL_THREAD,
+
+	REQ_TEST_BARRIER,
+	REQ_TEST_LOCK,
+	REQ_TEST_SEM,
+	REQ_TEST_COND,
+	REQ_TEST_RES,
+	TEST_DATA,
+	INIT_DATA,
+
+/* Aragorn Message Types */
+	/* gthread server Types, reflected in the command_name table */
+	GTHREAD_MSG_TYPE,
+	KRG_THREAD_DETACH_REQUEST,
+	KRG_THREAD_STOP_NOTIFICATION,
+	KRG_THREAD_ADD_THREAD_INFO,
+	KRG_THREAD_REMOVE_THREAD_INFO,
+	KRG_THREAD_UPDATE_STATE_INFO,
+	KRG_THREAD_WAS_DESTROYED_NOTIFICATION,
+	KRG_THREAD_DO_EXIT_REQUEST,
+	KRG_THREAD_HAS_EXITED_NOTIFICATION,
+	KRG_THREAD_FINISH_JOIN_REQUEST,
+	KRG_THREAD_BLOCKED_ON_JOIN_NOTIFICATION,
+	KRG_THREAD_MIGRATION_STARTED_NOTIFICATION,
+	KRG_THREAD_MIGRATION_FINISHED_NOTIFICATION,
+	STOP_ALL_THREADS_REQUEST,
+	END_REQUEST,
+	ARAG_TIMER_TYPE,
+	SIGNAL_MGR_SEND_NODEID,
+	SIGNAL_MGR_SEND_PID,
+	SIGNAL_MGR_SEND_TID,
+	SIGNAL_MGR_SEND_REQTYPE,
+	SIGNAL_MGR_SEND_SIG,
+	SIGNAL_MGR_SEND_SIGINFO,
+	PGM_MSG_TYPE,
+	SCHEDULER_MSG_TYPE,
+
+/* Aragorn Messages for Synch */
+
+	APP_STOP,
+	APP_CONTINUE,
+	APP_KILL,
+
+	APP_REMOTE_CHKPT,
+	APP_DO_CHKPT,
+
+	APP_INIT_RESTART,
+	APP_DO_RESTART,
+
+	APP_EXCL_MM_REGION,
+
+	RPC_EPM_REMOTE_CLONE,
+	RPC_EPM_MIGRATE,
+	EPM_CHECKPOINT_REQ,
+
+	ARAGORN_SYS_KILL,
+	PROC_KILL_PROC_INFO,
+	PROC_SCHED_SETSCHEDULER,
+	PROC_SCHED_GETPARAM,
+	PROC_SCHED_GETSCHEDULER,
+	PROC_KILL_PG_INFO,
+	ARAGORN_SESSION_GROUP,
+	ARAGORN_WILL_NOT_BECOME_ORPHANED_PGRP,
+	ARAGORN_SYS_SETPGID,
+	PROC_FORWARD_SETPGID,
+	PROC_GETPGID,
+	PROC_GETSID,
+	ARAGORN_UPDATE_PGRP,
+	PROC_WAKE_UP_PROCESS,
+	PROC_GET_PID_CAP,
+	PROC_SET_PID_CAP,
+	PROC_REQUEST_MIGRATION,
+	PROC_DO_NOTIFY_PARENT,
+	PROC_NOTIFY_REMOTE_CHILD_REAPER,
+	PROC_WAIT_TASK_ZOMBIE,
+	PROC_VFORK_DONE,
+	PROC_RESERVE_PID,
+	PROC_PID_LINK_TASK,
+	PROC_END_PID_RESERVATION,
+	EPM_PIDMAP_STEAL,
+
+/** System V IPC messages types */
+	IPC_MSG_SEND,
+	IPC_MSG_RCV,
+	IPC_MSG_CHKPT,
+
+/** System V IPC semaphores */
+	IPC_SEM_WAKEUP,
+
+/** group management */
+	REQ_CREATE_ARAGORN_GROUP,
+	REQ_ADD_PROCESS_IN_ARAGORN_GROUP,
+	REQ_ADD_THREAD_IN_ARAGORN_GROUP,
+	REQ_REMOVE_PROCESS_FROM_ARAGORN_GROUP,
+	REQ_REMOVE_THREAD_FROM_ARAGORN_GROUP,
+	REQ_ADD_NODE_IN_ARAGORN_GROUP,
+	REQ_REMOVE_NODE_FROM_ARAGORN_GROUP,
+	REQ_GET_ARAGORN_GROUP,
+	REQ_GET_ARAGORN_GROUP_NODE_USAGE,
+	REQ_DELETE_ARAGORN_GROUP,
+
+	SYNC_SELFTEST_SYNC,
+
+	REQ_DYNAMIC_INFO_UPDATE,
+	REQ_AVAILABLE_TGIDS,
+	REQ_TASK_LOCATION,
+	REQ_PROC_PID_ENVIRON,
+	REQ_PROC_PID_CMDLINE,
+	REQ_PROC_PID_AUXV,
+	REQ_PROC_PID_LIMITS,
+	REQ_PROC_PID_SYSCALL,
+	REQ_PROC_PID_WCHAN,
+	REQ_PROC_PID_SCHEDSTAT,
+	REQ_PROC_PID_OOM_SCORE,
+	REQ_PROC_TGID_IO_ACCOUNTING,
+	REQ_PROC_PID_STATUS,
+	REQ_PROC_PID_PERSONALITY,
+	REQ_PROC_TGID_STAT,
+	REQ_PROC_PID_STATM,
+	REQ_PROC_PID_STACK,
+	REQ_PROC_PID_FD,
+
+	HEARTBEAT_SIGNAL,
+	HEARTBEAT_FAILURE,
+	HEARTBEAT_MONCHANGE,
+
+	KDDM_COPYSET,
+	KDDM_SELECT_OWNER,
+	KDDM_CHANGE_PROB_OWNER,
+
+	NODE_ADD,
+	NODE_REMOVE,
+	NODE_REMOVE_ADVERTISE,
+	NODE_REMOVE_ACK,
+	NODE_REMOVE_CONFIRM,
+	NODE_FAIL,
+	NODE_POWEROFF,
+	NODE_REBOOT,
+
+	NODE_FWD_ADD,
+	NODE_FWD_REMOVE,
+
+	RPC_MM_MMAP_REGION,
+	RPC_MM_MUNMAP,
+	RPC_MM_MREMAP,
+	RPC_MM_DO_BRK,
+	RPC_MM_EXPAND_STACK,
+	RPC_MM_MPROTECT,
+	RPC_MM_NOTIFY_LOW_MEM,
+
+	CLUSTER_START,
+	CLUSTER_STOP,
+
+	GLOBAL_CONFIG_OP,
+	SCHED_PIPE_GET_REMOTE_VALUE,
+
+	RPC_ENTER_BARRIER,
+	RPC_EXIT_BARRIER,
+
+	KDDM_BENCH,
+
+	RPCID_MAX // Must be the last one
+};
+
+#endif
diff -ruN linux-2.6.29/include/net/tipc/tipc_bearer.h android_cluster/linux-2.6.29/include/net/tipc/tipc_bearer.h
--- linux-2.6.29/include/net/tipc/tipc_bearer.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/net/tipc/tipc_bearer.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,138 +0,0 @@
-/*
- * include/net/tipc/tipc_bearer.h: Include file for privileged access to TIPC bearers
- * 
- * Copyright (c) 2003-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _NET_TIPC_BEARER_H_
-#define _NET_TIPC_BEARER_H_
-
-#ifdef __KERNEL__
-
-#include <linux/tipc_config.h>
-#include <linux/skbuff.h>
-#include <linux/spinlock.h>
-
-/*
- * Identifiers of supported TIPC media types
- */
-
-#define TIPC_MEDIA_TYPE_ETH	1
-
-/* 
- * Destination address structure used by TIPC bearers when sending messages
- * 
- * IMPORTANT: The fields of this structure MUST be stored using the specified
- * byte order indicated below, as the structure is exchanged between nodes
- * as part of a link setup process.
- */
-
-struct tipc_media_addr {
-	__be32  type;			/* bearer type (network byte order) */
-	union {
-		__u8   eth_addr[6];	/* 48 bit Ethernet addr (byte array) */ 
-#if 0
-		/* Prototypes for other possible bearer types */
-
-		struct {
-			__u16 sin_family;
-			__u16 sin_port;
-			struct {
-				__u32 s_addr;
-			} sin_addr;
-			char pad[4];
-		} addr_in;		/* IP-based bearer */
-		__u16  sock_descr;	/* generic socket bearer */
-#endif
-	} dev_addr;
-};
-
-/**
- * struct tipc_bearer - TIPC bearer info available to privileged users
- * @usr_handle: pointer to additional user-defined information about bearer
- * @mtu: max packet size bearer can support
- * @blocked: non-zero if bearer is blocked
- * @lock: spinlock for controlling access to bearer
- * @addr: media-specific address associated with bearer
- * @name: bearer name (format = media:interface)
- * 
- * Note: TIPC initializes "name" and "lock" fields; user is responsible for
- * initialization all other fields when a bearer is enabled.
- */
-
-struct tipc_bearer {
-	void *usr_handle;
-	u32 mtu;
-	int blocked;
-	spinlock_t lock;
-	struct tipc_media_addr addr;
-	char name[TIPC_MAX_BEARER_NAME];
-};
-
-/*
- * TIPC routines available to supported media types
- */
-
-int  tipc_register_media(u32 media_type,
-			 char *media_name, 
-			 int (*enable)(struct tipc_bearer *), 
-			 void (*disable)(struct tipc_bearer *), 
-			 int (*send_msg)(struct sk_buff *, 
-					 struct tipc_bearer *,
-					 struct tipc_media_addr *), 
-			 char *(*addr2str)(struct tipc_media_addr *a,
-					   char *str_buf,
-					   int str_size),
-			 struct tipc_media_addr *bcast_addr,
-			 const u32 bearer_priority,
-			 const u32 link_tolerance,  /* [ms] */
-			 const u32 send_window_limit); 
-
-void tipc_recv_msg(struct sk_buff *buf, struct tipc_bearer *tb_ptr);
-
-int  tipc_block_bearer(const char *name);
-void tipc_continue(struct tipc_bearer *tb_ptr); 
-
-int tipc_enable_bearer(const char *bearer_name, u32 bcast_scope, u32 priority);
-int tipc_disable_bearer(const char *name);
-
-/*
- * Routines made available to TIPC by supported media types
- */
-
-int  tipc_eth_media_start(void);
-void tipc_eth_media_stop(void);
-
-#endif
-
-#endif
diff -ruN linux-2.6.29/include/net/tipc/tipc.h android_cluster/linux-2.6.29/include/net/tipc/tipc.h
--- linux-2.6.29/include/net/tipc/tipc.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/net/tipc/tipc.h	2014-05-27 23:04:10.394028589 -0700
@@ -2,7 +2,7 @@
  * include/net/tipc/tipc.h: Main include file for TIPC users
  * 
  * Copyright (c) 2003-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
+ * Copyright (c) 2005,2008 Wind River Systems
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -46,6 +46,11 @@
  * Native API
  */
 
+#ifdef CONFIG_KRGRPC
+extern int tipc_net_id;
+int tipc_core_start_net(unsigned long addr);
+#endif
+
 /*
  * TIPC operating mode routines
  */
@@ -126,7 +131,7 @@
 		    tipc_msg_event message_cb, 
 		    tipc_named_msg_event named_message_cb, 
 		    tipc_conn_msg_event conn_message_cb, 
-		    tipc_continue_event continue_event_cb,/* May be zero */
+		    tipc_continue_event continue_event_cb,
 		    u32 *portref);
 
 int tipc_deleteport(u32 portref);
@@ -145,13 +150,13 @@
 int tipc_publish(u32 portref, unsigned int scope, 
 		 struct tipc_name_seq const *name_seq);
 int tipc_withdraw(u32 portref, unsigned int scope,
-		  struct tipc_name_seq const *name_seq); /* 0: all */
+		  struct tipc_name_seq const *name_seq);
 
 int tipc_connect2port(u32 portref, struct tipc_portid const *port);
 
 int tipc_disconnect(u32 portref);
 
-int tipc_shutdown(u32 ref); /* Sends SHUTDOWN msg */
+int tipc_shutdown(u32 ref);
 
 int tipc_isconnected(u32 portref, int *isconnected);
 
@@ -176,7 +181,7 @@
 
 int tipc_send2name(u32 portref, 
 		   struct tipc_name const *name, 
-		   u32 domain,	/* 0:own zone */
+		   u32 domain,
 		   unsigned int num_sect,
 		   struct iovec const *msg_sect);
 
@@ -188,7 +193,7 @@
 
 int tipc_forward2name(u32 portref, 
 		      struct tipc_name const *name, 
-		      u32 domain,   /*0: own zone */
+		      u32 domain,
 		      unsigned int section_count,
 		      struct iovec const *msg_sect,
 		      struct tipc_portid const *origin,
@@ -228,14 +233,14 @@
 
 int tipc_multicast(u32 portref, 
 		   struct tipc_name_seq const *seq, 
-		   u32 domain,	/* 0:own zone */
+		   u32 domain,	/* currently unused */
 		   unsigned int section_count,
 		   struct iovec const *msg);
 
 #if 0
 int tipc_multicast_buf(u32 portref, 
 		       struct tipc_name_seq const *seq, 
-		       u32 domain,	/* 0:own zone */
+		       u32 domain,
 		       void *buf,
 		       unsigned int size);
 #endif
diff -ruN linux-2.6.29/include/net/tipc/tipc_msg.h android_cluster/linux-2.6.29/include/net/tipc/tipc_msg.h
--- linux-2.6.29/include/net/tipc/tipc_msg.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/net/tipc/tipc_msg.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,207 +0,0 @@
-/*
- * include/net/tipc/tipc_msg.h: Include file for privileged access to TIPC message headers
- * 
- * Copyright (c) 2003-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _NET_TIPC_MSG_H_
-#define _NET_TIPC_MSG_H_
-
-#ifdef __KERNEL__
-
-struct tipc_msg {
-	__be32 hdr[15];
-};
-
-
-/*
-		TIPC user data message header format, version 2:
-
-
-       1 0 9 8 7 6 5 4|3 2 1 0 9 8 7 6|5 4 3 2 1 0 9 8|7 6 5 4 3 2 1 0 
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w0:|vers | user  |hdr sz |n|d|s|-|          message size           |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w1:|mstyp| error |rer cnt|lsc|opt p|      broadcast ack no         |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w2:|        link level ack no      |   broadcast/link level seq no |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w3:|                       previous node                           |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w4:|                      originating port                         |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w5:|                      destination port                         |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+    
-   w6:|                      originating node                         |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w7:|                      destination node                         |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w8:|            name type / transport sequence number              |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w9:|              name instance/multicast lower bound              |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+    
-   wA:|                    multicast upper bound                      |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+    
-      /                                                               /
-      \                           options                             \
-      /                                                               /
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-
-*/
-
-#define TIPC_CONN_MSG	0
-#define TIPC_MCAST_MSG	1
-#define TIPC_NAMED_MSG	2
-#define TIPC_DIRECT_MSG	3
-
-
-static inline u32 msg_word(struct tipc_msg *m, u32 pos)
-{
-	return ntohl(m->hdr[pos]);
-}
-
-static inline u32 msg_bits(struct tipc_msg *m, u32 w, u32 pos, u32 mask)
-{
-	return (msg_word(m, w) >> pos) & mask;
-}
-
-static inline u32 msg_importance(struct tipc_msg *m)
-{
-	return msg_bits(m, 0, 25, 0xf);
-}
-
-static inline u32 msg_hdr_sz(struct tipc_msg *m)
-{
-	return msg_bits(m, 0, 21, 0xf) << 2;
-}
-
-static inline int msg_short(struct tipc_msg *m)
-{
-	return (msg_hdr_sz(m) == 24);
-}
-
-static inline u32 msg_size(struct tipc_msg *m)
-{
-	return msg_bits(m, 0, 0, 0x1ffff);
-}
-
-static inline u32 msg_data_sz(struct tipc_msg *m)
-{
-	return (msg_size(m) - msg_hdr_sz(m));
-}
-
-static inline unchar *msg_data(struct tipc_msg *m)
-{
-	return ((unchar *)m) + msg_hdr_sz(m);
-}
-
-static inline u32 msg_type(struct tipc_msg *m)
-{
-	return msg_bits(m, 1, 29, 0x7);
-}
-
-static inline u32 msg_named(struct tipc_msg *m)
-{
-	return (msg_type(m) == TIPC_NAMED_MSG);
-}
-
-static inline u32 msg_mcast(struct tipc_msg *m)
-{
-	return (msg_type(m) == TIPC_MCAST_MSG);
-}
-
-static inline u32 msg_connected(struct tipc_msg *m)
-{
-	return (msg_type(m) == TIPC_CONN_MSG);
-}
-
-static inline u32 msg_errcode(struct tipc_msg *m)
-{
-	return msg_bits(m, 1, 25, 0xf);
-}
-
-static inline u32 msg_prevnode(struct tipc_msg *m)
-{
-	return msg_word(m, 3);
-}
-
-static inline u32 msg_origport(struct tipc_msg *m)
-{
-	return msg_word(m, 4);
-}
-
-static inline u32 msg_destport(struct tipc_msg *m)
-{
-	return msg_word(m, 5);
-}
-
-static inline u32 msg_mc_netid(struct tipc_msg *m)
-{
-	return msg_word(m, 5);
-}
-
-static inline u32 msg_orignode(struct tipc_msg *m)
-{
-	if (likely(msg_short(m)))
-		return msg_prevnode(m);
-	return msg_word(m, 6);
-}
-
-static inline u32 msg_destnode(struct tipc_msg *m)
-{
-	return msg_word(m, 7);
-}
-
-static inline u32 msg_nametype(struct tipc_msg *m)
-{
-	return msg_word(m, 8);
-}
-
-static inline u32 msg_nameinst(struct tipc_msg *m)
-{
-	return msg_word(m, 9);
-}
-
-static inline u32 msg_namelower(struct tipc_msg *m)
-{
-	return msg_nameinst(m);
-}
-
-static inline u32 msg_nameupper(struct tipc_msg *m)
-{
-	return msg_word(m, 10);
-}
-
-#endif
-
-#endif
diff -ruN linux-2.6.29/include/net/tipc/tipc_plugin_if.h android_cluster/linux-2.6.29/include/net/tipc/tipc_plugin_if.h
--- linux-2.6.29/include/net/tipc/tipc_plugin_if.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/net/tipc/tipc_plugin_if.h	2014-05-27 23:04:10.394028589 -0700
@@ -0,0 +1,148 @@
+/*
+ * include/net/tipc/tipc_plugin_if.h: Include file for interface access by TIPC plugins
+ * 
+ * Copyright (c) 2003-2006, Ericsson AB
+ * Copyright (c) 2005-2006, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _NET_TIPC_BEARER_H_
+#define _NET_TIPC_BEARER_H_
+
+#ifdef __KERNEL__
+
+#include <linux/tipc_config.h>
+#include <linux/skbuff.h>
+#include <linux/spinlock.h>
+
+#define TIPC_MAX_BEARERS	8
+
+/*
+ * Identifiers of supported TIPC media types
+ */
+
+#define TIPC_MEDIA_ID_INVALID	0
+#define TIPC_MEDIA_ID_ETH	1
+
+/**
+ * struct tipc_media_addr - destination address used by TIPC bearers
+ * @value: media-specific address info (opaque to TIPC) 
+ * @media_id: TIPC media identifier
+ * @broadcast: non-zero if address is a broadcast address
+ */
+
+struct tipc_media_addr {
+        u8 value[20];
+	u8 media_id;
+        u8 broadcast;
+};
+
+/**
+ * struct tipc_bearer - TIPC bearer info available to privileged users
+ * @usr_handle: pointer to additional user-defined information about bearer
+ * @mtu: max packet size bearer can support
+ * @blocked: non-zero if bearer is blocked
+ * @addr: media-specific address associated with bearer
+ * @name: bearer name (format = media:interface)
+ * @lock: spinlock for controlling access to bearer
+ * 
+ * Note: TIPC initializes "name" and "lock" fields; user is responsible for
+ * initialization all other fields when a bearer is enabled.
+ */
+
+
+struct tipc_bearer {
+	void *usr_handle;
+	u32 mtu;
+	int blocked;
+	struct tipc_media_addr addr;
+	char name[TIPC_MAX_BEARER_NAME];
+	spinlock_t lock;
+};
+
+/**
+ * struct tipc_media - TIPC media information available to privileged users
+ * @type_id: TIPC media identifier
+ * @name: media name
+ * @priority: default link (and bearer) priority
+ * @tolerance: default time (in ms) before declaring link failure
+ * @window: default window (in packets) before declaring link congestion
+ * @send_msg: routine which handles buffer transmission
+ * @enable_bearer: routine which enables a bearer
+ * @disable_bearer: routine which disables a bearer
+ * @addr2str: routine which converts media address to string form
+ * @str2addr: routine which converts media address from string form
+ * @msg2addr: routine which converts media address to message header form
+ * @addr2msg: routine which converts media address from message header form
+ * @bcast_addr: media address used in broadcasting
+ */
+ 
+struct tipc_media {
+	u8 media_id;
+	char name[TIPC_MAX_MEDIA_NAME];
+	u32 priority;
+	u32 tolerance;
+	u32 window;
+	int (*send_msg)(struct sk_buff *buf, 
+			struct tipc_bearer *b_ptr,
+			struct tipc_media_addr *dest);
+	int (*enable_bearer)(struct tipc_bearer *b_ptr);
+	void (*disable_bearer)(struct tipc_bearer *b_ptr);
+	int (*addr2str)(struct tipc_media_addr *a, char *str_buf, int str_size);
+	int (*str2addr)(struct tipc_media_addr *a, char *str_buf);
+        int (*msg2addr)(struct tipc_media_addr *a, u32 *msg_area);
+        int (*addr2msg)(struct tipc_media_addr *a, u32 *msg_area);
+	struct tipc_media_addr bcast_addr;
+};
+
+/*
+ * TIPC routines available to supported media types
+ */
+
+int  tipc_register_media(struct tipc_media *m_ptr);
+void tipc_recv_msg(struct sk_buff *buf, struct tipc_bearer *tb_ptr);
+
+int  tipc_block_bearer(const char *name);
+void tipc_continue(struct tipc_bearer *tb_ptr); 
+
+int tipc_enable_bearer(const char *bearer_name, u32 disc_domain, u32 priority);
+int tipc_disable_bearer(const char *name);
+
+/*
+ * Routines made available to TIPC by supported media types
+ */
+
+int  tipc_eth_media_start(void);
+void tipc_eth_media_stop(void);
+
+#endif
+
+#endif
diff -ruN linux-2.6.29/include/net/tipc/tipc_plugin_msg.h android_cluster/linux-2.6.29/include/net/tipc/tipc_plugin_msg.h
--- linux-2.6.29/include/net/tipc/tipc_plugin_msg.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/net/tipc/tipc_plugin_msg.h	2014-05-27 23:04:10.394028589 -0700
@@ -0,0 +1,185 @@
+/*
+ * include/net/tipc/tipc_msg.h: Include file for message access by TIPC plugins
+ * 
+ * Copyright (c) 2003-2006, Ericsson AB
+ * Copyright (c) 2005-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _NET_TIPC_MSG_H_
+#define _NET_TIPC_MSG_H_
+
+#ifdef __KERNEL__
+
+/*
+ * Public TIPC message header definitions
+ *
+ * The following definitions are available to plugins that supplement TIPC's
+ * standard API and interface code.  (Additional definitions used internally
+ * by TIPC are found in tipc_msg.h.)
+ */
+
+struct tipc_msg {
+	__be32 hdr[15];
+};
+
+
+#define TIPC_CONN_MSG	0
+#define TIPC_MCAST_MSG	1
+#define TIPC_NAMED_MSG	2
+#define TIPC_DIRECT_MSG	3
+
+
+static inline u32 msg_word(struct tipc_msg *m, u32 pos)
+{
+	return ntohl(m->hdr[pos]);
+}
+
+static inline u32 msg_bits(struct tipc_msg *m, u32 w, u32 pos, u32 mask)
+{
+	return (msg_word(m, w) >> pos) & mask;
+}
+
+static inline u32 msg_importance(struct tipc_msg *m)
+{
+	return msg_bits(m, 0, 25, 0xf);
+}
+
+static inline u32 msg_hdr_sz(struct tipc_msg *m)
+{
+	return msg_bits(m, 0, 21, 0xf) << 2;
+}
+
+static inline int msg_short(struct tipc_msg *m)
+{
+	return (msg_hdr_sz(m) == 24);
+}
+
+static inline u32 msg_size(struct tipc_msg *m)
+{
+	return msg_bits(m, 0, 0, 0x1ffff);
+}
+
+static inline u32 msg_data_sz(struct tipc_msg *m)
+{
+	return (msg_size(m) - msg_hdr_sz(m));
+}
+
+static inline unchar *msg_data(struct tipc_msg *m)
+{
+	return ((unchar *)m) + msg_hdr_sz(m);
+}
+
+static inline u32 msg_type(struct tipc_msg *m)
+{
+	return msg_bits(m, 1, 29, 0x7);
+}
+
+static inline u32 msg_direct(struct tipc_msg *m)
+{
+	return (msg_type(m) == TIPC_DIRECT_MSG);
+}
+
+static inline u32 msg_named(struct tipc_msg *m)
+{
+	return (msg_type(m) == TIPC_NAMED_MSG);
+}
+
+static inline u32 msg_mcast(struct tipc_msg *m)
+{
+	return (msg_type(m) == TIPC_MCAST_MSG);
+}
+
+static inline u32 msg_connected(struct tipc_msg *m)
+{
+	return (msg_type(m) == TIPC_CONN_MSG);
+}
+
+static inline u32 msg_errcode(struct tipc_msg *m)
+{
+	return msg_bits(m, 1, 25, 0xf);
+}
+
+static inline u32 msg_prevnode(struct tipc_msg *m)
+{
+	return msg_word(m, 3);
+}
+
+static inline u32 msg_origport(struct tipc_msg *m)
+{
+	return msg_word(m, 4);
+}
+
+static inline u32 msg_destport(struct tipc_msg *m)
+{
+	return msg_word(m, 5);
+}
+
+static inline u32 msg_mc_netid(struct tipc_msg *m)
+{
+	return msg_word(m, 5);
+}
+
+static inline u32 msg_orignode(struct tipc_msg *m)
+{
+	if (likely(msg_short(m)))
+		return msg_prevnode(m);
+	return msg_word(m, 6);
+}
+
+static inline u32 msg_destnode(struct tipc_msg *m)
+{
+	return msg_word(m, 7);
+}
+
+static inline u32 msg_nametype(struct tipc_msg *m)
+{
+	return msg_word(m, 8);
+}
+
+static inline u32 msg_nameinst(struct tipc_msg *m)
+{
+	return msg_word(m, 9);
+}
+
+static inline u32 msg_namelower(struct tipc_msg *m)
+{
+	return msg_nameinst(m);
+}
+
+static inline u32 msg_nameupper(struct tipc_msg *m)
+{
+	return msg_word(m, 10);
+}
+
+#endif
+
+#endif
diff -ruN linux-2.6.29/include/net/tipc/tipc_plugin_port.h android_cluster/linux-2.6.29/include/net/tipc/tipc_plugin_port.h
--- linux-2.6.29/include/net/tipc/tipc_plugin_port.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/include/net/tipc/tipc_plugin_port.h	2014-05-27 23:04:10.394028589 -0700
@@ -0,0 +1,109 @@
+/*
+ * include/net/tipc/tipc_port.h: Include file for port access by TIPC plugins
+ * 
+ * Copyright (c) 1994-2007, Ericsson AB
+ * Copyright (c) 2005-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _NET_TIPC_PORT_H_
+#define _NET_TIPC_PORT_H_
+
+#ifdef __KERNEL__
+
+#include <linux/tipc.h>
+#include <linux/skbuff.h>
+#include <net/tipc/tipc_plugin_msg.h>
+
+#define TIPC_FLOW_CONTROL_WIN 512
+
+/**
+ * struct tipc_port - native TIPC port info available to privileged users
+ * @usr_handle: pointer to additional user-defined information about port
+ * @lock: pointer to spinlock for controlling access to port
+ * @connected: non-zero if port is currently connected to a peer port
+ * @conn_type: TIPC type used when connection was established
+ * @conn_instance: TIPC instance used when connection was established
+ * @conn_unacked: number of unacknowledged messages received from peer port
+ * @published: non-zero if port has one or more associated names
+ * @congested: non-zero if cannot send because of link or port congestion
+ * @max_pkt: maximum packet size "hint" used when building messages sent by port
+ * @ref: unique reference to port in TIPC object registry
+ * @phdr: preformatted message header used when sending messages
+ */
+
+struct tipc_port {
+        void *usr_handle;
+        spinlock_t *lock;
+	int connected;
+        u32 conn_type;
+        u32 conn_instance;
+	u32 conn_unacked;
+	int published;
+	u32 congested;
+	u32 max_pkt;
+	u32 ref;
+	struct tipc_msg phdr;
+};
+
+
+#ifdef CONFIG_KRGRPC
+struct tipc_port *tipc_createport_raw(void *usr_handle,
+			u32 (*dispatcher)(struct tipc_port *, struct sk_buff *),
+			void (*wakeup)(struct tipc_port *),
+			const u32 importance, void* user_port);
+#else
+struct tipc_port *tipc_createport_raw(void *usr_handle,
+			u32 (*dispatcher)(struct tipc_port *, struct sk_buff *),
+			void (*wakeup)(struct tipc_port *),
+			const u32 importance);
+#endif
+
+int tipc_reject_msg(struct sk_buff *buf, u32 err);
+
+int tipc_send_buf_fast(struct sk_buff *buf, u32 destnode);
+
+void tipc_acknowledge(u32 port_ref, u32 ack);
+
+struct tipc_port *tipc_get_port(const u32 ref);
+
+void *tipc_get_handle(const u32 ref);
+
+/*
+ * The following routines require that the port be locked on entry
+ */
+
+int tipc_disconnect_port(struct tipc_port *tp_ptr);
+
+#endif
+
+#endif
+
diff -ruN linux-2.6.29/include/net/tipc/tipc_port.h android_cluster/linux-2.6.29/include/net/tipc/tipc_port.h
--- linux-2.6.29/include/net/tipc/tipc_port.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/include/net/tipc/tipc_port.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,103 +0,0 @@
-/*
- * include/net/tipc/tipc_port.h: Include file for privileged access to TIPC ports
- * 
- * Copyright (c) 1994-2007, Ericsson AB
- * Copyright (c) 2005-2008, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _NET_TIPC_PORT_H_
-#define _NET_TIPC_PORT_H_
-
-#ifdef __KERNEL__
-
-#include <linux/tipc.h>
-#include <linux/skbuff.h>
-#include <net/tipc/tipc_msg.h>
-
-#define TIPC_FLOW_CONTROL_WIN 512
-
-/**
- * struct tipc_port - native TIPC port info available to privileged users
- * @usr_handle: pointer to additional user-defined information about port
- * @lock: pointer to spinlock for controlling access to port
- * @connected: non-zero if port is currently connected to a peer port
- * @conn_type: TIPC type used when connection was established
- * @conn_instance: TIPC instance used when connection was established
- * @conn_unacked: number of unacknowledged messages received from peer port
- * @published: non-zero if port has one or more associated names
- * @congested: non-zero if cannot send because of link or port congestion
- * @max_pkt: maximum packet size "hint" used when building messages sent by port
- * @ref: unique reference to port in TIPC object registry
- * @phdr: preformatted message header used when sending messages
- */
-
-struct tipc_port {
-        void *usr_handle;
-        spinlock_t *lock;
-	int connected;
-        u32 conn_type;
-        u32 conn_instance;
-	u32 conn_unacked;
-	int published;
-	u32 congested;
-	u32 max_pkt;
-	u32 ref;
-	struct tipc_msg phdr;
-};
-
-
-struct tipc_port *tipc_createport_raw(void *usr_handle,
-			u32 (*dispatcher)(struct tipc_port *, struct sk_buff *),
-			void (*wakeup)(struct tipc_port *),
-			const u32 importance);
-
-int tipc_reject_msg(struct sk_buff *buf, u32 err);
-
-int tipc_send_buf_fast(struct sk_buff *buf, u32 destnode);
-
-void tipc_acknowledge(u32 port_ref,u32 ack);
-
-struct tipc_port *tipc_get_port(const u32 ref);
-
-void *tipc_get_handle(const u32 ref);
-
-/*
- * The following routines require that the port be locked on entry
- */
-
-int tipc_disconnect_port(struct tipc_port *tp_ptr);
-
-
-#endif
-
-#endif
-
diff -ruN linux-2.6.29/init/main.c android_cluster/linux-2.6.29/init/main.c
--- linux-2.6.29/init/main.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/init/main.c	2014-05-27 23:04:10.418028090 -0700
@@ -100,6 +100,10 @@
 enum system_states system_state __read_mostly;
 EXPORT_SYMBOL(system_state);
 
+#ifdef CONFIG_KRGRPC
+extern void kerrighed_init(void);
+#endif
+
 /*
  * Boot command-line arguments
  */
@@ -788,8 +792,21 @@
  */
 static noinline int init_post(void)
 {
+#if defined(CONFIG_KERRIGHED) || defined(CONFIG_KRGRPC)
+	/* 
+	 * In a perfect world we would like to call global_pid_init and
+	 * kerrighed_init just after async_synchronize_full
+	 * Since we use early_kerrighed_* vars and such vars are tagged by
+	 * __initdata, this raise some WARNINGs.
+	 * In order to fix that we move async_synchronise_full just before
+	 * to call init_post
+	 */
+	
+#else
 	/* need to finish all async __init code before freeing the memory */
 	async_synchronize_full();
+#endif
+
 	free_initmem();
 	unlock_kernel();
 	mark_rodata_ro();
@@ -873,6 +890,13 @@
 		prepare_namespace();
 	}
 
+#if defined(CONFIG_KERRIGHED) || defined(CONFIG_KRGRPC)
+	/* need to finish all async __init code before freeing the memory */
+	async_synchronize_full();
+
+	kerrighed_init();
+#endif
+
 	/*
 	 * Ok, we have completed the initial bootup, and
 	 * we're essentially up and running. Get rid of the
diff -ruN linux-2.6.29/ipc/ipc_checkpoint.c android_cluster/linux-2.6.29/ipc/ipc_checkpoint.c
--- linux-2.6.29/ipc/ipc_checkpoint.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/ipc_checkpoint.c	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,140 @@
+/** IPC snapshot API
+ *  @file ipc_checkpoint.c
+ *
+ *  @author Matthieu Fertré
+ */
+
+#define MODULE_NAME "IPC checkpoint"
+#include <linux/sched.h>
+#include <linux/uaccess.h>
+#include <linux/syscalls.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/file_ghost.h>
+#include "krgipc_mobility.h"
+#include "ipc_checkpoint.h"
+
+int sys_msgq_checkpoint(int msqid, int fd)
+{
+	int r;
+
+	r = __sys_msgq_checkpoint(msqid, fd);
+
+	return r;
+}
+
+int sys_msgq_restart(int fd)
+{
+	int r;
+	ghost_fs_t oldfs;
+	ghost_t *ghost;
+
+	__set_ghost_fs(&oldfs);
+
+	ghost = create_file_ghost_from_fd(GHOST_READ, fd);
+
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		goto exit;
+	}
+
+	r = import_full_sysv_msgq(ghost);
+
+	ghost_close(ghost);
+exit:
+	unset_ghost_fs(&oldfs);
+	return r;
+}
+
+int sys_sem_checkpoint(int semid, int fd)
+{
+	int r;
+	ghost_fs_t oldfs;
+	ghost_t *ghost;
+
+	__set_ghost_fs(&oldfs);
+
+	ghost = create_file_ghost_from_fd(GHOST_WRITE, fd);
+
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		goto exit;
+	}
+
+	r = export_full_sysv_sem(ghost, semid);
+
+	ghost_close(ghost);
+exit:
+	unset_ghost_fs(&oldfs);
+	return r;
+}
+
+int sys_sem_restart(int fd)
+{
+	int r;
+	ghost_fs_t oldfs;
+	ghost_t *ghost;
+
+	__set_ghost_fs(&oldfs);
+
+	ghost = create_file_ghost_from_fd(GHOST_READ, fd);
+
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		goto exit;
+	}
+
+	r = import_full_sysv_sem(ghost);
+
+	ghost_close(ghost);
+exit:
+	unset_ghost_fs(&oldfs);
+	return r;
+}
+
+int sys_shm_checkpoint(int shmid, int fd)
+{
+	int r;
+	ghost_fs_t oldfs;
+	ghost_t *ghost;
+
+	__set_ghost_fs(&oldfs);
+
+	ghost = create_file_ghost_from_fd(GHOST_WRITE, fd);
+
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		goto exit;
+	}
+
+	r = export_full_sysv_shm(ghost, shmid);
+
+	ghost_close(ghost);
+exit:
+	unset_ghost_fs(&oldfs);
+	return r;
+}
+
+int sys_shm_restart(int fd)
+{
+	int r;
+	ghost_fs_t oldfs;
+	ghost_t *ghost;
+
+	__set_ghost_fs(&oldfs);
+
+	ghost = create_file_ghost_from_fd(GHOST_READ, fd);
+
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		goto exit;
+	}
+
+	r = import_full_sysv_shm(ghost);
+
+	ghost_close(ghost);
+exit:
+	unset_ghost_fs(&oldfs);
+	return r;
+}
+
+
diff -ruN linux-2.6.29/ipc/ipc_checkpoint.h android_cluster/linux-2.6.29/ipc/ipc_checkpoint.h
--- linux-2.6.29/ipc/ipc_checkpoint.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/ipc_checkpoint.h	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,13 @@
+
+int sys_msgq_checkpoint(int msqid, int fd);
+
+int sys_msgq_restart(int fd);
+
+int sys_sem_checkpoint(int semid, int fd);
+
+int sys_sem_restart(int fd);
+
+int sys_shm_checkpoint(int semid, int fd);
+
+int sys_shm_restart(int fd);
+
diff -ruN linux-2.6.29/ipc/ipc_handler.c android_cluster/linux-2.6.29/ipc/ipc_handler.c
--- linux-2.6.29/ipc/ipc_handler.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/ipc_handler.c	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,382 @@
+/** Common code for IPC mechanism accross the cluster
+ *  @file ipc_handler.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#ifndef NO_IPC
+
+#include <linux/proc_fs.h>
+#include <asm/uaccess.h>
+#include <linux/ipc.h>
+#include <linux/ipc_namespace.h>
+#include <linux/nsproxy.h>
+#include <linux/msg.h>
+#include <kddm/kddm.h>
+#include <kerrighed/namespace.h>
+#include <kerrighed/krg_syscalls.h>
+#include <kerrighed/krg_services.h>
+#include <kerrighed/procfs.h>
+#include "ipc_checkpoint.h"
+#include "ipcmap_io_linker.h"
+#include "ipc_handler.h"
+#include "util.h"
+#include "krgmsg.h"
+
+
+struct ipc_namespace *find_get_krg_ipcns(void)
+{
+	struct krg_namespace *krg_ns;
+	struct ipc_namespace *ipc_ns;
+
+	krg_ns = find_get_krg_ns();
+	if (!krg_ns)
+		goto error;
+
+	if (!krg_ns->root_nsproxy.ipc_ns)
+		goto error_ipcns;
+
+	ipc_ns = get_ipc_ns(krg_ns->root_nsproxy.ipc_ns);
+
+	put_krg_ns(krg_ns);
+
+	return ipc_ns;
+
+error_ipcns:
+	put_krg_ns(krg_ns);
+error:
+	return NULL;
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                                KERNEL HOOKS                               */
+/*                                                                           */
+/*****************************************************************************/
+
+int krg_ipc_get_maxid(struct ipc_ids* ids)
+{
+	ipcmap_object_t *ipc_map;
+	int max_id;
+
+	ipc_map = _kddm_get_object(ids->krgops->map_kddm_set, 0);
+	max_id = ipc_map->alloc_map - 1;
+	_kddm_put_object(ids->krgops->map_kddm_set, 0);
+
+	return max_id;
+}
+
+int krg_ipc_get_new_id(struct ipc_ids* ids)
+{
+	ipcmap_object_t *ipc_map, *max_id;
+	int i = 1, id = -1, offset;
+
+	max_id = _kddm_grab_object(ids->krgops->map_kddm_set, 0);
+
+	while (id == -1) {
+		ipc_map = _kddm_grab_object(ids->krgops->map_kddm_set, i);
+
+		if (ipc_map->alloc_map != ULONG_MAX) {
+			offset = find_first_zero_bit(&ipc_map->alloc_map,
+						     BITS_PER_LONG);
+
+			if (offset < BITS_PER_LONG) {
+
+				id = (i-1) * BITS_PER_LONG + offset;
+				set_bit(offset, &ipc_map->alloc_map);
+				if (id >= max_id->alloc_map)
+					max_id->alloc_map = id + 1;
+			}
+		}
+
+		_kddm_put_object(ids->krgops->map_kddm_set, i);
+		i++;
+	}
+
+	_kddm_put_object(ids->krgops->map_kddm_set, 0);
+
+	return id;
+}
+
+int krg_ipc_get_this_id(struct ipc_ids *ids, int id)
+{
+	ipcmap_object_t *ipc_map, *max_id;
+	int i, offset, ret = 0;
+
+	max_id = _kddm_grab_object(ids->krgops->map_kddm_set, 0);
+
+	offset = id % BITS_PER_LONG;
+	i = (id - offset)/BITS_PER_LONG +1;
+
+	ipc_map = _kddm_grab_object(ids->krgops->map_kddm_set, i);
+
+	if (test_and_set_bit(offset, &ipc_map->alloc_map)) {
+		ret = -EBUSY;
+		goto out_id_unavailable;
+	}
+
+	if (id >= max_id->alloc_map)
+		max_id->alloc_map = id + 1;
+
+out_id_unavailable:
+	_kddm_put_object(ids->krgops->map_kddm_set, i);
+	_kddm_put_object(ids->krgops->map_kddm_set, 0);
+
+	return ret;
+}
+
+void krg_ipc_rmid(struct ipc_ids* ids, int index)
+{
+	ipcmap_object_t *ipc_map, *max_id;
+	int i, offset;
+
+	/* Clear the corresponding entry in the bit field */
+
+	i = 1 + index / BITS_PER_LONG;
+	offset = index % BITS_PER_LONG;
+
+	ipc_map = _kddm_grab_object(ids->krgops->map_kddm_set, i);
+
+	BUG_ON(!test_bit(offset, &ipc_map->alloc_map));
+
+	clear_bit(offset, &ipc_map->alloc_map);
+
+	_kddm_put_object(ids->krgops->map_kddm_set, i);
+
+	/* Check if max_id must be adjusted */
+
+	max_id = _kddm_grab_object(ids->krgops->map_kddm_set, 0);
+
+	if (max_id->alloc_map != index + 1)
+		goto done;
+
+	for (; i > 0; i--) {
+
+		ipc_map = _kddm_grab_object(ids->krgops->map_kddm_set, i);
+		if (ipc_map->alloc_map != 0) {
+			for (; offset >= 0; offset--) {
+				if (test_bit (offset, &ipc_map->alloc_map)) {
+					max_id->alloc_map = 1 + offset +
+						(i - 1) * BITS_PER_LONG;
+					_kddm_put_object(
+						ids->krgops->map_kddm_set, i);
+					goto done;
+				}
+			}
+		}
+		offset = 31;
+		_kddm_put_object(ids->krgops->map_kddm_set, i);
+	}
+
+	max_id->alloc_map = 0;
+done:
+	_kddm_put_object(ids->krgops->map_kddm_set, 0);
+
+	return;
+}
+
+/*****************************************************************************/
+
+int proc_msgq_chkpt(void *arg)
+{
+	int r;
+	int args[2];
+	int msgqid, fd;
+
+	if (copy_from_user((void *) args, arg, 2*sizeof(int))) {
+		PANIC("cannot set the arg of proc_msgq_checkpoint system call\n");
+		return -EINVAL;
+	}
+
+	msgqid = args[0];
+	fd = args[1];
+
+	r = sys_msgq_checkpoint(msgqid, fd);
+
+	return r;
+}
+
+int proc_msgq_restart(void *arg)
+{
+	int r;
+	int fd;
+
+	if (copy_from_user(&fd, arg, sizeof(int))) {
+		PANIC("cannot set the arg of proc_msgq_restart system call\n");
+		return -EINVAL;
+	}
+
+	r = sys_msgq_restart(fd);
+
+	return r;
+}
+
+int proc_sem_chkpt(void *arg)
+{
+	int r;
+	int args[2];
+	int semid, fd;
+
+	if (copy_from_user((void *) args, arg, 2*sizeof(int))) {
+		PANIC("cannot set the arg of proc_sem_checkpoint system call\n");
+		return -EINVAL;
+	}
+
+	semid = args[0];
+	fd = args[1];
+
+	r = sys_sem_checkpoint(semid, fd);
+
+	return r;
+}
+
+int proc_sem_restart(void *arg)
+{
+	int r;
+	int fd;
+
+	if (copy_from_user(&fd, arg, sizeof(int))) {
+		PANIC("cannot set the arg of proc_sem_restart system call\n");
+		return -EINVAL;
+	}
+
+	r = sys_sem_restart(fd);
+
+	return r;
+}
+
+int proc_shm_chkpt(void *arg)
+{
+	int r;
+	int args[2];
+	int shmid, fd;
+
+	if (copy_from_user((void *) args, arg, 2*sizeof(int))) {
+		PANIC("cannot set the arg of proc_shm_checkpoint system call\n");
+		return -EINVAL;
+	}
+
+	shmid = args[0];
+	fd = args[1];
+
+	r = sys_shm_checkpoint(shmid, fd);
+
+	return r;
+}
+
+int proc_shm_restart(void *arg)
+{
+	int r;
+	int fd;
+
+	if (copy_from_user(&fd, arg, sizeof(int))) {
+		PANIC("cannot set the arg of proc_shm_restart system call\n");
+		return -EINVAL;
+	}
+
+	r = sys_shm_restart(fd);
+
+	return r;
+}
+
+
+static void do_cleanup_ipc_objects (unique_id_t set_id)
+{
+	ipcmap_object_t *ipc_map;
+
+	ipc_map = kddm_grab_object_no_ft(kddm_def_ns, set_id, 0);
+	if (ipc_map) {
+		BUG_ON (ipc_map->alloc_map != 0);
+		kddm_remove_frozen_object(kddm_def_ns, set_id, 0);
+	}
+	else
+		kddm_put_object(kddm_def_ns, set_id, 0);
+}
+
+
+
+/* Get rid of possible conflicting objects before node addition.
+ */
+void cleanup_ipc_objects ()
+{
+	do_cleanup_ipc_objects (MSGMAP_KDDM_ID);
+	do_cleanup_ipc_objects (SEMMAP_KDDM_ID);
+	do_cleanup_ipc_objects (SHMMAP_KDDM_ID);
+}
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              INITIALIZATION                               */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+static int ipc_procfs_start(void)
+{
+	int r;
+	int err = -EINVAL;
+
+	r = register_proc_service(KSYS_IPC_MSGQ_CHKPT, proc_msgq_chkpt);
+	if (r != 0)
+		goto err;
+
+	r = register_proc_service(KSYS_IPC_MSGQ_RESTART, proc_msgq_restart);
+	if (r != 0)
+		goto unreg_msgq_chkpt;
+
+	r = register_proc_service(KSYS_IPC_SEM_CHKPT, proc_sem_chkpt);
+	if (r != 0)
+		goto unreg_msgq_restart;
+
+	r = register_proc_service(KSYS_IPC_SEM_RESTART, proc_sem_restart);
+	if (r != 0)
+		goto unreg_sem_chkpt;
+
+	r = register_proc_service(KSYS_IPC_SHM_CHKPT, proc_shm_chkpt);
+	if (r != 0)
+		goto unreg_sem_restart;
+
+	r = register_proc_service(KSYS_IPC_SHM_RESTART, proc_shm_restart);
+	if (r != 0)
+		goto unreg_shm_chkpt;
+
+	return 0;
+
+unreg_shm_chkpt:
+	unregister_proc_service(KSYS_IPC_SHM_CHKPT);
+unreg_sem_restart:
+	unregister_proc_service(KSYS_IPC_SEM_RESTART);
+unreg_sem_chkpt:
+	unregister_proc_service(KSYS_IPC_SEM_CHKPT);
+unreg_msgq_restart:
+	unregister_proc_service(KSYS_IPC_MSGQ_RESTART);
+unreg_msgq_chkpt:
+	unregister_proc_service(KSYS_IPC_MSGQ_CHKPT);
+err:
+	return err;
+}
+
+void ipc_procfs_exit(void)
+{
+	unregister_proc_service(KSYS_IPC_MSGQ_CHKPT);
+	unregister_proc_service(KSYS_IPC_MSGQ_RESTART);
+	unregister_proc_service(KSYS_IPC_SEM_CHKPT);
+	unregister_proc_service(KSYS_IPC_SEM_RESTART);
+	unregister_proc_service(KSYS_IPC_SHM_CHKPT);
+	unregister_proc_service(KSYS_IPC_SHM_RESTART);
+}
+
+void ipc_handler_init(void)
+{
+	ipc_procfs_start();
+}
+
+void ipc_handler_finalize(void)
+{
+	ipc_procfs_exit();
+}
+
+#endif
diff -ruN linux-2.6.29/ipc/ipc_handler.h android_cluster/linux-2.6.29/ipc/ipc_handler.h
--- linux-2.6.29/ipc/ipc_handler.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/ipc_handler.h	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,24 @@
+/** Interface of IPC management.
+ *  @file ipc_handler.h
+ *
+ *  @author Renaud Lottiaux, Matthieu Fertré
+ */
+
+
+#ifndef IPC_HANDLER_H
+#define IPC_HANDLER_H
+
+#include <linux/ipc.h>
+#include <linux/ipc_namespace.h>
+
+int krg_ipc_get_maxid(struct ipc_ids *ids);
+int krg_ipc_get_new_id(struct ipc_ids *ids);
+void krg_ipc_rmid(struct ipc_ids *ids, int index);
+int krg_ipc_get_this_id(struct ipc_ids *ids, int id);
+
+struct ipc_namespace *find_get_krg_ipcns(void);
+
+void ipc_handler_finalize (void);
+void ipc_handler_init (void);
+
+#endif // IPC_HANDLER_H
diff -ruN linux-2.6.29/ipc/ipcmap_io_linker.c android_cluster/linux-2.6.29/ipc/ipcmap_io_linker.c
--- linux-2.6.29/ipc/ipcmap_io_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/ipcmap_io_linker.c	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,87 @@
+/** KDDM IPC allocation bitmap Linker.
+ *  @file ipcmap_io_linker.c
+ *
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#define MODULE_NAME "IPC map linker  "
+#include <kddm/kddm.h>
+#include "ipcmap_io_linker.h"
+
+struct kmem_cache *ipcmap_object_cachep;
+
+/*****************************************************************************/
+/*                                                                           */
+/*                           SHMID KDDM IO FUNCTIONS                         */
+/*                                                                           */
+/*****************************************************************************/
+
+int ipcmap_alloc_object (struct kddm_obj * obj_entry,
+			 struct kddm_set * set,
+			 objid_t objid)
+{
+	obj_entry->object = kmem_cache_alloc(ipcmap_object_cachep, GFP_KERNEL);
+	if (obj_entry->object == NULL)
+		return -ENOMEM;
+	return 0;
+}
+
+int ipcmap_remove_object (void *object,
+			  struct kddm_set * set,
+			  objid_t objid)
+{
+	kmem_cache_free (ipcmap_object_cachep, object);
+	return 0;
+}
+
+/** First touch a kddm ipcmap object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  obj_entr  Descriptor of the object to invalidate.
+ *  @param  set       KDDM descriptor
+ *  @param  objid     Id of the object to invalidate
+ */
+int ipcmap_first_touch_object (struct kddm_obj * obj_entry,
+			       struct kddm_set * set,
+			       objid_t objid,
+			       int flags)
+{
+	ipcmap_object_t *info;
+
+	info = kmem_cache_alloc(ipcmap_object_cachep, GFP_KERNEL);
+	if (!info)
+		return -ENOMEM;
+
+	info->alloc_map = 0;
+
+	obj_entry->object = info;
+	return 0;
+}
+
+/** Invalidate a KDDM ipcmap object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  obj_entry  Descriptor of the object to invalidate.
+ *  @param  set        KDDM descriptor
+ *  @param  objid      Id of the object to invalidate
+ */
+int ipcmap_invalidate_object (struct kddm_obj * obj_entry,
+			      struct kddm_set * set,
+			      objid_t objid)
+{
+	kmem_cache_free (ipcmap_object_cachep, obj_entry->object);
+	return 0;
+}
+
+/****************************************************************************/
+
+/* Init the shm info IO linker */
+
+struct iolinker_struct ipcmap_linker = {
+	first_touch:       ipcmap_first_touch_object,
+	alloc_object:      ipcmap_alloc_object,
+	remove_object:     ipcmap_remove_object,
+	invalidate_object: ipcmap_invalidate_object,
+	linker_name:       "ipcmap",
+	linker_id:         IPCMAP_LINKER,
+};
diff -ruN linux-2.6.29/ipc/ipcmap_io_linker.h android_cluster/linux-2.6.29/ipc/ipcmap_io_linker.h
--- linux-2.6.29/ipc/ipcmap_io_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/ipcmap_io_linker.h	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,30 @@
+/** KDDM IPC allocation bitmap Linker.
+ *  @file ipcmap_io_linker.h
+ *
+ *  Manage a distributed allocation bitmap for IPC ids.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __IPCMAP_IO_LINKER__
+#define __IPCMAP_IO_LINKER__
+
+extern struct kmem_cache *ipcmap_object_cachep;
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+typedef struct ipcmap_object {
+	unsigned long alloc_map;
+} ipcmap_object_t;
+
+
+extern struct iolinker_struct ipcmap_linker;
+
+
+#endif
diff -ruN linux-2.6.29/ipc/krgipc_mobility.c android_cluster/linux-2.6.29/ipc/krgipc_mobility.c
--- linux-2.6.29/ipc/krgipc_mobility.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/krgipc_mobility.c	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,1353 @@
+/*
+ *  Kerrighed/modules/ipc/mobility.c
+ *
+ *  Copyright (C) 2007 Louis Rilling - Kerlabs
+ *  Copyright (C) 2007-2008 Matthieu Fertré - INRIA
+ */
+
+#include <linux/sched.h>
+#include <linux/nsproxy.h>
+//#include <linux/ima.h>
+#include <linux/ipc.h>
+#include <linux/ipc_namespace.h>
+#include <linux/file.h>
+#include <linux/hugetlb.h>
+#include <linux/msg.h>
+#include <linux/security.h>
+#include <linux/sem.h>
+#include <linux/shm.h>
+#include <linux/syscalls.h>
+#include <linux/unique_id.h>
+#include <net/krgrpc/rpc.h>
+#include <net/krgrpc/rpcid.h>
+#include <kddm/kddm.h>
+#include <kerrighed/namespace.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/ghost_helpers.h>
+#include <kerrighed/action.h>
+#include <kerrighed/application.h>
+#include <kerrighed/app_shared.h>
+#include <kerrighed/faf.h>
+#include <kerrighed/faf_file_mgr.h>
+#include <kerrighed/file.h>
+#include <kerrighed/regular_file_mgr.h>
+#include <kerrighed/pid.h>
+#include "ipc_handler.h"
+#include "krgipc_mobility.h"
+#include "krgshm.h"
+#include "krgmsg.h"
+#include "krgsem.h"
+#include "msg_handler.h"
+#include "sem_handler.h"
+#include "semundolst_io_linker.h"
+
+extern struct kddm_set *sem_undo_list_kddm_set;
+
+/** Return a kerrighed descriptor corresponding to the given file.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file       The file to get a Kerrighed descriptor for.
+ *  @param desc       The returned descriptor.
+ *  @param desc_size  Size of the returned descriptor.
+ *
+ *  @return   0 if everything ok.
+ *            Negative value otherwise.
+ */
+int get_shm_file_krg_desc (struct file *file,
+			   void **desc,
+			   int *desc_size)
+{
+	struct regular_file_krg_desc *data;
+	int size, r = -ENOENT;
+
+	size = sizeof(struct regular_file_krg_desc);
+
+	data = kmalloc (size, GFP_KERNEL);
+	if (!data) {
+		r = -ENOMEM;
+		goto exit;
+	}
+
+	data->type = SHM;
+	data->shm.shmid = file->f_dentry->d_inode->i_ino;
+	data->shm.f_mode = file->f_mode;
+	*desc = data;
+	*desc_size = size;
+
+	r = 0;
+exit:
+	return r;
+}
+
+struct file *reopen_shm_file_entry_from_krg_desc(struct task_struct *task,
+						 void *_desc)
+{
+	int shmid;
+	int err = 0;
+	struct shmid_kernel *shp;
+	struct shm_file_data *sfd;
+	struct file *file = NULL;
+	struct regular_file_krg_desc *desc = _desc;
+	struct ipc_namespace *ns;
+	struct path path;
+
+	BUG_ON (!task);
+	BUG_ON (!desc);
+
+	ns = find_get_krg_ipcns();
+	BUG_ON(!ns);
+
+	shmid = desc->shm.shmid;
+
+	down_read(&shm_ids(ns).rw_mutex);
+	shp = shm_lock_check(ns, shmid);
+	if (IS_ERR(shp)) {
+		err = PTR_ERR(shp);
+		up_read(&shm_ids(ns).rw_mutex);
+		goto out;
+	}
+
+	path.dentry = dget(shp->shm_file->f_path.dentry);
+	path.mnt    = shp->shm_file->f_path.mnt;
+	shm_unlock(shp);
+	up_read(&shm_ids(ns).rw_mutex);
+
+	sfd = kzalloc(sizeof(*sfd), GFP_KERNEL);
+	if (!sfd) {
+		err = -ENOMEM;
+		goto out_put_dentry;
+	}
+
+	file = alloc_file(path.mnt, path.dentry, desc->shm.f_mode,
+			  &shm_file_operations);
+	if (!file) {
+		err = -ENOMEM;
+		goto out_free;
+	}
+//	ima_shm_check(file);
+
+	file->private_data = sfd;
+	file->f_mapping = shp->shm_file->f_mapping;
+
+	sfd->id = shp->shm_perm.id;
+	sfd->ns = get_ipc_ns(ns);
+	sfd->file = shp->shm_file;
+	sfd->file->private_data = sfd;
+	sfd->vm_ops = &krg_shmem_vm_ops;
+out:
+	put_ipc_ns(ns);
+
+	if (err)
+		file = ERR_PTR(err);
+
+	return file;
+
+out_free:
+	kfree(sfd);
+out_put_dentry:
+	dput(path.dentry);
+
+	down_write(&shm_ids(ns).rw_mutex);
+	shp = shm_lock(ns, shmid);
+	BUG_ON(IS_ERR(shp));
+	shp->shm_nattch--;
+	if (shp->shm_nattch == 0 &&
+	    shp->shm_perm.mode & SHM_DEST)
+		krg_ipc_shm_destroy(ns, shp);
+	else
+		shm_unlock(shp);
+	up_write(&shm_ids(ns).rw_mutex);
+
+	goto out;
+}
+
+int export_ipc_namespace(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task)
+{
+	if (task->nsproxy->ipc_ns != task->nsproxy->krg_ns->root_nsproxy.ipc_ns)
+		return -EPERM;
+
+	return 0;
+}
+
+int import_ipc_namespace(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task)
+{
+	task->nsproxy->ipc_ns = find_get_krg_ipcns();
+	BUG_ON(!task->nsproxy->ipc_ns);
+
+	return 0;
+}
+
+void unimport_ipc_namespace(struct task_struct *task)
+{
+	put_ipc_ns(task->nsproxy->ipc_ns);
+}
+
+
+static int cr_export_semundos(ghost_t *ghost, struct task_struct *task)
+{
+	int r = 0;
+	struct semundo_list_object *undo_list;
+	struct kddm_set *undo_list_kddm_set;
+	struct semundo_id *undo_id;
+	long nb_semundo;
+
+	if (task->sysvsem.undo_list_id == UNIQUE_ID_NONE)
+		goto exit;
+
+	undo_list_kddm_set = task_undolist_set(task);
+	if (IS_ERR(undo_list_kddm_set)) {
+		r = PTR_ERR(undo_list_kddm_set);
+		goto exit;
+	}
+
+	/* get the list of semaphores for which we have a semundo */
+	undo_list = _kddm_grab_object_no_ft(undo_list_kddm_set,
+					    task->sysvsem.undo_list_id);
+	if (!undo_list) {
+		r = -ENOMEM;
+		goto exit_put;
+	}
+
+	nb_semundo = 0;
+	for (undo_id = undo_list->list; undo_id; undo_id = undo_id->next)
+		nb_semundo++;
+
+	r = ghost_write(ghost, &nb_semundo, sizeof(long));
+	if (r)
+		goto exit_put;
+
+	for (undo_id = undo_list->list; undo_id; undo_id = undo_id->next) {
+
+		struct ipc_namespace *ns = task_nsproxy(task)->ipc_ns;
+		struct sem_undo *undo;
+		struct sem_array *sma = sem_lock(ns, undo_id->semid);
+
+		if (IS_ERR(sma)) {
+			BUG();
+			r = PTR_ERR(sma);
+			goto exit_put;
+		}
+
+		list_for_each_entry(undo, &sma->list_id, list_id) {
+			if (undo->proc_list_id == task->sysvsem.undo_list_id) {
+				int size;
+
+				r = ghost_write(ghost,
+						&sma->sem_perm.id, sizeof(int));
+				if (r)
+					goto exit_put;
+
+				size = sizeof(struct sem_undo) +
+					sma->sem_nsems * sizeof(short);
+				r = ghost_write(ghost, &size, sizeof(int));
+				if (r)
+					goto exit_put;
+
+				r = ghost_write(ghost, undo, size);
+				if (r)
+					goto exit_put;
+
+				goto next_sma;
+			}
+		}
+		BUG();
+	next_sma:
+		sem_unlock(sma);
+	}
+
+exit_put:
+	 _kddm_put_object(undo_list_kddm_set, task->sysvsem.undo_list_id);
+exit:
+	return r;
+}
+
+static int cr_export_later_sysv_sem(struct epm_action *action,
+				    ghost_t *ghost,
+				    struct task_struct *task)
+{
+	int r;
+	long key;
+
+	BUG_ON(action->type != EPM_CHECKPOINT);
+	BUG_ON(action->checkpoint.shared != CR_SAVE_LATER);
+
+	/* if this is not true anymore, it's time to change
+	   this implementation ... */
+	BUG_ON(sizeof(unique_id_t) != sizeof(long));
+
+	key = (long)(task->sysvsem.undo_list_id);
+
+	r = ghost_write(ghost, &key, sizeof(long));
+	if (r)
+		goto err;
+
+	if (task->sysvsem.undo_list_id != UNIQUE_ID_NONE) {
+		r = add_to_shared_objects_list(task->application, SEMUNDO_LIST,
+					       key, SHARED_ANY, task,
+					       NULL, 0);
+
+		if (r == -ENOKEY) /* the semundo list was already in the list */
+			r = 0;
+	}
+err:
+	return r;
+}
+
+int export_sysv_sem(struct epm_action *action,
+		    ghost_t *ghost, struct task_struct *task)
+{
+	int r;
+	unique_id_t undo_list_id = UNIQUE_ID_NONE;
+
+	BUG_ON(task->sysvsem.undo_list);
+
+	if (action->type == EPM_CHECKPOINT) {
+		BUG_ON(action->checkpoint.shared != CR_SAVE_LATER);
+		r = cr_export_later_sysv_sem(action, ghost, task);
+		return r;
+	}
+
+	/* lazy creation of semundo list:
+	   - nothing to do on migration
+	   - nothing to do on remote fork if CLONE_SYSVSEM is not set
+	   - need to create it if CLONE_SYSVSEM and still not created */
+	if (task->sysvsem.undo_list_id == UNIQUE_ID_NONE
+	    && action->type == EPM_REMOTE_CLONE
+	    && (action->remote_clone.clone_flags & CLONE_SYSVSEM)) {
+		r = create_semundo_proc_list(task);
+		if (r)
+			goto err;
+	}
+
+	/* does the remote process will use our undo_list ? */
+	if (action->type == EPM_MIGRATE
+	    || (action->type == EPM_REMOTE_CLONE
+		&& (action->remote_clone.clone_flags & CLONE_SYSVSEM)))
+		undo_list_id = task->sysvsem.undo_list_id;
+
+	r = ghost_write(ghost, &undo_list_id, sizeof(unique_id_t));
+
+err:
+	return r;
+}
+
+static int cr_import_one_semundo(ghost_t *ghost, struct task_struct *task,
+				 struct semundo_list_object *undo_list)
+{
+	int size, semid, r = 0;
+	struct sem_array *sma;
+	struct sem_undo *undo;
+
+	struct ipc_namespace *ns = task_nsproxy(task)->ipc_ns;
+
+	r = ghost_read(ghost, &semid, sizeof(int));
+	if (r)
+		goto end;
+
+	sma = sem_lock_check(ns, semid);
+	if (IS_ERR(sma)) {
+		r = PTR_ERR(sma);
+		goto end;
+	}
+
+	r = ghost_read(ghost, &size, sizeof(int));
+	if (r)
+		goto unlock_sma;
+
+	if (size != sizeof(struct sem_undo) + sma->sem_nsems * sizeof(short)) {
+		printk("This is not the good semaphore... no way to restart\n");
+		r = -EFAULT;
+		goto unlock_sma;
+	}
+
+	undo = kzalloc(size, GFP_KERNEL);
+	if (!undo)
+		goto unlock_sma;
+
+	r = ghost_read(ghost, undo, size);
+	if (r)
+		goto free_undo;
+
+	INIT_LIST_HEAD(&undo->list_proc); /* list_proc is useless!*/
+	undo->proc_list_id = undo_list->id; /* id may have changed */
+	undo->semadj = (short *) &undo[1];
+
+	r = add_semundo_to_proc_list(undo_list, sma->sem_perm.id);
+	if (r)
+		goto free_undo;
+
+	list_add(&undo->list_id, &sma->list_id);
+
+unlock_sma:
+	sem_unlock(sma);
+end:
+	return r;
+
+free_undo:
+	kfree(undo);
+	goto unlock_sma;
+}
+
+static int cr_import_semundos(ghost_t *ghost, struct task_struct *task)
+{
+	int r = 0;
+	struct kddm_set *undo_list_set;
+	struct semundo_list_object *undo_list;
+	long i, nb_semundo;
+
+	if (task->sysvsem.undo_list_id == UNIQUE_ID_NONE)
+		goto err;
+
+	undo_list_set = task_undolist_set(task);
+	if (IS_ERR(undo_list_set)) {
+		r = PTR_ERR(undo_list_set);
+		goto err;
+	}
+
+	/* get the list of semaphores for which we have a semundo */
+	undo_list = _kddm_grab_object_no_ft(undo_list_set,
+					    task->sysvsem.undo_list_id);
+	if (!undo_list) {
+		r = -ENOMEM;
+		goto exit_put;
+	}
+
+	r = ghost_read(ghost, &nb_semundo, sizeof(long));
+	if (r || !nb_semundo)
+		goto exit_put;
+
+	for (i = 0; i < nb_semundo; i++) {
+		r = cr_import_one_semundo(ghost, task, undo_list);
+		if (r)
+			goto unimport_semundos;
+	}
+
+exit_put:
+	_kddm_put_object(undo_list_set, task->sysvsem.undo_list_id);
+err:
+	return r;
+
+unimport_semundos:
+	_kddm_remove_frozen_object(undo_list_set, task->sysvsem.undo_list_id);
+	goto err;
+}
+
+static int cr_link_to_sysv_sem(struct epm_action *action,
+			       ghost_t *ghost,
+			       struct task_struct *task)
+{
+	int r;
+	long key;
+	unique_id_t undo_list_id;
+
+	r = ghost_read(ghost, &key, sizeof(long));
+	if (r)
+		goto err;
+
+	if ((unique_id_t)key == UNIQUE_ID_NONE) {
+		task->sysvsem.undo_list_id = UNIQUE_ID_NONE;
+	} else {
+		undo_list_id = (unique_id_t)get_imported_shared_object(
+			action->restart.app,
+			SEMUNDO_LIST, key);
+
+		BUG_ON(undo_list_id == UNIQUE_ID_NONE);
+		r = share_existing_semundo_proc_list(task, undo_list_id);
+	}
+err:
+	return r;
+}
+
+int import_sysv_sem(struct epm_action *action,
+		    ghost_t *ghost, struct task_struct *task)
+{
+	int r;
+	unique_id_t undo_list_id;
+
+	if (action->type == EPM_CHECKPOINT) {
+		BUG_ON(action->restart.shared != CR_LINK_ONLY);
+		r = cr_link_to_sysv_sem(action, ghost, task);
+		return r;
+	}
+
+	task->sysvsem.undo_list_id = UNIQUE_ID_NONE;
+	/*BUG_ON(task->sysvsem.undo_list);*/
+
+	r = ghost_read(ghost, &undo_list_id, sizeof(unique_id_t));
+	if (r)
+		goto err;
+
+	if (undo_list_id == UNIQUE_ID_NONE)
+		goto err;
+
+	r = share_existing_semundo_proc_list(task, undo_list_id);
+
+err:
+	return r;
+}
+
+void unimport_sysv_sem(struct task_struct *task)
+{
+	task->sysvsem.undo_list_id = UNIQUE_ID_NONE;
+}
+
+static int cr_export_now_sysv_sem(struct epm_action *action, ghost_t *ghost,
+				  struct task_struct *task,
+				  union export_args *args)
+{
+	int r;
+
+	unique_id_t undo_list_id = task->sysvsem.undo_list_id;
+
+	r = ghost_write(ghost, &undo_list_id, sizeof(unique_id_t));
+	if (r)
+		goto err;
+
+	r = cr_export_semundos(ghost, task);
+err:
+	if (r)
+		ckpt_err(action, r,
+			 "Fail to save semundos of process %d (%s)",
+			 task_pid_knr(task), task->comm);
+
+	return r;
+}
+
+
+static int cr_import_now_sysv_sem(struct epm_action *action, ghost_t *ghost,
+				  struct task_struct *fake, int local_only,
+				  void ** returned_data, size_t *data_size)
+{
+	int r;
+	unique_id_t undo_list_id;
+
+	BUG_ON(*returned_data != NULL);
+
+	r = ghost_read(ghost, &undo_list_id, sizeof(unique_id_t));
+	if (r)
+		goto err;
+
+	if (undo_list_id == UNIQUE_ID_NONE)
+		goto err;
+
+	fake->sysvsem.undo_list = NULL; /* fake task_struct ... */
+	fake->sysvsem.undo_list_id = UNIQUE_ID_NONE;
+
+	r = create_semundo_proc_list(fake);
+	if (r)
+		goto err;
+
+	r = cr_import_semundos(ghost, fake);
+	if (r)
+		goto err;
+
+	*returned_data = (void*)fake->sysvsem.undo_list_id;
+err:
+	if (r)
+		ckpt_err(action, r,
+			 "App %ld - Fail to restore semundos",
+			 action->restart.app->app_id);
+	return r;
+}
+
+static int cr_import_complete_sysv_sem(struct task_struct * fake,
+				       void * _undo_list_id)
+{
+	unique_id_t undo_list_id = (unique_id_t)_undo_list_id;
+
+	fake->sysvsem.undo_list = NULL;
+	fake->sysvsem.undo_list_id = undo_list_id;
+
+	exit_sem(fake);
+
+	return 0;
+}
+
+static int cr_delete_sysv_sem(struct task_struct * fake, void * _undo_list_id)
+{
+	unique_id_t undo_list_id = (unique_id_t)_undo_list_id;
+
+	fake->sysvsem.undo_list = NULL;
+	fake->sysvsem.undo_list_id = undo_list_id;
+
+	exit_sem(fake);
+
+	return 0;
+}
+
+struct shared_object_operations cr_shared_semundo_ops = {
+        .export_now        = cr_export_now_sysv_sem,
+	.export_user_info  = NULL,
+	.import_now        = cr_import_now_sysv_sem,
+	.import_complete   = cr_import_complete_sysv_sem,
+	.delete            = cr_delete_sysv_sem,
+};
+
+/******************************************************************************/
+
+static const int CR_MSG_MAGIC = 0x33333311;
+
+/* mostly copy/paste from store_msg */
+static int export_full_one_msg(ghost_t *ghost, struct msg_msg *msg)
+{
+	int r = 0;
+
+	int alen;
+	int len;
+	struct msg_msgseg *seg;
+
+	r = ghost_write(ghost, &msg->m_ts, sizeof(int));
+	if (r)
+		goto out;
+
+	r = ghost_write(ghost, msg, sizeof(struct msg_msg));
+	if (r)
+		goto out;
+
+	len = msg->m_ts;
+	alen = len;
+	if (alen > DATALEN_MSG)
+		alen = DATALEN_MSG;
+
+	r = ghost_write(ghost, msg + 1, alen);
+	if (r)
+		goto out;
+
+	len -= alen;
+	seg = msg->next;
+	while (len > 0) {
+		alen = len;
+		if (alen > DATALEN_SEG)
+			alen = DATALEN_SEG;
+
+		r = ghost_write(ghost, seg + 1, alen);
+		if (r)
+			goto out;
+
+		len -= alen;
+		seg = seg->next;
+	}
+out:
+	return r;
+}
+
+static int export_full_all_msgs(ghost_t * ghost, struct msg_queue *msq)
+{
+	int r = 0;
+	struct msg_msg *msg;
+
+	r = ghost_write(ghost, &msq->q_qnum, sizeof(unsigned long));
+	if (r)
+		goto out;
+
+	list_for_each_entry(msg, &msq->q_messages, m_list) {
+		r = export_full_one_msg(ghost, msg);
+		if (r)
+			goto out;
+	}
+out:
+	return r;
+}
+
+static int export_full_local_sysv_msgq(ghost_t *ghost, int msgid)
+{
+	int r = 0;
+	struct msg_queue *msq;
+	struct ipc_namespace *ns;
+
+	ns = find_get_krg_ipcns();
+	if (!ns)
+		return -ENOSYS;
+
+	msq = msg_lock(ns, msgid);
+	if (IS_ERR(msq)) {
+		r = PTR_ERR(msq);
+		goto out;
+	}
+
+	if (!msq->is_master) {
+		r = -EPERM;
+		goto out_unlock;
+	}
+
+	r = ghost_write(ghost, &CR_MSG_MAGIC, sizeof(int));
+	if (r)
+		goto out_unlock;
+
+	r = ghost_write(ghost, msq, sizeof(struct msg_queue));
+	if (r)
+		goto out_unlock;
+
+	r = export_full_all_msgs(ghost, msq);
+
+out_unlock:
+	msg_unlock(msq);
+out:
+	put_ipc_ns(ns);
+	return r;
+}
+
+static int local_sys_msgq_checkpoint(int msqid, int fd)
+{
+	int r;
+	ghost_fs_t oldfs;
+	ghost_t *ghost;
+
+	__set_ghost_fs(&oldfs);
+
+	ghost = create_file_ghost_from_fd(GHOST_WRITE, fd);
+
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		goto exit;
+	}
+
+	r = export_full_local_sysv_msgq(ghost, msqid);
+
+	ghost_close(ghost);
+exit:
+	unset_ghost_fs(&oldfs);
+	return r;
+}
+
+static int receive_fd_from_network(struct rpc_desc *desc)
+{
+	int r, fd;
+	struct file *file;
+
+	fd = get_unused_fd();
+	if (fd < 0) {
+		r = fd;
+		goto out;
+	}
+
+	file = rcv_faf_file_desc(desc);
+	if (IS_ERR(file)) {
+		r = PTR_ERR(file);
+		goto out_put_fd;
+	}
+
+	fd_install(fd, file);
+
+	r = fd;
+
+out:
+	return r;
+
+out_put_fd:
+	put_unused_fd(fd);
+	goto out;
+}
+
+struct msgq_checkpoint_msg
+{
+	int msqid;
+};
+
+void handle_msg_checkpoint(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	int r, fd;
+	struct msgq_checkpoint_msg *msg = _msg;
+
+	fd = receive_fd_from_network(desc);
+	if (fd < 0) {
+		r = fd;
+		goto error;
+	}
+
+	r = __sys_msgq_checkpoint(msg->msqid, fd);
+
+	sys_close (fd);
+
+error:
+	r = rpc_pack_type(desc, r);
+	if (r)
+		rpc_cancel(desc);
+}
+
+int __sys_msgq_checkpoint(int msqid, int fd)
+{
+	int r, index, err;
+	struct msgq_checkpoint_msg msg;
+	struct kddm_set *master_set;
+	kerrighed_node_t *master_node;
+	struct ipc_namespace *ns;
+	struct file *file;
+	struct rpc_desc *desc;
+
+	ns = find_get_krg_ipcns();
+
+	index = ipcid_to_idx(msqid);
+
+	master_set = krgipc_ops_master_set(msg_ids(ns).krgops);
+
+	master_node = _kddm_get_object_no_ft(master_set, index);
+	if (!master_node) {
+		_kddm_put_object(master_set, index);
+		r = -EINVAL;
+		goto out;
+	}
+
+	if (*master_node == kerrighed_node_id) {
+		_kddm_put_object(master_set, index);
+		r = local_sys_msgq_checkpoint(msqid, fd);
+		goto out;
+	}
+
+	file = fget(fd);
+
+	desc = rpc_begin(IPC_MSG_CHKPT, *master_node);
+	_kddm_put_object(master_set, index);
+
+	msg.msqid = msqid;
+	r = rpc_pack_type(desc, msg);
+	if (r)
+		goto err_rpc;
+
+	r = send_faf_file_desc(desc, file);
+	if (r)
+		goto err_rpc;
+
+	r = rpc_unpack_type(desc, err);
+	if (r)
+		goto err_rpc;
+
+	r = err;
+
+out_put_file:
+	fput(file);
+out:
+	put_ipc_ns(ns);
+	return r;
+
+err_rpc:
+	rpc_cancel(desc);
+	goto out_put_file;
+}
+
+/* mostly copy/paste from load_msg */
+static struct msg_msg *import_full_one_msg(ghost_t * ghost)
+{
+	struct msg_msg *msg = NULL;
+	struct msg_msgseg **pseg;
+	int r;
+	int len, alen;
+
+	r = ghost_read(ghost, &len, sizeof(int));
+	if (r)
+		goto out_err;
+
+	alen = len;
+	if (alen > DATALEN_MSG)
+		alen = DATALEN_MSG;
+
+	msg = kmalloc(sizeof(*msg) + alen, GFP_KERNEL);
+	if (msg == NULL)
+		return ERR_PTR(-ENOMEM);
+
+	r = ghost_read(ghost, msg, sizeof(struct msg_msg));
+	if (r)
+		goto out_err;
+
+	r = ghost_read(ghost, msg + 1, alen);
+	if (r)
+		goto out_err;
+
+	msg->next = NULL;
+	msg->security = NULL;
+
+	len -= alen;
+	pseg = &msg->next;
+	while (len > 0) {
+		struct msg_msgseg *seg;
+		alen = len;
+		if (alen > DATALEN_SEG)
+			alen = DATALEN_SEG;
+
+		seg = kmalloc(sizeof(*seg) + alen, GFP_KERNEL);
+
+		if (!seg) {
+			r = -ENOMEM;
+			goto out_err;
+		}
+		*pseg = seg;
+		seg->next = NULL;
+
+		r = ghost_read(ghost, seg + 1, alen);
+		if (r)
+			goto out_err;
+
+		pseg = &seg->next;
+		len -= alen;
+	}
+
+	r = security_msg_msg_alloc(msg);
+	if (r)
+		goto out_err;
+
+	return msg;
+
+out_err:
+	free_msg(msg);
+	return ERR_PTR(r);
+}
+
+static int import_full_all_msgs(ghost_t *ghost, struct ipc_namespace *ns,
+				struct msg_queue *msq)
+{
+	int i;
+	int r = 0;
+	unsigned long nbmsg;
+
+	r = ghost_read(ghost, &nbmsg, sizeof(unsigned long));
+	if (r)
+		goto out;
+
+	for (i = 0; i < nbmsg; i++) {
+		struct msg_msg * msg = import_full_one_msg(ghost);
+		if (IS_ERR(msg)) {
+			r = PTR_ERR(msg);
+			goto out;
+		}
+		list_add_tail(&msg->m_list, &msq->q_messages);
+		atomic_inc(&ns->msg_hdrs);
+		atomic_add(msg->m_ts, &ns->msg_bytes);
+		msq->q_qnum++;
+		msq->q_cbytes += msg->m_ts;
+	}
+
+out:
+	return r;
+}
+
+int import_full_sysv_msgq(ghost_t *ghost)
+{
+	int r, magic;
+	struct ipc_namespace *ns;
+	struct msg_queue copy_msq, *msq;
+	struct ipc_params params;
+
+	r = ghost_read(ghost, &magic, sizeof(int));
+	if (r)
+		goto out;
+
+	if (magic != CR_MSG_MAGIC) {
+		r = -EINVAL;
+		goto out;
+	}
+
+	r = ghost_read(ghost, &copy_msq, sizeof(struct msg_queue));
+	if (r)
+		goto out;
+
+	ns = find_get_krg_ipcns();
+	if (!ns)
+		return -ENOSYS;
+
+	down_write(&msg_ids(ns).rw_mutex);
+
+	params.requested_id = copy_msq.q_perm.id;
+	params.key = copy_msq.q_perm.key;
+	params.flg = copy_msq.q_perm.mode;
+
+	r = newque(ns, &params);
+	if (r < 0)
+		goto out_put_ns;
+
+	BUG_ON(r != params.requested_id);
+
+	/* the message queue cannot disappear since we hold the ns mutex */
+	msq = msg_lock(ns, params.requested_id);
+	if (IS_ERR(msq)) {
+		r = PTR_ERR(msq);
+		goto out_put_ns;
+	}
+
+	r = import_full_all_msgs(ghost, ns, msq);
+	if (r)
+		goto out_freeque;
+
+	msq->q_stime = copy_msq.q_stime;
+	msq->q_rtime = copy_msq.q_rtime;
+	msq->q_ctime = copy_msq.q_ctime;
+
+	msq->q_lspid = copy_msq.q_lspid;
+	msq->q_lrpid = copy_msq.q_lrpid;
+
+	msg_unlock(msq);
+out_put_ns:
+	up_write(&msg_ids(ns).rw_mutex);
+
+	put_ipc_ns(ns);
+out:
+	return r;
+
+out_freeque:
+	krg_ipc_msg_freeque(ns, &msq->q_perm);
+	goto out_put_ns;
+}
+
+/******************************************************************************/
+
+static const int CR_SEM_MAGIC = 0x33333322;
+
+int export_full_sysv_sem(ghost_t *ghost, int semid)
+{
+	int r;
+	struct ipc_namespace *ns;
+	struct sem_array *sma;
+
+	ns = find_get_krg_ipcns();
+	if (!ns)
+		return -ENOSYS;
+
+	sma = sem_lock(ns, semid);
+	if (IS_ERR(sma)) {
+		r = PTR_ERR(sma);
+		goto out;
+	}
+
+	r = ghost_write(ghost, &CR_SEM_MAGIC, sizeof(int));
+	if (r)
+		goto out;
+
+	r = ghost_write(ghost, sma, sizeof(struct sem_array));
+	if (r)
+		goto out;
+
+	r = ghost_write(ghost, sma->sem_base, sma->sem_nsems * sizeof(struct sem));
+	if (r)
+		goto out;
+
+	sem_unlock(sma);
+
+out:
+	put_ipc_ns(ns);
+	return r;
+}
+
+int import_full_sysv_sem(ghost_t *ghost)
+{
+	int r, magic;
+	struct ipc_namespace *ns;
+	struct sem_array copy_sma, *sma;
+	struct ipc_params params;
+
+	r = ghost_read(ghost, &magic, sizeof(int));
+	if (r)
+		goto out;
+
+	if (magic != CR_SEM_MAGIC) {
+		r = -EINVAL;
+		goto out;
+	}
+
+	r = ghost_read(ghost, &copy_sma, sizeof(struct sem_array));
+	if (r)
+		goto out;
+
+	ns = find_get_krg_ipcns();
+	if (!ns)
+		return -ENOSYS;
+
+	down_write(&sem_ids(ns).rw_mutex);
+
+	params.requested_id = copy_sma.sem_perm.id;
+	params.key = copy_sma.sem_perm.key;
+	params.flg = copy_sma.sem_perm.mode;
+	params.u.nsems = copy_sma.sem_nsems;
+
+	r = newary(ns, &params);
+	if (r < 0)
+		goto out_put_ns;
+
+	BUG_ON(r != params.requested_id);
+
+	/* the semaphore array cannot disappear since we hold the ns mutex */
+	sma = sem_lock(ns, params.requested_id);
+	if (IS_ERR(sma)) {
+		r = PTR_ERR(sma);
+		goto out_put_ns;
+	}
+
+	r = ghost_read(ghost, sma->sem_base, sma->sem_nsems * sizeof(struct sem));
+	if (r)
+		goto out_freeary;
+
+	sma->sem_otime = copy_sma.sem_otime;
+	sma->sem_ctime = copy_sma.sem_ctime;
+
+	sem_unlock(sma);
+
+out_put_ns:
+	up_write(&sem_ids(ns).rw_mutex);
+
+	put_ipc_ns(ns);
+out:
+	return r;
+
+out_freeary:
+	krg_ipc_sem_freeary(ns, &sma->sem_perm);
+	goto out_put_ns;
+}
+
+/******************************************************************************/
+
+static const int CR_SHM_MAGIC = 0x33333333;
+
+static int export_full_one_shm_page(ghost_t * ghost, struct kddm_set *kset,
+				    unsigned long pageid, unsigned long size)
+{
+	int r;
+	struct page *page;
+	char *data;
+	const int no_page=0, page_used = 1;
+
+	page = _kddm_get_object_no_ft(kset, (objid_t)pageid);
+	if (page) {
+		r = ghost_write(ghost, &page_used, sizeof(int));
+		if (r)
+			goto put_page;
+		data = (char *)kmap(page);
+		r = ghost_write(ghost, data, size);
+		kunmap(page);
+	} else
+		r = ghost_write(ghost, &no_page, sizeof(int));
+
+put_page:
+	_kddm_put_object(kset, (objid_t)pageid);
+	return r;
+}
+
+/* shp must be locked */
+static int export_full_shm_content(ghost_t * ghost, struct ipc_namespace *ns,
+				   struct shmid_kernel **shp)
+{
+	int r = 0;
+	int shmid;
+	struct kddm_set *kset;
+	unsigned long i;
+	unsigned long nb_pages;
+	unsigned long left_size;
+
+	shmid = (*shp)->shm_perm.id;
+
+	nb_pages = (*shp)->shm_segsz / PAGE_SIZE;
+	left_size = (*shp)->shm_segsz % PAGE_SIZE;
+
+	kset = (*shp)->shm_file->f_dentry->d_inode->i_mapping->kddm_set;
+
+	/* to ensure the SHP will stay alive without deadlocking
+	 * with the IO Linker...
+	 */
+	(*shp)->shm_nattch++;
+	shm_unlock(*shp);
+
+	for (i = 0; i < nb_pages && r == 0; i++)
+		r = export_full_one_shm_page(ghost, kset, i, PAGE_SIZE);
+
+	if (r)
+		goto error;
+
+	if (left_size)
+		r = export_full_one_shm_page(ghost, kset, i, left_size);
+
+error:
+	*shp = shm_lock(ns, shmid);
+	BUG_ON(IS_ERR(*shp));
+	(*shp)->shm_nattch--;
+
+	return r;
+}
+
+int export_full_sysv_shm(ghost_t *ghost, int shmid)
+{
+	int r, flag;
+	struct ipc_namespace *ns;
+	struct shmid_kernel *shp;
+
+	ns = find_get_krg_ipcns();
+	if (!ns)
+		return -ENOSYS;
+
+	shp = shm_lock(ns, shmid);
+	if (IS_ERR(shp)) {
+		r = PTR_ERR(shp);
+		goto out;
+	}
+
+	r = ghost_write(ghost, &CR_SHM_MAGIC, sizeof(int));
+	if (r)
+		goto out_shm_unlock;
+
+	r = ghost_write(ghost, shp, sizeof(struct shmid_kernel));
+	if (r)
+		goto out_shm_unlock;
+
+	flag = shp->shm_perm.mode;
+#ifdef CONFIG_HUGETLB_PAGE
+	if (shp->shm_file->f_op == &hugetlbfs_file_operations)
+		flag |= SHM_HUGETLB;
+#endif
+	/* SHM_NORESERVE not handled */
+
+	r = ghost_write(ghost, &flag, sizeof(int));
+	if (r)
+		goto out_shm_unlock;
+
+	r = export_full_shm_content(ghost, ns, &shp);
+
+out_shm_unlock:
+	shm_unlock(shp);
+out:
+	put_ipc_ns(ns);
+	return r;
+}
+
+static int import_full_one_shm_page(ghost_t * ghost, struct kddm_set *kset,
+				    unsigned long pageid,
+				    unsigned long size)
+{
+	int r;
+	struct page *page;
+	char* data;
+	int page_used;
+
+	r = ghost_read(ghost, &page_used, sizeof(page_used));
+	if (r)
+		goto out;
+
+	if (!page_used)
+		goto out;
+
+	/* it should return an existing but empty object */
+	page = _kddm_grab_object(kset, (objid_t)pageid);
+	if (!page)
+		goto put_page;
+
+	data = (char *)kmap(page);
+	r = ghost_read(ghost, data, size);
+	kunmap(page);
+
+put_page:
+	_kddm_put_object(kset, (objid_t)pageid);
+out:
+	return r;
+}
+
+/* shp must be locked */
+static int import_full_shm_content(ghost_t * ghost, struct ipc_namespace *ns,
+				   struct shmid_kernel **shp)
+{
+	int r = 0;
+	int shmid;
+	struct kddm_set *kset;
+	unsigned long nb_pages;
+	unsigned long left_size;
+	unsigned long i;
+
+	shmid = (*shp)->shm_perm.id;
+
+	nb_pages = (*shp)->shm_segsz / PAGE_SIZE;
+	left_size = (*shp)->shm_segsz % PAGE_SIZE;
+
+	kset = (*shp)->shm_file->f_dentry->d_inode->i_mapping->kddm_set;
+
+	/* to ensure the SHP will stay alive without deadlocking
+	 * with the IO Linker...
+	 */
+	(*shp)->shm_nattch++;
+	shm_unlock(*shp);
+
+	for (i = 0; i < nb_pages && r == 0; i++)
+		r = import_full_one_shm_page(ghost, kset, i, PAGE_SIZE);
+
+	if (r)
+		goto error;
+
+	if (left_size)
+		r = import_full_one_shm_page(ghost, kset, i, left_size);
+
+error:
+	*shp = shm_lock(ns, shmid);
+	BUG_ON(IS_ERR(*shp));
+	(*shp)->shm_nattch--;
+
+	return r;
+}
+
+int import_full_sysv_shm(ghost_t *ghost)
+{
+	int r, flag, magic;
+	struct ipc_namespace *ns;
+	struct shmid_kernel copy_shp, *shp;
+	struct ipc_params params;
+
+	r = ghost_read(ghost, &magic, sizeof(int));
+	if (r)
+		goto out;
+
+	if (magic != CR_SHM_MAGIC) {
+		r = -EINVAL;
+		goto out;
+	}
+
+	r = ghost_read(ghost, &copy_shp, sizeof(struct shmid_kernel));
+	if (r)
+		goto out;
+
+	r = ghost_read(ghost, &flag, sizeof(int));
+	if (r)
+		goto out;
+
+	ns = find_get_krg_ipcns();
+	if (!ns)
+		return -ENOSYS;
+
+	down_write(&shm_ids(ns).rw_mutex);
+
+	params.requested_id = copy_shp.shm_perm.id;
+	params.key = copy_shp.shm_perm.key;
+	params.flg = flag;
+	params.u.size = copy_shp.shm_segsz;
+
+	r = newseg(ns, &params);
+	if (r < 0)
+		goto out_put_ns;
+
+	BUG_ON(r != params.requested_id);
+
+	/* the memory segment cannot disappear since we hold the ns mutex */
+	shp = shm_lock(ns, params.requested_id);
+	if (IS_ERR(shp)) {
+		r = PTR_ERR(shp);
+		goto out_put_ns;
+	}
+
+	r = import_full_shm_content(ghost, ns, &shp);
+	if (r)
+		goto out_freeshm;
+
+	shp->shm_atim = copy_shp.shm_atim;
+	shp->shm_dtim = copy_shp.shm_dtim;
+	shp->shm_ctim = copy_shp.shm_ctim;
+	shp->shm_cprid = copy_shp.shm_cprid;
+	shp->shm_lprid = copy_shp.shm_lprid;
+
+	shm_unlock(shp);
+
+out_put_ns:
+	up_write(&shm_ids(ns).rw_mutex);
+
+	put_ipc_ns(ns);
+out:
+	return r;
+out_freeshm:
+	BUG_ON(shp->shm_nattch);
+	krg_ipc_shm_destroy(ns, shp);
+	goto out;
+}
+
+
diff -ruN linux-2.6.29/ipc/krgipc_mobility.h android_cluster/linux-2.6.29/ipc/krgipc_mobility.h
--- linux-2.6.29/ipc/krgipc_mobility.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/krgipc_mobility.h	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,15 @@
+#include <kerrighed/ghost.h>
+
+int __sys_msgq_checkpoint(int msqid, int fd);
+
+void handle_msg_checkpoint(struct rpc_desc *desc, void *_msg, size_t size);
+
+int import_full_sysv_msgq(ghost_t *ghost);
+
+int export_full_sysv_sem(ghost_t *ghost, int semid);
+
+int import_full_sysv_sem(ghost_t *ghost);
+
+int export_full_sysv_shm(ghost_t *ghost, int shmid);
+
+int import_full_sysv_shm(ghost_t *ghost);
diff -ruN linux-2.6.29/ipc/krgmsg.h android_cluster/linux-2.6.29/ipc/krgmsg.h
--- linux-2.6.29/ipc/krgmsg.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/krgmsg.h	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,72 @@
+#ifndef __KKRG_MSG__
+#define __KKRG_MSG__
+
+
+struct msg_msgseg {
+	struct msg_msgseg* next;
+	/* the next part of the message follows immediately */
+};
+
+#define DATALEN_MSG	(PAGE_SIZE-sizeof(struct msg_msg))
+#define DATALEN_SEG	(PAGE_SIZE-sizeof(struct msg_msgseg))
+
+/** Kerrighed Hooks **/
+
+int krg_ipc_msg_newque(struct ipc_namespace *ns,
+		       struct msg_queue *msq);
+
+void krg_ipc_msg_freeque(struct ipc_namespace *ns,
+			 struct kern_ipc_perm *ipcp);
+
+long krg_ipc_msgsnd(int msqid, long mtype, void __user *mtext,
+		    size_t msgsz, int msgflg, struct ipc_namespace *ns,
+		    pid_t tgid);
+
+long krg_ipc_msgrcv(int msqid, long *pmtype, void __user *mtext,
+		    size_t msgsz, long msgtyp, int msgflg,
+		    struct ipc_namespace *ns, pid_t tgid);
+
+
+int newque(struct ipc_namespace *ns, struct ipc_params *params);
+
+static inline struct msg_queue *msg_lock(struct ipc_namespace *ns, int id)
+{
+	struct kern_ipc_perm *ipcp = ipc_lock(&msg_ids(ns), id);
+
+	if (IS_ERR(ipcp))
+		return (struct msg_queue *)ipcp;
+
+	return container_of(ipcp, struct msg_queue, q_perm);
+}
+
+static inline void msg_unlock(struct msg_queue *msq)
+{
+	ipc_unlock(&(msq)->q_perm);
+}
+
+static inline struct msg_queue *local_msg_lock(struct ipc_namespace *ns, int id)
+{
+	struct kern_ipc_perm *ipcp = local_ipc_lock(&msg_ids(ns), id);
+
+	if (IS_ERR(ipcp))
+		return (struct msg_queue *)ipcp;
+
+	return container_of(ipcp, struct msg_queue, q_perm);
+}
+
+static inline void local_msg_unlock(struct msg_queue *msq)
+{
+	local_ipc_unlock(&(msq)->q_perm);
+}
+
+long __do_msgsnd(int msqid, long mtype, void __user *mtext,
+		 size_t msgsz, int msgflg, struct ipc_namespace *ns,
+		 pid_t tgid);
+
+long __do_msgrcv(int msqid, long *pmtype, void __user *mtext,
+		 size_t msgsz, long msgtyp, int msgflg,
+		 struct ipc_namespace *ns, pid_t tgid);
+
+void local_master_freeque(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp);
+
+#endif // __KKRG_MSG__
diff -ruN linux-2.6.29/ipc/krgsem.h android_cluster/linux-2.6.29/ipc/krgsem.h
--- linux-2.6.29/ipc/krgsem.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/krgsem.h	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,58 @@
+#ifndef __KKRG_SEM__
+#define __KKRG_SEM__
+
+#define sc_semmni       sem_ctls[3]
+
+int krg_ipc_sem_newary(struct ipc_namespace *ns, struct sem_array *sma);
+
+void krg_ipc_sem_freeary(struct ipc_namespace *ns,
+			 struct kern_ipc_perm *ipcp);
+
+void krg_ipc_sem_wakeup_process(struct sem_queue *q, int error);
+
+int krg_ipc_sem_copy_semundo(unsigned long clone_flags,
+			     struct task_struct *tsk);
+
+struct sem_undo *krg_ipc_sem_find_undo(struct sem_array* sma);
+
+void krg_ipc_sem_exit_sem(struct ipc_namespace *ns, struct task_struct * tsk);
+
+int newary(struct ipc_namespace *ns, struct ipc_params *params);
+
+struct sem_array *sem_lock(struct ipc_namespace *ns, int id);
+
+struct sem_array *sem_lock_check(struct ipc_namespace *ns, int id);
+
+static inline struct sem_array *local_sem_lock(struct ipc_namespace *ns, int id)
+{
+	struct kern_ipc_perm *ipcp = local_ipc_lock(&sem_ids(ns), id);
+
+	if (IS_ERR(ipcp))
+		return (struct sem_array *)ipcp;
+
+	return container_of(ipcp, struct sem_array, sem_perm);
+}
+
+static inline void sem_unlock(struct sem_array *sma)
+{
+	ipc_unlock(&(sma)->sem_perm);
+}
+
+static inline void local_sem_unlock(struct sem_array *sma)
+{
+	local_ipc_unlock(&(sma)->sem_perm);
+}
+
+/* caller is responsible to call kfree(q->undo) before if needed */
+static inline void free_semqueue(struct sem_queue *q)
+{
+	if (q->sops)
+		kfree(q->sops);
+	kfree(q);
+}
+
+void local_freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp);
+
+void __exit_sem_found(struct sem_array *sma, struct sem_undo *un);
+
+#endif
diff -ruN linux-2.6.29/ipc/krgshm.h android_cluster/linux-2.6.29/ipc/krgshm.h
--- linux-2.6.29/ipc/krgshm.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/krgshm.h	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,62 @@
+#ifndef __KKRG_SHM__
+#define __KKRG_SHM__
+
+#include <linux/init.h>
+#include <linux/ipc.h>
+#include "util.h"
+
+struct vm_area_struct;
+
+
+/** Kerrighed Hooks **/
+int krg_ipc_shm_newseg(struct ipc_namespace *ns, struct shmid_kernel * shp);
+void krg_ipc_shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp);
+void krg_ipc_shm_rmkey(struct ipc_namespace *ns, key_t key);
+
+/** Exported variables  **/
+
+extern struct vm_operations_struct shm_vm_ops;
+extern struct vm_operations_struct krg_shmem_vm_ops;
+
+extern const struct file_operations shm_file_operations;
+
+/** Exported functions  **/
+
+int newseg(struct ipc_namespace *ns, struct ipc_params *params);
+
+struct shm_file_data {
+	int id;
+	struct ipc_namespace *ns;
+	struct file *file;
+	const struct vm_operations_struct *vm_ops;
+};
+
+#define shm_file_data(file) (*((struct shm_file_data **)&(file)->private_data))
+
+static inline struct shmid_kernel* local_shm_lock(struct ipc_namespace *ns,
+						  int id)
+{
+	struct kern_ipc_perm *ipcp = local_ipc_lock(&shm_ids(ns), id);
+
+	if (IS_ERR(ipcp))
+		return (struct shmid_kernel *)ipcp;
+
+	return container_of(ipcp, struct shmid_kernel, shm_perm);
+}
+
+struct shmid_kernel *shm_lock(struct ipc_namespace *ns, int id);
+struct shmid_kernel *shm_lock_check(struct ipc_namespace *ns, int id);
+
+static inline void local_shm_unlock(struct shmid_kernel *shp)
+{
+	local_ipc_unlock(&(shp)->shm_perm);
+}
+
+static inline void shm_unlock(struct shmid_kernel *shp)
+{
+	ipc_unlock(&(shp)->shm_perm);
+}
+
+void local_shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp);
+
+#endif // __KKRG_SHM__
diff -ruN linux-2.6.29/ipc/Makefile android_cluster/linux-2.6.29/ipc/Makefile
--- linux-2.6.29/ipc/Makefile	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/ipc/Makefile	2014-05-27 23:04:10.422028007 -0700
@@ -9,3 +9,5 @@
 obj-$(CONFIG_POSIX_MQUEUE) += mqueue.o msgutil.o $(obj_mq-y)
 obj-$(CONFIG_IPC_NS) += namespace.o
 
+obj-$(CONFIG_KRG_IPC) += ipcmap_io_linker.o ipc_handler.o msg_io_linker.o msg_handler.o semarray_io_linker.o semundolst_io_linker.o sem_handler.o shmid_io_linker.o shm_memory_linker.o shm_handler.o
+obj-$(CONFIG_KRG_IPC_EPM) += krgipc_mobility.o ipc_checkpoint.o
diff -ruN linux-2.6.29/ipc/msg.c android_cluster/linux-2.6.29/ipc/msg.c
--- linux-2.6.29/ipc/msg.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/ipc/msg.c	2014-05-27 23:04:10.422028007 -0700
@@ -42,6 +42,12 @@
 #include <asm/current.h>
 #include <asm/uaccess.h>
 #include "util.h"
+#ifdef CONFIG_KRG_IPC
+#include "krgmsg.h"
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/action.h>
+#endif
+#endif
 
 /*
  * one msg_receiver structure for each sleeping receiver:
@@ -68,12 +74,15 @@
 #define SEARCH_NOTEQUAL		3
 #define SEARCH_LESSEQUAL	4
 
+#ifndef CONFIG_KRG_IPC
 #define msg_ids(ns)	((ns)->ids[IPC_MSG_IDS])
-
 #define msg_unlock(msq)		ipc_unlock(&(msq)->q_perm)
+#endif
 
 static void freeque(struct ipc_namespace *, struct kern_ipc_perm *);
+#ifndef CONFIG_KRG_IPC
 static int newque(struct ipc_namespace *, struct ipc_params *);
+#endif
 #ifdef CONFIG_PROC_FS
 static int sysvipc_msg_proc_show(struct seq_file *s, void *it);
 #endif
@@ -125,6 +134,7 @@
 void msg_exit_ns(struct ipc_namespace *ns)
 {
 	free_ipcs(ns, &msg_ids(ns), freeque);
+	idr_destroy(&ns->ids[IPC_MSG_IDS].ipcs_idr);
 }
 #endif
 
@@ -140,6 +150,7 @@
 				IPC_MSG_IDS, sysvipc_msg_proc_show);
 }
 
+#ifndef CONFIG_KRG_IPC
 /*
  * msg_lock_(check_) routines are called in the paths where the rw_mutex
  * is not held.
@@ -153,6 +164,7 @@
 
 	return container_of(ipcp, struct msg_queue, q_perm);
 }
+#endif
 
 static inline struct msg_queue *msg_lock_check(struct ipc_namespace *ns,
 						int id)
@@ -177,7 +189,10 @@
  *
  * Called with msg_ids.rw_mutex held (writer)
  */
-static int newque(struct ipc_namespace *ns, struct ipc_params *params)
+#ifndef CONFIG_KRG_IPC
+static
+#endif
+int newque(struct ipc_namespace *ns, struct ipc_params *params)
 {
 	struct msg_queue *msq;
 	int id, retval;
@@ -201,7 +216,12 @@
 	/*
 	 * ipc_addid() locks msq
 	 */
+#ifdef CONFIG_KRG_IPC
+	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni,
+		       params->requested_id);
+#else
 	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
+#endif
 	if (id < 0) {
 		security_msg_queue_free(msq);
 		ipc_rcu_putref(msq);
@@ -217,6 +237,21 @@
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
+#ifdef CONFIG_KRG_IPC
+	if (is_krg_ipc(&msg_ids(ns))) {
+		retval = krg_ipc_msg_newque(ns, msq) ;
+		if (retval) {
+			/* release locks held by ipc_addid */
+			local_ipc_unlock(&msq->q_perm);
+
+			security_msg_queue_free(msq);
+			ipc_rcu_putref(msq);
+			return retval;
+		}
+	} else
+		msq->q_perm.krgops = NULL;
+#endif
+
 	msg_unlock(msq);
 
 	return msq->q_perm.id;
@@ -276,15 +311,31 @@
  * msg_ids.rw_mutex (writer) and the spinlock for this message queue are held
  * before freeque() is called. msg_ids.rw_mutex remains locked on exit.
  */
+#ifdef CONFIG_KRG_IPC
 static void freeque(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)
 {
+	if (is_krg_ipc(&msg_ids(ns)))
+		krg_ipc_msg_freeque(ns, ipcp);
+	else
+		local_master_freeque(ns, ipcp);
+}
+
+void local_master_freeque(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)
+#else
+static void freeque(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)
+#endif
+{
 	struct list_head *tmp;
 	struct msg_queue *msq = container_of(ipcp, struct msg_queue, q_perm);
 
 	expunge_all(msq, -EIDRM);
 	ss_wakeup(&msq->q_senders, 1);
 	msg_rmid(ns, msq);
+#ifdef CONFIG_KRG_IPC
+	local_msg_unlock(msq);
+#else
 	msg_unlock(msq);
+#endif
 
 	tmp = msq->q_messages.next;
 	while (tmp != &msq->q_messages) {
@@ -527,15 +578,32 @@
 		if (!buf)
 			return -EFAULT;
 
+#ifdef CONFIG_KRG_IPC
+		down_read(&msg_ids(ns).rw_mutex);
+#endif
 		if (cmd == MSG_STAT) {
 			msq = msg_lock(ns, msqid);
 			if (IS_ERR(msq))
+#ifdef CONFIG_KRG_IPC
+			{
+				up_read(&msg_ids(ns).rw_mutex);
+				return PTR_ERR(msq);
+			}
+#else
 				return PTR_ERR(msq);
+#endif
 			success_return = msq->q_perm.id;
 		} else {
 			msq = msg_lock_check(ns, msqid);
 			if (IS_ERR(msq))
+#ifdef CONFIG_KRG_IPC
+			{
+				up_read(&msg_ids(ns).rw_mutex);
 				return PTR_ERR(msq);
+			}
+#else
+				return PTR_ERR(msq);
+#endif
 			success_return = 0;
 		}
 		err = -EACCES;
@@ -558,6 +626,9 @@
 		tbuf.msg_lspid  = msq->q_lspid;
 		tbuf.msg_lrpid  = msq->q_lrpid;
 		msg_unlock(msq);
+#ifdef CONFIG_KRG_IPC
+		up_read(&msg_ids(ns).rw_mutex);
+#endif
 		if (copy_msqid_to_user(buf, &tbuf, version))
 			return -EFAULT;
 		return success_return;
@@ -572,6 +643,9 @@
 
 out_unlock:
 	msg_unlock(msq);
+#ifdef CONFIG_KRG_IPC
+	up_read(&msg_ids(ns).rw_mutex);
+#endif
 	return err;
 }
 
@@ -632,12 +706,42 @@
 	return 0;
 }
 
+#ifdef CONFIG_KRG_IPC
+long do_msgsnd(int msqid, long mtype, void __user *mtext,
+	       size_t msgsz, int msgflg)
+{
+	long r;
+	struct ipc_namespace *ns;
+	ns = current->nsproxy->ipc_ns;
+
+	if (msgsz > ns->msg_ctlmax || (long) msgsz < 0 || msqid < 0)
+		return -EINVAL;
+	if (mtype < 1)
+		return -EINVAL;
+
+	if (is_krg_ipc(&msg_ids(ns)))
+		r = krg_ipc_msgsnd(msqid, mtype, mtext, msgsz, msgflg,
+				  ns, task_tgid_vnr(current));
+	else {
+		r = __do_msgsnd(msqid, mtype, mtext, msgsz, msgflg,
+				ns, task_tgid_vnr(current));
+	}
+
+	return r;
+}
+
+long __do_msgsnd(int msqid, long mtype, void __user *mtext,
+		 size_t msgsz, int msgflg, struct ipc_namespace *ns,
+		 pid_t tgid)
+#else
 long do_msgsnd(int msqid, long mtype, void __user *mtext,
 		size_t msgsz, int msgflg)
+#endif
 {
 	struct msg_queue *msq;
 	struct msg_msg *msg;
 	int err;
+#ifndef CONFIG_KRG_IPC
 	struct ipc_namespace *ns;
 
 	ns = current->nsproxy->ipc_ns;
@@ -646,6 +750,7 @@
 		return -EINVAL;
 	if (mtype < 1)
 		return -EINVAL;
+#endif
 
 	msg = load_msg(mtext, msgsz);
 	if (IS_ERR(msg))
@@ -654,6 +759,9 @@
 	msg->m_type = mtype;
 	msg->m_ts = msgsz;
 
+#ifdef CONFIG_KRG_IPC
+	down_read(&msg_ids(ns).rw_mutex);
+#endif
 	msq = msg_lock_check(ns, msqid);
 	if (IS_ERR(msq)) {
 		err = PTR_ERR(msq);
@@ -682,25 +790,54 @@
 			goto out_unlock_free;
 		}
 		ss_add(msq, &s);
+#ifndef CONFIG_KRG_IPC
 		ipc_rcu_getref(msq);
+#endif
 		msg_unlock(msq);
+#ifdef CONFIG_KRG_IPC
+		up_read(&msg_ids(ns).rw_mutex);
+#endif
+
 		schedule();
 
+#ifdef CONFIG_KRG_IPC
+		down_read(&msg_ids(ns).rw_mutex);
+		msq = msg_lock_check(ns, msqid);
+		if (IS_ERR(msq)) {
+			err = -EIDRM;
+			goto out_free;
+		}
+#else
 		ipc_lock_by_ptr(&msq->q_perm);
 		ipc_rcu_putref(msq);
 		if (msq->q_perm.deleted) {
 			err = -EIDRM;
 			goto out_unlock_free;
 		}
+#endif
 		ss_del(&s);
 
+#if defined(CONFIG_KRG_IPC) && defined(CONFIG_KRG_EPM)
+		if (krg_action_any_pending(current)) {
+#ifdef CONFIG_KRG_DEBUG
+			printk("%s:%d - action kerrighed! --> need replay!!\n",
+			       __PRETTY_FUNCTION__, __LINE__);
+#endif
+			err = -ERESTARTSYS;
+			goto out_unlock_free;
+		}
+#endif
 		if (signal_pending(current)) {
 			err = -ERESTARTNOHAND;
 			goto out_unlock_free;
 		}
 	}
 
+#ifdef CONFIG_KRG_IPC
+	msq->q_lspid = tgid;
+#else
 	msq->q_lspid = task_tgid_vnr(current);
+#endif
 	msq->q_stime = get_seconds();
 
 	if (!pipelined_send(msq, msg)) {
@@ -720,6 +857,11 @@
 out_free:
 	if (msg != NULL)
 		free_msg(msg);
+
+#ifdef CONFIG_KRG_IPC
+	up_read(&msg_ids(ns).rw_mutex);
+#endif
+
 	return err;
 }
 
@@ -752,22 +894,58 @@
 	return SEARCH_EQUAL;
 }
 
+#ifdef CONFIG_KRG_IPC
+long do_msgrcv(int msqid, long *pmtype, void __user *mtext,
+	       size_t msgsz, long msgtyp, int msgflg)
+{
+	long r;
+	struct ipc_namespace *ns = current->nsproxy->ipc_ns;
+
+	if (is_krg_ipc(&msg_ids(ns)))
+		r = krg_ipc_msgrcv(msqid, pmtype, mtext, msgsz, msgtyp, msgflg,
+				  ns, task_tgid_vnr(current));
+	else
+		r = __do_msgrcv(msqid, pmtype, mtext, msgsz, msgtyp, msgflg,
+				ns, task_tgid_vnr(current));
+
+	return r;
+}
+
+long __do_msgrcv(int msqid, long *pmtype, void __user *mtext,
+		 size_t msgsz, long msgtyp, int msgflg,
+		 struct ipc_namespace *ns, pid_t tgid)
+#else
 long do_msgrcv(int msqid, long *pmtype, void __user *mtext,
 		size_t msgsz, long msgtyp, int msgflg)
+#endif
 {
 	struct msg_queue *msq;
 	struct msg_msg *msg;
 	int mode;
+#ifndef CONFIG_KRG_IPC
 	struct ipc_namespace *ns;
+#endif
 
 	if (msqid < 0 || (long) msgsz < 0)
 		return -EINVAL;
 	mode = convert_mode(&msgtyp, msgflg);
+#ifndef CONFIG_KRG_IPC
 	ns = current->nsproxy->ipc_ns;
+#endif
 
+#ifdef CONFIG_KRG_IPC
+	down_read(&msg_ids(ns).rw_mutex);
+#endif
 	msq = msg_lock_check(ns, msqid);
 	if (IS_ERR(msq))
+#ifdef CONFIG_KRG_IPC
+	{
+		up_read(&msg_ids(ns).rw_mutex);
 		return PTR_ERR(msq);
+	}
+#else
+		return PTR_ERR(msq);
+#endif
 
 	for (;;) {
 		struct msg_receiver msr_d;
@@ -811,12 +989,19 @@
 			list_del(&msg->m_list);
 			msq->q_qnum--;
 			msq->q_rtime = get_seconds();
+#ifdef CONFIG_KRG_IPC
+			msq->q_lrpid = tgid;
+#else
 			msq->q_lrpid = task_tgid_vnr(current);
+#endif
 			msq->q_cbytes -= msg->m_ts;
 			atomic_sub(msg->m_ts, &ns->msg_bytes);
 			atomic_dec(&ns->msg_hdrs);
 			ss_wakeup(&msq->q_senders, 0);
 			msg_unlock(msq);
+#ifdef CONFIG_KRG_IPC
+			up_read(&msg_ids(ns).rw_mutex);
+#endif
 			break;
 		}
 		/* No message waiting. Wait for a message */
@@ -835,9 +1020,21 @@
 		msr_d.r_msg = ERR_PTR(-EAGAIN);
 		current->state = TASK_INTERRUPTIBLE;
 		msg_unlock(msq);
+#ifdef CONFIG_KRG_IPC
+		up_read(&msg_ids(ns).rw_mutex);
+#endif
 
 		schedule();
 
+
+#ifdef CONFIG_KRG_IPC
+		down_read(&msg_ids(ns).rw_mutex);
+		msq = msg_lock_check(ns, msqid);
+		if (IS_ERR(msq)) {
+			up_read(&msg_ids(ns).rw_mutex);
+			return -EIDRM;
+		}
+#else
 		/* Lockless receive, part 1:
 		 * Disable preemption.  We don't hold a reference to the queue
 		 * and getting a reference would defeat the idea of a lockless
@@ -879,15 +1076,30 @@
 		/* Lockless receive, part 4:
 		 * Repeat test after acquiring the spinlock.
 		 */
+#endif
 		msg = (struct msg_msg*)msr_d.r_msg;
 		if (msg != ERR_PTR(-EAGAIN))
 			goto out_unlock;
 
 		list_del(&msr_d.r_list);
+
+#if defined(CONFIG_KRG_IPC) && defined(CONFIG_KRG_EPM)
+		if (krg_action_any_pending(current)) {
+#ifdef CONFIG_KRG_DEBUG
+			printk("%s:%d - action kerrighed! --> need replay!!\n",
+			       __PRETTY_FUNCTION__, __LINE__);
+#endif
+			msg = ERR_PTR(-ERESTARTSYS);
+			goto out_unlock;
+		}
+#endif
 		if (signal_pending(current)) {
 			msg = ERR_PTR(-ERESTARTNOHAND);
 out_unlock:
 			msg_unlock(msq);
+#ifdef CONFIG_KRG_IPC
+			up_read(&msg_ids(ns).rw_mutex);
+#endif
 			break;
 		}
 	}
diff -ruN linux-2.6.29/ipc/msg_handler.c android_cluster/linux-2.6.29/ipc/msg_handler.c
--- linux-2.6.29/ipc/msg_handler.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/msg_handler.c	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,585 @@
+/** All the code for IPC messages accross the cluster
+ *  @file msg_handler.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2007-2008 Matthieu Fertré - INRIA
+ */
+
+#ifndef NO_MSG
+
+#include <linux/ipc.h>
+#include <linux/ipc_namespace.h>
+#include <linux/msg.h>
+#include <linux/syscalls.h>
+#include <linux/remote_sleep.h>
+
+#include <kddm/kddm.h>
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/hotplug.h>
+#include "ipc_handler.h"
+#include "msg_handler.h"
+#include "msg_io_linker.h"
+#include "ipcmap_io_linker.h"
+#include "util.h"
+#include "krgmsg.h"
+#include "krgipc_mobility.h"
+
+struct msgkrgops {
+	struct krgipc_ops krgops;
+	struct kddm_set *master_kddm_set;
+};
+
+struct kddm_set *krgipc_ops_master_set(struct krgipc_ops *ipcops)
+{
+	struct msgkrgops *msgops;
+
+	msgops = container_of(ipcops, struct msgkrgops, krgops);
+
+	return msgops->master_kddm_set;
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                                KERNEL HOOKS                               */
+/*                                                                           */
+/*****************************************************************************/
+
+static struct kern_ipc_perm *kcb_ipc_msg_lock(struct ipc_ids *ids, int id)
+{
+	msq_object_t *msq_object;
+	struct msg_queue *msq;
+	int index;
+
+	rcu_read_lock();
+
+	index = ipcid_to_idx(id);
+
+	msq_object = _kddm_grab_object_no_ft(ids->krgops->data_kddm_set, index);
+
+	if (!msq_object)
+		goto error;
+
+	msq = msq_object->local_msq;
+
+	BUG_ON(!msq);
+
+	mutex_lock(&msq->q_perm.mutex);
+
+	if (msq->q_perm.deleted) {
+		mutex_unlock(&msq->q_perm.mutex);
+		goto error;
+	}
+
+	return &(msq->q_perm);
+
+error:
+	_kddm_put_object(ids->krgops->data_kddm_set, index);
+	rcu_read_unlock();
+
+	return ERR_PTR(-EINVAL);
+}
+
+static void kcb_ipc_msg_unlock(struct kern_ipc_perm *ipcp)
+{
+	int index, deleted = 0;
+
+	index = ipcid_to_idx(ipcp->id);
+
+	if (ipcp->deleted)
+		deleted = 1;
+
+	_kddm_put_object(ipcp->krgops->data_kddm_set, index);
+
+	if (!deleted)
+		mutex_unlock(&ipcp->mutex);
+
+	rcu_read_unlock();
+}
+
+static struct kern_ipc_perm *kcb_ipc_msg_findkey(struct ipc_ids *ids, key_t key)
+{
+	long *key_index;
+	int id = -1;
+
+	key_index = _kddm_get_object_no_ft(ids->krgops->key_kddm_set, key);
+
+	if (key_index)
+		id = *key_index;
+
+	_kddm_put_object(ids->krgops->key_kddm_set, key);
+
+	if (id != -1)
+		return kcb_ipc_msg_lock(ids, id);
+
+	return NULL;
+}
+
+/** Notify the creation of a new IPC msg queue to Kerrighed.
+ *
+ *  @author Matthieu Fertré
+ */
+int krg_ipc_msg_newque(struct ipc_namespace *ns, struct msg_queue *msq)
+{
+	struct kddm_set *master_set;
+	msq_object_t *msq_object;
+	kerrighed_node_t *master_node;
+	long *key_index;
+	int index, err = 0;
+
+	BUG_ON(!msg_ids(ns).krgops);
+
+	index = ipcid_to_idx(msq->q_perm.id);
+
+	msq_object = _kddm_grab_object_manual_ft(
+		msg_ids(ns).krgops->data_kddm_set, index);
+
+	BUG_ON(msq_object);
+
+	msq_object = kmem_cache_alloc(msq_object_cachep, GFP_KERNEL);
+	if (!msq_object) {
+		err = -ENOMEM;
+		goto err_put;
+	}
+
+	msq_object->local_msq = msq;
+	msq_object->local_msq->is_master = 1;
+	msq_object->mobile_msq.q_perm.id = -1;
+
+	_kddm_set_object(msg_ids(ns).krgops->data_kddm_set, index, msq_object);
+
+	if (msq->q_perm.key != IPC_PRIVATE)
+	{
+		key_index = _kddm_grab_object(msg_ids(ns).krgops->key_kddm_set,
+					      msq->q_perm.key);
+		*key_index = index;
+		_kddm_put_object(msg_ids(ns).krgops->key_kddm_set,
+				 msq->q_perm.key);
+	}
+
+	master_set = krgipc_ops_master_set(msg_ids(ns).krgops);
+
+	master_node = _kddm_grab_object(master_set, index);
+	*master_node = kerrighed_node_id;
+
+	msq->q_perm.krgops = msg_ids(ns).krgops;
+
+	_kddm_put_object(master_set, index);
+
+err_put:
+	_kddm_put_object(msg_ids(ns).krgops->data_kddm_set, index);
+
+	return err;
+}
+
+void krg_ipc_msg_freeque(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)
+{
+	int index;
+	key_t key;
+	struct kddm_set *master_set;
+	struct msg_queue *msq = container_of(ipcp, struct msg_queue, q_perm);
+
+	index = ipcid_to_idx(msq->q_perm.id);
+	key = msq->q_perm.key;
+
+	if (key != IPC_PRIVATE) {
+		_kddm_grab_object_no_ft(ipcp->krgops->key_kddm_set, key);
+		_kddm_remove_frozen_object(ipcp->krgops->key_kddm_set, key);
+	}
+
+	master_set = krgipc_ops_master_set(ipcp->krgops);
+
+	_kddm_grab_object_no_ft(master_set, index);
+	_kddm_remove_frozen_object(master_set, index);
+
+	local_msg_unlock(msq);
+
+	_kddm_remove_frozen_object(ipcp->krgops->data_kddm_set, index);
+
+	krg_ipc_rmid(&msg_ids(ns), index);
+}
+
+/*****************************************************************************/
+
+struct msgsnd_msg
+{
+	kerrighed_node_t requester;
+	int msqid;
+	int msgflg;
+	long mtype;
+	pid_t tgid;
+	size_t msgsz;
+};
+
+long krg_ipc_msgsnd(int msqid, long mtype, void __user *mtext,
+		    size_t msgsz, int msgflg, struct ipc_namespace *ns,
+		    pid_t tgid)
+{
+	struct rpc_desc * desc;
+	struct kddm_set *master_set;
+	kerrighed_node_t* master_node;
+	void *buffer;
+	long r;
+	int err;
+	int index;
+	struct msgsnd_msg msg;
+
+	index = ipcid_to_idx(msqid);
+
+	master_set = krgipc_ops_master_set(msg_ids(ns).krgops);
+
+	master_node = _kddm_get_object_no_ft(master_set, index);
+	if (!master_node) {
+		_kddm_put_object(master_set, index);
+		r = -EINVAL;
+		goto exit;
+	}
+
+	if (*master_node == kerrighed_node_id) {
+		/* inverting the following 2 lines can conduct to deadlock
+		 * if the send is blocked */
+		_kddm_put_object(master_set, index);
+		r = __do_msgsnd(msqid, mtype, mtext, msgsz,
+				msgflg, ns, tgid);
+		goto exit;
+	}
+
+	msg.requester = kerrighed_node_id;
+	msg.msqid = msqid;
+	msg.mtype = mtype;
+	msg.msgflg = msgflg;
+	msg.tgid = tgid;
+	msg.msgsz = msgsz;
+
+	buffer = kmalloc(msgsz, GFP_KERNEL);
+	if (!buffer) {
+		r = -ENOMEM;
+		goto exit;
+	}
+
+	r = copy_from_user(buffer, mtext, msgsz);
+	if (r)
+		goto exit_free_buffer;
+
+	desc = rpc_begin(IPC_MSG_SEND, *master_node);
+	_kddm_put_object(master_set, index);
+
+	r = rpc_pack_type(desc, msg);
+	if (r)
+		goto exit_rpc;
+
+	r = rpc_pack(desc, 0, buffer, msgsz);
+	if (r)
+		goto exit_rpc;
+
+	r = unpack_remote_sleep_res_prepare(desc);
+	if (r)
+		goto exit_rpc;
+
+	err = unpack_remote_sleep_res_type(desc, r);
+	if (err)
+		r = err;
+
+exit_rpc:
+	rpc_end(desc, 0);
+exit_free_buffer:
+	kfree(buffer);
+exit:
+	return r;
+}
+
+static void handle_do_msg_send(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	void *mtext;
+	long r;
+	struct msgsnd_msg *msg = _msg;
+	struct ipc_namespace *ns;
+
+	ns = find_get_krg_ipcns();
+	BUG_ON(!ns);
+
+	mtext = kmalloc(msg->msgsz, GFP_KERNEL);
+	if (!mtext) {
+		r = -ENOMEM;
+		goto exit_put_ns;
+	}
+
+	r = rpc_unpack(desc, 0, mtext, msg->msgsz);
+	if (r)
+		goto exit_free_text;
+
+	r = remote_sleep_prepare(desc);
+	if (r)
+		goto exit_free_text;
+
+	r = __do_msgsnd(msg->msqid, msg->mtype, mtext, msg->msgsz, msg->msgflg,
+			ns, msg->tgid);
+
+	remote_sleep_finish();
+
+	r = rpc_pack_type(desc, r);
+
+exit_free_text:
+	kfree(mtext);
+exit_put_ns:
+	put_ipc_ns(ns);
+}
+
+struct msgrcv_msg
+{
+	kerrighed_node_t requester;
+	int msqid;
+	int msgflg;
+	long msgtyp;
+	pid_t tgid;
+	size_t msgsz;
+};
+
+long krg_ipc_msgrcv(int msqid, long *pmtype, void __user *mtext,
+		    size_t msgsz, long msgtyp, int msgflg,
+		    struct ipc_namespace *ns, pid_t tgid)
+{
+	struct rpc_desc * desc;
+	enum rpc_error err;
+	struct kddm_set *master_set;
+	kerrighed_node_t *master_node;
+	void * buffer;
+	long r;
+	int retval;
+	int index;
+	struct msgrcv_msg msg;
+
+	/* TODO: manage ipc namespace */
+	index = ipcid_to_idx(msqid);
+
+	master_set = krgipc_ops_master_set(msg_ids(ns).krgops);
+
+	master_node = _kddm_get_object_no_ft(master_set, index);
+	if (!master_node) {
+		_kddm_put_object(master_set, index);
+		return -EINVAL;
+	}
+
+	if (*master_node == kerrighed_node_id) {
+		/*inverting the following 2 lines can conduct to deadlock
+		 * if the receive is blocked */
+		_kddm_put_object(master_set, index);
+		r = __do_msgrcv(msqid, pmtype, mtext, msgsz, msgtyp,
+				msgflg, ns, tgid);
+		return r;
+	}
+
+	msg.requester = kerrighed_node_id;
+	msg.msqid = msqid;
+	msg.msgtyp = msgtyp;
+	msg.msgflg = msgflg;
+	msg.tgid = tgid;
+	msg.msgsz = msgsz;
+
+	desc = rpc_begin(IPC_MSG_RCV, *master_node);
+	_kddm_put_object(master_set, index);
+
+	r = rpc_pack_type(desc, msg);
+	if (r)
+		goto exit;
+
+	r = unpack_remote_sleep_res_prepare(desc);
+	if (r)
+		goto exit;
+
+	err = unpack_remote_sleep_res_type(desc, r);
+	if (!err) {
+		if (r > 0) {
+			/* get the real msg type */
+			err = rpc_unpack(desc, 0, pmtype, sizeof(long));
+			if (err)
+				goto err_rpc;
+
+			buffer = kmalloc(r, GFP_KERNEL);
+			if (!buffer) {
+				r = -ENOMEM;
+				goto exit;
+			}
+
+			err = rpc_unpack(desc, 0, buffer, r);
+			if (err) {
+				kfree(buffer);
+				goto err_rpc;
+			}
+
+			retval = copy_to_user(mtext, buffer, r);
+			kfree(buffer);
+			if (retval)
+				r = retval;
+		}
+	} else {
+		r = err;
+	}
+
+exit:
+	rpc_end(desc, 0);
+	return r;
+
+err_rpc:
+	r = -EPIPE;
+	goto exit;
+}
+
+static void handle_do_msg_rcv(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	void *mtext;
+	long msgsz, pmtype;
+	int r;
+	struct msgrcv_msg *msg = _msg;
+	struct ipc_namespace *ns;
+
+	ns = find_get_krg_ipcns();
+	BUG_ON(!ns);
+
+	mtext = kmalloc(msg->msgsz, GFP_KERNEL);
+	if (!mtext)
+		goto exit_put_ns;
+
+	r = remote_sleep_prepare(desc);
+	if (r)
+		goto exit_free_text;
+
+	msgsz = __do_msgrcv(msg->msqid, &pmtype, mtext, msg->msgsz,
+			    msg->msgtyp, msg->msgflg, ns, msg->tgid);
+
+	remote_sleep_finish();
+
+	r = rpc_pack_type(desc, msgsz);
+	if (r || msgsz <= 0)
+		goto exit_free_text;
+
+	r = rpc_pack_type(desc, pmtype); /* send the real type of msg */
+	if (r)
+		goto exit_free_text;
+
+	r = rpc_pack(desc, 0, mtext, msgsz);
+	if (r)
+		goto exit_free_text;
+
+exit_free_text:
+	kfree(mtext);
+exit_put_ns:
+	put_ipc_ns(ns);
+}
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              INITIALIZATION                               */
+/*                                                                           */
+/*****************************************************************************/
+
+int krg_msg_init_ns(struct ipc_namespace *ns)
+{
+	int r;
+
+	struct msgkrgops *msg_ops = kmalloc(sizeof(struct msgkrgops),
+					    GFP_KERNEL);
+	if (!msg_ops) {
+		r = -ENOMEM;
+		goto err;
+	}
+
+	msg_ops->krgops.map_kddm_set = create_new_kddm_set(
+		kddm_def_ns, MSGMAP_KDDM_ID, IPCMAP_LINKER,
+		KDDM_RR_DEF_OWNER, sizeof(ipcmap_object_t),
+		KDDM_LOCAL_EXCLUSIVE);
+
+	if (IS_ERR(msg_ops->krgops.map_kddm_set)) {
+		r = PTR_ERR(msg_ops->krgops.map_kddm_set);
+		goto err_map;
+	}
+
+	msg_ops->krgops.key_kddm_set = create_new_kddm_set(
+		kddm_def_ns, MSGKEY_KDDM_ID, MSGKEY_LINKER,
+		KDDM_RR_DEF_OWNER, sizeof(long),
+		KDDM_LOCAL_EXCLUSIVE);
+
+	if (IS_ERR(msg_ops->krgops.key_kddm_set)) {
+		r = PTR_ERR(msg_ops->krgops.key_kddm_set);
+		goto err_key;
+	}
+
+	msg_ops->krgops.data_kddm_set = create_new_kddm_set(
+		kddm_def_ns, MSG_KDDM_ID, MSG_LINKER,
+		KDDM_RR_DEF_OWNER, sizeof(msq_object_t),
+		KDDM_LOCAL_EXCLUSIVE);
+
+	if (IS_ERR(msg_ops->krgops.data_kddm_set)) {
+		r = PTR_ERR(msg_ops->krgops.data_kddm_set);
+		goto err_data;
+	}
+
+	msg_ops->master_kddm_set = create_new_kddm_set(
+		kddm_def_ns, MSGMASTER_KDDM_ID, MSGMASTER_LINKER,
+		KDDM_RR_DEF_OWNER, sizeof(kerrighed_node_t),
+		KDDM_LOCAL_EXCLUSIVE);
+
+	if (IS_ERR(msg_ops->master_kddm_set)) {
+		r = PTR_ERR(msg_ops->master_kddm_set);
+		goto err_master;
+	}
+
+	msg_ops->krgops.ipc_lock = kcb_ipc_msg_lock;
+	msg_ops->krgops.ipc_unlock = kcb_ipc_msg_unlock;
+	msg_ops->krgops.ipc_findkey = kcb_ipc_msg_findkey;
+
+	msg_ids(ns).krgops = &msg_ops->krgops;
+
+	return 0;
+
+err_master:
+	_destroy_kddm_set(msg_ops->krgops.data_kddm_set);
+err_data:
+	_destroy_kddm_set(msg_ops->krgops.key_kddm_set);
+err_key:
+	_destroy_kddm_set(msg_ops->krgops.map_kddm_set);
+err_map:
+	kfree(msg_ops);
+err:
+	return r;
+}
+
+void krg_msg_exit_ns(struct ipc_namespace *ns)
+{
+	if (msg_ids(ns).krgops) {
+		struct msgkrgops *msg_ops;
+
+		msg_ops = container_of(msg_ids(ns).krgops, struct msgkrgops,
+				       krgops);
+
+		_destroy_kddm_set(msg_ops->krgops.map_kddm_set);
+		_destroy_kddm_set(msg_ops->krgops.key_kddm_set);
+		_destroy_kddm_set(msg_ops->krgops.data_kddm_set);
+		_destroy_kddm_set(msg_ops->master_kddm_set);
+
+		kfree(msg_ops);
+	}
+}
+
+void msg_handler_init(void)
+{
+	msq_object_cachep = kmem_cache_create("msg_queue_object",
+					      sizeof(msq_object_t),
+					      0, SLAB_PANIC, NULL);
+
+	register_io_linker(MSG_LINKER, &msq_linker);
+	register_io_linker(MSGKEY_LINKER, &msqkey_linker);
+	register_io_linker(MSGMASTER_LINKER, &msqmaster_linker);
+
+	rpc_register_void(IPC_MSG_SEND, handle_do_msg_send, 0);
+	rpc_register_void(IPC_MSG_RCV, handle_do_msg_rcv, 0);
+	rpc_register_void(IPC_MSG_CHKPT, handle_msg_checkpoint, 0);
+}
+
+
+
+void msg_handler_finalize(void)
+{
+}
+
+#endif
diff -ruN linux-2.6.29/ipc/msg_handler.h android_cluster/linux-2.6.29/ipc/msg_handler.h
--- linux-2.6.29/ipc/msg_handler.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/msg_handler.h	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,20 @@
+/** Interface of IPC msg management.
+ *  @file msg_handler.h
+ *
+ *  @author Matthieu Fertré
+ */
+
+
+#ifndef MSG_HANDLER_H
+#define MSG_HANDLER_H
+
+#include <linux/msg.h>
+
+struct kddm_set;
+
+struct kddm_set *krgipc_ops_master_set(struct krgipc_ops *ipcops);
+
+void msg_handler_init(void);
+void msg_handler_finalize(void);
+
+#endif // MSG_HANDLER_H
diff -ruN linux-2.6.29/ipc/msg_io_linker.c android_cluster/linux-2.6.29/ipc/msg_io_linker.c
--- linux-2.6.29/ipc/msg_io_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/msg_io_linker.c	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,354 @@
+/*
+ *  Kerrighed/modules/ipc/msg_io_linker.c
+ *
+ *  KDDM IPC msg_queue id Linker.
+ *
+ *  Copyright (C) 2007-2008 Matthieu Fertré - INRIA
+ */
+#include <linux/shm.h>
+#include <linux/lockdep.h>
+#include <linux/security.h>
+#include <linux/ipc_namespace.h>
+#include <linux/ipc.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+
+#include "ipc_handler.h"
+#include "msg_io_linker.h"
+#include "util.h"
+#include "krgmsg.h"
+
+struct kmem_cache *msq_object_cachep;
+
+/** Create a local instance of a remotly existing IPC message queue.
+ *
+ *  @author Matthieu Fertré
+ */
+static struct msg_queue *create_local_msq(struct ipc_namespace *ns,
+					  struct msg_queue *received_msq)
+{
+	struct msg_queue *msq;
+	int retval;
+
+	msq = ipc_rcu_alloc(sizeof(*msq));
+	if (!msq)
+		return ERR_PTR(-ENOMEM);
+
+	*msq = *received_msq;
+	retval = security_msg_queue_alloc(msq);
+	if (retval)
+		goto err_putref;
+
+	/*
+	 * ipc_reserveid() locks msq
+	 */
+	retval = local_ipc_reserveid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
+	if (retval)
+		goto err_security_free;
+
+	msq->is_master = 0;
+	INIT_LIST_HEAD(&msq->q_messages);
+	INIT_LIST_HEAD(&msq->q_receivers);
+	INIT_LIST_HEAD(&msq->q_senders);
+
+	msq->q_perm.krgops = msg_ids(ns).krgops;
+	local_msg_unlock(msq);
+
+	return msq;
+
+err_security_free:
+	security_msg_queue_free(msq);
+err_putref:
+	ipc_rcu_putref(msq);
+	return ERR_PTR(retval);
+}
+
+/** Remove a local instance of a removed IPC message queue.
+ *
+ *  @author Matthieu Fertré
+ */
+static void delete_local_msq(struct ipc_namespace *ns, struct msg_queue *local_msq)
+{
+	struct msg_queue *msq;
+
+	msq = local_msq;
+
+	security_msg_queue_free(msq);
+
+	ipc_rmid(&msg_ids(ns), &msq->q_perm);
+
+	local_msg_unlock(msq);
+
+	ipc_rcu_putref(msq);
+}
+
+/** Update a local instance of a remotly existing IPC message queue.
+ *
+ *  @author Matthieu Fertré
+ */
+static void update_local_msq (struct msg_queue *local_msq,
+			      struct msg_queue *received_msq)
+{
+	/* local_msq->q_perm = received_msq->q_perm;*/
+	local_msq->q_stime = received_msq->q_stime;
+	local_msq->q_rtime = received_msq->q_rtime;
+	local_msq->q_ctime = received_msq->q_ctime;
+	local_msq->q_cbytes = received_msq->q_cbytes;
+	local_msq->q_qnum = received_msq->q_qnum;
+	local_msq->q_qbytes = received_msq->q_qbytes;
+	local_msq->q_lspid = received_msq->q_lspid;
+	local_msq->q_lrpid = received_msq->q_lrpid;
+
+	/* Do not modify the list_head else you will loose
+	   information on master node */
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                         MSQID KDDM IO FUNCTIONS                           */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+int msq_alloc_object (struct kddm_obj * obj_entry,
+		      struct kddm_set * set,
+		      objid_t objid)
+{
+	msq_object_t *msq_object;
+
+	msq_object = kmem_cache_alloc(msq_object_cachep, GFP_KERNEL);
+	if (!msq_object)
+		return -ENOMEM;
+
+	msq_object->local_msq = NULL;
+	obj_entry->object = msq_object;
+
+	return 0;
+}
+
+
+
+/** Handle a kddm set msq_queue id first touch
+ *  @author Matthieu Fertré
+ *
+ *  @param  obj_entry  Kddm object descriptor.
+ *  @param  set       Kddm set descriptor
+ *  @param  objid     Id of the object to create.
+ *
+ *  @return  0 if everything is ok. Negative value otherwise.
+ */
+int msq_first_touch (struct kddm_obj * obj_entry,
+		     struct kddm_set * set,
+		     objid_t objid,
+		     int flags)
+{
+	BUG(); // I should never get here !
+
+	return 0;
+}
+
+
+
+/** Insert a new msg_queue id in local structures.
+ *  @author Matthieu Fertré
+ *
+ *  @param  obj_entry  Descriptor of the object to insert.
+ *  @param  set       Kddm set descriptor
+ *  @param  objid     Id of the object to insert.
+ */
+int msq_insert_object (struct kddm_obj * obj_entry,
+		       struct kddm_set * set,
+		       objid_t objid)
+{
+	msq_object_t *msq_object;
+	struct msg_queue *msq;
+	int r = 0;
+
+	msq_object = obj_entry->object;
+	BUG_ON(!msq_object);
+
+	/* Regular case, the kernel msg_queue struct is already allocated */
+	if (msq_object->local_msq) {
+		if (msq_object->mobile_msq.q_perm.id != -1)
+			update_local_msq(msq_object->local_msq,
+					 &msq_object->mobile_msq);
+
+	} else {
+		struct ipc_namespace *ns;
+
+		ns = find_get_krg_ipcns();
+		BUG_ON(!ns);
+
+		/* This is the first time the object is inserted locally. We need
+		 * to allocate kernel msq structures.
+		 */
+		msq = create_local_msq(ns, &msq_object->mobile_msq);
+		msq_object->local_msq = msq;
+
+		if (IS_ERR(msq)) {
+			r = PTR_ERR(msq);
+			BUG();
+		}
+
+		put_ipc_ns(ns);
+	}
+
+	return r;
+}
+
+
+
+/** Invalidate a kddm object msqid.
+ *  @author Matthieu Fertré
+ *
+ *  @param  obj_entry  Descriptor of the object to invalidate.
+ *  @param  set       Kddm set descriptor
+ *  @param  objid     Id of the object to invalidate
+ */
+int msq_invalidate_object (struct kddm_obj * obj_entry,
+			   struct kddm_set * set,
+			   objid_t objid)
+{
+	return KDDM_IO_KEEP_OBJECT;
+}
+
+
+
+/** Handle a msg queue remove.
+ *  @author Matthieu Fertré
+ *
+ *  @param  obj_entry  Descriptor of the object to remove.
+ *  @param  set       Kddm set descriptor.
+ *  @param  padeid    Id of the object to remove.
+ */
+int msq_remove_object(void *object, struct kddm_set *set, objid_t objid)
+{
+	msq_object_t *msq_object;
+	struct msg_queue *msq;
+	struct ipc_namespace *ns;
+
+	ns = find_get_krg_ipcns();
+	BUG_ON(!ns);
+
+	msq_object = object;
+	if (msq_object) {
+		msq = msq_object->local_msq;
+		local_msg_lock(ns, msq->q_perm.id);
+		if (msq->is_master)
+			local_master_freeque(ns, &msq->q_perm);
+		else
+			delete_local_msq(ns, msq);
+
+		kmem_cache_free (msq_object_cachep, msq_object);
+	}
+
+	put_ipc_ns(ns);
+
+	return 0;
+}
+
+
+
+/** Export an object
+ *  @author Matthieu Fertré
+ *
+ *  @param  buffer    Buffer to export object data in.
+ *  @param  object    The object to export data from.
+ */
+int msq_export_object (struct rpc_desc *desc,
+		       struct kddm_set *set,
+		       struct kddm_obj *obj_entry,
+		       objid_t objid,
+		       int flags)
+{
+	msq_object_t *msq_object;
+	int r;
+
+	msq_object = obj_entry->object;
+	msq_object->mobile_msq = *msq_object->local_msq;
+
+	r = rpc_pack(desc, 0, msq_object, sizeof(msq_object_t));
+
+	return r;
+}
+
+
+
+/** Import an object
+ *  @author Matthieu Fertré
+ *
+ *  @param  object    The object to import data in.
+ *  @param  buffer    Data to import in the object.
+ */
+int msq_import_object (struct rpc_desc *desc,
+		       struct kddm_set *set,
+		       struct kddm_obj *obj_entry,
+		       objid_t objid,
+		       int flags)
+{
+	msq_object_t *msq_object, buffer;
+	struct msg_queue *msq;
+	int r;
+
+	msq_object = obj_entry->object;
+
+	r = rpc_unpack(desc, 0, &buffer, sizeof(msq_object_t));
+	if (r)
+		goto error;
+
+	msq_object->mobile_msq = buffer.mobile_msq;
+
+	if (msq_object->local_msq) {
+		msq = msq_object->local_msq;
+	}
+
+error:
+	return r;
+}
+
+
+
+/****************************************************************************/
+
+/* Init the msg queue id IO linker */
+
+struct iolinker_struct msq_linker = {
+	first_touch:       msq_first_touch,
+	remove_object:     msq_remove_object,
+	invalidate_object: msq_invalidate_object,
+	insert_object:     msq_insert_object,
+	linker_name:       "msg_queue",
+	linker_id:         MSG_LINKER,
+	alloc_object:      msq_alloc_object,
+	export_object:     msq_export_object,
+	import_object:     msq_import_object
+};
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                  MSG QUEUE KEY KDDM IO FUNCTIONS                          */
+/*                                                                           */
+/*****************************************************************************/
+
+/* Init the msg queue key IO linker */
+
+struct iolinker_struct msqkey_linker = {
+	linker_name:       "msqkey",
+	linker_id:         MSGKEY_LINKER,
+};
+
+/*****************************************************************************/
+/*                                                                           */
+/*                  MSG MASTER KDDM IO FUNCTIONS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+/* Init the msg master node IO linker */
+
+struct iolinker_struct msqmaster_linker = {
+	linker_name:       "msqmaster",
+	linker_id:         MSGMASTER_LINKER,
+};
diff -ruN linux-2.6.29/ipc/msg_io_linker.h android_cluster/linux-2.6.29/ipc/msg_io_linker.h
--- linux-2.6.29/ipc/msg_io_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/msg_io_linker.h	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,32 @@
+/** KDDM IPC MSG id Linker.
+ *  @file msgid_io_linker.h
+ *
+ *  Link KDDM and Linux IPC msg id mechanisms.
+ *  @author Matthieu Fertré
+ */
+
+#ifndef __MSGID_IO_LINKER__
+#define __MSGID_IO_LINKER__
+
+#include <kddm/kddm_types.h>
+
+extern struct kmem_cache *msq_object_cachep;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+typedef struct msq_object {
+	struct msg_queue mobile_msq;
+	struct msg_queue *local_msq;
+} msq_object_t;
+
+
+extern struct iolinker_struct msq_linker;
+extern struct iolinker_struct msqkey_linker;
+extern struct iolinker_struct msqmaster_linker;
+
+#endif
diff -ruN linux-2.6.29/ipc/namespace.c android_cluster/linux-2.6.29/ipc/namespace.c
--- linux-2.6.29/ipc/namespace.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/ipc/namespace.c	2014-05-27 23:04:10.422028007 -0700
@@ -12,20 +12,61 @@
 
 #include "util.h"
 
+#ifdef CONFIG_KRG_IPC
+static int krg_init_ipc_ns(struct ipc_namespace *ns)
+{
+	int err = 0;
+
+	if (!current->create_krg_ns)
+		goto exit;
+
+	err = krg_sem_init_ns(ns);
+	if (err)
+		goto err_sem;
+
+	err = krg_msg_init_ns(ns);
+	if (err)
+		goto err_msg;
+
+	err = krg_shm_init_ns(ns);
+	if (err)
+		goto err_shm;
+
+	return err;
+
+err_shm:
+	krg_msg_exit_ns(ns);
+err_msg:
+	krg_sem_exit_ns(ns);
+err_sem:
+exit:
+	return err;
+}
+#endif
+
 static struct ipc_namespace *clone_ipc_ns(struct ipc_namespace *old_ns)
 {
 	struct ipc_namespace *ns;
+	int err;
 
 	ns = kmalloc(sizeof(struct ipc_namespace), GFP_KERNEL);
 	if (ns == NULL)
 		return ERR_PTR(-ENOMEM);
 
-	atomic_inc(&nr_ipc_ns);
-
 	sem_init_ns(ns);
 	msg_init_ns(ns);
 	shm_init_ns(ns);
 
+#ifdef CONFIG_KRG_IPC
+	err = krg_init_ipc_ns(ns);
+	if (err) {
+		kfree(ns);
+		return ERR_PTR(err);
+	}
+#endif
+
+	atomic_inc(&nr_ipc_ns);
+
 	/*
 	 * msgmni has already been computed for the new ipc ns.
 	 * Thus, do the ipcns creation notification before registering that
@@ -101,6 +142,11 @@
 	sem_exit_ns(ns);
 	msg_exit_ns(ns);
 	shm_exit_ns(ns);
+#ifdef CONFIG_KRG_IPC
+	krg_sem_exit_ns(ns);
+	krg_msg_exit_ns(ns);
+	krg_shm_exit_ns(ns);
+#endif
 	kfree(ns);
 	atomic_dec(&nr_ipc_ns);
 
diff -ruN linux-2.6.29/ipc/semarray_io_linker.c android_cluster/linux-2.6.29/ipc/semarray_io_linker.c
--- linux-2.6.29/ipc/semarray_io_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/semarray_io_linker.c	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,644 @@
+/*
+ *  Kerrighed/modules/ipc/semarray_io_linker.c
+ *
+ *  KDDM SEM array Linker.
+ *
+ *  Copyright (C) 2007-2008 Matthieu Fertré - INRIA
+ */
+
+#include <linux/sem.h>
+#include <linux/lockdep.h>
+#include <linux/security.h>
+#include <linux/ipc_namespace.h>
+#include <linux/ipc.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+#include <kerrighed/pid.h>
+
+#include "ipc_handler.h"
+#include "semarray_io_linker.h"
+#include "util.h"
+#include "krgsem.h"
+
+struct kmem_cache *semarray_object_cachep;
+
+/** Create a local instance of an remotly existing Semaphore.
+ *
+ *  @author Matthieu Fertré
+ */
+struct sem_array *create_local_sem(struct ipc_namespace *ns,
+				   struct sem_array *received_sma)
+{
+	struct sem_array *sma;
+	int size_sems;
+	int retval;
+
+	size_sems = received_sma->sem_nsems * sizeof (struct sem);
+	sma = ipc_rcu_alloc(sizeof (*sma) + size_sems);
+	if (!sma) {
+		return ERR_PTR(-ENOMEM);
+	}
+	*sma = *received_sma;
+
+	sma->sem_base = (struct sem *) &sma[1];
+	memcpy(sma->sem_base, received_sma->sem_base, size_sems);
+
+	retval = security_sem_alloc(sma);
+	if (retval)
+		goto err_putref;
+
+	/*
+	 * ipc_reserveid() locks msq
+	 */
+	retval = local_ipc_reserveid(&sem_ids(ns), &sma->sem_perm, ns->sc_semmni);
+	if (retval)
+		goto err_security_free;
+
+	INIT_LIST_HEAD(&sma->sem_pending);
+	INIT_LIST_HEAD(&sma->list_id);
+	INIT_LIST_HEAD(&sma->remote_sem_pending);
+
+	sma->sem_perm.krgops = sem_ids(ns).krgops;
+	local_sem_unlock(sma);
+
+	return sma;
+
+err_security_free:
+	security_sem_free(sma);
+err_putref:
+	ipc_rcu_putref(sma);
+	return ERR_PTR(retval);
+}
+
+#define IN_WAKEUP 1
+
+static inline void update_sem_queues(struct sem_array *sma,
+				     struct sem_array *received_sma)
+{
+	struct sem_queue *q, *tq, *local_q;
+
+	BUG_ON(!list_empty(&received_sma->sem_pending));
+
+	/* adding (to local sem) semqueues that are not local */
+	list_for_each_entry_safe(q, tq, &received_sma->remote_sem_pending, list) {
+
+		int is_local = 0;
+
+		/* checking if the sem_queue is local */
+		list_for_each_entry(local_q, &sma->sem_pending, list) {
+
+			/* comparing local_q->pid to q->pid is not sufficient
+			 *  as they contains only tgid, two or more threads
+			 *  can be pending.
+			 */
+			if (task_pid_knr(local_q->sleeper) == remote_sleeper_pid(q)) {
+				/* the sem_queue is local */
+				is_local = 1;
+
+				BUG_ON(q->undo && !local_q->undo);
+				BUG_ON(local_q->undo && !q->undo);
+				local_q->undo = q->undo;
+				/* No need to update q->status, as it is done when
+				   needed in handle_ipcsem_wake_up_process */
+				BUG_ON(q->status == IN_WAKEUP);
+				BUG_ON(local_q->status != q->status);
+
+				goto next;
+			}
+		}
+	next:
+		list_del(&q->list);
+		if (is_local)
+			free_semqueue(q);
+		else
+			list_add(&q->list, &sma->remote_sem_pending);
+	}
+
+	BUG_ON(!list_empty(&received_sma->remote_sem_pending));
+}
+
+/** Update a local instance of a remotly existing IPC semaphore.
+ *
+ *  @author Matthieu Fertré
+ */
+static void update_local_sem(struct sem_array *local_sma,
+			     struct sem_array *received_sma)
+{
+	int size_sems;
+
+	size_sems = local_sma->sem_nsems * sizeof (struct sem);
+
+	/* getting new values from received semaphore */
+	local_sma->sem_otime = received_sma->sem_otime;
+	local_sma->sem_ctime = received_sma->sem_ctime;
+	memcpy(local_sma->sem_base, received_sma->sem_base, size_sems);
+
+	/* updating sem_undos list */
+	list_splice_init(&received_sma->list_id, &local_sma->list_id);
+
+	/* updating semqueues list */
+	update_sem_queues(local_sma, received_sma);
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                         SEM Array KDDM IO FUNCTIONS                       */
+/*                                                                           */
+/*****************************************************************************/
+
+int semarray_alloc_object (struct kddm_obj * obj_entry,
+			   struct kddm_set * set,
+			   objid_t objid)
+{
+	semarray_object_t *sem_object;
+
+	sem_object = kmem_cache_alloc(semarray_object_cachep, GFP_KERNEL);
+	if (!sem_object)
+		return -ENOMEM;
+
+	sem_object->local_sem = NULL;
+	sem_object->mobile_sem_base = NULL;
+	obj_entry->object = sem_object;
+
+	return 0;
+}
+
+
+
+/** Handle a kddm set sem_array first touch
+ *  @author Matthieu Fertré, Renaud Lottiaux
+ *
+ *  @param  obj_entry  Kddm object descriptor.
+ *  @param  set       Kddm set descriptor
+ *  @param  objid     Id of the object to create.
+ *
+ *  @return  0 if everything is ok. Negative value otherwise.
+ */
+int semarray_first_touch (struct kddm_obj * obj_entry,
+			  struct kddm_set * set,
+			  objid_t objid,
+			  int flags)
+{
+	BUG(); // I should never get here !
+
+	return 0;
+}
+
+
+
+/** Insert a new sem_array in local structures.
+ *  @author Matthieu Fertré, Renaud Lottiaux
+ *
+ *  @param  obj_entry  Descriptor of the object to insert.
+ *  @param  set       Kddm set descriptor
+ *  @param  objid     Id of the object to insert.
+ */
+int semarray_insert_object (struct kddm_obj * obj_entry,
+			    struct kddm_set * set,
+			    objid_t objid)
+{
+	semarray_object_t *sem_object;
+	struct sem_array *sem;
+	int r = 0;
+
+	sem_object = obj_entry->object;
+	BUG_ON(!sem_object);
+
+	if (!sem_object->local_sem) {
+		struct ipc_namespace *ns;
+
+		ns = find_get_krg_ipcns();
+		BUG_ON(!ns);
+
+		/* This is the first time the object is inserted locally.
+		 * We need to allocate kernel sem_array structure.
+		 */
+		sem = create_local_sem(ns, &sem_object->imported_sem);
+		sem_object->local_sem = sem;
+
+		if (IS_ERR(sem)) {
+			r = PTR_ERR(sem);
+			BUG();
+		}
+
+		put_ipc_ns(ns);
+	}
+
+	if (!r)
+		update_local_sem(sem_object->local_sem,
+				 &sem_object->imported_sem);
+
+	return r;
+}
+
+
+
+/** Invalidate a kddm object semarray.
+ *  @author Matthieu Fertré, Renaud Lottiaux
+ *
+ *  @param  obj_entry  Descriptor of the object to invalidate.
+ *  @param  set       Kddm set descriptor
+ *  @param  objid     Id of the object to invalidate
+ */
+int semarray_invalidate_object (struct kddm_obj * obj_entry,
+				struct kddm_set * set,
+				objid_t objid)
+{
+	semarray_object_t *sem_object = obj_entry->object;
+	struct sem_array *sma = sem_object->local_sem;
+	struct sem_undo *un, *tu;
+	struct sem_queue *q, *tq;
+
+	BUG_ON(!list_empty(&sem_object->imported_sem.list_id));
+	BUG_ON(!list_empty(&sem_object->imported_sem.sem_pending));
+	BUG_ON(!list_empty(&sem_object->imported_sem.remote_sem_pending));
+
+	/* freeing the semundo list */
+	list_for_each_entry_safe(un, tu, &sma->list_id, list_id) {
+		list_del(&un->list_id);
+		kfree(un);
+	}
+
+	/* freeing the remote semqueues */
+	list_for_each_entry_safe(q, tq, &sma->remote_sem_pending, list) {
+		list_del(&q->list);
+		free_semqueue(q);
+	}
+
+	return KDDM_IO_KEEP_OBJECT;
+}
+
+/** Handle a kddm semaphore remove.
+ *  @author Matthieu Fertré, Renaud Lottiaux
+ *
+ *  @param  obj_entry  Descriptor of the object to remove.
+ *  @param  set       Kddm set descriptor.
+ *  @param  padeid    Id of the object to remove.
+ */
+int semarray_remove_object(void *object, struct kddm_set * set,
+			   objid_t objid)
+{
+	semarray_object_t *sem_object;
+	struct sem_array *sma;
+
+	sem_object = object;
+	if (sem_object) {
+		struct ipc_namespace *ns;
+
+		ns = find_get_krg_ipcns();
+		BUG_ON(!ns);
+
+		sma = sem_object->local_sem;
+
+		local_sem_lock(ns, sma->sem_perm.id);
+		local_freeary(ns, &sma->sem_perm);
+
+		kfree(sem_object->mobile_sem_base);
+		sem_object->mobile_sem_base = NULL;
+		kmem_cache_free(semarray_object_cachep, sem_object);
+
+		put_ipc_ns(ns);
+	}
+
+	return 0;
+}
+
+static inline void __export_semarray(struct rpc_desc *desc,
+				     const semarray_object_t *sem_object,
+				     const struct sem_array* sma)
+{
+	rpc_pack(desc, 0, sma, sizeof(struct sem_array));
+	rpc_pack(desc, 0, sma->sem_base, sma->sem_nsems * sizeof (struct sem));
+}
+
+static inline void __export_semundos(struct rpc_desc *desc,
+				     const struct sem_array* sma)
+{
+	long nb_semundo = 0;
+	struct sem_undo *un;
+
+	list_for_each_entry(un, &sma->list_id, list_id)
+		nb_semundo++;
+
+	rpc_pack_type(desc, nb_semundo);
+
+	list_for_each_entry(un, &sma->list_id, list_id) {
+		BUG_ON(!list_empty(&un->list_proc));
+
+		rpc_pack(desc, 0, un, sizeof(struct sem_undo) +
+			 sma->sem_nsems * sizeof(short));
+	}
+}
+
+static inline void __export_one_local_semqueue(struct rpc_desc *desc,
+					       const struct sem_queue* q)
+{
+	/* Fill q2->sleeper with the pid (and not tgid) of q->sleeper
+	   (needed to be thread aware) */
+	struct sem_queue q2 = *q;
+
+	/* Make remote_sleeper_pid(q2) equal to q->sleeper's pid */
+	q2.sleeper = (void*)((long)(task_pid_knr(q->sleeper)));
+
+	rpc_pack_type(desc, q2);
+	if (q->nsops)
+		rpc_pack(desc, 0, q->sops,
+			 q->nsops * sizeof(struct sembuf));
+
+	if (q->undo) {
+		BUG_ON(!list_empty(&q->undo->list_proc));
+		rpc_pack_type(desc, q->undo->proc_list_id);
+	}
+}
+
+static inline void __export_one_remote_semqueue(struct rpc_desc *desc,
+						const struct sem_queue* q)
+{
+	rpc_pack(desc, 0, q, sizeof(struct sem_queue));
+	if (q->nsops)
+		rpc_pack(desc, 0, q->sops,
+			 q->nsops * sizeof(struct sembuf));
+
+	if (q->undo) {
+		BUG_ON(!list_empty(&q->undo->list_proc));
+		rpc_pack_type(desc, q->undo->proc_list_id);
+	}
+}
+
+static inline void __export_semqueues(struct rpc_desc *desc,
+				      const struct sem_array* sma)
+{
+	struct sem_queue *q;
+	long nb_sem_pending = 0;
+
+	/* count local sem_pending */
+	list_for_each_entry(q, &sma->sem_pending, list)
+		nb_sem_pending++;
+
+	/* count remote sem_pending */
+	list_for_each_entry(q, &sma->remote_sem_pending, list)
+		nb_sem_pending++;
+
+	rpc_pack_type(desc, nb_sem_pending);
+
+	/* send local sem_queues */
+	list_for_each_entry(q, &sma->sem_pending, list)
+		__export_one_local_semqueue(desc, q);
+
+	/* send remote sem_queues */
+	list_for_each_entry(q, &sma->remote_sem_pending, list)
+		__export_one_remote_semqueue(desc, q);
+}
+
+/** Export an object
+ *  @author Matthieu Fertré, Renaud Lottiaux
+ *
+ *  @param  buffer    Buffer to export object data in.
+ *  @param  object    The object to export data from.
+ */
+int semarray_export_object (struct rpc_desc *desc,
+			    struct kddm_set *set,
+			    struct kddm_obj *obj_entry,
+			    objid_t objid,
+			    int flags)
+{
+	semarray_object_t *sem_object;
+	struct sem_array *sma;
+
+	sem_object = obj_entry->object;
+	sma = sem_object->local_sem;
+
+	BUG_ON(!sma);
+
+	__export_semarray(desc, sem_object, sma);
+	__export_semundos(desc, sma);
+	__export_semqueues(desc, sma);
+
+	return 0;
+}
+
+static inline int __import_semarray(struct rpc_desc *desc,
+				    semarray_object_t *sem_object)
+{
+	struct sem_array buffer;
+	int size_sems;
+
+	rpc_unpack_type(desc, buffer);
+	sem_object->imported_sem = buffer;
+
+	size_sems = sem_object->imported_sem.sem_nsems * sizeof(struct sem);
+	if (!sem_object->mobile_sem_base)
+		sem_object->mobile_sem_base = kmalloc(size_sems, GFP_KERNEL);
+	if (!sem_object->mobile_sem_base)
+		return -ENOMEM;
+
+	rpc_unpack(desc, 0, sem_object->mobile_sem_base, size_sems);
+	sem_object->imported_sem.sem_base = sem_object->mobile_sem_base;
+
+	INIT_LIST_HEAD(&sem_object->imported_sem.sem_pending);
+	INIT_LIST_HEAD(&sem_object->imported_sem.remote_sem_pending);
+	INIT_LIST_HEAD(&sem_object->imported_sem.list_id);
+
+	return 0;
+}
+
+static inline int __import_semundos(struct rpc_desc *desc,
+				    struct sem_array *sma)
+{
+	struct sem_undo* undo;
+	long nb_semundo, i;
+	int size_undo;
+	size_undo = sizeof(struct sem_undo) +
+		sma->sem_nsems * sizeof(short);
+
+	rpc_unpack_type(desc, nb_semundo);
+
+	BUG_ON(!list_empty(&sma->list_id));
+
+	for (i=0; i < nb_semundo; i++) {
+		undo = kzalloc(size_undo, GFP_KERNEL);
+		if (!undo)
+			goto unalloc_undos;
+
+		rpc_unpack(desc, 0, undo, size_undo);
+		INIT_LIST_HEAD(&undo->list_id);
+		INIT_LIST_HEAD(&undo->list_proc);
+
+		undo->semadj = (short *) &undo[1];
+		list_add(&undo->list_id, &sma->list_id);
+	}
+
+	return 0;
+
+unalloc_undos:
+	return -ENOMEM;
+}
+
+static inline void __unimport_semundos(struct sem_array *sma)
+{
+	struct sem_undo * un, *tu;
+
+	list_for_each_entry_safe(un, tu, &sma->list_id, list_id) {
+		list_del(&un->list_id);
+		kfree(un);
+	}
+}
+
+static inline int import_one_semqueue(struct rpc_desc *desc,
+				      struct sem_array *sma)
+{
+	unique_id_t undo_proc_list_id;
+	struct sem_undo* undo;
+	int r = -ENOMEM;
+	struct sem_queue *q = kmalloc(sizeof(struct sem_queue), GFP_KERNEL);
+	if (!q)
+		goto exit;
+
+	rpc_unpack(desc, 0, q, sizeof(struct sem_queue));
+	INIT_LIST_HEAD(&q->list);
+
+	if (q->nsops) {
+		q->sops = kzalloc(q->nsops * sizeof(struct sembuf),
+				  GFP_KERNEL);
+		if (!q->sops)
+			goto unalloc_q;
+		rpc_unpack(desc, 0, q->sops, q->nsops * sizeof(struct sembuf));
+	}
+
+	if (q->undo) {
+		rpc_unpack_type(desc, undo_proc_list_id);
+
+		list_for_each_entry(undo, &sma->list_id, list_id) {
+			if (undo->proc_list_id == undo_proc_list_id) {
+				q->undo = undo;
+				goto undo_found;
+			}
+		}
+	}
+
+undo_found:
+	r = 0;
+
+	/* split between remote and local
+	   queues is done in update_local_sem */
+	list_add(&q->list, &sma->remote_sem_pending);
+
+	BUG_ON(!q->sleeper);
+	return r;
+
+unalloc_q:
+	kfree(q);
+
+exit:
+	return r;
+}
+
+static inline int __import_semqueues(struct rpc_desc *desc,
+				     struct sem_array *sma)
+{
+	long nb_sempending, i;
+	int r;
+
+	r = rpc_unpack_type(desc, nb_sempending);
+	if (r)
+		goto err;
+
+	BUG_ON(!list_empty(&sma->remote_sem_pending));
+
+	for (i=0; i < nb_sempending; i++) {
+		r = import_one_semqueue(desc, sma);
+		if (r)
+			goto err;
+	}
+
+#ifdef CONFIG_KRG_DEBUG
+	{
+		struct sem_queue *q;
+		i=0;
+		list_for_each_entry(q, &sma->remote_sem_pending, list)
+			i++;
+
+		BUG_ON(nb_sempending != i);
+	}
+#endif
+
+err:
+	return r;
+}
+
+static inline void __unimport_semqueues(struct sem_array *sma)
+{
+	struct sem_queue *q, *tq;
+
+	list_for_each_entry_safe(q, tq, &sma->remote_sem_pending, list) {
+		list_del(&q->list);
+		free_semqueue(q);
+	}
+}
+
+/** Import an object
+ *  @author Matthieu Fertré, Renaud Lottiaux
+ *
+ *  @param  object    The object to import data in.
+ *  @param  buffer    Data to import in the object.
+ */
+int semarray_import_object (struct rpc_desc *desc,
+			    struct kddm_set *set,
+			    struct kddm_obj *obj_entry,
+			    objid_t objid,
+			    int flags)
+{
+	semarray_object_t *sem_object;
+	int r = 0;
+	sem_object = obj_entry->object;
+
+	r = __import_semarray(desc, sem_object);
+	if (r)
+		goto err;
+
+	r = __import_semundos(desc, &sem_object->imported_sem);
+	if (r)
+		goto unimport_semundos;
+
+	r = __import_semqueues(desc, &sem_object->imported_sem);
+	if (r)
+		goto unimport_semqueues;
+
+	goto err;
+
+unimport_semqueues:
+	__unimport_semqueues(&sem_object->imported_sem);
+
+unimport_semundos:
+	__unimport_semundos(&sem_object->imported_sem);
+
+err:
+	return r;
+}
+
+/****************************************************************************/
+
+/* Init the semarray IO linker */
+struct iolinker_struct semarray_linker = {
+	first_touch:       semarray_first_touch,
+	remove_object:     semarray_remove_object,
+	invalidate_object: semarray_invalidate_object,
+	insert_object:     semarray_insert_object,
+	linker_name:       "semarray",
+	linker_id:         SEMARRAY_LINKER,
+	alloc_object:      semarray_alloc_object,
+	export_object:     semarray_export_object,
+	import_object:     semarray_import_object
+};
+
+/*****************************************************************************/
+/*                                                                           */
+/*                         SEMKEY KDDM IO FUNCTIONS                          */
+/*                                                                           */
+/*****************************************************************************/
+
+/* Init the sem key IO linker */
+struct iolinker_struct semkey_linker = {
+	linker_name:       "semkey",
+	linker_id:         SEMKEY_LINKER,
+};
diff -ruN linux-2.6.29/ipc/semarray_io_linker.h android_cluster/linux-2.6.29/ipc/semarray_io_linker.h
--- linux-2.6.29/ipc/semarray_io_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/semarray_io_linker.h	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,34 @@
+/** KDDM SEM Array Linker.
+ *  @file semarray_io_linker.h
+ *
+ *  Link KDDM and Linux SEM Array mechanisms.
+ *  @author Matthieu Fertré
+ */
+
+#ifndef __SEMARRAY_IO_LINKER__
+#define __SEMARRAY_IO_LINKER__
+
+#include <kddm/kddm_types.h>
+
+extern struct kmem_cache *semarray_object_cachep;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+typedef struct semarray_object {
+	struct sem_array imported_sem;
+	struct sem_array *local_sem;
+	struct sem* mobile_sem_base;
+} semarray_object_t;
+
+
+extern struct iolinker_struct semarray_linker;
+extern struct iolinker_struct semkey_linker;
+
+#define remote_sleeper_pid(q) ((pid_t)((long)(q->sleeper)))
+
+#endif
diff -ruN linux-2.6.29/ipc/sem.c android_cluster/linux-2.6.29/ipc/sem.c
--- linux-2.6.29/ipc/sem.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/ipc/sem.c	2014-05-27 23:04:10.422028007 -0700
@@ -86,13 +86,29 @@
 
 #include <asm/uaccess.h>
 #include "util.h"
+#ifdef CONFIG_KRG_IPC
+#include <linux/random.h>
+#include <kerrighed/pid.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/action.h>
+#endif
+#include "krgsem.h"
+#endif
+
+#ifdef CONFIG_KRG_IPC
+#define assert_mutex_locked(x) BUG_ON(!mutex_is_locked(x))
+#endif
 
 #define sem_ids(ns)	((ns)->ids[IPC_SEM_IDS])
 
+#ifndef CONFIG_KRG_IPC
 #define sem_unlock(sma)		ipc_unlock(&(sma)->sem_perm)
+#endif
 #define sem_checkid(sma, semid)	ipc_checkid(&sma->sem_perm, semid)
 
+#ifndef CONFIG_KRG_IPC
 static int newary(struct ipc_namespace *, struct ipc_params *);
+#endif
 static void freeary(struct ipc_namespace *, struct kern_ipc_perm *);
 #ifdef CONFIG_PROC_FS
 static int sysvipc_sem_proc_show(struct seq_file *s, void *it);
@@ -129,6 +145,7 @@
 void sem_exit_ns(struct ipc_namespace *ns)
 {
 	free_ipcs(ns, &sem_ids(ns), freeary);
+	idr_destroy(&ns->ids[IPC_SEM_IDS].ipcs_idr);
 }
 #endif
 
@@ -144,7 +161,11 @@
  * sem_lock_(check_) routines are called in the paths where the rw_mutex
  * is not held.
  */
+#ifdef CONFIG_KRG_IPC
+struct sem_array *sem_lock(struct ipc_namespace *ns, int id)
+#else
 static inline struct sem_array *sem_lock(struct ipc_namespace *ns, int id)
+#endif
 {
 	struct kern_ipc_perm *ipcp = ipc_lock(&sem_ids(ns), id);
 
@@ -154,7 +175,10 @@
 	return container_of(ipcp, struct sem_array, sem_perm);
 }
 
-static inline struct sem_array *sem_lock_check(struct ipc_namespace *ns,
+#ifndef CONFIG_KRG_IPC
+static inline
+#endif
+struct sem_array *sem_lock_check(struct ipc_namespace *ns,
 						int id)
 {
 	struct kern_ipc_perm *ipcp = ipc_lock_check(&sem_ids(ns), id);
@@ -165,6 +189,7 @@
 	return container_of(ipcp, struct sem_array, sem_perm);
 }
 
+#ifndef CONFIG_KRG_IPC
 static inline void sem_lock_and_putref(struct sem_array *sma)
 {
 	ipc_lock_by_ptr(&sma->sem_perm);
@@ -183,6 +208,7 @@
 	ipc_rcu_putref(sma);
 	ipc_unlock(&(sma)->sem_perm);
 }
+#endif
 
 static inline void sem_rmid(struct ipc_namespace *ns, struct sem_array *s)
 {
@@ -230,8 +256,10 @@
  *
  * Called with sem_ids.rw_mutex held (as a writer)
  */
-
-static int newary(struct ipc_namespace *ns, struct ipc_params *params)
+#ifndef CONFIG_KRG_IPC
+static
+#endif
+int newary(struct ipc_namespace *ns, struct ipc_params *params)
 {
 	int id;
 	int retval;
@@ -263,7 +291,12 @@
 		return retval;
 	}
 
+#ifdef CONFIG_KRG_IPC
+	id = ipc_addid(&sem_ids(ns), &sma->sem_perm, ns->sc_semmni,
+		       params->requested_id);
+#else
 	id = ipc_addid(&sem_ids(ns), &sma->sem_perm, ns->sc_semmni);
+#endif
 	if (id < 0) {
 		security_sem_free(sma);
 		ipc_rcu_putref(sma);
@@ -276,6 +309,20 @@
 	INIT_LIST_HEAD(&sma->list_id);
 	sma->sem_nsems = nsems;
 	sma->sem_ctime = get_seconds();
+#ifdef CONFIG_KRG_IPC
+	INIT_LIST_HEAD(&sma->remote_sem_pending);
+
+	if (is_krg_ipc(&sem_ids(ns))) {
+		retval = krg_ipc_sem_newary(ns, sma);
+		if (retval) {
+			security_sem_free(sma);
+			ipc_rcu_putref(sma);
+			return retval;
+		}
+	} else
+
+	sma->sem_perm.krgops = NULL;
+#endif
 	sem_unlock(sma);
 
 	return sma->sem_perm.id;
@@ -405,8 +452,27 @@
 	int error;
 	struct sem_queue * q;
 
+#ifdef CONFIG_KRG_IPC
+	/* the following is used to ensure that a node would not
+	   keep the sem for it */
+	int remote = 0, loop = 0;
+	if (sma->sem_perm.krgops) {
+		remote = get_random_int()%2;
+		loop = 1;
+	}
+begin:
+	if (remote)
+		q = list_entry(sma->remote_sem_pending.next, struct sem_queue, list);
+	else
+#endif
+
 	q = list_entry(sma->sem_pending.next, struct sem_queue, list);
+#ifdef CONFIG_KRG_IPC
+	while ((!remote && &q->list != &sma->sem_pending)
+	       || (remote && &q->list != &sma->remote_sem_pending)) {
+#else
 	while (&q->list != &sma->sem_pending) {
+#endif
 		error = try_atomic_semop(sma, q->sops, q->nsops,
 					 q->undo, q->pid);
 
@@ -433,6 +499,12 @@
 			 */
 			if (q->alter) {
 				list_del(&q->list);
+#ifdef CONFIG_KRG_IPC
+				if (remote)
+					n = list_entry(sma->remote_sem_pending.next,
+						       struct sem_queue, list);
+				else
+#endif
 				n = list_entry(sma->sem_pending.next,
 						struct sem_queue, list);
 			} else {
@@ -444,6 +516,11 @@
 			/* wake up the waiting thread */
 			q->status = IN_WAKEUP;
 
+#ifdef CONFIG_KRG_IPC
+			if (remote)
+				krg_ipc_sem_wakeup_process(q, error);
+			else
+#endif
 			wake_up_process(q->sleeper);
 			/* hands-off: q will disappear immediately after
 			 * writing q->status.
@@ -455,6 +532,13 @@
 			q = list_entry(q->list.next, struct sem_queue, list);
 		}
 	}
+#ifdef CONFIG_KRG_IPC
+	if (loop) {
+		remote = !remote;
+		loop = 0;
+		goto begin;
+	}
+#endif
 }
 
 /* The following counts are associated to each semaphore:
@@ -482,6 +566,18 @@
 			    && !(sops[i].sem_flg & IPC_NOWAIT))
 				semncnt++;
 	}
+#ifdef CONFIG_KRG_IPC
+	list_for_each_entry(q, &sma->remote_sem_pending, list) {
+		struct sembuf * sops = q->sops;
+		int nsops = q->nsops;
+		int i;
+		for (i = 0; i < nsops; i++)
+			if (sops[i].sem_num == semnum
+			    && (sops[i].sem_op < 0)
+			    && !(sops[i].sem_flg & IPC_NOWAIT))
+				semncnt++;
+	}
+#endif
 	return semncnt;
 }
 
@@ -501,6 +597,18 @@
 			    && !(sops[i].sem_flg & IPC_NOWAIT))
 				semzcnt++;
 	}
+#ifdef CONFIG_KRG_IPC
+	list_for_each_entry(q, &sma->remote_sem_pending, list) {
+		struct sembuf * sops = q->sops;
+		int nsops = q->nsops;
+		int i;
+		for (i = 0; i < nsops; i++)
+			if (sops[i].sem_num == semnum
+			    && (sops[i].sem_op == 0)
+			    && !(sops[i].sem_flg & IPC_NOWAIT))
+				semzcnt++;
+	}
+#endif
 	return semzcnt;
 }
 
@@ -514,14 +622,35 @@
  * as a writer and the spinlock for this semaphore set hold. sem_ids.rw_mutex
  * remains locked on exit.
  */
+#ifdef CONFIG_KRG_IPC
 static void freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)
 {
+	if (is_krg_ipc(&sem_ids(ns)))
+		krg_ipc_sem_freeary(ns, ipcp);
+	else
+		local_freeary(ns, ipcp);
+}
+
+void local_freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)
+#else
+static void freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)
+#endif
+{
 	struct sem_undo *un, *tu;
 	struct sem_queue *q, *tq;
 	struct sem_array *sma = container_of(ipcp, struct sem_array, sem_perm);
 
+#ifdef CONFIG_KRG_IPC
+	if (is_krg_ipc(&sem_ids(ns)))
+		BUG_ON(!list_empty(&sma->list_id));
+#endif
+
 	/* Free the existing undo structures for this semaphore set.  */
+#ifdef CONFIG_KRG_IPC
+	assert_mutex_locked(&sma->sem_perm.mutex);
+#else
 	assert_spin_locked(&sma->sem_perm.lock);
+#endif
 	list_for_each_entry_safe(un, tu, &sma->list_id, list_id) {
 		list_del(&un->list_id);
 		spin_lock(&un->ulp->lock);
@@ -540,10 +669,26 @@
 		smp_wmb();
 		q->status = -EIDRM;	/* hands-off q */
 	}
+#ifdef CONFIG_KRG_IPC
+	list_for_each_entry_safe(q, tq, &sma->remote_sem_pending, list) {
+		list_del(&q->list);
+
+		/* __freeary is called on every nodes where the semarray exists:
+		 * no need to care about remote pending processes */
+		if (q->undo)
+			kfree(q->undo);
+
+		free_semqueue(q);
+	}
+#endif
 
 	/* Remove the semaphore set from the IDR */
 	sem_rmid(ns, sma);
+#ifdef CONFIG_KRG_IPC
+	local_sem_unlock(sma);
+#else
 	sem_unlock(sma);
+#endif
 
 	ns->used_sems -= sma->sem_nsems;
 	security_sem_free(sma);
@@ -618,15 +763,32 @@
 		struct semid64_ds tbuf;
 		int id;
 
+#ifdef CONFIG_KRG_IPC
+		down_read(&sem_ids(ns).rw_mutex);
+#endif
 		if (cmd == SEM_STAT) {
 			sma = sem_lock(ns, semid);
 			if (IS_ERR(sma))
+#ifdef CONFIG_KRG_IPC
+			{
+				up_read(&sem_ids(ns).rw_mutex);
 				return PTR_ERR(sma);
+			}
+#else
+				return PTR_ERR(sma);
+#endif
 			id = sma->sem_perm.id;
 		} else {
 			sma = sem_lock_check(ns, semid);
 			if (IS_ERR(sma))
+#ifdef CONFIG_KRG_IPC
+			{
+				up_read(&sem_ids(ns).rw_mutex);
 				return PTR_ERR(sma);
+			}
+#else
+				return PTR_ERR(sma);
+#endif
 			id = 0;
 		}
 
@@ -645,6 +807,9 @@
 		tbuf.sem_ctime  = sma->sem_ctime;
 		tbuf.sem_nsems  = sma->sem_nsems;
 		sem_unlock(sma);
+#ifdef CONFIG_KRG_IPC
+		up_read(&sem_ids(ns).rw_mutex);
+#endif
 		if (copy_semid_to_user (arg.buf, &tbuf, version))
 			return -EFAULT;
 		return id;
@@ -655,6 +820,9 @@
 	return err;
 out_unlock:
 	sem_unlock(sma);
+#ifdef CONFIG_KRG_IPC
+	up_read(&sem_ids(ns).rw_mutex);
+#endif
 	return err;
 }
 
@@ -668,9 +836,19 @@
 	ushort* sem_io = fast_sem_io;
 	int nsems;
 
+#ifdef CONFIG_KRG_IPC
+	down_read(&sem_ids(ns).rw_mutex);
+#endif
 	sma = sem_lock_check(ns, semid);
 	if (IS_ERR(sma))
+#ifdef CONFIG_KRG_IPC
+	{
+		up_read(&sem_ids(ns).rw_mutex);
 		return PTR_ERR(sma);
+	}
+#else
+		return PTR_ERR(sma);
+#endif
 
 	nsems = sma->sem_nsems;
 
@@ -690,10 +868,18 @@
 		int i;
 
 		if(nsems > SEMMSL_FAST) {
+#ifndef CONFIG_KRG_IPC
 			sem_getref_and_unlock(sma);
+#endif
 
 			sem_io = ipc_alloc(sizeof(ushort)*nsems);
 			if(sem_io == NULL) {
+#ifdef CONFIG_KRG_IPC
+				err = -ENOMEM;
+				goto out_unlock;
+			}
+			BUG_ON(sma->sem_perm.deleted);
+#else
 				sem_putref(sma);
 				return -ENOMEM;
 			}
@@ -704,6 +890,7 @@
 				err = -EIDRM;
 				goto out_free;
 			}
+#endif
 		}
 
 		for (i = 0; i < sma->sem_nsems; i++)
@@ -718,41 +905,65 @@
 	{
 		int i;
 		struct sem_undo *un;
-
+#ifndef CONFIG_KRG_IPC
 		sem_getref_and_unlock(sma);
+#endif
 
 		if(nsems > SEMMSL_FAST) {
 			sem_io = ipc_alloc(sizeof(ushort)*nsems);
 			if(sem_io == NULL) {
+#ifdef CONFIG_KRG_IPC
+				err = -ENOMEM;
+				goto out_unlock;
+#else
 				sem_putref(sma);
 				return -ENOMEM;
+#endif
 			}
 		}
 
 		if (copy_from_user (sem_io, arg.array, nsems*sizeof(ushort))) {
+#ifdef CONFIG_KRG_IPC
+			err = -EFAULT;
+			goto out_unlock;
+#else
 			sem_putref(sma);
 			err = -EFAULT;
 			goto out_free;
+#endif
 		}
 
 		for (i = 0; i < nsems; i++) {
 			if (sem_io[i] > SEMVMX) {
+#ifdef CONFIG_KRG_IPC
+				err = -ERANGE;
+				goto out_unlock;
+#else
 				sem_putref(sma);
 				err = -ERANGE;
 				goto out_free;
+#endif
 			}
 		}
+#ifdef CONFIG_KRG_IPC
+		BUG_ON(sma->sem_perm.deleted);
+#else
 		sem_lock_and_putref(sma);
 		if (sma->sem_perm.deleted) {
 			sem_unlock(sma);
 			err = -EIDRM;
 			goto out_free;
 		}
+#endif
 
 		for (i = 0; i < nsems; i++)
 			sma->sem_base[i].semval = sem_io[i];
 
+#ifdef CONFIG_KRG_IPC
+		assert_mutex_locked(&sma->sem_perm.mutex);
+#else
 		assert_spin_locked(&sma->sem_perm.lock);
+#endif
 		list_for_each_entry(un, &sma->list_id, list_id) {
 			for (i = 0; i < nsems; i++)
 				un->semadj[i] = 0;
@@ -793,7 +1004,12 @@
 		if (val > SEMVMX || val < 0)
 			goto out_unlock;
 
+
+#ifdef CONFIG_KRG_IPC
+		assert_mutex_locked(&sma->sem_perm.mutex);
+#else
 		assert_spin_locked(&sma->sem_perm.lock);
+#endif
 		list_for_each_entry(un, &sma->list_id, list_id)
 			un->semadj[semnum] = 0;
 
@@ -809,6 +1025,9 @@
 out_unlock:
 	sem_unlock(sma);
 out_free:
+#ifdef CONFIG_KRG_IPC
+	up_read(&sem_ids(ns).rw_mutex);
+#endif
 	if(sem_io != fast_sem_io)
 		ipc_free(sem_io, sizeof(ushort)*nsems);
 	return err;
@@ -1005,20 +1224,41 @@
 
 	/* no undo structure around - allocate one. */
 	/* step 1: figure out the size of the semaphore array */
+#ifdef CONFIG_KRG_IPC
+	down_read(&sem_ids(ns).rw_mutex);
+#endif
 	sma = sem_lock_check(ns, semid);
 	if (IS_ERR(sma))
+#ifdef CONFIG_KRG_IPC
+	{
+		up_read(&sem_ids(ns).rw_mutex);
 		return ERR_PTR(PTR_ERR(sma));
+	}
+#else
+		return ERR_PTR(PTR_ERR(sma));
+#endif
 
 	nsems = sma->sem_nsems;
+
+#ifndef CONFIG_KRG_IPC
 	sem_getref_and_unlock(sma);
+#endif
 
 	/* step 2: allocate new undo structure */
 	new = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);
 	if (!new) {
+#ifdef CONFIG_KRG_IPC
+		sem_unlock(sma);
+		up_read(&sem_ids(ns).rw_mutex);
+#else
 		sem_putref(sma);
+#endif
 		return ERR_PTR(-ENOMEM);
 	}
 
+#ifdef CONFIG_KRG_IPC
+	BUG_ON(sma->sem_perm.deleted);
+#else
 	/* step 3: Acquire the lock on semaphore array */
 	sem_lock_and_putref(sma);
 	if (sma->sem_perm.deleted) {
@@ -1027,6 +1267,7 @@
 		un = ERR_PTR(-EIDRM);
 		goto out;
 	}
+#endif
 	spin_lock(&ulp->lock);
 
 	/*
@@ -1043,7 +1284,11 @@
 	new->semid = semid;
 	assert_spin_locked(&ulp->lock);
 	list_add_rcu(&new->list_proc, &ulp->list_proc);
+#ifdef CONFIG_KRG_IPC
+	assert_mutex_locked(&sma->sem_perm.mutex);
+#else
 	assert_spin_locked(&sma->sem_perm.lock);
+#endif
 	list_add(&new->list_id, &sma->list_id);
 	un = new;
 
@@ -1051,6 +1296,9 @@
 	spin_unlock(&ulp->lock);
 	rcu_read_lock();
 	sem_unlock(sma);
+#ifdef CONFIG_KRG_IPC
+	up_read(&sem_ids(ns).rw_mutex);
+#endif
 out:
 	return un;
 }
@@ -1106,6 +1354,11 @@
 			alter = 1;
 	}
 
+#ifdef CONFIG_KRG_IPC
+	if (is_krg_ipc(&sem_ids(ns)))
+		un = NULL;
+	else
+#endif
 	if (undos) {
 		un = find_alloc_undo(ns, semid);
 		if (IS_ERR(un)) {
@@ -1115,11 +1368,17 @@
 	} else
 		un = NULL;
 
+#ifdef CONFIG_KRG_IPC
+	down_read(&sem_ids(ns).rw_mutex);
+#endif
 	sma = sem_lock_check(ns, semid);
 	if (IS_ERR(sma)) {
 		if (un)
 			rcu_read_unlock();
 		error = PTR_ERR(sma);
+#ifdef CONFIG_KRG_IPC
+		up_read(&sem_ids(ns).rw_mutex);
+#endif
 		goto out_free;
 	}
 
@@ -1160,6 +1419,16 @@
 	if (error)
 		goto out_unlock_free;
 
+#ifdef CONFIG_KRG_IPC
+	if (undos && sma->sem_perm.krgops) {
+		un = krg_ipc_sem_find_undo(sma);
+		if (IS_ERR(un)) {
+			error = PTR_ERR(un);
+			goto out_unlock_free;
+		}
+	}
+#endif
+
 	error = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));
 	if (error <= 0) {
 		if (alter && error == 0)
@@ -1176,6 +1445,10 @@
 	queue.undo = un;
 	queue.pid = task_tgid_vnr(current);
 	queue.alter = alter;
+#ifdef CONFIG_KRG_IPC
+	queue.semid = sma->sem_perm.id;
+	queue.node = kerrighed_node_id;
+#endif
 	if (alter)
 		list_add_tail(&queue.list, &sma->sem_pending);
 	else
@@ -1185,6 +1458,9 @@
 	queue.sleeper = current;
 	current->state = TASK_INTERRUPTIBLE;
 	sem_unlock(sma);
+#ifdef CONFIG_KRG_IPC
+	up_read(&sem_ids(ns).rw_mutex);
+#endif
 
 	if (timeout)
 		jiffies_left = schedule_timeout(jiffies_left);
@@ -1203,12 +1479,30 @@
 		goto out_free;
 	}
 
+#ifdef CONFIG_KRG_IPC
+	down_read(&sem_ids(ns).rw_mutex);
+#endif
 	sma = sem_lock(ns, semid);
 	if (IS_ERR(sma)) {
 		error = -EIDRM;
+#ifdef CONFIG_KRG_IPC
+		up_read(&sem_ids(ns).rw_mutex);
+#endif
 		goto out_free;
 	}
 
+#if defined(CONFIG_KRG_IPC) && defined(CONFIG_KRG_EPM)
+	if (krg_action_any_pending(current)) {
+#ifdef CONFIG_KRG_DEBUG
+		printk("%s:%d - action kerrighed! --> need replay!!\n",
+		       __PRETTY_FUNCTION__, __LINE__);
+#endif
+		list_del(&queue.list);
+		error = -ERESTARTSYS;
+		goto out_unlock_free;
+	}
+#endif
+
 	/*
 	 * If queue.status != -EINTR we are woken up by another process
 	 */
@@ -1226,6 +1520,9 @@
 
 out_unlock_free:
 	sem_unlock(sma);
+#ifdef CONFIG_KRG_IPC
+	up_read(&sem_ids(ns).rw_mutex);
+#endif
 out_free:
 	if(sops != fast_sops)
 		kfree(sops);
@@ -1242,11 +1539,35 @@
  * parent and child tasks.
  */
 
+#ifdef CONFIG_KRG_IPC
+int __copy_semundo(unsigned long clone_flags, struct task_struct *tsk);
+
 int copy_semundo(unsigned long clone_flags, struct task_struct *tsk)
 {
+	struct ipc_namespace *ns;
+
+	ns = task_nsproxy(tsk)->ipc_ns;
+
+	if (is_krg_ipc(&sem_ids(ns)))
+		return krg_ipc_sem_copy_semundo(clone_flags, tsk);
+
+	return __copy_semundo(clone_flags, tsk);
+}
+
+int __copy_semundo(unsigned long clone_flags, struct task_struct *tsk)
+#else
+int copy_semundo(unsigned long clone_flags, struct task_struct *tsk)
+#endif
+{
 	struct sem_undo_list *undo_list;
 	int error;
 
+#ifdef CONFIG_KRG_IPC
+	BUG_ON((clone_flags & CLONE_SYSVSEM)
+	       && current->sysvsem.undo_list_id != UNIQUE_ID_NONE);
+	tsk->sysvsem.undo_list_id = UNIQUE_ID_NONE;
+#endif
+
 	if (clone_flags & CLONE_SYSVSEM) {
 		error = get_undo_list(&undo_list);
 		if (error)
@@ -1259,6 +1580,41 @@
 	return 0;
 }
 
+void __exit_sem_found(struct sem_array *sma, struct sem_undo *un)
+{
+	int i;
+
+	/* perform adjustments registered in un */
+	for (i = 0; i < sma->sem_nsems; i++) {
+		struct sem * semaphore = &sma->sem_base[i];
+		if (un->semadj[i]) {
+			semaphore->semval += un->semadj[i];
+			/*
+			 * Range checks of the new semaphore value,
+			 * not defined by sus:
+			 * - Some unices ignore the undo entirely
+			 *   (e.g. HP UX 11i 11.22, Tru64 V5.1)
+			 * - some cap the value (e.g. FreeBSD caps
+			 *   at 0, but doesn't enforce SEMVMX)
+			 *
+			 * Linux caps the semaphore value, both at 0
+			 * and at SEMVMX.
+			 *
+			 * 	Manfred <manfred@colorfullife.com>
+			 */
+			if (semaphore->semval < 0)
+				semaphore->semval = 0;
+			if (semaphore->semval > SEMVMX)
+				semaphore->semval = SEMVMX;
+			semaphore->sempid = task_tgid_vnr(current);
+		}
+	}
+	sma->sem_otime = get_seconds();
+	/* maybe some queued-up processes were waiting for this */
+	update_queue(sma);
+}
+
+
 /*
  * add semadj values to semaphores, free undo structures.
  * undo structures are not freed when semaphore arrays are destroyed
@@ -1271,7 +1627,11 @@
  * The current implementation does not do so. The POSIX standard
  * and SVID should be consulted to determine what behavior is mandated.
  */
+#ifdef CONFIG_KRG_IPC
+void __exit_sem(struct task_struct *tsk)
+#else
 void exit_sem(struct task_struct *tsk)
+#endif
 {
 	struct sem_undo_list *ulp;
 
@@ -1287,7 +1647,6 @@
 		struct sem_array *sma;
 		struct sem_undo *un;
 		int semid;
-		int i;
 
 		rcu_read_lock();
 		un = list_entry(rcu_dereference(ulp->list_proc.next),
@@ -1301,64 +1660,83 @@
 		if (semid == -1)
 			break;
 
+#ifdef CONFIG_KRG_IPC
+		down_read(&sem_ids(tsk->nsproxy->ipc_ns).rw_mutex);
+#endif
 		sma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);
 
 		/* exit_sem raced with IPC_RMID, nothing to do */
 		if (IS_ERR(sma))
+#ifdef CONFIG_KRG_IPC
+		{
+			up_read(&sem_ids(tsk->nsproxy->ipc_ns).rw_mutex);
+			continue;
+		}
+#else
 			continue;
+#endif
 
 		un = lookup_undo(ulp, semid);
 		if (un == NULL) {
 			/* exit_sem raced with IPC_RMID+semget() that created
 			 * exactly the same semid. Nothing to do.
 			 */
+#ifdef CONFIG_KRG_IPC
+			up_read(&sem_ids(tsk->nsproxy->ipc_ns).rw_mutex);
+#endif
 			sem_unlock(sma);
 			continue;
 		}
 
 		/* remove un from the linked lists */
+#ifdef CONFIG_KRG_IPC
+		assert_mutex_locked(&sma->sem_perm.mutex);
+#else
 		assert_spin_locked(&sma->sem_perm.lock);
+#endif
+
 		list_del(&un->list_id);
 
 		spin_lock(&ulp->lock);
 		list_del_rcu(&un->list_proc);
 		spin_unlock(&ulp->lock);
 
-		/* perform adjustments registered in un */
-		for (i = 0; i < sma->sem_nsems; i++) {
-			struct sem * semaphore = &sma->sem_base[i];
-			if (un->semadj[i]) {
-				semaphore->semval += un->semadj[i];
-				/*
-				 * Range checks of the new semaphore value,
-				 * not defined by sus:
-				 * - Some unices ignore the undo entirely
-				 *   (e.g. HP UX 11i 11.22, Tru64 V5.1)
-				 * - some cap the value (e.g. FreeBSD caps
-				 *   at 0, but doesn't enforce SEMVMX)
-				 *
-				 * Linux caps the semaphore value, both at 0
-				 * and at SEMVMX.
-				 *
-				 * 	Manfred <manfred@colorfullife.com>
-				 */
-				if (semaphore->semval < 0)
-					semaphore->semval = 0;
-				if (semaphore->semval > SEMVMX)
-					semaphore->semval = SEMVMX;
-				semaphore->sempid = task_tgid_vnr(current);
-			}
-		}
-		sma->sem_otime = get_seconds();
-		/* maybe some queued-up processes were waiting for this */
-		update_queue(sma);
+		__exit_sem_found(sma, un);
 		sem_unlock(sma);
-
+#ifdef CONFIG_KRG_IPC
+		up_read(&sem_ids(tsk->nsproxy->ipc_ns).rw_mutex);
+#endif
 		call_rcu(&un->rcu, free_un);
 	}
 	kfree(ulp);
 }
 
+#ifdef CONFIG_KRG_IPC
+void exit_sem(struct task_struct *tsk)
+{
+	struct ipc_namespace *ipcns;
+	struct nsproxy *ns;
+
+	ns = task_nsproxy(tsk);
+	if (!ns) { /* it happens when cleaning a failing fork */
+		if (krg_current)
+			ns = task_nsproxy(krg_current);
+		else
+			ns = task_nsproxy(current);
+	}
+
+	ipcns = ns->ipc_ns;
+
+	if (is_krg_ipc(&sem_ids(ipcns)))
+		krg_ipc_sem_exit_sem(ipcns, tsk);
+
+	/* let call __exit_sem in case process has been created
+	 * before the Kerrighed loading
+	 */
+	__exit_sem(tsk);
+}
+#endif
+
 #ifdef CONFIG_PROC_FS
 static int sysvipc_sem_proc_show(struct seq_file *s, void *it)
 {
diff -ruN linux-2.6.29/ipc/sem_handler.c android_cluster/linux-2.6.29/ipc/sem_handler.c
--- linux-2.6.29/ipc/sem_handler.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/sem_handler.c	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,780 @@
+/*
+ *  Kerrighed/modules/ipc/sem_handler.c
+ *
+ *  All the code for sharing IPC semaphore accross the cluster
+ *
+ *  Copyright (C) 2007-2008 Matthieu Fertré - INRIA
+ */
+#include <linux/ipc.h>
+#include <linux/ipc_namespace.h>
+#include <linux/msg.h>
+#include <linux/sem.h>
+#include <linux/nsproxy.h>
+#include <kddm/kddm.h>
+#include <kerrighed/pid.h>
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/hotplug.h>
+#include "ipc_handler.h"
+#include "sem_handler.h"
+#include "semarray_io_linker.h"
+#include "semundolst_io_linker.h"
+#include "ipcmap_io_linker.h"
+#include "util.h"
+#include "krgsem.h"
+
+struct semkrgops {
+	struct krgipc_ops krgops;
+	struct kddm_set *undo_list_kddm_set;
+
+	/* unique_id generator for sem_undo_list identifier */
+	unique_id_root_t undo_list_unique_id_root;
+};
+
+struct kddm_set *krgipc_ops_undolist_set(struct krgipc_ops *ipcops)
+{
+	struct semkrgops *semops;
+
+	semops = container_of(ipcops, struct semkrgops, krgops);
+
+	return semops->undo_list_kddm_set;
+}
+
+struct kddm_set *task_undolist_set(struct task_struct *task)
+{
+	struct ipc_namespace *ns;
+
+	ns = task_nsproxy(task)->ipc_ns;
+	if (!sem_ids(ns).krgops)
+		return ERR_PTR(-EINVAL);
+
+	return krgipc_ops_undolist_set(sem_ids(ns).krgops);
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                                KERNEL HOOKS                               */
+/*                                                                           */
+/*****************************************************************************/
+
+static struct kern_ipc_perm *kcb_ipc_sem_lock(struct ipc_ids *ids, int id)
+{
+	semarray_object_t *sem_object;
+	struct sem_array *sma;
+	int index ;
+
+	rcu_read_lock();
+
+	index = ipcid_to_idx(id);
+
+	sem_object = _kddm_grab_object_no_ft(ids->krgops->data_kddm_set, index);
+
+	if (!sem_object)
+		goto error;
+
+	sma = sem_object->local_sem;
+
+	BUG_ON(!sma);
+
+	mutex_lock(&sma->sem_perm.mutex);
+
+	if (sma->sem_perm.deleted) {
+		mutex_unlock(&sma->sem_perm.mutex);
+		goto error;
+	}
+
+	return &(sma->sem_perm);
+
+error:
+	_kddm_put_object(ids->krgops->data_kddm_set, index);
+	rcu_read_unlock();
+
+	return ERR_PTR(-EINVAL);
+}
+
+static void kcb_ipc_sem_unlock(struct kern_ipc_perm *ipcp)
+{
+	int index, deleted = 0;
+
+	index = ipcid_to_idx(ipcp->id);
+
+	if (ipcp->deleted)
+		deleted = 1;
+
+	_kddm_put_object(ipcp->krgops->data_kddm_set, index);
+
+	if (!deleted)
+		mutex_unlock(&ipcp->mutex);
+
+	rcu_read_unlock();
+}
+
+static struct kern_ipc_perm *kcb_ipc_sem_findkey(struct ipc_ids *ids, key_t key)
+{
+	long *key_index;
+	int id = -1;
+
+	key_index = _kddm_get_object_no_ft(ids->krgops->key_kddm_set, key);
+
+	if (key_index)
+		id = *key_index;
+
+	_kddm_put_object(ids->krgops->key_kddm_set, key);
+
+	if (id != -1)
+		return kcb_ipc_sem_lock(ids, id);
+
+	return NULL;
+}
+
+/** Notify the creation of a new IPC sem_array to Kerrighed.
+ *
+ *  @author Matthieu Fertré
+ */
+int krg_ipc_sem_newary(struct ipc_namespace *ns, struct sem_array *sma)
+{
+	semarray_object_t *sem_object;
+	long *key_index;
+	int index ;
+
+	BUG_ON(!sem_ids(ns).krgops);
+
+	index = ipcid_to_idx(sma->sem_perm.id);
+
+	sem_object = _kddm_grab_object_manual_ft(
+		sem_ids(ns).krgops->data_kddm_set, index);
+
+	BUG_ON(sem_object);
+
+	sem_object = kmem_cache_alloc(semarray_object_cachep, GFP_KERNEL);
+	if (!sem_object)
+		return -ENOMEM;
+
+	sem_object->local_sem = sma;
+	sem_object->mobile_sem_base = NULL;
+	sem_object->imported_sem = *sma;
+
+	/* there are no pending objects for the moment */
+	BUG_ON(!list_empty(&sma->sem_pending));
+	BUG_ON(!list_empty(&sma->remote_sem_pending));
+
+	INIT_LIST_HEAD(&sem_object->imported_sem.list_id);
+	INIT_LIST_HEAD(&sem_object->imported_sem.sem_pending);
+	INIT_LIST_HEAD(&sem_object->imported_sem.remote_sem_pending);
+
+	_kddm_set_object(sem_ids(ns).krgops->data_kddm_set, index, sem_object);
+
+	if (sma->sem_perm.key != IPC_PRIVATE)
+	{
+		key_index = _kddm_grab_object(sem_ids(ns).krgops->key_kddm_set,
+					      sma->sem_perm.key);
+		*key_index = index;
+		_kddm_put_object(sem_ids(ns).krgops->key_kddm_set,
+				 sma->sem_perm.key);
+	}
+
+	_kddm_put_object(sem_ids(ns).krgops->data_kddm_set, index);
+
+	sma->sem_perm.krgops = sem_ids(ns).krgops;
+
+	return 0;
+}
+
+static inline void __remove_semundo_from_proc_list(struct sem_array *sma,
+						   unique_id_t proc_list_id)
+{
+	struct semundo_id * undo_id, *next, *prev;
+	struct kddm_set *undo_list_set;
+	struct semundo_list_object *undo_list;
+
+	undo_list_set = krgipc_ops_undolist_set(sma->sem_perm.krgops);
+
+	undo_list = _kddm_grab_object_no_ft(undo_list_set, proc_list_id);
+
+	if (!undo_list)
+		goto exit;
+
+	prev = NULL;
+	for (undo_id = undo_list->list; undo_id; undo_id = next) {
+		next = undo_id->next;
+
+		if (undo_id->semid == sma->sem_perm.id) {
+			atomic_dec(&undo_list->semcnt);
+			kfree(undo_id);
+			if (!prev)
+				undo_list->list = next;
+			else
+				prev->next = next;
+
+			goto exit;
+		}
+		prev = undo_id;
+	}
+	BUG();
+
+exit:
+	_kddm_put_object(undo_list_set, proc_list_id);
+}
+
+void krg_ipc_sem_freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)
+{
+	int index;
+	struct sem_undo* un, *tu;
+	struct sem_array *sma = container_of(ipcp, struct sem_array, sem_perm);
+
+	index = ipcid_to_idx(ipcp->id);
+
+	/* removing the related semundo from the list per process */
+	list_for_each_entry_safe(un, tu, &sma->list_id, list_id) {
+		list_del(&un->list_id);
+		__remove_semundo_from_proc_list(sma, un->proc_list_id);
+		kfree(un);
+	}
+
+	if (ipcp->key != IPC_PRIVATE) {
+		_kddm_grab_object(sem_ids(ns).krgops->key_kddm_set,
+				  ipcp->key);
+		_kddm_remove_frozen_object(sem_ids(ns).krgops->key_kddm_set,
+					   ipcp->key);
+	}
+
+	local_sem_unlock(sma);
+	_kddm_remove_frozen_object(sem_ids(ns).krgops->data_kddm_set, index);
+
+	krg_ipc_rmid(&sem_ids(ns), index);
+}
+
+struct ipcsem_wakeup_msg {
+	kerrighed_node_t requester;
+	int sem_id;
+	pid_t pid;
+	int error;
+};
+
+void handle_ipcsem_wakeup_process(struct rpc_desc *desc, void *_msg,
+				  size_t size)
+{
+	struct ipcsem_wakeup_msg *msg = _msg;
+	struct sem_array *sma;
+	struct sem_queue *q, *tq;
+	struct ipc_namespace *ns;
+
+	ns = find_get_krg_ipcns();
+	BUG_ON(!ns);
+
+	/* take only a local lock because the requester node has the kddm lock
+	   on the semarray */
+	sma = local_sem_lock(ns, msg->sem_id);
+	BUG_ON(IS_ERR(sma));
+
+	list_for_each_entry_safe(q, tq, &sma->sem_pending, list) {
+		/* compare to q->sleeper's pid instead of q->pid
+		   because q->pid == q->sleeper's tgid */
+		if (task_pid_knr(q->sleeper) == msg->pid) {
+			list_del(&q->list);
+			goto found;
+		}
+	}
+
+	BUG();
+found:
+	q->status = 1; /* IN_WAKEUP; */
+
+	BUG_ON(!q->sleeper);
+	BUG_ON(q->pid != task_tgid_nr_ns(q->sleeper,
+					 task_active_pid_ns(q->sleeper)));
+
+	wake_up_process(q->sleeper);
+	smp_wmb();
+	q->status = msg->error;
+
+	local_sem_unlock(sma);
+
+	rpc_pack_type(desc, msg->error);
+
+	put_ipc_ns(ns);
+}
+
+void krg_ipc_sem_wakeup_process(struct sem_queue *q, int error)
+{
+	struct ipcsem_wakeup_msg msg;
+	struct rpc_desc *desc;
+
+	msg.requester = kerrighed_node_id;
+	msg.sem_id = q->semid;
+	msg.pid = remote_sleeper_pid(q); /* q->pid contains the tgid */
+	msg.error = error;
+
+	desc = rpc_begin(IPC_SEM_WAKEUP, q->node);
+	rpc_pack_type(desc, msg);
+	rpc_unpack_type(desc, msg.error);
+	rpc_end(desc, 0);
+}
+
+static inline struct semundo_list_object * __create_semundo_proc_list(
+	struct task_struct *task, struct kddm_set *undo_list_set)
+{
+	unique_id_t undo_list_id;
+	struct semundo_list_object *undo_list;
+	struct ipc_namespace *ns;
+	struct semkrgops *semops;
+
+	ns = task_nsproxy(task)->ipc_ns;
+	if (!sem_ids(ns).krgops)
+		return ERR_PTR(-EINVAL);
+
+	semops = container_of(sem_ids(ns).krgops, struct semkrgops, krgops);
+
+	/* get a random id */
+	undo_list_id = get_unique_id(&semops->undo_list_unique_id_root);
+
+	undo_list = _kddm_grab_object_manual_ft(undo_list_set, undo_list_id);
+
+	BUG_ON(undo_list);
+
+	undo_list = kzalloc(sizeof(struct semundo_list_object), GFP_KERNEL);
+	if (!undo_list) {
+		undo_list = ERR_PTR(-ENOMEM);
+		goto err_alloc;
+	}
+
+	undo_list->id = undo_list_id;
+	atomic_inc(&undo_list->refcnt);
+
+	_kddm_set_object(undo_list_set, undo_list_id, undo_list);
+
+	task->sysvsem.undo_list_id = undo_list_id;
+exit:
+	return undo_list;
+
+err_alloc:
+	_kddm_put_object(undo_list_set, undo_list_id);
+	goto exit;
+}
+
+int create_semundo_proc_list(struct task_struct *task)
+{
+	int r = 0;
+	struct kddm_set *undo_list_set;
+	struct semundo_list_object *undo_list;
+
+	BUG_ON(task->sysvsem.undo_list_id != UNIQUE_ID_NONE);
+
+	undo_list_set = task_undolist_set(task);
+	if (IS_ERR(undo_list_set)) {
+		undo_list = ERR_PTR(PTR_ERR(undo_list_set));
+		goto err;
+	}
+
+	undo_list = __create_semundo_proc_list(task, undo_list_set);
+
+	if (IS_ERR(undo_list)) {
+		r = PTR_ERR(undo_list);
+		goto err;
+	}
+
+	BUG_ON(atomic_read(&undo_list->refcnt) != 1);
+
+	_kddm_put_object(undo_list_set, task->sysvsem.undo_list_id);
+err:
+	return r;
+}
+
+
+static int __share_new_semundo(struct task_struct *task)
+{
+	int r = 0;
+	struct semundo_list_object *undo_list;
+	struct kddm_set *undo_list_set;
+
+	BUG_ON(krg_current);
+	BUG_ON(current->sysvsem.undo_list_id != UNIQUE_ID_NONE);
+
+	undo_list_set = task_undolist_set(task);
+	if (IS_ERR(undo_list_set)) {
+		r = PTR_ERR(undo_list_set);
+		goto exit;
+	}
+
+	undo_list = __create_semundo_proc_list(current, undo_list_set);
+
+	if (IS_ERR(undo_list)) {
+		r = PTR_ERR(undo_list);
+		goto exit;
+	}
+
+	task->sysvsem.undo_list_id = current->sysvsem.undo_list_id;
+	atomic_inc(&undo_list->refcnt);
+
+	BUG_ON(atomic_read(&undo_list->refcnt) != 2);
+
+	_kddm_put_object(undo_list_set, current->sysvsem.undo_list_id);
+exit:
+	return r;
+}
+
+int share_existing_semundo_proc_list(struct task_struct *task,
+				     unique_id_t undo_list_id)
+{
+	int r = 0;
+	struct semundo_list_object *undo_list;
+	struct kddm_set *undo_list_set;
+
+	undo_list_set = task_undolist_set(task);
+	if (IS_ERR(undo_list_set)) {
+		r = PTR_ERR(undo_list_set);
+		goto exit;
+	}
+
+	BUG_ON(undo_list_id == UNIQUE_ID_NONE);
+
+	undo_list = _kddm_grab_object_no_ft(undo_list_set, undo_list_id);
+
+	if (!undo_list) {
+		r = -ENOMEM;
+		goto exit_put;
+	}
+
+	task->sysvsem.undo_list_id = undo_list_id;
+	atomic_inc(&undo_list->refcnt);
+
+exit_put:
+	_kddm_put_object(undo_list_set, undo_list_id);
+exit:
+	return r;
+}
+
+int krg_ipc_sem_copy_semundo(unsigned long clone_flags,
+			     struct task_struct *tsk)
+{
+	int r = 0;
+
+	BUG_ON(!tsk);
+
+	if (krg_current)
+		goto exit;
+
+	if (clone_flags & CLONE_SYSVSEM) {
+
+		/* Do not support fork of process which had used semaphore
+		   before Kerrighed was loaded */
+		if (current->sysvsem.undo_list) {
+			printk("ERROR: Do not support fork of process (%d - %s)"
+			       "that had used semaphore before Kerrighed was "
+			       "started\n", tsk->tgid, tsk->comm);
+			r = -EPERM;
+			goto exit;
+		}
+
+		if (current->sysvsem.undo_list_id != UNIQUE_ID_NONE)
+			r = share_existing_semundo_proc_list(
+				tsk, current->sysvsem.undo_list_id);
+		else
+			r = __share_new_semundo(tsk);
+
+	} else
+		/* undolist will be only created when needed */
+		tsk->sysvsem.undo_list_id = UNIQUE_ID_NONE;
+
+	/* pointer to undo_list is useless in KRG implementation of semaphores */
+	tsk->sysvsem.undo_list = NULL;
+
+exit:
+	return r;
+}
+
+int add_semundo_to_proc_list(struct semundo_list_object *undo_list, int semid)
+{
+	struct semundo_id *undo_id;
+	int r = 0;
+	BUG_ON(!undo_list);
+
+#ifdef CONFIG_KRG_DEBUG
+	/* WARNING: this is a paranoiac checking */
+	for (undo_id = undo_list->list; undo_id; undo_id = undo_id->next) {
+		if (undo_id->semid == semid) {
+			printk("%p %p %d %d\n", undo_id,
+			       undo_list, semid,
+			       atomic_read(&undo_list->semcnt));
+			BUG();
+		}
+	}
+#endif
+
+	undo_id = kmalloc(sizeof(struct semundo_id), GFP_KERNEL);
+	if (!undo_id) {
+		r = -ENOMEM;
+		goto exit;
+	}
+
+	atomic_inc(&undo_list->semcnt);
+	undo_id->semid = semid;
+	undo_id->next = undo_list->list;
+	undo_list->list = undo_id;
+exit:
+	return r;
+}
+
+struct sem_undo * krg_ipc_sem_find_undo(struct sem_array* sma)
+{
+	struct sem_undo * undo;
+	int r = 0;
+	struct kddm_set *undo_list_set;
+	struct semundo_list_object *undo_list = NULL;
+	unique_id_t undo_list_id;
+
+	undo_list_set = krgipc_ops_undolist_set(sma->sem_perm.krgops);
+	if (IS_ERR(undo_list_set)) {
+		undo = ERR_PTR(PTR_ERR(undo_list_set));
+		goto exit;
+	}
+
+	if (current->sysvsem.undo_list_id == UNIQUE_ID_NONE) {
+
+		/* create a undolist if not yet allocated */
+		undo_list = __create_semundo_proc_list(current, undo_list_set);
+
+		if (IS_ERR(undo_list)) {
+			undo = ERR_PTR(PTR_ERR(undo_list));
+			goto exit;
+		}
+
+		BUG_ON(atomic_read(&undo_list->semcnt) != 0);
+
+	} else {
+		/* check in the undo list of the sma */
+		list_for_each_entry(undo, &sma->list_id, list_id) {
+			if (undo->proc_list_id ==
+			    current->sysvsem.undo_list_id) {
+				goto exit;
+			}
+		}
+	}
+
+	undo_list_id = current->sysvsem.undo_list_id;
+
+	/* allocate one */
+	undo = kzalloc(sizeof(struct sem_undo) +
+		       sizeof(short)*(sma->sem_nsems), GFP_KERNEL);
+	if (!undo) {
+		undo = ERR_PTR(-ENOMEM);
+		goto exit_put_kddm;
+	}
+
+	INIT_LIST_HEAD(&undo->list_proc);
+	undo->proc_list_id = undo_list_id;
+	undo->semid = sma->sem_perm.id;
+	undo->semadj = (short *) &undo[1];
+
+	list_add(&undo->list_id, &sma->list_id);
+
+	/* reference it in the undo_list per process*/
+	BUG_ON(undo_list_id == UNIQUE_ID_NONE);
+
+	if (!undo_list)
+		undo_list = _kddm_grab_object_no_ft(undo_list_set,
+						    undo_list_id);
+
+	if (!undo_list) {
+		r = -ENOMEM;
+		goto exit_free_undo;
+	}
+
+	r = add_semundo_to_proc_list(undo_list, undo->semid);
+
+exit_free_undo:
+	if (r) {
+		list_del(&undo->list_id);
+		kfree(undo);
+		undo = ERR_PTR(r);
+	}
+
+exit_put_kddm:
+	if (undo_list && !IS_ERR(undo_list))
+		_kddm_put_object(undo_list_set, undo_list_id);
+exit:
+	return undo;
+}
+
+static inline void __remove_semundo_from_sem_list(struct ipc_namespace *ns,
+						  int semid,
+						  unique_id_t undo_list_id)
+{
+	struct sem_array *sma;
+	struct sem_undo *un, *tu;
+
+	sma = sem_lock(ns, semid);
+	if (IS_ERR(sma))
+		return;
+
+	list_for_each_entry_safe(un, tu, &sma->list_id, list_id) {
+		if (un->proc_list_id == undo_list_id) {
+			list_del(&un->list_id);
+			__exit_sem_found(sma, un);
+
+			kfree(un);
+			goto exit_unlock;
+		}
+	}
+	BUG();
+
+exit_unlock:
+	sem_unlock(sma);
+}
+
+void krg_ipc_sem_exit_sem(struct ipc_namespace *ns,
+			  struct task_struct * task)
+{
+	struct kddm_set *undo_list_kddm_set;
+	unique_id_t undo_list_id;
+	struct semundo_list_object * undo_list;
+	struct semundo_id * undo_id, *next;
+
+	if (task->sysvsem.undo_list_id == UNIQUE_ID_NONE)
+		return;
+
+	undo_list_kddm_set = krgipc_ops_undolist_set(sem_ids(ns).krgops);
+	if (IS_ERR(undo_list_kddm_set)) {
+		BUG();
+		return;
+	}
+
+	undo_list_id = task->sysvsem.undo_list_id;
+
+	undo_list = _kddm_grab_object_no_ft(undo_list_kddm_set, undo_list_id);
+	if (!undo_list) {
+		printk("undo_list_id: %lu\n", undo_list_id);
+		BUG();
+	}
+	if (!atomic_dec_and_test(&undo_list->refcnt))
+		goto exit_wo_action;
+
+	for (undo_id = undo_list->list; undo_id; undo_id = next) {
+		next = undo_id->next;
+		__remove_semundo_from_sem_list(ns, undo_id->semid,
+					       undo_list_id);
+		kfree(undo_id);
+	}
+	undo_list->list = NULL;
+	atomic_set(&undo_list->semcnt, 0);
+
+	_kddm_remove_frozen_object(undo_list_kddm_set, undo_list_id);
+
+	return;
+
+exit_wo_action:
+	_kddm_put_object(undo_list_kddm_set, undo_list_id);
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              INITIALIZATION                               */
+/*                                                                           */
+/*****************************************************************************/
+
+int krg_sem_init_ns(struct ipc_namespace *ns)
+{
+	int r;
+
+	struct semkrgops *sem_ops = kmalloc(sizeof(struct semkrgops),
+					     GFP_KERNEL);
+	if (!sem_ops) {
+		r = -ENOMEM;
+		goto err;
+	}
+
+	sem_ops->krgops.map_kddm_set = create_new_kddm_set(
+		kddm_def_ns, SEMMAP_KDDM_ID, IPCMAP_LINKER,
+		KDDM_RR_DEF_OWNER, sizeof(ipcmap_object_t),
+		KDDM_LOCAL_EXCLUSIVE);
+
+	if (IS_ERR(sem_ops->krgops.map_kddm_set)) {
+		r = PTR_ERR(sem_ops->krgops.map_kddm_set);
+		goto err_map;
+	}
+
+	sem_ops->krgops.key_kddm_set = create_new_kddm_set(
+		kddm_def_ns, SEMKEY_KDDM_ID, SEMKEY_LINKER,
+		KDDM_RR_DEF_OWNER, sizeof(long), KDDM_LOCAL_EXCLUSIVE);
+
+	if (IS_ERR(sem_ops->krgops.key_kddm_set)) {
+		r = PTR_ERR(sem_ops->krgops.key_kddm_set);
+		goto err_key;
+	}
+
+	sem_ops->krgops.data_kddm_set = create_new_kddm_set(
+		kddm_def_ns, SEMARRAY_KDDM_ID, SEMARRAY_LINKER,
+		KDDM_RR_DEF_OWNER, sizeof(semarray_object_t),
+		KDDM_LOCAL_EXCLUSIVE);
+
+	if (IS_ERR(sem_ops->krgops.data_kddm_set)) {
+		r = PTR_ERR(sem_ops->krgops.data_kddm_set);
+		goto err_data;
+	}
+
+	sem_ops->undo_list_kddm_set = create_new_kddm_set(
+		kddm_def_ns, SEMUNDO_KDDM_ID, SEMUNDO_LINKER,
+		KDDM_RR_DEF_OWNER, sizeof(struct semundo_list_object),
+		KDDM_LOCAL_EXCLUSIVE);
+
+	if (IS_ERR(sem_ops->undo_list_kddm_set)) {
+		r = PTR_ERR(sem_ops->undo_list_kddm_set);
+		goto err_undolist;
+	}
+
+	init_unique_id_root(&sem_ops->undo_list_unique_id_root);
+
+	sem_ops->krgops.ipc_lock = kcb_ipc_sem_lock;
+	sem_ops->krgops.ipc_unlock = kcb_ipc_sem_unlock;
+	sem_ops->krgops.ipc_findkey = kcb_ipc_sem_findkey;
+
+	sem_ids(ns).krgops = &sem_ops->krgops;
+
+	return 0;
+
+err_undolist:
+	_destroy_kddm_set(sem_ops->krgops.data_kddm_set);
+err_data:
+	_destroy_kddm_set(sem_ops->krgops.key_kddm_set);
+err_key:
+	_destroy_kddm_set(sem_ops->krgops.map_kddm_set);
+err_map:
+	kfree(sem_ops);
+err:
+	return r;
+}
+
+void krg_sem_exit_ns(struct ipc_namespace *ns)
+{
+	if (sem_ids(ns).krgops) {
+		struct semkrgops *sem_ops;
+
+		sem_ops = container_of(sem_ids(ns).krgops, struct semkrgops,
+				      krgops);
+
+		_destroy_kddm_set(sem_ops->undo_list_kddm_set);
+		_destroy_kddm_set(sem_ops->krgops.data_kddm_set);
+		_destroy_kddm_set(sem_ops->krgops.key_kddm_set);
+		_destroy_kddm_set(sem_ops->krgops.map_kddm_set);
+
+		kfree(sem_ops);
+	}
+}
+
+void sem_handler_init (void)
+{
+	semarray_object_cachep = kmem_cache_create("semarray_object",
+						   sizeof(semarray_object_t),
+						   0, SLAB_PANIC, NULL);
+
+	register_io_linker(SEMARRAY_LINKER, &semarray_linker);
+	register_io_linker(SEMKEY_LINKER, &semkey_linker);
+	register_io_linker(SEMUNDO_LINKER, &semundo_linker);
+
+	rpc_register_void(IPC_SEM_WAKEUP, handle_ipcsem_wakeup_process, 0);
+}
+
+
+
+void sem_handler_finalize (void)
+{
+}
diff -ruN linux-2.6.29/ipc/sem_handler.h android_cluster/linux-2.6.29/ipc/sem_handler.h
--- linux-2.6.29/ipc/sem_handler.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/sem_handler.h	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,26 @@
+/** Interface of IPC semaphore (sem) management.
+ *  @file sem_handler.h
+ *
+ *  @author Matthieu Fertré
+ */
+#ifndef SEM_HANDLER_H
+#define SEM_HANDLER_H
+
+#include <linux/sem.h>
+
+int share_existing_semundo_proc_list(struct task_struct *tsk,
+				     unique_id_t undo_list_id);
+int create_semundo_proc_list(struct task_struct *tsk);
+
+struct kddm_set;
+
+struct kddm_set *krgipc_ops_undolist_set(struct krgipc_ops *ipcops);
+struct kddm_set *task_undolist_set(struct task_struct *task);
+
+struct semundo_list_object;
+int add_semundo_to_proc_list(struct semundo_list_object *undo_list, int semid);
+
+void sem_handler_init(void);
+void sem_handler_finalize(void);
+
+#endif // SEM_HANDLER_H
diff -ruN linux-2.6.29/ipc/semundolst_io_linker.c android_cluster/linux-2.6.29/ipc/semundolst_io_linker.c
--- linux-2.6.29/ipc/semundolst_io_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/semundolst_io_linker.c	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,231 @@
+/*
+ *  Kerrighed/modules/ipc/semundolst_io_linker.c
+ *
+ *  KDDM SEM undo proc list Linker.
+ *
+ *  Copyright (C) 2007-2008 Matthieu Fertré - INRIA
+ */
+#include <linux/sem.h>
+#include <linux/lockdep.h>
+#include <linux/security.h>
+#include <linux/ipc.h>
+#include <linux/ipc_namespace.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+
+#include "semundolst_io_linker.h"
+
+/*****************************************************************************/
+/*                                                                           */
+/*                         SEM Undo list KDDM IO FUNCTIONS                   */
+/*                                                                           */
+/*****************************************************************************/
+
+static inline void __undolist_remove(struct semundo_list_object *undo_list)
+{
+	struct semundo_id *id, *next;
+
+	if (undo_list) {
+		for (id = undo_list->list; id; id = next) {
+			next = id->next;
+			kfree(id);
+		}
+		undo_list->list = NULL;
+	}
+}
+
+static inline struct semundo_list_object * __undolist_alloc(void)
+{
+	struct semundo_list_object *undo_list;
+
+	undo_list = kzalloc(sizeof(struct semundo_list_object), GFP_KERNEL);
+	if (!undo_list)
+		return ERR_PTR(-ENOMEM);
+
+	return undo_list;
+}
+
+/** Handle a kddm set sem_undo_list alloc
+ *  @author Matthieu Fertré
+ *
+ *  @param  obj_entry  Kddm object descriptor.
+ *  @param  set       Kddm set descriptor
+ *  @param  objid     Id of the object to create.
+ */
+int undolist_alloc_object (struct kddm_obj * obj_entry,
+			   struct kddm_set * set,
+			   objid_t objid)
+{
+	struct semundo_list_object *undo_list;
+
+	undo_list = __undolist_alloc();
+	if (IS_ERR(undo_list))
+		return PTR_ERR(undo_list);
+
+	obj_entry->object = undo_list;
+	return 0;
+}
+
+
+/** Handle a kddm set sem_undo_list first touch
+ *  @author Matthieu Fertré
+ *
+ *  @param  obj_entry  Kddm object descriptor.
+ *  @param  set       Kddm set descriptor
+ *  @param  objid     Id of the object to create.
+ *
+ *  @return  0 if everything is ok. Negative value otherwise.
+ */
+int undolist_first_touch (struct kddm_obj * obj_entry,
+			  struct kddm_set * set,
+			  objid_t objid,
+			  int flags)
+{
+	BUG();
+	return -EINVAL;
+}
+
+/** Handle a kddm sem_undo_list remove.
+ *  @author Matthieu Fertré
+ *
+ *  @param  obj_entry  Descriptor of the object to remove.
+ *  @param  set       Kddm set descriptor.
+ *  @param  padeid    Id of the object to remove.
+ */
+int undolist_remove_object (void *object,
+			    struct kddm_set * set,
+			    objid_t objid)
+{
+	struct semundo_list_object *undo_list;
+	undo_list = object;
+
+	__undolist_remove(undo_list);
+	kfree(undo_list);
+	object = NULL;
+
+	return 0;
+}
+
+/** Invalidate a kddm sem_undo_list
+ *  @author Matthieu Fertré
+ *
+ *  @param  obj_entry  Descriptor of the object to invalidate.
+ *  @param  set       Kddm set descriptor
+ *  @param  objid     Id of the object to invalidate
+ */
+int undolist_invalidate_object (struct kddm_obj * obj_entry,
+				struct kddm_set * set,
+				objid_t objid)
+{
+	struct semundo_list_object *undo_list;
+	undo_list = obj_entry->object;
+
+	__undolist_remove(undo_list);
+	obj_entry->object = NULL;
+
+	return 0;
+}
+
+/** Export a sem_undo_list
+ *  @author Matthieu Fertré
+ *
+ *  @param  buffer    Buffer to export object data in.
+ *  @param  object    The object to export data from.
+ */
+int undolist_export_object (struct rpc_desc *desc,
+			    struct kddm_set *set,
+			    struct kddm_obj *obj_entry,
+			    objid_t objid,
+			    int flags)
+{
+	struct semundo_list_object *undo_list;
+	struct semundo_id *un;
+	int nb_semundo = 0, r;
+
+	undo_list = obj_entry->object;
+
+	r = rpc_pack_type(desc, *undo_list);
+	if (r)
+		goto error;
+
+	/* counting number of semundo to send */
+	for (un = undo_list->list; un;  un = un->next)
+		nb_semundo++;
+
+	r = rpc_pack_type(desc, nb_semundo);
+
+	BUG_ON(nb_semundo != atomic_read(&undo_list->semcnt));
+
+	/* really sending the semundo identifier */
+	for (un = undo_list->list; un;  un = un->next) {
+		r = rpc_pack_type(desc, *un);
+		if (r)
+			goto error;
+	}
+error:
+	return r;
+}
+
+/** Import a sem_undo_list
+ *  @author Matthieu Fertré
+ *
+ *  @param  object    The object to import data in.
+ *  @param  buffer    Data to import in the object.
+ */
+int undolist_import_object (struct rpc_desc *desc,
+			    struct kddm_set *set,
+			    struct kddm_obj *obj_entry,
+			    objid_t objid,
+			    int flags)
+{
+	struct semundo_list_object *undo_list;
+	struct semundo_id *un, *prev = NULL;
+	int nb_semundo = 0, i=0, r;
+
+	undo_list = obj_entry->object;
+
+	r = rpc_unpack_type(desc, *undo_list);
+	if (r)
+		goto error;
+
+	r = rpc_unpack_type(desc, nb_semundo);
+	if (r)
+		goto error;
+
+	BUG_ON(nb_semundo != atomic_read(&undo_list->semcnt));
+
+	for (i=0; i < nb_semundo; i++) {
+		un = kmalloc(sizeof(struct semundo_id), GFP_KERNEL);
+		if (!un) {
+			r = -ENOMEM;
+			goto error;
+		}
+
+		r = rpc_unpack_type(desc, *un);
+		if (r)
+			goto error;
+
+		un->next = NULL;
+		if (prev)
+			prev->next = un;
+		else
+			undo_list->list = un;
+		prev = un;
+	}
+error:
+	return r;
+}
+
+/****************************************************************************/
+
+/* Init the sem_undo_list IO linker */
+struct iolinker_struct semundo_linker = {
+	first_touch:       undolist_first_touch,
+	remove_object:     undolist_remove_object,
+	invalidate_object: undolist_invalidate_object,
+	linker_name:       "semundo",
+	linker_id:         SEMUNDO_LINKER,
+	alloc_object:      undolist_alloc_object,
+	export_object:     undolist_export_object,
+	import_object:     undolist_import_object
+};
diff -ruN linux-2.6.29/ipc/semundolst_io_linker.h android_cluster/linux-2.6.29/ipc/semundolst_io_linker.h
--- linux-2.6.29/ipc/semundolst_io_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/semundolst_io_linker.h	2014-05-27 23:04:10.422028007 -0700
@@ -0,0 +1,33 @@
+/** KDDM SEM_UNDO proc_list Linker.
+ *  @file semundolst_io_linker.h
+ *
+ *  Link KDDM and Linux SEM_UNDO proc list mechanisms.
+ *  @author Matthieu Fertré
+ */
+
+#ifndef __SEMUNDOLST_IO_LINKER__
+#define __SEMUNDOLST_IO_LINKER__
+
+#include <kddm/kddm_types.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+struct semundo_id {
+	int semid;
+	struct semundo_id *next;
+};
+
+struct semundo_list_object {
+	unique_id_t id;
+	atomic_t refcnt;
+	atomic_t semcnt;
+	struct semundo_id *list;
+};
+
+extern struct iolinker_struct semundo_linker;
+
+#endif
diff -ruN linux-2.6.29/ipc/shm.c android_cluster/linux-2.6.29/ipc/shm.c
--- linux-2.6.29/ipc/shm.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/ipc/shm.c	2014-05-27 23:04:10.422028007 -0700
@@ -41,9 +41,13 @@
 #include <linux/ipc_namespace.h>
 
 #include <asm/uaccess.h>
+#ifdef CONFIG_KRG_IPC
+#include "krgshm.h"
+#endif
 
 #include "util.h"
 
+#ifndef CONFIG_KRG_IPC
 struct shm_file_data {
 	int id;
 	struct ipc_namespace *ns;
@@ -52,18 +56,30 @@
 };
 
 #define shm_file_data(file) (*((struct shm_file_data **)&(file)->private_data))
+#endif
 
-static const struct file_operations shm_file_operations;
-static struct vm_operations_struct shm_vm_ops;
+#ifndef CONFIG_KRG_IPC
+static
+#endif
+const struct file_operations shm_file_operations;
+#ifndef CONFIG_KRG_IPC
+static
+#endif
+struct vm_operations_struct shm_vm_ops;
 
+#ifndef CONFIG_KRG_IPC
 #define shm_ids(ns)	((ns)->ids[IPC_SHM_IDS])
 
 #define shm_unlock(shp)			\
 	ipc_unlock(&(shp)->shm_perm)
+#endif
 
+#ifndef CONFIG_KRG_IPC
 static int newseg(struct ipc_namespace *, struct ipc_params *);
+#endif
 static void shm_open(struct vm_area_struct *vma);
 static void shm_close(struct vm_area_struct *vma);
+
 static void shm_destroy (struct ipc_namespace *ns, struct shmid_kernel *shp);
 #ifdef CONFIG_PROC_FS
 static int sysvipc_shm_proc_show(struct seq_file *s, void *it);
@@ -88,6 +104,11 @@
 	shp = container_of(ipcp, struct shmid_kernel, shm_perm);
 
 	if (shp->shm_nattch){
+#ifdef CONFIG_KRG_IPC
+		if (is_krg_ipc(&shm_ids(ns))
+		    && shp->shm_perm.key != IPC_PRIVATE)
+			krg_ipc_shm_rmkey(ns, shp->shm_perm.key);
+#endif
 		shp->shm_perm.mode |= SHM_DEST;
 		/* Do not find it any more */
 		shp->shm_perm.key = IPC_PRIVATE;
@@ -100,6 +121,7 @@
 void shm_exit_ns(struct ipc_namespace *ns)
 {
 	free_ipcs(ns, &shm_ids(ns), do_shm_rmid);
+	idr_destroy(&ns->ids[IPC_SHM_IDS].ipcs_idr);
 }
 #endif
 
@@ -115,7 +137,10 @@
  * shm_lock_(check_) routines are called in the paths where the rw_mutex
  * is not necessarily held.
  */
-static inline struct shmid_kernel *shm_lock(struct ipc_namespace *ns, int id)
+#ifndef CONFIG_KRG_IPC
+static inline
+#endif
+struct shmid_kernel *shm_lock(struct ipc_namespace *ns, int id)
 {
 	struct kern_ipc_perm *ipcp = ipc_lock(&shm_ids(ns), id);
 
@@ -125,7 +150,10 @@
 	return container_of(ipcp, struct shmid_kernel, shm_perm);
 }
 
-static inline struct shmid_kernel *shm_lock_check(struct ipc_namespace *ns,
+#ifndef CONFIG_KRG_IPC
+static inline
+#endif
+struct shmid_kernel *shm_lock_check(struct ipc_namespace *ns,
 						int id)
 {
 	struct kern_ipc_perm *ipcp = ipc_lock_check(&shm_ids(ns), id);
@@ -149,12 +177,18 @@
 	struct shm_file_data *sfd = shm_file_data(file);
 	struct shmid_kernel *shp;
 
+#ifdef CONFIG_KRG_IPC
+	down_read(&shm_ids(sfd->ns).rw_mutex);
+#endif
 	shp = shm_lock(sfd->ns, sfd->id);
 	BUG_ON(IS_ERR(shp));
 	shp->shm_atim = get_seconds();
 	shp->shm_lprid = task_tgid_vnr(current);
 	shp->shm_nattch++;
 	shm_unlock(shp);
+#ifdef CONFIG_KRG_IPC
+	up_read(&shm_ids(sfd->ns).rw_mutex);
+#endif
 }
 
 /*
@@ -166,11 +200,19 @@
  * It has to be called with shp and shm_ids.rw_mutex (writer) locked,
  * but returns with shp unlocked and freed.
  */
+#ifdef CONFIG_KRG_IPC
+void local_shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp)
+#else
 static void shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp)
+#endif
 {
 	ns->shm_tot -= (shp->shm_segsz + PAGE_SIZE - 1) >> PAGE_SHIFT;
 	shm_rmid(ns, shp);
+#ifdef CONFIG_KRG_IPC
+	local_shm_unlock(shp);
+#else
 	shm_unlock(shp);
+#endif
 	if (!is_file_hugepages(shp->shm_file))
 		shmem_lock(shp->shm_file, 0, shp->mlock_user);
 	else
@@ -181,6 +223,16 @@
 	ipc_rcu_putref(shp);
 }
 
+#ifdef CONFIG_KRG_IPC
+static void shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp)
+{
+	if (is_krg_ipc(&shm_ids(ns)))
+		krg_ipc_shm_destroy(ns, shp);
+	else
+		local_shm_destroy(ns, shp);
+}
+#endif
+
 /*
  * remove the attach descriptor vma.
  * free memory for segment if it is marked destroyed.
@@ -217,6 +269,21 @@
 	return sfd->vm_ops->fault(vma, vmf);
 }
 
+#ifdef CONFIG_KRG_IPC
+static struct page *shm_wppage (struct vm_area_struct *vma,
+				unsigned long address,
+				struct page *old_page)
+{
+	struct file *file = vma->vm_file;
+	struct shm_file_data *sfd = shm_file_data(file);
+
+	if (sfd->vm_ops->wppage)
+		return sfd->vm_ops->wppage(vma, address, old_page);
+	else
+		return ERR_PTR(EPERM);
+}
+#endif
+
 #ifdef CONFIG_NUMA
 static int shm_set_policy(struct vm_area_struct *vma, struct mempolicy *new)
 {
@@ -249,6 +316,9 @@
 	struct shm_file_data *sfd = shm_file_data(file);
 	int ret;
 
+#ifdef CONFIG_KRG_IPC
+	sfd->file->private_data = sfd;
+#endif
 	ret = sfd->file->f_op->mmap(sfd->file, vma);
 	if (ret != 0)
 		return ret;
@@ -304,17 +374,26 @@
 	return ret;
 }
 
-static const struct file_operations shm_file_operations = {
+#ifndef CONFIG_KRG_IPC
+static
+#endif
+const struct file_operations shm_file_operations = {
 	.mmap		= shm_mmap,
 	.fsync		= shm_fsync,
 	.release	= shm_release,
 	.get_unmapped_area	= shm_get_unmapped_area,
 };
 
-static struct vm_operations_struct shm_vm_ops = {
+#ifndef CONFIG_KRG_IPC
+static
+#endif
+struct vm_operations_struct shm_vm_ops = {
 	.open	= shm_open,	/* callback for a new vm-area open */
 	.close	= shm_close,	/* callback for when the vm-area is released */
 	.fault	= shm_fault,
+#ifdef CONFIG_KRG_IPC
+	.wppage	= shm_wppage,
+#endif
 #if defined(CONFIG_NUMA)
 	.set_policy = shm_set_policy,
 	.get_policy = shm_get_policy,
@@ -328,8 +407,10 @@
  *
  * Called with shm_ids.rw_mutex held as a writer.
  */
-
-static int newseg(struct ipc_namespace *ns, struct ipc_params *params)
+#ifndef CONFIG_KRG_IPC
+static
+#endif
+int newseg(struct ipc_namespace *ns, struct ipc_params *params)
 {
 	key_t key = params->key;
 	int shmflg = params->flg;
@@ -384,7 +465,12 @@
 	if (IS_ERR(file))
 		goto no_file;
 
+#ifdef CONFIG_KRG_IPC
+	id = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni,
+		       params->requested_id);
+#else
 	id = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);
+#endif
 	if (id < 0) {
 		error = id;
 		goto no_id;
@@ -404,7 +490,16 @@
 	file->f_dentry->d_inode->i_ino = shp->shm_perm.id;
 
 	ns->shm_tot += numpages;
+#ifdef CONFIG_KRG_IPC
+	if (is_krg_ipc(&shm_ids(ns))) {
+		error = krg_ipc_shm_newseg(ns, shp) ;
+		if (error)
+			goto no_file;
+	} else
+		shp->shm_perm.krgops = NULL;
+#endif
 	error = shp->shm_perm.id;
+
 	shm_unlock(shp);
 	return error;
 
@@ -698,18 +793,29 @@
 		struct shmid64_ds tbuf;
 		int result;
 
+#ifdef CONFIG_KRG_IPC
+		down_read(&shm_ids(ns).rw_mutex);
+#endif
 		if (cmd == SHM_STAT) {
 			shp = shm_lock(ns, shmid);
 			if (IS_ERR(shp)) {
 				err = PTR_ERR(shp);
+#ifdef CONFIG_KRG_IPC
+				goto out_unlock_ns;
+#else
 				goto out;
+#endif
 			}
 			result = shp->shm_perm.id;
 		} else {
 			shp = shm_lock_check(ns, shmid);
 			if (IS_ERR(shp)) {
 				err = PTR_ERR(shp);
+#ifdef CONFIG_KRG_IPC
+				goto out_unlock_ns;
+#else
 				goto out;
+#endif
 			}
 			result = 0;
 		}
@@ -729,6 +835,9 @@
 		tbuf.shm_lpid	= shp->shm_lprid;
 		tbuf.shm_nattch	= shp->shm_nattch;
 		shm_unlock(shp);
+#ifdef CONFIG_KRG_IPC
+		up_read(&shm_ids(ns).rw_mutex);
+#endif
 		if(copy_shmid_to_user (buf, &tbuf, version))
 			err = -EFAULT;
 		else
@@ -742,10 +851,17 @@
 
 		lru_add_drain_all();  /* drain pagevecs to lru lists */
 
+#ifdef CONFIG_KRG_IPC
+		down_read(&shm_ids(ns).rw_mutex);
+#endif
 		shp = shm_lock_check(ns, shmid);
 		if (IS_ERR(shp)) {
 			err = PTR_ERR(shp);
+#ifdef CONFIG_KRG_IPC
+			goto out_unlock_ns;
+#else
 			goto out;
+#endif
 		}
 
 		audit_ipc_obj(&(shp->shm_perm));
@@ -780,7 +896,11 @@
 			shp->mlock_user = NULL;
 		}
 		shm_unlock(shp);
+#ifdef CONFIG_KRG_IPC
+		goto out_unlock_ns;
+#else
 		goto out;
+#endif
 	}
 	case IPC_RMID:
 	case IPC_SET:
@@ -792,6 +912,10 @@
 
 out_unlock:
 	shm_unlock(shp);
+#ifdef CONFIG_KRG_IPC
+out_unlock_ns:
+	up_read(&shm_ids(ns).rw_mutex);
+#endif
 out:
 	return err;
 }
@@ -859,10 +983,17 @@
 	 * additional creator id...
 	 */
 	ns = current->nsproxy->ipc_ns;
+#ifdef CONFIG_KRG_IPC
+	down_read(&shm_ids(ns).rw_mutex);
+#endif
 	shp = shm_lock_check(ns, shmid);
 	if (IS_ERR(shp)) {
 		err = PTR_ERR(shp);
+#ifdef CONFIG_KRG_IPC
+		goto out_unlock_ns;
+#else
 		goto out;
+#endif
 	}
 
 	err = -EACCES;
@@ -878,6 +1009,9 @@
 	shp->shm_nattch++;
 	size = i_size_read(path.dentry->d_inode);
 	shm_unlock(shp);
+#ifdef CONFIG_KRG_IPC
+	up_read(&shm_ids(ns).rw_mutex);
+#endif
 
 	err = -ENOMEM;
 	sfd = kzalloc(sizeof(*sfd), GFP_KERNEL);
@@ -936,6 +1070,10 @@
 
 out_unlock:
 	shm_unlock(shp);
+#ifdef CONFIG_KRG_IPC
+out_unlock_ns:
+	up_read(&shm_ids(ns).rw_mutex);
+#endif
 	goto out;
 
 out_free:
diff -ruN linux-2.6.29/ipc/shm_handler.c android_cluster/linux-2.6.29/ipc/shm_handler.c
--- linux-2.6.29/ipc/shm_handler.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/shm_handler.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,290 @@
+/** All the code for sharing sys V shared memory segments accross the cluster
+ *  @file shm_handler.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#ifndef NO_SHM
+
+#include <linux/ipc.h>
+#include <linux/ipc_namespace.h>
+#include <linux/shm.h>
+#include <linux/msg.h>
+#include <kddm/kddm.h>
+#include <kerrighed/hotplug.h>
+#include "krgshm.h"
+#include "ipc_handler.h"
+#include "shm_handler.h"
+#include "shmid_io_linker.h"
+#include "ipcmap_io_linker.h"
+#include "shm_memory_linker.h"
+
+/*****************************************************************************/
+/*                                                                           */
+/*                                KERNEL HOOKS                               */
+/*                                                                           */
+/*****************************************************************************/
+
+static struct kern_ipc_perm *kcb_ipc_shm_lock(struct ipc_ids *ids, int id)
+{
+	shmid_object_t *shp_object;
+	struct shmid_kernel *shp;
+	int index;
+
+	rcu_read_lock();
+
+	index = ipcid_to_idx(id);
+
+	shp_object = _kddm_grab_object_no_ft(ids->krgops->data_kddm_set, index);
+
+	if (!shp_object)
+		goto error;
+
+	shp = shp_object->local_shp;
+
+	BUG_ON(!shp);
+
+	mutex_lock(&shp->shm_perm.mutex);
+
+	if (shp->shm_perm.deleted) {
+		mutex_unlock(&shp->shm_perm.mutex);
+		goto error;
+	}
+
+	return &(shp->shm_perm);
+
+error:
+	_kddm_put_object(ids->krgops->data_kddm_set, index);
+	rcu_read_unlock();
+
+	return ERR_PTR(-EINVAL);
+}
+
+static void kcb_ipc_shm_unlock(struct kern_ipc_perm *ipcp)
+{
+	int index, deleted = 0;
+
+	index = ipcid_to_idx(ipcp->id);
+
+	if (ipcp->deleted)
+		deleted = 1;
+
+	_kddm_put_object(ipcp->krgops->data_kddm_set, index);
+
+	if (!deleted)
+		mutex_unlock(&ipcp->mutex);
+
+	rcu_read_unlock();
+}
+
+static struct kern_ipc_perm *kcb_ipc_shm_findkey(struct ipc_ids *ids, key_t key)
+{
+	long *key_index;
+	int id = -1;
+
+	key_index = _kddm_get_object_no_ft(ids->krgops->key_kddm_set, key);
+
+	if (key_index)
+		id = *key_index;
+
+	_kddm_put_object(ids->krgops->key_kddm_set, key);
+
+	if (id != -1)
+		return kcb_ipc_shm_lock(ids, id);
+
+	return NULL;
+}
+
+/** Notify the creation of a new shm segment to Kerrighed.
+ *
+ *  @author Renaud Lottiaux
+ */
+int krg_ipc_shm_newseg (struct ipc_namespace *ns, struct shmid_kernel *shp)
+{
+	shmid_object_t *shp_object;
+	struct kddm_set *kddm;
+	long *key_index;
+	int index, err;
+
+	BUG_ON(!shm_ids(ns).krgops);
+
+	index = ipcid_to_idx(shp->shm_perm.id);
+
+	shp_object = _kddm_grab_object_manual_ft(
+		shm_ids(ns).krgops->data_kddm_set, index);
+
+	BUG_ON(shp_object);
+
+	shp_object = kmem_cache_alloc(shmid_object_cachep, GFP_KERNEL);
+	if (!shp_object) {
+		err = -ENOMEM;
+		goto err_put;
+	}
+
+	/* Create a KDDM set to host segment pages */
+	kddm = _create_new_kddm_set (kddm_def_ns, 0, SHM_MEMORY_LINKER,
+				     kerrighed_node_id, PAGE_SIZE,
+				     &shp->shm_perm.id, sizeof(int), 0);
+
+	if (IS_ERR(kddm)) {
+		err = PTR_ERR(kddm);
+		goto err_put;
+	}
+
+	shp->shm_file->f_dentry->d_inode->i_mapping->kddm_set = kddm;
+	shp->shm_file->f_op = &krg_shm_file_operations;
+
+	shp_object->set_id = kddm->id;
+
+	shp_object->local_shp = shp;
+
+	_kddm_set_object(shm_ids(ns).krgops->data_kddm_set, index, shp_object);
+
+	if (shp->shm_perm.key != IPC_PRIVATE)
+	{
+		key_index = _kddm_grab_object(shm_ids(ns).krgops->key_kddm_set,
+					      shp->shm_perm.key);
+		*key_index = index;
+		_kddm_put_object (shm_ids(ns).krgops->key_kddm_set,
+				  shp->shm_perm.key);
+	}
+
+	shp->shm_perm.krgops = shm_ids(ns).krgops;
+
+err_put:
+	_kddm_put_object(shm_ids(ns).krgops->data_kddm_set, index);
+
+	return 0;
+
+}
+
+void krg_ipc_shm_rmkey(struct ipc_namespace *ns, key_t key)
+{
+	_kddm_remove_object(shm_ids(ns).krgops->key_kddm_set, key);
+}
+
+void krg_ipc_shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp)
+{
+	struct kddm_set *mm_set;
+	int index;
+	key_t key;
+
+	index = ipcid_to_idx(shp->shm_perm.id);
+	key = shp->shm_perm.key;
+
+	mm_set = shp->shm_file->f_dentry->d_inode->i_mapping->kddm_set;
+
+	if (key != IPC_PRIVATE) {
+		_kddm_grab_object_no_ft(shm_ids(ns).krgops->key_kddm_set, key);
+		_kddm_remove_frozen_object(shm_ids(ns).krgops->key_kddm_set, key);
+	}
+
+	local_shm_unlock(shp);
+
+	_kddm_remove_frozen_object(shm_ids(ns).krgops->data_kddm_set, index);
+	_destroy_kddm_set(mm_set);
+
+	krg_ipc_rmid(&shm_ids(ns), index);
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              INITIALIZATION                               */
+/*                                                                           */
+/*****************************************************************************/
+
+int krg_shm_init_ns(struct ipc_namespace *ns)
+{
+	int r;
+
+	struct krgipc_ops *shm_ops = kmalloc(sizeof(struct krgipc_ops),
+					     GFP_KERNEL);
+	if (!shm_ops) {
+		r = -ENOMEM;
+		goto err;
+	}
+
+	shm_ops->map_kddm_set = create_new_kddm_set(kddm_def_ns,
+						    SHMMAP_KDDM_ID,
+						    IPCMAP_LINKER,
+						    KDDM_RR_DEF_OWNER,
+						    sizeof(ipcmap_object_t),
+						    KDDM_LOCAL_EXCLUSIVE);
+	if (IS_ERR(shm_ops->map_kddm_set)) {
+		r = PTR_ERR(shm_ops->map_kddm_set);
+		goto err_map;
+	}
+
+	shm_ops->key_kddm_set = create_new_kddm_set(kddm_def_ns,
+						    SHMKEY_KDDM_ID,
+						    SHMKEY_LINKER,
+						    KDDM_RR_DEF_OWNER,
+						    sizeof(long),
+						    KDDM_LOCAL_EXCLUSIVE);
+	if (IS_ERR(shm_ops->key_kddm_set)) {
+		r = PTR_ERR(shm_ops->key_kddm_set);
+		goto err_key;
+	}
+
+	shm_ops->data_kddm_set = create_new_kddm_set(kddm_def_ns,
+						     SHMID_KDDM_ID,
+						     SHMID_LINKER,
+						     KDDM_RR_DEF_OWNER,
+						     sizeof(shmid_object_t),
+						     KDDM_LOCAL_EXCLUSIVE);
+	if (IS_ERR(shm_ops->data_kddm_set)) {
+		r = PTR_ERR(shm_ops->data_kddm_set);
+		goto err_data;
+	}
+
+	shm_ops->ipc_lock = kcb_ipc_shm_lock;
+	shm_ops->ipc_unlock = kcb_ipc_shm_unlock;
+	shm_ops->ipc_findkey = kcb_ipc_shm_findkey;
+
+	shm_ids(ns).krgops = shm_ops;
+
+	return 0;
+
+err_data:
+	_destroy_kddm_set(shm_ops->key_kddm_set);
+err_key:
+	_destroy_kddm_set(shm_ops->map_kddm_set);
+err_map:
+	kfree(shm_ops);
+err:
+	return r;
+}
+
+void krg_shm_exit_ns(struct ipc_namespace *ns)
+{
+	if (shm_ids(ns).krgops) {
+
+		_destroy_kddm_set(shm_ids(ns).krgops->data_kddm_set);
+		_destroy_kddm_set(shm_ids(ns).krgops->key_kddm_set);
+		_destroy_kddm_set(shm_ids(ns).krgops->map_kddm_set);
+
+		kfree(shm_ids(ns).krgops);
+	}
+}
+
+void shm_handler_init(void)
+{
+	shmid_object_cachep = kmem_cache_create("shmid_object",
+						sizeof(shmid_object_t),
+						0, SLAB_PANIC, NULL);
+
+	register_io_linker(SHM_MEMORY_LINKER, &shm_memory_linker);
+	register_io_linker(SHMID_LINKER, &shmid_linker);
+	register_io_linker(SHMKEY_LINKER, &shmkey_linker);
+
+	krgsyms_register(KRGSYMS_VM_OPS_SHM, &shm_vm_ops);
+
+	printk("Shm Server configured\n");
+}
+
+void shm_handler_finalize (void)
+{
+}
+
+#endif
diff -ruN linux-2.6.29/ipc/shm_handler.h android_cluster/linux-2.6.29/ipc/shm_handler.h
--- linux-2.6.29/ipc/shm_handler.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/shm_handler.h	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,19 @@
+/** Interface of system V shared memory (shm) management.
+ *  @file shm_handler.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+
+#ifndef SHM_HANDLER_H
+#define SHM_HANDLER_H
+
+#include <linux/mm.h>
+#include <linux/shm.h>
+
+extern struct iolinker_struct shm_memory_linker;
+
+void shm_handler_finalize(void);
+void shm_handler_init(void);
+
+#endif // SHM_HANDLER_H
diff -ruN linux-2.6.29/ipc/shmid_io_linker.c android_cluster/linux-2.6.29/ipc/shmid_io_linker.c
--- linux-2.6.29/ipc/shmid_io_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/shmid_io_linker.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,321 @@
+/** KDDM SHM id Linker.
+ *  @file shmid_io_linker.c
+ *
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/sched.h>
+#include <linux/shm.h>
+#include <linux/lockdep.h>
+#include <linux/security.h>
+#include <linux/ipc_namespace.h>
+#include <linux/ipc.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+#include "ipc_handler.h"
+#include "krgshm.h"
+#include "util.h"
+#include "shmid_io_linker.h"
+#include "shm_memory_linker.h"
+
+#define shm_flags	shm_perm.mode
+struct kmem_cache *shmid_object_cachep;
+
+
+
+/** Create a local instance of an remotly existing SHM segment.
+ *
+ *  @author Renaud Lottiaux
+ */
+struct shmid_kernel *create_local_shp (struct ipc_namespace *ns,
+				       struct shmid_kernel *received_shp,
+				       kddm_set_id_t set_id)
+{
+	struct shmid_kernel *shp;
+	struct kddm_set *set;
+	char name[13];
+	int retval;
+
+	shp = ipc_rcu_alloc(sizeof(*shp));
+	if (!shp)
+		return ERR_PTR(-ENOMEM);
+
+	*shp = *received_shp;
+	shp->shm_perm.security = NULL;
+	retval = security_shm_alloc(shp);
+	if (retval)
+		goto err_putref;
+
+	/*
+	 * ipc_reserveid() locks shp
+	 */
+	retval = local_ipc_reserveid(&shm_ids(ns), &shp->shm_perm,
+				     ns->shm_ctlmni);
+	if (retval)
+		goto err_security_free;
+
+	sprintf (name, "SYSV%08x", received_shp->shm_perm.key);
+	shp->shm_file = shmem_file_setup(name, shp->shm_segsz, shp->shm_flags);
+
+	if (IS_ERR(shp->shm_file)) {
+		retval = PTR_ERR(shp->shm_file);
+		goto err_security_free;
+	}
+
+	set = _find_get_kddm_set(kddm_def_ns, set_id);
+	BUG_ON(!set);
+
+	shp->shm_file->f_dentry->d_inode->i_ino = shp->shm_perm.id;
+	shp->shm_file->f_dentry->d_inode->i_mapping->kddm_set = set;
+	shp->shm_file->f_op = &krg_shm_file_operations;
+	shp->mlock_user = NULL;
+
+	put_kddm_set(set);
+
+	ns->shm_tot += (shp->shm_segsz + PAGE_SIZE -1) >> PAGE_SHIFT;
+
+	shp->shm_perm.krgops = shm_ids(ns).krgops;
+
+	local_shm_unlock(shp);
+
+	return shp;
+
+err_security_free:
+	security_shm_free(shp);
+err_putref:
+	ipc_rcu_putref(shp);
+	return ERR_PTR(retval);
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                         SHMID KDDM IO FUNCTIONS                           */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+int shmid_alloc_object (struct kddm_obj * obj_entry,
+			struct kddm_set * set,
+			objid_t objid)
+{
+	shmid_object_t *shp_object;
+
+	shp_object = kmem_cache_alloc (shmid_object_cachep, GFP_KERNEL);
+	if (!shp_object)
+		return -ENOMEM;
+
+	shp_object->local_shp = NULL;
+	obj_entry->object = shp_object;
+
+	return 0;
+}
+
+
+
+/** Handle a kddm set shmid first touch
+ *  @author Renaud Lottiaux
+ *
+ *  @param  obj_entry  Kddm object descriptor.
+ *  @param  set       Kddm set descriptor
+ *  @param  objid     Id of the object to create.
+ *
+ *  @return  0 if everything is ok. Negative value otherwise.
+ */
+int shmid_first_touch (struct kddm_obj * obj_entry,
+		       struct kddm_set * set,
+		       objid_t objid,
+		       int flags)
+{
+	BUG(); // I should never get here !
+
+	return -EINVAL;
+}
+
+
+
+/** Insert a new shmid in local structures.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  obj_entry  Descriptor of the object to insert.
+ *  @param  set       Kddm set descriptor
+ *  @param  objid     Id of the object to insert.
+ */
+int shmid_insert_object (struct kddm_obj * obj_entry,
+			 struct kddm_set * set,
+			 objid_t objid)
+{
+	shmid_object_t *shp_object;
+	struct shmid_kernel *shp;
+	struct ipc_namespace *ns;
+	int r = 0;
+
+	shp_object = obj_entry->object;
+	BUG_ON(!shp_object);
+
+	/* Regular case, the kernel shm struct is already allocated */
+	if (shp_object->local_shp)
+		goto done;
+
+	/* This is the first time the object is inserted locally. We need
+	 * to allocate kernel shm structures.
+	 */
+	ns = find_get_krg_ipcns();
+	BUG_ON(!ns);
+
+	shp = create_local_shp(ns, &shp_object->mobile_shp, shp_object->set_id);
+	shp_object->local_shp = shp;
+
+	if (IS_ERR(shp)) {
+		r = PTR_ERR(shp);
+		BUG();
+	}
+
+	put_ipc_ns(ns);
+done:
+	return r;
+}
+
+
+
+/** Invalidate a kddm object shmid.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  obj_entry  Descriptor of the object to invalidate.
+ *  @param  set       Kddm set descriptor
+ *  @param  objid     Id of the object to invalidate
+ */
+int shmid_invalidate_object (struct kddm_obj * obj_entry,
+			     struct kddm_set * set,
+			     objid_t objid)
+{
+	return KDDM_IO_KEEP_OBJECT;
+}
+
+
+
+/** Handle a kddm memory page remove.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  obj_entry  Descriptor of the object to remove.
+ *  @param  set       Kddm set descriptor.
+ *  @param  padeid    Id of the object to remove.
+ */
+int shmid_remove_object (void *object,
+			 struct kddm_set * set,
+			 objid_t objid)
+{
+	shmid_object_t *shp_object;
+	struct shmid_kernel *shp;
+
+	shp_object = object;
+
+	if (shp_object) {
+		struct ipc_namespace *ns;
+
+		ns = find_get_krg_ipcns();
+		BUG_ON(!ns);
+
+		shp = shp_object->local_shp;
+		local_shm_lock(ns, shp->shm_perm.id);
+		local_shm_destroy(ns, shp);
+		kmem_cache_free(shmid_object_cachep, shp_object);
+
+		put_ipc_ns(ns);
+	}
+
+	return 0;
+}
+
+
+
+/** Export an object
+ *  @author Renaud Lottiaux
+ *
+ *  @param  buffer    Buffer to export object data in.
+ *  @param  object    The object to export data from.
+ */
+int shmid_export_object (struct rpc_desc *desc,
+			 struct kddm_set *set,
+			 struct kddm_obj *obj_entry,
+			 objid_t objid,
+			 int flags)
+{
+	shmid_object_t *shp_object;
+
+	shp_object = obj_entry->object;
+	shp_object->mobile_shp = *shp_object->local_shp;
+
+	rpc_pack(desc, 0, shp_object, sizeof(shmid_object_t));
+	return 0;
+}
+
+
+
+/** Import an object
+ *  @author Renaud Lottiaux
+ *
+ *  @param  object    The object to import data in.
+ *  @param  buffer    Data to import in the object.
+ */
+int shmid_import_object (struct rpc_desc *desc,
+			 struct kddm_set *set,
+			 struct kddm_obj *obj_entry,
+			 objid_t objid,
+			 int flags)
+{
+	shmid_object_t *shp_object, buffer;
+	struct shmid_kernel *shp;
+
+	shp_object = obj_entry->object;
+	rpc_unpack(desc, 0, &buffer, sizeof(shmid_object_t));
+
+	shp_object->mobile_shp = buffer.mobile_shp;
+	shp_object->set_id = buffer.set_id;
+
+	if (shp_object->local_shp) {
+		shp = shp_object->local_shp;
+		shp->shm_nattch = shp_object->mobile_shp.shm_nattch;
+		shp->shm_perm.mode = shp_object->mobile_shp.shm_perm.mode;
+	}
+
+	return 0;
+}
+
+
+
+/****************************************************************************/
+
+/* Init the shm id IO linker */
+
+struct iolinker_struct shmid_linker = {
+	first_touch:       shmid_first_touch,
+	remove_object:     shmid_remove_object,
+	invalidate_object: shmid_invalidate_object,
+	insert_object:     shmid_insert_object,
+	linker_name:       "shmid",
+	linker_id:         SHMID_LINKER,
+	alloc_object:      shmid_alloc_object,
+	export_object:     shmid_export_object,
+	import_object:     shmid_import_object
+};
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                         SHMKEY KDDM IO FUNCTIONS                          */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+/****************************************************************************/
+
+/* Init the shm key IO linker */
+
+struct iolinker_struct shmkey_linker = {
+	linker_name:       "shmkey",
+	linker_id:         SHMKEY_LINKER,
+};
diff -ruN linux-2.6.29/ipc/shmid_io_linker.h android_cluster/linux-2.6.29/ipc/shmid_io_linker.h
--- linux-2.6.29/ipc/shmid_io_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/shmid_io_linker.h	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,33 @@
+/** KDDM SHM id Linker.
+ *  @file shmid_io_linker.h
+ *
+ *  Link KDDM and Linux SHM id mechanisms.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __SHMID_IO_LINKER__
+#define __SHMID_IO_LINKER__
+
+#include <kddm/kddm_types.h>
+
+extern struct kmem_cache *shmid_object_cachep;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+typedef struct shmid_object {
+	struct shmid_kernel mobile_shp;
+	kddm_set_id_t set_id;
+	struct shmid_kernel *local_shp;
+} shmid_object_t;
+
+
+extern struct iolinker_struct shmid_linker;
+extern struct iolinker_struct shmkey_linker;
+
+
+#endif
diff -ruN linux-2.6.29/ipc/shm_memory_linker.c android_cluster/linux-2.6.29/ipc/shm_memory_linker.c
--- linux-2.6.29/ipc/shm_memory_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/shm_memory_linker.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,340 @@
+/** KDDM shared memory linker.
+ *  @file shm_memory_linker.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/mm.h>
+#include <linux/shm.h>
+#include <asm/pgtable.h>
+#include <asm/system.h>
+#include <asm/string.h>
+#include <linux/rmap.h>
+#include <linux/pagemap.h>
+#include <linux/ipc.h>
+#include <linux/ipc_namespace.h>
+#include <linux/msg.h>
+#include <linux/mm_inline.h>
+#include <linux/kernel.h>
+#include <linux/swap.h>
+#include <kddm/kddm.h>
+#include "krgshm.h"
+#include "ipc_handler.h"
+
+extern int memory_first_touch (struct kddm_obj * obj_entry,
+			       struct kddm_set * set, objid_t objid,int flags);
+
+void memory_change_state (struct kddm_obj * objEntry, struct kddm_set * kddm,
+			  objid_t objid, kddm_obj_state_t state);
+
+extern int memory_remove_page (void *object,
+			       struct kddm_set * kddm, objid_t objid);
+extern int memory_alloc_object (struct kddm_obj * objEntry,
+				struct kddm_set * kddm, objid_t objid);
+
+extern int memory_import_object (struct rpc_desc *desc, struct kddm_set *set,
+				 struct kddm_obj *objEntry, objid_t objid,
+				 int flags);
+extern int memory_export_object (struct rpc_desc *desc, struct kddm_set *set,
+				 struct kddm_obj *objEntry, objid_t objid,
+				 int flags);
+
+extern void map_kddm_page (struct vm_area_struct *vma, unsigned long address,
+			   struct page *page, int write);
+
+/*****************************************************************************/
+/*                                                                           */
+/*                            SHM KDDM IO FUNCTIONS                          */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+/** Insert a new shm memory page in the corresponding mapping.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  objEntry  Descriptor of the page to insert.
+ *  @param  kddm      KDDM descriptor
+ *  @param  padeid    Id of the page to insert.
+ */
+int shm_memory_insert_page(struct kddm_obj *objEntry, struct kddm_set *kddm,
+			   objid_t objid)
+{
+	struct page *page;
+	struct shmid_kernel *shp;
+	struct address_space *mapping = NULL;
+	int ret, shm_id;
+	struct ipc_namespace *ns;
+
+	ns = find_get_krg_ipcns();
+	BUG_ON(!ns);
+
+	shm_id = *(int *) kddm->private_data;
+
+	shp = local_shm_lock(ns, shm_id);
+
+	if (IS_ERR(shp)) {
+		ret = PTR_ERR(shp);
+		goto error;
+	}
+
+	mapping = shp->shm_file->f_dentry->d_inode->i_mapping;
+
+	local_shm_unlock(shp);
+
+	page = objEntry->object;
+	page->index = objid;
+	ret = add_to_page_cache_lru(page, mapping, objid, GFP_ATOMIC);
+	if (ret) {
+		printk("shm_memory_insert_page: add_to_page_cache_lru returns %d\n",
+		       ret);
+		BUG();
+	}
+	unlock_page(page);
+
+error:
+	put_ipc_ns(ns);
+
+	return ret;
+}
+
+
+
+/** Invalidate a KDDM memory page.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  kddm     KDDM descriptor
+ *  @param  objid    Id of the page to invalidate
+ */
+int shm_memory_invalidate_page (struct kddm_obj * objEntry,
+				struct kddm_set * kddm,
+				objid_t objid)
+{
+	int res ;
+
+	if (objEntry->object) {
+		struct page *page = (struct page *) objEntry->object;
+
+		BUG_ON (page->mapping == NULL);
+		BUG_ON (trylock_page(page));
+
+		SetPageToInvalidate(page);
+		res = try_to_unmap(page, 0);
+
+		ClearPageToInvalidate(page);
+		remove_from_page_cache (page);
+
+		if (PageDirty(page)) {
+			printk ("Check why the page is dirty...\n");
+			ClearPageDirty(page);
+		}
+		unlock_page(page);
+
+		if (TestClearPageLRU(page))
+			del_page_from_lru(page_zone(page), page);
+
+		page_cache_release (page);
+
+#ifdef IPCDEBUG_PAGEALLOC
+		int extra_count = 0;
+
+		if (PageInVec(page))
+			extra_count = 1;
+
+		BUG_ON (page_mapcount(page) != 0);
+
+		if ((page_count (page) != objEntry->countx + extra_count)) {
+			WARNING ("Hum... page %p (%ld;%ld) has count %d;%d "
+				 "(against %d)\n", page, kddm->id, objid,
+				 page_count (page), page_mapcount(page),
+				 objEntry->countx + extra_count);
+		}
+
+		if (PageActive(page)) {
+			WARNING ("Hum. page %p (%ld;%ld) has Active bit set\n",
+				 page, kddm->id, objid);
+			while (1)
+				schedule();
+		}
+#endif
+	}
+
+	return 0;
+}
+
+
+
+/** Handle a kddm set memory page remove.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  set      Kddm Set descriptor
+ *  @param  padeid   Id of the page to remove
+ */
+int shm_memory_remove_page (void *object,
+			    struct kddm_set * set,
+			    objid_t objid)
+{
+	if (object)
+		page_cache_release ((struct page *) object);
+
+	return 0;
+}
+
+
+
+/****************************************************************************/
+
+/* Init the memory IO linker */
+
+struct iolinker_struct shm_memory_linker = {
+	first_touch:       memory_first_touch,
+	remove_object:     shm_memory_remove_page,
+	invalidate_object: shm_memory_invalidate_page,
+	change_state:      memory_change_state,
+	insert_object:     shm_memory_insert_page,
+	linker_name:       "shm",
+	linker_id:         SHM_MEMORY_LINKER,
+	alloc_object:      memory_alloc_object,
+	export_object:     memory_export_object,
+	import_object:     memory_import_object
+};
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              SHM VM OPERATIONS                            */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+/** Handle a nopage fault on an anonymous VMA.
+ * @author Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param  vma           vm_area of the faulting address area
+ *  @param  vmf
+ */
+int shmem_memory_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
+{
+	struct inode *inode = vma->vm_file->f_dentry->d_inode;
+	struct page *page;
+	struct kddm_set *kddm;
+	unsigned long address;
+	objid_t objid;
+	int write_access = vmf->flags & FAULT_FLAG_WRITE;
+
+	address = (unsigned long)(vmf->virtual_address) & PAGE_MASK;
+
+	kddm = inode->i_mapping->kddm_set;
+
+	BUG_ON(!kddm);
+	objid = vma->vm_pgoff + (address - vma->vm_start) / PAGE_SIZE;
+
+	if (write_access)
+		page = kddm_grab_object(kddm_def_ns, kddm->id, objid);
+	else
+		page = kddm_get_object(kddm_def_ns, kddm->id, objid);
+
+	page_cache_get(page);
+
+	if (!page->mapping) {
+		printk ("Hum... NULL mapping in shmem_memory_nopage\n");
+		page->mapping = inode->i_mapping;
+	}
+
+	map_kddm_page (vma, address, page, write_access);
+	ClearPageMigratable(page);
+
+	inc_mm_counter(vma->vm_mm, file_rss);
+	page_add_file_rmap(page);
+
+	kddm_put_object (kddm_def_ns, kddm->id, objid);
+
+	vmf->page = page;
+	return 0;
+}
+
+/** Handle a wppage fault on a memory KDDM set.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  vma       vm_area of the faulting address area
+ *  @param  virtaddr  Virtual address of the page fault
+ *  @return           Physical address of the page
+ */
+struct page *shmem_memory_wppage (struct vm_area_struct *vma,
+				  unsigned long address,
+				  struct page *old_page)
+{
+	struct inode *inode = vma->vm_file->f_dentry->d_inode;
+	struct page *page;
+	struct kddm_set *kddm;
+	objid_t objid;
+
+	BUG_ON(!vma);
+
+	kddm = inode->i_mapping->kddm_set;
+
+	BUG_ON(!kddm);
+	objid = vma->vm_pgoff + (address - vma->vm_start) / PAGE_SIZE;
+
+	page = kddm_grab_object (kddm_def_ns, kddm->id, objid);
+
+	if (!page->mapping)
+		page->mapping = inode->i_mapping;
+
+	map_kddm_page (vma, address, page, 1);
+
+	if (page != old_page) {
+		page_add_file_rmap(page);
+		page_cache_get(page);
+	}
+
+	kddm_put_object (kddm_def_ns, kddm->id, objid);
+
+	return page;
+}
+
+/****************************************************************************/
+
+/* Init the Kerrighed SHM file operations structure */
+
+struct vm_operations_struct krg_shmem_vm_ops = {
+	fault:	shmem_memory_fault,
+	wppage:	shmem_memory_wppage,
+};
+
+/****************************************************************************/
+
+static int krg_shmem_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	struct shm_file_data *sfd;
+
+	BUG_ON(!file->private_data); /*shm_file_data(file) */
+
+	sfd = shm_file_data(file);
+#ifdef CONFIG_KRG_DEBUG
+	{
+		struct ipc_namespace *ns;
+
+		ns = find_get_krg_ipcns();
+		BUG_ON(!ns);
+
+		BUG_ON(sfd->ns != ns);
+
+		put_ipc_ns(ns);
+	}
+#endif
+        file_accessed(file);
+	vma->vm_ops = &krg_shmem_vm_ops;
+	vma->vm_flags |= VM_KDDM;
+
+	return 0;
+}
+
+/* Init the Kerrighed SHM file operations structure */
+
+struct file_operations krg_shm_file_operations = {
+	.mmap = krg_shmem_mmap,
+};
diff -ruN linux-2.6.29/ipc/shm_memory_linker.h android_cluster/linux-2.6.29/ipc/shm_memory_linker.h
--- linux-2.6.29/ipc/shm_memory_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/ipc/shm_memory_linker.h	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,21 @@
+/** KDDM SHM Memory Linker.
+ *  @file shm_memory_linker.h
+ *
+ *  Link KDDM and Linux SHM memory system.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __SHM_MEMORY_LINKER__
+#define __SHM_MEMORY_LINKER__
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+extern struct vm_operations_struct _krg_shmem_vmops;
+extern struct file_operations krg_shm_file_operations;
+
+#endif
diff -ruN linux-2.6.29/ipc/util.c android_cluster/linux-2.6.29/ipc/util.c
--- linux-2.6.29/ipc/util.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/ipc/util.c	2014-05-27 23:04:10.426027924 -0700
@@ -40,6 +40,15 @@
 
 #include "util.h"
 
+#ifdef CONFIG_KRG_IPC
+#include <kddm/kddm.h>
+#include "ipcmap_io_linker.h"
+#include "ipc_handler.h"
+#include "msg_handler.h"
+#include "sem_handler.h"
+#include "shm_handler.h"
+#endif
+
 struct ipc_proc_iface {
 	const char *path;
 	const char *header;
@@ -140,6 +149,9 @@
 	}
 
 	idr_init(&ids->ipcs_idr);
+#ifdef CONFIG_KRG_IPC
+	ids->krgops = NULL;
+#endif
 }
 
 #ifdef CONFIG_PROC_FS
@@ -193,6 +205,15 @@
 	int next_id;
 	int total;
 
+#ifdef CONFIG_KRG_IPC
+	if (is_krg_ipc(ids)) {
+		ipc = ids->krgops->ipc_findkey(ids, key);
+		if (IS_ERR(ipc))
+			ipc = NULL;
+		return ipc;
+	}
+#endif
+
 	for (total = 0, next_id = 0; total < ids->in_use; next_id++) {
 		ipc = idr_find(&ids->ipcs_idr, next_id);
 
@@ -224,6 +245,11 @@
 	int max_id = -1;
 	int total, id;
 
+#ifdef CONFIG_KRG_IPC
+	if (is_krg_ipc(ids))
+		return krg_ipc_get_maxid(ids);
+#endif
+
 	if (ids->in_use == 0)
 		return -1;
 
@@ -242,6 +268,114 @@
 	return max_id;
 }
 
+#ifdef CONFIG_KRG_IPC
+bool ipc_used(struct ipc_namespace *ns)
+{
+	bool used = false;
+	int i;
+	struct ipc_ids *ids;
+
+	for (i = 0; i < ARRAY_SIZE(ns->ids); i++) {
+		ids = &ns->ids[i];
+
+		down_read(&ids->rw_mutex);
+		used |= ipc_get_maxid(ids) != -1;
+		up_read(&ids->rw_mutex);
+	}
+
+	return used;
+}
+
+static int krg_idr_get_new(struct ipc_ids *ids, struct kern_ipc_perm *new, int *id)
+{
+	int err;
+
+	if (is_krg_ipc(ids)) {
+		int ipcid, lid;
+
+		ipcid = krg_ipc_get_new_id(ids);
+		if (ipcid == -1) {
+			err = -ENOMEM;
+			goto error;
+		}
+
+		lid = ipcid_to_idx(ipcid);
+		err = idr_get_new_above(&ids->ipcs_idr, new, lid, id);
+		if (!err && lid != *id) {
+			idr_remove(&ids->ipcs_idr, *id);
+			err = -EINVAL;
+		}
+	} else
+		err = idr_get_new(&ids->ipcs_idr, new, id);
+
+error:
+	return err;
+}
+
+static int ipc_reserveid(struct ipc_ids *ids, struct kern_ipc_perm *new,
+			 int requested_id)
+{
+	uid_t euid;
+	gid_t egid;
+	int lid, id, err;
+
+	mutex_init(&new->mutex);
+	new->deleted = 0;
+	rcu_read_lock();
+
+	mutex_lock(&new->mutex);
+
+	lid = ipcid_to_idx(requested_id);
+
+	err = krg_ipc_get_this_id(ids, lid);
+	if (err)
+		goto out;
+
+	err = idr_pre_get(&ids->ipcs_idr, GFP_KERNEL);
+	if (!err) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	err = idr_get_new_above(&ids->ipcs_idr, new, lid, &id);
+	if (err)
+		goto out_free_krg_id;
+
+	if (lid != id) {
+		err = -EINVAL;
+		goto out_free_idr_id;
+	}
+
+	ids->in_use++;
+
+	current_euid_egid(&euid, &egid);
+	new->cuid = new->uid = euid;
+	new->gid = new->cgid = egid;
+
+	new->seq = (requested_id - lid) / SEQ_MULTIPLIER;
+
+	if (ids->seq <= new->seq)
+		ids->seq = new->seq+1;
+
+	if (ids->seq > ids->seq_max)
+		ids->seq = 0;
+
+	new->id = requested_id;
+
+	return requested_id;
+
+out_free_idr_id:
+	idr_remove(&ids->ipcs_idr, id);
+out_free_krg_id:
+	krg_ipc_rmid(ids, lid);
+out:
+	mutex_unlock(&new->mutex);
+	rcu_read_unlock();
+
+	return err;
+}
+#endif
+
 /**
  *	ipc_addid 	-	add an IPC identifier
  *	@ids: IPC identifier set
@@ -256,7 +390,12 @@
  *	Called with ipc_ids.rw_mutex held as a writer.
  */
  
+#ifdef CONFIG_KRG_IPC
+int ipc_addid(struct ipc_ids* ids, struct kern_ipc_perm* new, int size,
+	      int requested_id)
+#else
 int ipc_addid(struct ipc_ids* ids, struct kern_ipc_perm* new, int size)
+#endif
 {
 	uid_t euid;
 	gid_t egid;
@@ -268,14 +407,35 @@
 	if (ids->in_use >= size)
 		return -ENOSPC;
 
+#ifdef CONFIG_KRG_IPC
+	if (requested_id != -1)
+		return ipc_reserveid(ids, new, requested_id);
+#endif
+
+#ifdef CONFIG_KRG_IPC
+	mutex_init(&new->mutex);
+#else
 	spin_lock_init(&new->lock);
+#endif
 	new->deleted = 0;
 	rcu_read_lock();
+#ifdef CONFIG_KRG_IPC
+	mutex_lock(&new->mutex);
+#else
 	spin_lock(&new->lock);
+#endif
 
+#ifdef CONFIG_KRG_IPC
+	err = krg_idr_get_new(ids, new, &id);
+#else
 	err = idr_get_new(&ids->ipcs_idr, new, &id);
+#endif
 	if (err) {
+#ifdef CONFIG_KRG_IPC
+		mutex_unlock(&new->mutex);
+#else
 		spin_unlock(&new->lock);
+#endif
 		rcu_read_unlock();
 		return err;
 	}
@@ -294,6 +454,68 @@
 	return id;
 }
 
+#ifdef CONFIG_KRG_IPC
+int local_ipc_reserveid(struct ipc_ids* ids, struct kern_ipc_perm* new,
+                        int size)
+{
+	int original_idx, idx, err;
+
+	if (size > IPCMNI)
+		size = IPCMNI;
+
+	if (ids->in_use >= size) {
+		/* IPC quota is not clusterwide, returning an error here
+		   might lead to kernel crash within the IO linker */
+		printk("%s:%d - Number of Kerrighed IPC objects is locally"
+		       " exceeding quota (%d >= %d)\n",
+		       __PRETTY_FUNCTION__, __LINE__,
+		       ids->in_use, size);
+		/*return -ENOSPC;*/
+	}
+
+	err = idr_pre_get(&ids->ipcs_idr, GFP_KERNEL);
+	if (!err)
+		return -ENOMEM;
+
+	mutex_init(&new->mutex);
+
+	new->deleted = 0;
+
+	rcu_read_lock();
+
+	mutex_lock(&new->mutex);
+
+	original_idx = ipcid_to_idx(new->id);
+
+	BUG_ON(new->id != SEQ_MULTIPLIER * new->seq + original_idx);
+
+	err = idr_get_new_above(&ids->ipcs_idr, new, original_idx, &idx);
+
+	if (err)
+		goto error;
+
+	if (original_idx != idx) {
+		idr_remove(&ids->ipcs_idr, idx);
+		err = -EINVAL;
+		goto error;
+	}
+
+	ids->in_use++;
+
+	if (ids->seq <= new->seq)
+		ids->seq = new->seq+1;
+
+	if (ids->seq > ids->seq_max)
+		ids->seq = 0;
+
+	return 0;
+
+error:
+	mutex_unlock(&new->mutex);
+	return err;
+}
+#endif
+
 /**
  *	ipcget_new	-	create a new ipc object
  *	@ns: namespace
@@ -417,7 +639,6 @@
 	return err;
 }
 
-
 /**
  *	ipc_rmid	-	remove an IPC identifier
  *	@ids: IPC identifier set
@@ -697,8 +918,11 @@
  *
  * The ipc object is locked on exit.
  */
-
+#ifdef CONFIG_KRG_IPC
+struct kern_ipc_perm *local_ipc_lock(struct ipc_ids *ids, int id)
+#else
 struct kern_ipc_perm *ipc_lock(struct ipc_ids *ids, int id)
+#endif
 {
 	struct kern_ipc_perm *out;
 	int lid = ipcid_to_idx(id);
@@ -709,14 +933,21 @@
 		rcu_read_unlock();
 		return ERR_PTR(-EINVAL);
 	}
-
+#ifdef CONFIG_KRG_IPC
+	mutex_lock(&out->mutex);
+#else
 	spin_lock(&out->lock);
+#endif
 	
 	/* ipc_rmid() may have already freed the ID while ipc_lock
 	 * was spinning: here verify that the structure is still valid
 	 */
 	if (out->deleted) {
+#ifdef CONFIG_KRG_IPC
+		mutex_unlock(&out->mutex);
+#else
 		spin_unlock(&out->lock);
+#endif
 		rcu_read_unlock();
 		return ERR_PTR(-EINVAL);
 	}
@@ -724,6 +955,16 @@
 	return out;
 }
 
+#ifdef CONFIG_KRG_IPC
+struct kern_ipc_perm *ipc_lock(struct ipc_ids *ids, int id)
+{
+	if (is_krg_ipc(ids))
+		return ids->krgops->ipc_lock(ids, id);
+
+	return local_ipc_lock(ids, id);
+}
+#endif
+
 struct kern_ipc_perm *ipc_lock_check(struct ipc_ids *ids, int id)
 {
 	struct kern_ipc_perm *out;
@@ -740,6 +981,22 @@
 	return out;
 }
 
+#ifdef CONFIG_KRG_IPC
+void local_ipc_unlock(struct kern_ipc_perm *perm)
+{
+	mutex_unlock(&perm->mutex);
+	rcu_read_unlock();
+}
+
+void ipc_unlock(struct kern_ipc_perm *perm)
+{
+	if (perm->krgops)
+		perm->krgops->ipc_unlock(perm);
+	else
+		local_ipc_unlock(perm);
+}
+#endif
+
 /**
  * ipcget - Common sys_*get() code
  * @ns : namsepace
@@ -753,6 +1010,9 @@
 int ipcget(struct ipc_namespace *ns, struct ipc_ids *ids,
 			struct ipc_ops *ops, struct ipc_params *params)
 {
+#ifdef CONFIG_KRG_IPC
+	params->requested_id = -1;
+#endif
 	if (params->key == IPC_PRIVATE)
 		return ipcget_new(ns, ids, ops, params);
 	else
@@ -856,6 +1116,19 @@
 					      loff_t *new_pos)
 {
 	struct kern_ipc_perm *ipc;
+#ifdef CONFIG_KRG_IPC
+	int total;
+
+	total = ipc_get_maxid(ids);
+
+	for (; pos <= total && pos < IPCMNI; pos++) {
+		ipc = ipc_lock(ids, pos);
+		if (!IS_ERR(ipc)) {
+			*new_pos = pos + 1;
+			return ipc;
+		}
+	}
+#else
 	int total, id;
 
 	total = 0;
@@ -876,6 +1149,7 @@
 			return ipc;
 		}
 	}
+#endif
 
 	/* Out of range - return NULL to terminate iteration */
 	return NULL;
@@ -1000,3 +1274,42 @@
 	.release = sysvipc_proc_release,
 };
 #endif /* CONFIG_PROC_FS */
+
+#ifdef CONFIG_KRG_IPC
+
+int is_krg_ipc(struct ipc_ids *ids)
+{
+	if (ids->krgops)
+		return 1;
+
+	return 0;
+}
+
+int init_keripc(void)
+{
+	printk("KrgIPC initialisation : start\n");
+
+	ipcmap_object_cachep = kmem_cache_create("ipcmap_object",
+						 sizeof(ipcmap_object_t),
+						 0, SLAB_PANIC, NULL);
+	register_io_linker (IPCMAP_LINKER, &ipcmap_linker);
+
+	ipc_handler_init();
+
+	msg_handler_init();
+
+	sem_handler_init();
+
+	shm_handler_init();
+
+	printk("KrgIPC initialisation done\n");
+
+	return 0;
+}
+
+void cleanup_keripc(void)
+{
+	ipc_handler_finalize();
+}
+
+#endif
diff -ruN linux-2.6.29/ipc/util.h android_cluster/linux-2.6.29/ipc/util.h
--- linux-2.6.29/ipc/util.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/ipc/util.h	2014-05-27 23:04:10.426027924 -0700
@@ -11,6 +11,10 @@
 #define _IPC_UTIL_H
 
 #include <linux/err.h>
+#ifdef CONFIG_KRG_IPC
+#include <kerrighed/types.h>
+#include <kddm/kddm_types.h>
+#endif
 
 #define SEQ_MULTIPLIER	(IPCMNI)
 
@@ -39,8 +43,17 @@
 		size_t size;	/* for shared memories */
 		int nsems;	/* for semaphores */
 	} u;			/* holds the getnew() specific param */
+#ifdef CONFIG_KRG_IPC
+	int requested_id;
+#endif
 };
 
+#ifdef CONFIG_KRG_IPC
+#define sem_ids(ns)     ((ns)->ids[IPC_SEM_IDS])
+#define msg_ids(ns)     ((ns)->ids[IPC_MSG_IDS])
+#define shm_ids(ns)     ((ns)->ids[IPC_SHM_IDS])
+#endif
+
 /*
  * Structure that holds some ipc operations. This structure is used to unify
  * the calls to sys_msgget(), sys_semget(), sys_shmget()
@@ -75,7 +88,11 @@
 #define ipcid_to_idx(id) ((id) % SEQ_MULTIPLIER)
 
 /* must be called with ids->rw_mutex acquired for writing */
+#ifdef CONFIG_KRG_IPC
+int ipc_addid(struct ipc_ids *, struct kern_ipc_perm *, int, int);
+#else
 int ipc_addid(struct ipc_ids *, struct kern_ipc_perm *, int);
+#endif
 
 /* must be called with ids->rw_mutex acquired for reading */
 int ipc_get_maxid(struct ipc_ids *);
@@ -103,6 +120,9 @@
 void ipc_rcu_putref(void *ptr);
 
 struct kern_ipc_perm *ipc_lock(struct ipc_ids *, int);
+#ifdef CONFIG_KRG_IPC
+struct kern_ipc_perm *local_ipc_lock(struct ipc_ids *ids, int id);
+#endif
 
 void kernel_to_ipc64_perm(struct kern_ipc_perm *in, struct ipc64_perm *out);
 void ipc64_perm_to_ipc_perm(struct ipc64_perm *in, struct ipc_perm *out);
@@ -140,18 +160,58 @@
 
 static inline void ipc_lock_by_ptr(struct kern_ipc_perm *perm)
 {
+#ifdef CONFIG_KRG_IPC
+	BUG_ON(perm->krgops);
+#endif
 	rcu_read_lock();
+#ifdef CONFIG_KRG_IPC
+	mutex_lock(&perm->mutex);
+#else
 	spin_lock(&perm->lock);
+#endif
 }
 
+#ifdef CONFIG_KRG_IPC
+void ipc_unlock(struct kern_ipc_perm *perm);
+
+void local_ipc_unlock(struct kern_ipc_perm *perm);
+#else
 static inline void ipc_unlock(struct kern_ipc_perm *perm)
 {
 	spin_unlock(&perm->lock);
 	rcu_read_unlock();
 }
+#endif
 
 struct kern_ipc_perm *ipc_lock_check(struct ipc_ids *ids, int id);
 int ipcget(struct ipc_namespace *ns, struct ipc_ids *ids,
 			struct ipc_ops *ops, struct ipc_params *params);
 
+#ifdef CONFIG_KRG_IPC
+
+struct krgipc_ops {
+	struct kddm_set *map_kddm_set;
+	struct kddm_set *key_kddm_set;
+	struct kddm_set *data_kddm_set;
+
+	struct kern_ipc_perm *(*ipc_lock)(struct ipc_ids *, int);
+	void (*ipc_unlock)(struct kern_ipc_perm *);
+	struct kern_ipc_perm *(*ipc_findkey)(struct ipc_ids *, key_t);
+};
+
+int local_ipc_reserveid(struct ipc_ids* ids, struct kern_ipc_perm* new,
+                        int size);
+
+int is_krg_ipc(struct ipc_ids *ids);
+
+int krg_msg_init_ns(struct ipc_namespace *ns);
+int krg_sem_init_ns(struct ipc_namespace *ns);
+int krg_shm_init_ns(struct ipc_namespace *ns);
+
+void krg_msg_exit_ns(struct ipc_namespace *ns);
+void krg_sem_exit_ns(struct ipc_namespace *ns);
+void krg_shm_exit_ns(struct ipc_namespace *ns);
+
+#endif
+
 #endif
diff -ruN linux-2.6.29/kddm/io_linker.c android_cluster/linux-2.6.29/kddm/io_linker.c
--- linux-2.6.29/kddm/io_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/io_linker.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,488 @@
+/** KDDM IO linker interface.
+ *  @file io_linker.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/slab.h>
+#include <linux/mman.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/krgflags.h>
+
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+#include <kddm/io_linker.h>
+
+
+struct iolinker_struct *iolinker_list[MAX_IO_LINKER];
+
+krgnodemask_t krgnode_kddm_map;
+kerrighed_node_t kddm_nb_nodes;
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                     INSTANTIATE/UNINSTANTIATE FUNCTIONS                   */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+/** Instantiate a kddm set with an IO linker.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set           Kddm set to instantiate.
+ *  @param link          Node linked to the kddm set.
+ *  @param iolinker_id   Id of the linker to link to the kddm set.
+ *  @param private_data  Data used by the instantiator...
+ *
+ *  @return  Structure of the requested kddm set or NULL if not found.
+ */
+int kddm_io_instantiate (struct kddm_set * set,
+			 kerrighed_node_t def_owner,
+			 iolinker_id_t iolinker_id,
+			 void *private_data,
+			 int data_size,
+			 int master)
+{
+	int err = 0;
+
+	BUG_ON (set == NULL);
+	BUG_ON (iolinker_id < 0 || iolinker_id >= MAX_IO_LINKER);
+	BUG_ON (set->state != KDDM_SET_LOCKED);
+
+	while (iolinker_list[iolinker_id] == NULL) {
+		WARNING ("Instantiate a kddm set with a not registered IO "
+			 "linker (%d)... Retry in 1 second\n", iolinker_id);
+		set_current_state (TASK_INTERRUPTIBLE);
+		schedule_timeout (1 * HZ);
+	}
+
+	set->def_owner = def_owner;
+	set->iolinker = iolinker_list[iolinker_id];
+
+	if (data_size) {
+		set->private_data = kmalloc (data_size, GFP_KERNEL);
+		BUG_ON (set->private_data == NULL);
+		memcpy (set->private_data, private_data, data_size);
+		set->private_data_size = data_size;
+	}
+	else {
+		set->private_data = NULL;
+		set->private_data_size = 0;
+	}
+
+	if (set->iolinker->instantiate)
+		err = set->iolinker->instantiate (set, private_data,
+						  master);
+
+	return err;
+}
+
+
+
+/** Uninstantiate a kddm set.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set to uninstantiate
+ */
+void kddm_io_uninstantiate (struct kddm_set * set,
+                            int destroy)
+{
+	if (set->iolinker && set->iolinker->uninstantiate)
+		set->iolinker->uninstantiate (set, destroy);
+
+	if (set->private_data)
+		kfree(set->private_data);
+	set->private_data = NULL;
+	set->iolinker = NULL;
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                      MAIN IO LINKER INTERFACE FUNCTIONS                   */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+/** Request an IO linker to allocate an object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry    Object entry to export data from.
+ *  @param set          Kddm Set the object belong to.
+ */
+int kddm_io_alloc_object (struct kddm_obj * obj_entry,
+			  struct kddm_set * set,
+			  objid_t objid)
+{
+	int r = 0;
+
+	if (obj_entry->object != NULL)
+		goto done;
+
+	if (set->iolinker && set->iolinker->alloc_object)
+		r = set->iolinker->alloc_object (obj_entry, set, objid);
+	else {
+		/* Default allocation function */
+		obj_entry->object = kmalloc(set->obj_size, GFP_KERNEL);
+		if (obj_entry->object == NULL)
+			r = -ENOMEM;
+	}
+
+	if (obj_entry->object != NULL)
+		atomic_inc(&set->nr_objects);
+
+done:
+	return r;
+}
+
+
+
+/** Request an IO linker to do an object first touch.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to first touch.
+ *  @param obj_entry    Object entry the object belong to.
+ */
+int kddm_io_first_touch_object (struct kddm_obj * obj_entry,
+                                struct kddm_set * set,
+                                objid_t objid,
+				int flags)
+{
+	int res = 0 ;
+
+	BUG_ON (obj_entry->object != NULL);
+	BUG_ON (OBJ_STATE(obj_entry) != INV_FILLING);
+
+	if (set->iolinker && set->iolinker->first_touch) {
+		res = set->iolinker->first_touch (obj_entry, set,
+						  objid, flags);
+		if (obj_entry->object)
+			atomic_inc(&set->nr_objects);
+	}
+	else
+		res = kddm_io_alloc_object(obj_entry, set, objid);
+
+	return res ;
+}
+
+
+
+/** Request an IO linker to insert an object in a kddm set.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to insert.
+ *  @param obj_entry    Object entry the object belong to.
+ */
+int kddm_io_insert_object (struct kddm_obj * obj_entry,
+                           struct kddm_set * set,
+                           objid_t objid)
+{
+	int res = 0;
+
+	if (set->iolinker && set->iolinker->insert_object)
+		res = set->iolinker->insert_object (obj_entry, set,
+						    objid);
+
+	return res;
+}
+
+
+
+/** Request an IO linker to put a kddm object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to put.
+ *  @param obj_entry    Object entry the object belong to.
+ */
+int kddm_io_put_object (struct kddm_obj * obj_entry,
+                        struct kddm_set * set,
+                        objid_t objid)
+{
+	int res = 0;
+
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	if (set && set->iolinker->put_object)
+		res = set->iolinker->put_object (obj_entry, set,
+						 objid);
+
+	return res;
+}
+
+
+
+/** Request an IO linker to invalidate an object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to invalidate.
+ *  @param obj_entry    Object entry the object belong to.
+ */
+int kddm_io_invalidate_object (struct kddm_obj * obj_entry,
+			       struct kddm_set * set,
+			       objid_t objid)
+{
+	int res = 0;
+
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	if (obj_entry->object) {
+		if (set->iolinker && set->iolinker->invalidate_object) {
+			res = set->iolinker->invalidate_object (obj_entry,
+								set, objid);
+
+			if (res != KDDM_IO_KEEP_OBJECT)
+				obj_entry->object = NULL;
+		}
+
+		if (obj_entry->object == NULL)
+			atomic_dec(&set->nr_objects);
+	}
+
+	return res;
+}
+
+
+
+/** Request an IO linker to remove an object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to remove.
+ *  @param obj_entry    Object entry the object belong to.
+ */
+int kddm_io_remove_object (void *object,
+			   struct kddm_set * set,
+			   objid_t objid)
+{
+	int res = 0;
+
+	if (set->iolinker && set->iolinker->remove_object) {
+		might_sleep();
+		res = set->iolinker->remove_object (object, set, objid);
+	}
+	else
+		/* Default free function */
+		kfree (object);
+
+	atomic_dec(&set->nr_objects);
+
+	return res;
+}
+
+int kddm_io_remove_object_and_unlock (struct kddm_obj * obj_entry,
+				      struct kddm_set * set,
+				      objid_t objid)
+{
+	int res = 0;
+	void *object;
+
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	object = obj_entry->object;
+
+	if (object == NULL) {
+		put_kddm_obj_entry(set, obj_entry, objid);
+		goto done;
+	}
+
+	obj_entry->object = NULL;
+	put_kddm_obj_entry(set, obj_entry, objid);
+
+	res = kddm_io_remove_object (object, set, objid);
+
+done:
+	return res;
+}
+
+
+
+/** Request an IO linker to sync an object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to sync.
+ *  @param obj_entry    Object entry the object belong to.
+ */
+int kddm_io_sync_object (struct kddm_obj * obj_entry,
+                         struct kddm_set * set,
+                         objid_t objid)
+{
+	int res = 0 ;
+
+	if (set->iolinker && set->iolinker->sync_object)
+		res = set->iolinker->sync_object (obj_entry, set, objid);
+	else
+		BUG();
+
+	return res ;
+}
+
+
+
+/** Inform an IO linker that an object state has changed.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry    Object entry the object belong to.
+ *  @param set          Kddm Set the object belong to.
+ *  @param objid        Id of the object to sync.
+ *  @param new_state    New state for the object.
+ */
+int kddm_io_change_state (struct kddm_obj * obj_entry,
+			  struct kddm_set * set,
+			  objid_t objid,
+			  kddm_obj_state_t new_state)
+{
+	if (set->iolinker && set->iolinker->change_state)
+		set->iolinker->change_state (obj_entry, set, objid, new_state);
+
+	return 0 ;
+}
+
+
+
+/** Request an IO linker to import data into an object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param obj_entry    Object entry to import data into.
+ *  @param buffer       Buffer containing data to import.
+ */
+int kddm_io_import_object (struct rpc_desc *desc,
+                           struct kddm_set *set,
+                           struct kddm_obj *obj_entry,
+                           objid_t objid,
+			   int flags)
+{
+	struct iolinker_struct *io = set->iolinker;
+	int res;
+
+	BUG_ON (OBJ_STATE(obj_entry) != INV_FILLING);
+
+	might_sleep();
+
+	if (io && io->import_object)
+		res = io->import_object(desc, set, obj_entry, objid, flags);
+	else
+		res = rpc_unpack(desc, 0, obj_entry->object, set->obj_size);
+
+	return res;
+}
+
+
+
+/** Request an IO linker to export data from an object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set          Kddm Set the object belong to.
+ *  @param obj_entry    Object entry to export data from.
+ *  @param desc		RPC descriptor to export data on.
+ */
+int kddm_io_export_object (struct rpc_desc *desc,
+			   struct kddm_set *set,
+                           struct kddm_obj *obj_entry,
+                           objid_t objid,
+			   int flags)
+{
+	struct iolinker_struct *io = set->iolinker;
+	int res;
+
+	if (io && io->export_object)
+		res = io->export_object(desc, set, obj_entry, objid, flags);
+	else
+		res = rpc_pack(desc, 0, obj_entry->object, set->obj_size);
+
+	return res;
+}
+
+kerrighed_node_t __kddm_io_default_owner (struct kddm_set *set,
+					  objid_t objid,
+					  const krgnodemask_t *nodes,
+					  int nr_nodes)
+{
+	switch (set->def_owner) {
+	  case KDDM_RR_DEF_OWNER:
+		  if (likely(__krgnode_isset(kerrighed_node_id, nodes)))
+			  return __nth_krgnode(objid % nr_nodes, nodes);
+		  else
+			  return kerrighed_node_id;
+
+	  case KDDM_UNIQUE_ID_DEF_OWNER:
+		  return objid >> UNIQUE_ID_NODE_SHIFT;
+
+	  case KDDM_CUSTOM_DEF_OWNER:
+		  return set->iolinker->default_owner (set, objid,
+						       nodes, nr_nodes);
+
+	  default:
+		  return set->def_owner;
+	}
+}
+
+kerrighed_node_t kddm_io_default_owner (struct kddm_set * set, objid_t objid)
+{
+	return __kddm_io_default_owner (set, objid,
+					&krgnode_kddm_map,
+					kddm_nb_nodes);
+}
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                           IO LINKER INIT FUNCTIONS                        */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+/** Register a new kddm set IO linker.
+ *  @author Renaud Lottiaux
+ *
+ *  @param io_linker_id
+ *  @param linker
+ */
+int register_io_linker (int linker_id,
+                        struct iolinker_struct *io_linker)
+{
+	if(iolinker_list[linker_id] != NULL)
+		return -1;
+
+	iolinker_list[linker_id] = io_linker;
+
+	return 0;
+}
+
+
+
+/** Initialise the IO linker array with existing linker
+ */
+void io_linker_init (void)
+{
+	int i;
+
+	kddm_nb_nodes = kerrighed_nb_nodes;
+	krgnodes_copy(krgnode_kddm_map, krgnode_online_map);
+
+	for (i = 0; i < MAX_IO_LINKER; i++)
+		iolinker_list[i] = NULL;
+}
+
+
+
+/** Initialise the IO linker array with existing linker
+ */
+void io_linker_finalize (void)
+{
+}
diff -ruN linux-2.6.29/kddm/kddm_bench.c android_cluster/linux-2.6.29/kddm/kddm_bench.c
--- linux-2.6.29/kddm/kddm_bench.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm_bench.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,392 @@
+/** KDDM benchmark module.
+ *  @file kddm_bench.c
+ *
+ *  Copyright (C) 2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+
+#define NR_TEST_LOOPS 16
+#define NR_TESTS 12
+#define BARRIER_ID 100000
+
+
+/****************************************************************************/
+
+int test_first_touch (struct kddm_obj * obj_entry,
+		      struct kddm_set * set,
+		      objid_t objid,
+		      int flags)
+{
+	int *data;
+
+	if (obj_entry->object == NULL) {
+		data = kmalloc(set->obj_size, GFP_KERNEL);
+		obj_entry->object = data;
+		*data = 0;
+	}
+
+	return 0;
+}
+
+/* Init the KDDM test IO linker */
+
+struct iolinker_struct kddm_test_linker = {
+	first_touch:       test_first_touch,
+	linker_name:       "kddm_test",
+	linker_id:         KDDM_TEST_LINKER,
+};
+
+struct kddm_set *kddm_test4_dist = NULL;
+struct kddm_set *kddm_test4_loc = NULL;
+struct kddm_set *kddm_test4096_dist = NULL;
+
+static inline void start_timer (struct timeval *tv)
+{
+	do_gettimeofday(tv);
+}
+
+static inline long stop_timer (struct timeval *tv_start)
+{
+	u64 start = 0, end;
+	struct timeval tv_end;
+
+	do_gettimeofday(&tv_end);
+
+	start = tv_start->tv_usec + tv_start->tv_sec * 1000000;
+	end = tv_end.tv_usec + tv_end.tv_sec * 1000000;
+	return end - start;
+}
+
+void kddm_test_barrier(void)
+{
+	int *val, done = 0;
+
+	if (kerrighed_node_id == 0) {
+		val = _kddm_grab_object (kddm_test4_loc, BARRIER_ID);
+		*val = kerrighed_nb_nodes;
+		_kddm_put_object (kddm_test4_loc, BARRIER_ID);
+	}
+	else {
+		while (! done) {
+			val = _kddm_get_object (kddm_test4_loc, BARRIER_ID);
+			done = (*val != 0);
+			_kddm_put_object (kddm_test4_loc, BARRIER_ID);
+			schedule();
+		}
+	}
+
+	val = _kddm_grab_object (kddm_test4_loc, BARRIER_ID);
+	(*val)--;
+	_kddm_put_object (kddm_test4_loc, BARRIER_ID);
+
+	done = 0;
+
+	while (! done) {
+		val = _kddm_get_object (kddm_test4_loc, BARRIER_ID);
+		done = (*val == 0);
+		_kddm_put_object (kddm_test4_loc, BARRIER_ID);
+		schedule();
+	}
+}
+
+
+
+long do_grab(struct kddm_set *set, int start, int end)
+{
+	struct timeval tv;
+	long tot_time = 0;
+	int i;
+
+	for (i = start; i < end; i++) {
+		start_timer (&tv);
+		_kddm_grab_object (set, i);
+		tot_time += stop_timer(&tv);
+	}
+
+	for (i = start; i < end; i++)
+		_kddm_put_object (set, i);
+
+	return tot_time / NR_TEST_LOOPS;
+}
+
+long do_get(struct kddm_set *set, int start, int end)
+{
+	struct timeval tv;
+	long tot_time = 0;
+	int i;
+
+	for (i = start; i < end; i++) {
+		start_timer (&tv);
+		_kddm_get_object (set, i);
+		tot_time += stop_timer(&tv);
+	}
+
+	for (i = start; i < end; i++)
+		_kddm_put_object (set, i);
+
+	return tot_time / NR_TEST_LOOPS;
+}
+
+long do_remove(struct kddm_set *set, int start, int end)
+{
+	struct timeval tv;
+	long tot_time = 0;
+	int i ;
+
+	for (i = start; i < end; i++)
+		_kddm_get_object (set, i);
+
+	for (i = start; i < end; i++) {
+		start_timer (&tv);
+		_kddm_remove_frozen_object (set, i);
+		tot_time += stop_timer(&tv);
+	}
+
+	return tot_time / NR_TEST_LOOPS;
+}
+
+void alloc_test_kddm_sets(int master_node)
+{
+	kddm_test4_dist = create_new_kddm_set (kddm_def_ns,
+					       KDDM_TEST4_DIST,
+					       KDDM_TEST_LINKER,
+					       master_node + 1, 4, 0);
+
+	kddm_test4_loc = create_new_kddm_set (kddm_def_ns,
+					      KDDM_TEST4_LOC,
+					      KDDM_TEST_LINKER,
+					      master_node, 4, 0);
+
+	kddm_test4096_dist = create_new_kddm_set (kddm_def_ns,
+						  KDDM_TEST4096,
+						  KDDM_TEST_LINKER,
+						  master_node + 1, 4096, 0);
+}
+
+void prepare_bench (struct kddm_set *set, int master_node)
+{
+	int start, end, test_nr;
+
+	for (test_nr = 0; test_nr <= NR_TESTS; test_nr++) {
+
+		start = test_nr * NR_TEST_LOOPS;
+		end = start + NR_TEST_LOOPS;
+
+		switch (test_nr) {
+		case 0:  /* Get - 0 copies (FT) */
+			break;
+
+		case 1:  /* Get - Fetch from node 1 */
+			if (kerrighed_node_id == master_node + 1)
+				do_grab(set, start, end);
+			break;
+
+		case 2:  /* Get - Fetch from node 2 */
+			if (kerrighed_node_id == master_node + 2)
+				do_grab(set, start, end);
+			break;
+
+		case 3:  /* Grab - 0 copies (FT) */
+			break;
+
+		case 4:  /* Grab - 1 local copy */
+			if (kerrighed_node_id == master_node)
+				do_grab(set, start, end);
+			break;
+
+		case 5:  /* Grab - 1 remote copy */
+		case 6:  /* Grab - 2 remote copies */
+		case 7:  /* Grab - 3 remote copies */
+			if (kerrighed_node_id > master_node + (7 - test_nr))
+				do_get(set, start, end);
+			break;
+
+		case 8:  /* Grab - 1 local 3 remotes copies */
+			do_get(set, start, end);
+			break;
+
+		case 9:  /* Remove - 1 local copy */
+			if (kerrighed_node_id == master_node)
+				do_get(set, start, end);
+			break;
+
+		case 10: /* Remove - 1 remote copy */
+		case 11: /* Remove - 2 remote copies */
+		case 12: /* Remove - 3 remote copies */
+			if ((kerrighed_node_id > master_node + (12 - test_nr))
+			    || (kerrighed_node_id == master_node))
+				do_get(set, start, end);
+			break;
+		}
+	}
+}
+
+void do_one_bench (struct kddm_set *set, char *buff, int size, int *index)
+{
+	int start, end, test_nr;
+
+	for (test_nr = 0; test_nr <= NR_TESTS; test_nr++) {
+
+		start = test_nr * NR_TEST_LOOPS;
+		end = start + NR_TEST_LOOPS;
+
+		switch (test_nr) {
+		case 0:  /* Get - 0 copies (FT) */
+			*index += snprintf (&buff[*index], size - *index,
+					    "Get (FT): %ld\n",
+					    do_get(set, start, end));
+			break;
+
+		case 1:  /* Get - Fetch from node 1 */
+			*index += snprintf (&buff[*index], size - *index,
+					    "Get (Fetch from node 1): %ld\n",
+					    do_get(set, start, end));
+			break;
+
+		case 2:  /* Get - Fetch from node 2 */
+			*index += snprintf (&buff[*index], size - *index,
+					    "Get (Fetch from node 2): %ld\n",
+					    do_get(set, start, end));
+			break;
+
+		case 3:  /* Grab - 0 copies (FT) */
+			*index += snprintf (&buff[*index], size - *index,
+					    "Grab (FT): %ld\n",
+					    do_grab(set, start, end));
+			break;
+
+		case 4:  /* Grab - 1 local copy */
+			*index += snprintf (&buff[*index], size - *index,
+					    "Grab (1 local copy): %ld\n",
+					    do_grab(set, start, end));
+			break;
+
+		case 5:  /* Grab - 1 remote copy */
+		case 6:  /* Grab - 2 remote copies */
+		case 7:  /* Grab - 3 remote copies */
+			*index += snprintf (&buff[*index], size - *index,
+					    "Grab (%d remote copy): %ld\n",
+					    test_nr - 4,
+					    do_grab(set, start, end));
+			break;
+
+		case 8:  /* Grab - 4 copies */
+			*index += snprintf (&buff[*index], size - *index,
+					    "Grab (1 loc - 3 remote): %ld\n",
+					    do_grab(set, start, end));
+			break;
+
+		case 9:  /* Remove - 1 local copy */
+			*index += snprintf (&buff[*index], size - *index,
+					    "Remove (1 local copy): %ld\n",
+					    do_remove(set, start, end));
+			break;
+
+		case 10: /* Remove - 1 remote copy */
+		case 11: /* Remove - 2 remote copies */
+		case 12: /* Remove - 3 remote copies */
+			*index += snprintf (&buff[*index], size - *index,
+					    "Remove (%d remote copy): %ld\n",
+					    test_nr - 9,
+					    do_remove(set, start, end));
+			break;
+		}
+	}
+}
+
+
+
+void cleanup_bench (struct kddm_set *set)
+{
+	int start, end, test_nr;
+
+	for (test_nr = 0; test_nr <= NR_TESTS; test_nr++) {
+		start = test_nr * NR_TEST_LOOPS;
+		end = start + NR_TEST_LOOPS;
+		do_remove (set, start, end);
+	}
+}
+
+
+
+int do_bench (char *buff, int size, int master_node)
+{
+	int index = 0;
+
+	if (kddm_test4_dist == NULL)
+		alloc_test_kddm_sets(master_node);
+
+	prepare_bench(kddm_test4_loc, master_node);
+	prepare_bench(kddm_test4_dist, master_node);
+	prepare_bench(kddm_test4096_dist, master_node);
+
+	kddm_test_barrier();
+
+	if (kerrighed_node_id == master_node) {
+		index += snprintf (&buff[index], size - index, "----- KDDM "
+				   "BENCH - Local Manager   - Object size = "
+				   "4 -----\n");
+		do_one_bench(kddm_test4_loc, buff, size, &index);
+		index += snprintf (&buff[index], size - index, "----- KDDM "
+				   "BENCH - Distant Manager - Object size = "
+				   "4 -----\n");
+		do_one_bench(kddm_test4_dist, buff, size, &index);
+		index += snprintf (&buff[index], size - index, "----- KDDM "
+				   "BENCH - Distant Manager - Object size = "
+				   "4096 -----\n");
+		do_one_bench(kddm_test4096_dist, buff, size, &index);
+		cleanup_bench(kddm_test4_loc);
+		cleanup_bench(kddm_test4_dist);
+		cleanup_bench(kddm_test4096_dist);
+	}
+	return index;
+}
+
+
+
+int handle_kddm_bench (struct rpc_desc* desc, void *_msg, size_t size)
+{
+	int *master_node = _msg;
+
+	do_bench(NULL, 0, *master_node);
+	return 0;
+}
+
+
+
+int kddm_bench(char *buff, int size)
+{
+	int n, i, master_node = kerrighed_node_id;
+	krgnodemask_t nodes;
+
+	if (kerrighed_nb_nodes < 4) {
+		n = snprintf (buff, size, "Not enough nodes (min nodes: 4)\n");
+		return n;
+	}
+
+	krgnodes_clear(nodes);
+
+	for (i = master_node + 1; i <= master_node + 3; i++)
+		krgnode_set(i, nodes);
+
+	rpc_async_m(KDDM_BENCH, &nodes, &master_node, sizeof(int));
+
+	return do_bench(buff, size, master_node);
+}
+
+
+
+void init_kddm_test (void)
+{
+	struct rpc_synchro* test_server;
+
+	test_server = rpc_synchro_new(1, "kddm test", 0);
+
+	register_io_linker (KDDM_TEST_LINKER, &kddm_test_linker);
+
+	__rpc_register(KDDM_BENCH,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       test_server, handle_kddm_bench, 0);
+}
diff -ruN linux-2.6.29/kddm/kddm_bench.h android_cluster/linux-2.6.29/kddm/kddm_bench.h
--- linux-2.6.29/kddm/kddm_bench.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm_bench.h	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,2 @@
+void init_kddm_test (void);
+int kddm_bench(char *buff, int size);
diff -ruN linux-2.6.29/kddm/kddm.c android_cluster/linux-2.6.29/kddm/kddm.c
--- linux-2.6.29/kddm/kddm.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,146 @@
+/** KDDM module initialization.
+ *  @file kddm.c
+ *
+ *  Implementation of functions used to initialize and finalize the
+ *  KDDM module. It also implements some device file system functions for
+ *  testing purpose.
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+
+#include <kerrighed/hotplug.h>
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+#include "procfs.h"
+#include "protocol_action.h"
+#include <kddm/name_space.h>
+#include <kddm/kddm_set.h>
+#include "kddm_bench.h"
+
+#ifndef CONFIG_KRG_MONOLITHIC
+MODULE_AUTHOR ("Renaud Lottiaux");
+MODULE_DESCRIPTION ("Kerrighed Distributed Data Manager");
+MODULE_LICENSE ("GPL");
+#endif
+
+event_counter_t total_get_object_counter = 0;
+event_counter_t total_grab_object_counter = 0;
+event_counter_t total_remove_object_counter = 0;
+event_counter_t total_flush_object_counter = 0;
+
+int (*kh_copy_kddm_info)(unsigned long clone_flags, struct task_struct * tsk);
+
+struct kmem_cache *kddm_info_cachep;
+
+int kddm_hotplug_init(void);
+void kddm_hotplug_cleanup(void);
+
+
+/** Initialize the kddm field of the krg_task field of the given task.
+ *  @author  Renaud Lottiaux
+ *
+ *  @param tsk   Task to fill the kddm struct.
+ */
+int initialize_kddm_info_struct (struct task_struct *task)
+{
+	struct kddm_info_struct *kddm_info;
+
+	kddm_info = kmem_cache_alloc (kddm_info_cachep, GFP_KERNEL);
+	if (!kddm_info)
+		return -ENOMEM;
+
+	kddm_info->get_object_counter = 0;
+	kddm_info->grab_object_counter = 0;
+	kddm_info->remove_object_counter = 0;
+	kddm_info->flush_object_counter = 0;
+	kddm_info->wait_obj = NULL;
+
+	task->kddm_info = kddm_info;
+
+	return 0;
+}
+
+
+
+int kcb_copy_kddm_info(unsigned long clone_flags, struct task_struct * tsk)
+{
+	return initialize_kddm_info_struct(tsk);
+}
+
+
+
+/** Initialisation of the KDDM sub-system module.
+ *  @author Renaud Lottiaux
+ */
+int init_kddm (void)
+{
+	printk ("KDDM initialisation : start\n");
+
+        kddm_info_cachep = KMEM_CACHE(kddm_info_struct, SLAB_PANIC);
+
+	kddm_ns_init();
+
+	io_linker_init();
+
+	kddm_set_init();
+
+	init_kddm_objects();
+
+	procfs_kddm_init ();
+
+	object_server_init ();
+
+	start_run_queue_thread ();
+
+	hook_register(&kh_copy_kddm_info, kcb_copy_kddm_info);
+
+	kddm_hotplug_init();
+
+	init_kddm_test ();
+
+	/*
+	  process_add(0, kerrighed_nb_nodes);
+	  process_synchronize(0);
+	  process_remove(0);
+	*/
+
+	krgsyms_register (KRGSYMS_KDDM_TREE_OPS, &kddm_tree_set_ops);
+
+	printk ("KDDM initialisation done\n");
+
+	return 0;
+}
+
+
+
+/** Cleanup of the KDDM sub-system.
+ *  @author Renaud Lottiaux
+ */
+void cleanup_kddm (void)
+{
+	printk ("KDDM termination : start\n");
+
+	krgsyms_unregister (KRGSYMS_KDDM_TREE_OPS);
+
+	kddm_hotplug_cleanup();
+
+	stop_run_queue_thread ();
+
+	procfs_kddm_finalize ();
+
+	object_server_finalize ();
+
+	kddm_set_finalize();
+
+	io_linker_finalize();
+
+	kddm_ns_finalize();
+
+	printk ("KDDM termination done\n");
+}
diff -ruN linux-2.6.29/kddm/kddm_find_object.c android_cluster/linux-2.6.29/kddm/kddm_find_object.c
--- linux-2.6.29/kddm/kddm_find_object.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm_find_object.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,100 @@
+/** KDDM find object
+ *  @file kddm_find_object.c
+ *
+ *  Implementation of KDDM find object function.
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/module.h>
+
+#include <kddm/kddm.h>
+#include "protocol_action.h"
+
+
+/** Check the presence of a given object in local physical memory.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set        KDDM set hosting the object.
+ *  @param obj_entry  Object_entry to lookup for.
+ *  @param objid      Identifier of the requested object.
+ *
+ *  @return           Pointer to the object if it is present in memory,
+ *                    NULL otherwise.
+ */
+void *_kddm_find_object (struct kddm_set *set,
+			 objid_t objid)
+{
+	struct kddm_obj *obj_entry;
+	void *object = NULL;
+
+	obj_entry = __get_kddm_obj_entry(set, objid);
+
+	if (obj_entry == NULL)
+		return NULL;
+
+	if (object_frozen(obj_entry, set)) {
+		object = obj_entry->object;
+	}
+
+	switch (OBJ_STATE(obj_entry)) {
+	case INV_OWNER:
+	case INV_COPY:
+	case INV_FILLING:
+		/* No object... */
+		break;
+
+	case WAIT_ACK_INV:
+	case WAIT_CHG_OWN_ACK:
+		break;
+
+	case WAIT_OBJ_RM_DONE:
+	case WAIT_OBJ_RM_ACK:
+	case WAIT_OBJ_RM_ACK2:
+		/* There is an object but being destroyed... */
+		break;
+
+	case WAIT_OBJ_WRITE:
+	case WAIT_OBJ_READ:
+		break;
+
+	case WRITE_GHOST:
+		kddm_change_obj_state(set, obj_entry, objid, WRITE_OWNER);
+		/* Fall through */
+
+	case WAIT_ACK_WRITE:
+	case READ_COPY:
+	case READ_OWNER:
+	case WRITE_OWNER:
+		object = obj_entry->object;
+		break;
+
+	default:
+		STATE_MACHINE_ERROR(set->id, objid, obj_entry);
+		break;
+	}
+
+	if (object)
+		set_object_frozen(obj_entry, set);
+
+	put_kddm_obj_entry(set, obj_entry, objid);
+
+	return object;
+}
+EXPORT_SYMBOL(_kddm_find_object);
+
+
+
+void *kddm_find_object (struct kddm_ns *ns, kddm_set_id_t set_id,
+			objid_t objid)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = _kddm_find_object (set, objid);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(kddm_find_object);
diff -ruN linux-2.6.29/kddm/kddm_flush_object.c android_cluster/linux-2.6.29/kddm/kddm_flush_object.c
--- linux-2.6.29/kddm/kddm_flush_object.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm_flush_object.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,148 @@
+/** KDDM flush object
+ *  @file kddm_flush_object.c
+ *
+ *  Implementation of KDDM flush object function.
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/module.h>
+
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+#include "protocol_action.h"
+
+
+/** Remove an object from local physical memory.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set        KDDM set hosting the object.
+ *  @param objid      Identifier of the object to flush.
+ *  @param dest       Identifier of the node to send object to if needed.
+ *  @return           0 if everything OK, -1 otherwise.
+ *
+ *  Remove an object from local memory and send it to the given node if
+ *  needed. At least one copy is kept somewhere in the cluster memory. The
+ *  object is never swaped to disk through this function.
+ */
+int _kddm_flush_object(struct kddm_set *set,
+		       objid_t objid,
+		       kerrighed_node_t dest)
+{
+	kerrighed_node_t dest_from_copyset;
+	struct kddm_obj *obj_entry;
+	int res = -ENOENT;
+
+	BUG_ON(!set);
+
+	inc_flush_object_counter(set);
+
+	obj_entry = __get_kddm_obj_entry(set, objid);
+	if (obj_entry == NULL)
+		return res;
+
+try_again:
+	switch (OBJ_STATE(obj_entry)) {
+	case READ_COPY:
+		if (object_frozen_or_pinned(obj_entry, set)) {
+			__sleep_on_kddm_obj(set, obj_entry, objid, 0);
+			goto try_again;
+		}
+
+		/* There exist another copy in the cluster.
+		   Just invalidate the local one */
+		destroy_kddm_obj_entry(set, obj_entry, objid, 0);
+		send_invalidation_ack(set, objid, get_prob_owner(obj_entry));
+		res = 0;
+		goto exit_no_unlock;
+
+	case READ_OWNER:
+		if (object_frozen_or_pinned(obj_entry, set)) {
+			__sleep_on_kddm_obj(set, obj_entry, objid, 0);
+			goto try_again;
+		}
+
+		REMOVE_FROM_SET(COPYSET(obj_entry), kerrighed_node_id);
+		if (SET_IS_EMPTY(COPYSET(obj_entry))) {
+			/* I'm owner of the only existing object in the
+			 * cluster. Let's inject it ! */
+			goto send_copy;
+		}
+		/* There exist at least another copy. Send ownership */
+		dest_from_copyset = choose_injection_node_in_copyset(obj_entry);
+		BUG_ON (dest_from_copyset == -1);
+		send_change_ownership_req(set, obj_entry, objid,
+					  dest_from_copyset,
+					  &obj_entry->master_obj);
+
+		/* Wait for ack... The object is invalidated by the ack
+		   handler */
+
+		__sleep_on_kddm_obj(set, obj_entry, objid, 0);
+
+		destroy_kddm_obj_entry(set, obj_entry, objid, 0);
+		res = 0;
+		goto exit_no_unlock;
+
+	case WRITE_GHOST:
+	case WRITE_OWNER:
+		/* Local copy is the only one. Let's inject it ! */
+send_copy:
+		if (dest == KERRIGHED_NODE_ID_NONE) {
+			res = -ENOSPC;
+			break;
+		}
+
+		if (object_frozen_or_pinned(obj_entry, set)) {
+			__sleep_on_kddm_obj(set, obj_entry, objid, 0);
+			goto try_again;
+		}
+
+		send_copy_on_write(set, obj_entry, objid, dest,
+				   KDDM_REMOVE_ON_ACK | KDDM_IO_FLUSH);
+		res = 0;
+		goto exit_no_unlock;
+
+	case WAIT_ACK_INV:
+	case WAIT_OBJ_RM_DONE:
+	case WAIT_OBJ_RM_ACK:
+	case WAIT_OBJ_RM_ACK2:
+		res = 0;
+		break;
+
+	case INV_OWNER:
+	case INV_COPY:
+	case WAIT_ACK_WRITE:
+	case WAIT_CHG_OWN_ACK:
+	case WAIT_OBJ_READ:
+	case WAIT_OBJ_WRITE:
+	case INV_FILLING:
+		break;
+
+	default:
+		STATE_MACHINE_ERROR(set->id, objid, obj_entry);
+		break;
+	}
+	put_kddm_obj_entry(set, obj_entry, objid);
+
+exit_no_unlock:
+
+	return res;
+}
+EXPORT_SYMBOL(_kddm_flush_object);
+
+
+
+int kddm_flush_object(struct kddm_ns *ns, kddm_set_id_t set_id, objid_t objid,
+		      kerrighed_node_t dest)
+{
+	struct kddm_set *set;
+	int res;
+
+	set = _find_get_kddm_set (ns, set_id);
+	res = _kddm_flush_object(set, objid, dest);
+	put_kddm_set(set);
+
+	return res;
+}
+EXPORT_SYMBOL(kddm_flush_object);
diff -ruN linux-2.6.29/kddm/kddm_get_object.c android_cluster/linux-2.6.29/kddm/kddm_get_object.c
--- linux-2.6.29/kddm/kddm_get_object.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm_get_object.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,258 @@
+/** KDDM get object
+ *  @file kddm_get_object.c
+ *
+ *  Implementation of KDDM get object function.
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/module.h>
+
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+#include "protocol_action.h"
+
+
+/** Place a copy of a given object in local physical memory.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set        KDDM set hosting the object.
+ *  @param obj_entry  Object entry of the object to get.
+ *  @param objid      Identifier of the object to get.
+ *  @param flags      Sync / Async request, FT or not, etc...
+ *
+ *  @return        Address of the object if found.
+ *                 NULL if the object does not exist.
+ *                 Negative value if error.
+ *
+ *  A object is retreived from a remote node owning a copy. If there is
+ *  no copy in the cluster and the NO_FT flag is not set,
+ *  the object is created by the IO Linker. Otherwise NULL is returned.
+ */
+void *generic_kddm_get_object(struct kddm_set *set,
+			      objid_t objid,
+			      int flags)
+{
+	struct kddm_obj *obj_entry;
+	void *object = NULL;
+
+	inc_get_object_counter (set);
+
+	obj_entry = __get_kddm_obj_entry(set, objid);
+	if (likely(obj_entry != NULL))
+		goto try_again;
+
+	if (I_AM_DEFAULT_OWNER(set, objid) && (flags & KDDM_NO_FT_REQ) &&
+	    !(flags & KDDM_SEND_OWNERSHIP))
+		return NULL;
+
+	obj_entry = __get_alloc_kddm_obj_entry(set, objid);
+
+try_again:
+	switch (OBJ_STATE(obj_entry)) {
+	case INV_COPY:
+		request_object_on_read(set, obj_entry, objid, flags);
+		kddm_change_obj_state(set, obj_entry, objid, WAIT_OBJ_READ);
+		/* Fall through */
+
+	case WAIT_ACK_WRITE:
+	case WAIT_OBJ_WRITE:
+	case WAIT_OBJ_READ:
+	case INV_FILLING:
+		if (flags & KDDM_ASYNC_REQ)
+			goto exit_no_freeze;
+
+		sleep_on_kddm_obj(set, obj_entry, objid, flags);
+
+		if ((flags & KDDM_NO_FT_REQ) &&
+		    ((OBJ_STATE(obj_entry) == INV_COPY) ||
+		     (OBJ_STATE(obj_entry) == INV_OWNER)))
+			goto exit;
+
+		if (!(OBJ_STATE(obj_entry) & KDDM_READ_OBJ)) {
+			/* Argh, object has been invalidated before we
+			   woke up. */
+			goto try_again;
+		}
+		break;
+
+	case WAIT_CHG_OWN_ACK:
+	case READ_COPY:
+	case READ_OWNER:
+	case WRITE_OWNER:
+	case WAIT_ACK_INV:
+		break;
+
+	case WAIT_OBJ_RM_DONE:
+	case WAIT_OBJ_RM_ACK:
+	case WAIT_OBJ_RM_ACK2:
+		sleep_on_kddm_obj(set, obj_entry, objid, flags);
+		goto try_again;
+
+	case WRITE_GHOST:
+		kddm_change_obj_state(set, obj_entry, objid, WRITE_OWNER);
+		break;
+
+	case INV_OWNER:          /* First Touch */
+		if (flags & KDDM_NO_FT_REQ)
+			goto exit;
+
+		/*** The object can be created on the local node  ***/
+		if (object_first_touch(set, obj_entry, objid,
+				       READ_OWNER, flags) != 0)
+			BUG();
+		break;
+
+	default:
+		STATE_MACHINE_ERROR(set->id, objid, obj_entry);
+		break;
+	}
+
+exit:
+	if (flags & KDDM_ASYNC_REQ)
+		goto exit_no_freeze;
+
+	if (check_sleep_on_local_exclusive(set, obj_entry, objid, flags))
+		goto try_again;
+	if (!(flags & KDDM_NO_FREEZE))
+		set_object_frozen(obj_entry, set);
+
+exit_no_freeze:
+	object = obj_entry->object;
+	put_kddm_obj_entry(set, obj_entry, objid);
+
+	return object;
+}
+
+
+
+void *_kddm_get_object(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_get_object(set, objid, 0);
+}
+EXPORT_SYMBOL(_kddm_get_object);
+
+void *kddm_get_object(struct kddm_ns *ns, kddm_set_id_t set_id, objid_t objid)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_get_object(set, objid, 0);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(kddm_get_object);
+
+
+
+void *_fkddm_get_object(struct kddm_set *set, objid_t objid, int flags)
+{
+	return generic_kddm_get_object(set, objid, flags);
+}
+EXPORT_SYMBOL(_fkddm_get_object);
+
+void *fkddm_get_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+			objid_t objid, int flags)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_get_object(set, objid, flags);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(fkddm_get_object);
+
+
+
+void *_async_kddm_get_object(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_get_object(set, objid, KDDM_ASYNC_REQ);
+}
+EXPORT_SYMBOL(_async_kddm_get_object);
+
+void *async_kddm_get_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+			    objid_t objid)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_get_object(set, objid, KDDM_ASYNC_REQ);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(async_kddm_get_object);
+
+
+
+void *_kddm_get_object_no_ft(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_get_object(set, objid, KDDM_NO_FT_REQ);
+}
+EXPORT_SYMBOL(_kddm_get_object_no_ft);
+
+void *kddm_get_object_no_ft(struct kddm_ns *ns, kddm_set_id_t set_id,
+			    objid_t objid)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_get_object(set, objid, KDDM_NO_FT_REQ);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(kddm_get_object_no_ft);
+
+
+
+void *_kddm_get_object_manual_ft(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_get_object(set, objid, KDDM_NO_FT_REQ |
+				       KDDM_SEND_OWNERSHIP);
+}
+EXPORT_SYMBOL(_kddm_get_object_manual_ft);
+
+void *kddm_get_object_manual_ft(struct kddm_ns *ns, kddm_set_id_t set_id,
+				objid_t objid)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_get_object(set, objid, KDDM_NO_FT_REQ |
+				      KDDM_SEND_OWNERSHIP);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(kddm_get_object_manual_ft);
+
+
+
+void *_kddm_get_object_no_lock(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_get_object(set, objid, KDDM_NO_FREEZE);
+}
+EXPORT_SYMBOL(_kddm_get_object_no_lock);
+
+void *kddm_get_object_no_lock(struct kddm_ns *ns, kddm_set_id_t set_id,
+			      objid_t objid)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_get_object(set, objid, KDDM_NO_FREEZE);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(kddm_get_object_no_lock);
diff -ruN linux-2.6.29/kddm/kddm_grab_object.c android_cluster/linux-2.6.29/kddm/kddm_grab_object.c
--- linux-2.6.29/kddm/kddm_grab_object.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm_grab_object.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,394 @@
+/** KDDM grab object
+ *  @file kddm_grab_object.c
+ *
+ *  Implementation of KDDM grab object function.
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/module.h>
+
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+#include "protocol_action.h"
+
+static inline struct kddm_obj *check_cow (struct kddm_set *set,
+					  struct kddm_obj *obj_entry,
+					  objid_t objid,
+					  int flags,
+					  int *retry)
+{
+	*retry = 0;
+	if (flags & KDDM_COW_OBJECT) {
+		if (object_frozen(obj_entry, set)) {
+			if (!(flags & KDDM_ASYNC_REQ)) {
+				sleep_on_kddm_obj(set, obj_entry, objid,
+						  flags);
+				*retry = 1;
+			}
+		}
+		else
+			obj_entry = kddm_break_cow_object(set, obj_entry,
+							  objid,
+							  KDDM_BREAK_COW_COPY);
+	}
+	return obj_entry;
+}
+
+/** Get a copy of a object and invalidate any other existing copy.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set        KDDM set hosting the object.
+ *  @param obj_entry  Object entry of the object to grab.
+ *  @param objid      Identifier of the object to grab.
+ *  @param flags      Sync / Async request, FT or not, etc...
+ *
+ *  @return              Address of the object if found.
+ *                       NULL if the object does not exist.
+ *                       Negative value if error.
+ */
+void *generic_kddm_grab_object(struct kddm_set *set,
+			       objid_t objid,
+			       int flags)
+{
+	struct kddm_obj *obj_entry;
+	void *object;
+	int retry;
+
+	inc_grab_object_counter(set);
+
+	obj_entry = __get_kddm_obj_entry(set, objid);
+	if (likely(obj_entry != NULL))
+		goto try_again;
+
+	if (I_AM_DEFAULT_OWNER(set, objid) && (flags & KDDM_NO_FT_REQ) &&
+	    !(flags & KDDM_SEND_OWNERSHIP))
+		return NULL;
+
+	obj_entry = __get_alloc_kddm_obj_entry(set, objid);
+
+try_again:
+	switch (OBJ_STATE(obj_entry)) {
+	case READ_COPY:
+		if (object_frozen(obj_entry, set)) {
+			if (flags & KDDM_ASYNC_REQ)
+				BUG();
+			goto sleep;
+		}
+
+	case INV_COPY:
+		request_object_on_write(set, obj_entry, objid, flags);
+		CLEAR_SET(COPYSET(obj_entry));
+		kddm_change_obj_state(set, obj_entry, objid, WAIT_OBJ_WRITE);
+		if (flags & KDDM_TRY_GRAB)
+			goto sleep_on_wait_page;
+		/* Else Fall through */
+
+	case WAIT_ACK_WRITE:
+	case WAIT_OBJ_WRITE:
+	case INV_FILLING:
+		if (flags & KDDM_TRY_GRAB)
+			goto exit_try_failed;
+
+		if (flags & KDDM_ASYNC_REQ)
+			goto exit_no_freeze;
+
+sleep_on_wait_page:
+		sleep_on_kddm_obj(set, obj_entry, objid, flags);
+
+		if (OBJ_STATE(obj_entry) == WRITE_OWNER) {
+			obj_entry = check_cow (set, obj_entry, objid, flags,
+					       &retry);
+			if (retry)
+				goto try_again;
+			break;
+		}
+
+		if (flags & KDDM_NO_FT_REQ) {
+			if (OBJ_STATE(obj_entry) == INV_OWNER)
+				break;
+
+			if (OBJ_STATE(obj_entry) == INV_COPY) {
+				if (!(flags & KDDM_SEND_OWNERSHIP))
+					break;
+				BUG();
+			}
+		}
+
+		if (flags & KDDM_TRY_GRAB)
+			goto exit_try_failed;
+
+		/* Argh, object has been invalidated before we woke up. */
+		goto try_again;
+
+	case INV_OWNER:
+		if (flags & KDDM_NO_FT_REQ)
+			break;
+
+		/*** The object can be created on the local node  ***/
+		if (object_first_touch(set, obj_entry, objid,
+				       WRITE_OWNER, flags) != 0)
+			BUG();
+		break;
+
+	case READ_OWNER:
+		obj_entry = check_cow (set, obj_entry, objid, flags,
+				       &retry);
+		if (retry)
+			goto try_again;
+
+		if (!OBJ_EXCLUSIVE(obj_entry)) {
+			kddm_change_obj_state(set, obj_entry, objid,
+					      WAIT_ACK_WRITE);
+			request_copies_invalidation(set, obj_entry, objid,
+						    kerrighed_node_id);
+			if (flags & KDDM_ASYNC_REQ)
+				goto exit_no_freeze;
+			sleep_on_kddm_obj(set, obj_entry, objid, flags);
+
+			if (OBJ_STATE(obj_entry) != WRITE_OWNER) {
+				/* Argh, object has been invalidated before
+				   we woke up. */
+				goto try_again;
+			}
+		} else
+			kddm_change_obj_state(set, obj_entry, objid,
+					      WRITE_OWNER);
+		break;
+
+	case WRITE_OWNER:
+		obj_entry = check_cow (set, obj_entry, objid, flags, &retry);
+		if (retry)
+			goto try_again;
+		break;
+
+	case WRITE_GHOST:
+		obj_entry = check_cow (set, obj_entry, objid, flags, &retry);
+		if (retry)
+			goto try_again;
+		kddm_change_obj_state(set, obj_entry, objid, WRITE_OWNER);
+		break;
+
+	case WAIT_ACK_INV:
+	case WAIT_OBJ_READ:
+		if (flags & KDDM_TRY_GRAB)
+			goto exit_try_failed;
+
+		/* Fall through */
+	case WAIT_OBJ_RM_DONE:
+	case WAIT_OBJ_RM_ACK:
+	case WAIT_OBJ_RM_ACK2:
+	case WAIT_CHG_OWN_ACK:
+sleep:
+		if (flags & KDDM_ASYNC_REQ)
+			goto exit_no_freeze;
+
+		sleep_on_kddm_obj(set, obj_entry, objid, flags);
+		goto try_again;
+
+	default:
+		STATE_MACHINE_ERROR(set->id, objid, obj_entry);
+		break;
+	}
+
+	if (flags & KDDM_ASYNC_REQ)
+		goto exit_no_freeze;
+
+	if (object_frozen(obj_entry, set) &&
+	    (flags & KDDM_TRY_GRAB) &&
+	    (kddm_local_exclusive (set)))
+		goto exit_try_failed;
+
+	if (check_sleep_on_local_exclusive(set, obj_entry, objid, flags))
+		goto try_again;
+
+	if (!(flags & KDDM_NO_FREEZE))
+		set_object_frozen(obj_entry, set);
+
+exit_no_freeze:
+	object = obj_entry->object;
+	put_kddm_obj_entry(set, obj_entry, objid);
+
+	return object;
+
+exit_try_failed:
+	put_kddm_obj_entry(set, obj_entry, objid);
+	return ERR_PTR(-EBUSY);
+}
+
+
+
+void *_kddm_grab_object(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_grab_object(set, objid, 0);
+}
+EXPORT_SYMBOL(_kddm_grab_object);
+
+void *kddm_grab_object(struct kddm_ns *ns, kddm_set_id_t set_id, objid_t objid)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_grab_object(set, objid, 0);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(kddm_grab_object);
+
+void *fkddm_grab_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+			objid_t objid, int flags)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_grab_object(set, objid, flags);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(fkddm_grab_object);
+
+
+
+void *_async_kddm_grab_object(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_grab_object(set, objid, KDDM_ASYNC_REQ);
+}
+EXPORT_SYMBOL(_async_kddm_grab_object);
+
+void *async_kddm_grab_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+			     objid_t objid)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_grab_object(set, objid, KDDM_ASYNC_REQ);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(async_kddm_grab_object);
+
+
+
+void *_kddm_grab_object_no_ft(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_grab_object(set, objid, KDDM_NO_FT_REQ);
+}
+EXPORT_SYMBOL(_kddm_grab_object_no_ft);
+
+void *kddm_grab_object_no_ft(struct kddm_ns *ns, kddm_set_id_t set_id,
+			     objid_t objid)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_grab_object(set, objid, KDDM_NO_FT_REQ);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(kddm_grab_object_no_ft);
+
+
+
+void *_async_kddm_grab_object_no_ft(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_grab_object(set, objid, KDDM_NO_FT_REQ |
+					KDDM_ASYNC_REQ);
+}
+EXPORT_SYMBOL(_async_kddm_grab_object_no_ft);
+
+void *async_kddm_grab_object_no_ft(struct kddm_ns *ns, kddm_set_id_t set_id,
+				   objid_t objid)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_grab_object(set, objid, KDDM_NO_FT_REQ |
+				       KDDM_ASYNC_REQ);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(async_kddm_grab_object_no_ft);
+
+
+
+void *_kddm_grab_object_manual_ft(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_grab_object(set, objid, KDDM_NO_FT_REQ |
+					KDDM_SEND_OWNERSHIP);
+}
+EXPORT_SYMBOL(_kddm_grab_object_manual_ft);
+
+void *kddm_grab_object_manual_ft(struct kddm_ns *ns, kddm_set_id_t set_id,
+				 objid_t objid)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_grab_object(set, objid, KDDM_NO_FT_REQ |
+				       KDDM_SEND_OWNERSHIP);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(kddm_grab_object_manual_ft);
+
+
+
+void *_kddm_grab_object_no_lock(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_grab_object(set, objid, KDDM_NO_FREEZE);
+}
+EXPORT_SYMBOL(_kddm_grab_object_no_lock);
+
+void *kddm_grab_object_no_lock(struct kddm_ns *ns, kddm_set_id_t set_id,
+			       objid_t objid)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_grab_object(set, objid, KDDM_NO_FREEZE);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(kddm_grab_object_no_lock);
+
+
+void *_kddm_try_grab_object(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_grab_object(set, objid, KDDM_TRY_GRAB);
+}
+EXPORT_SYMBOL(_kddm_try_grab_object);
+
+void *_kddm_grab_object_cow(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_grab_object(set, objid, KDDM_COW_OBJECT);
+}
+EXPORT_SYMBOL(_kddm_grab_object_cow);
+
+
+void *kddm_try_grab_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+			   objid_t objid)
+{
+	struct kddm_set *set;
+	void *obj;
+
+	set = _find_get_kddm_set (ns, set_id);
+	obj = generic_kddm_grab_object(set, objid, KDDM_TRY_GRAB);
+	put_kddm_set(set);
+
+	return obj;
+}
+EXPORT_SYMBOL(kddm_try_grab_object);
diff -ruN linux-2.6.29/kddm/kddm_hotplug.c android_cluster/linux-2.6.29/kddm/kddm_hotplug.c
--- linux-2.6.29/kddm/kddm_hotplug.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm_hotplug.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,764 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+
+#include <linux/wait.h>
+#include <linux/list.h>
+#include <linux/kernel.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/krginit.h>
+#include <linux/hashtable.h>
+#include <linux/cluster_barrier.h>
+
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/hotplug.h>
+#include <kddm/kddm.h>
+#include "protocol_action.h"
+
+struct cluster_barrier *kddm_barrier;
+
+extern krgnodemask_t krgnode_kddm_map;
+extern kerrighed_node_t kddm_nb_nodes;
+extern kerrighed_node_t __kddm_io_default_owner (struct kddm_set *set,
+						 objid_t objid,
+						 const krgnodemask_t *nodes,
+						 int nr_nodes);
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                NODE ADDITION                             *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+struct browse_add_param {
+	struct kddm_set *set;
+	krgnodemask_t new_nodes_map;
+	kerrighed_node_t new_nb_nodes;
+};
+
+static int add_browse_objects(unsigned long objid,
+			      void *_obj_entry,
+			      void *_data)
+{
+	struct kddm_obj *obj_entry = (struct kddm_obj *)_obj_entry;
+	kerrighed_node_t old_def_owner, new_def_owner;
+	struct browse_add_param *param = _data;
+	struct kddm_set *set = param->set;
+
+	old_def_owner = kddm_io_default_owner (set, objid);
+	new_def_owner = __kddm_io_default_owner(set, objid,
+						&param->new_nodes_map,
+						param->new_nb_nodes);
+
+	if (new_def_owner == old_def_owner)
+		goto done;
+
+	switch (OBJ_STATE(obj_entry)) {
+	case READ_OWNER:
+	case WRITE_GHOST:
+	case WRITE_OWNER:
+	case WAIT_ACK_INV:
+	case WAIT_ACK_WRITE:
+	case WAIT_CHG_OWN_ACK:
+		BUG_ON (get_prob_owner(obj_entry) != kerrighed_node_id);
+		if (new_def_owner == kerrighed_node_id)
+			break;
+		/* Inform the new owner a copy already exist */
+		request_change_prob_owner(set, objid, new_def_owner,
+					  kerrighed_node_id);
+		break;
+
+	case INV_OWNER:
+		/* Update the local default owner to the new one */
+		change_prob_owner(obj_entry, new_def_owner);
+		break;
+
+	case INV_COPY:
+	case READ_COPY:
+	case WAIT_OBJ_READ:
+	case WAIT_OBJ_WRITE:
+	case WAIT_OBJ_RM_DONE:
+		BUG_ON(get_prob_owner(obj_entry) == kerrighed_node_id);
+		break;
+
+	case WAIT_OBJ_RM_ACK:
+		PANIC ("Case not yet managed\n");
+
+	case INV_FILLING:
+		BUG();
+
+	default:
+		STATE_MACHINE_ERROR (set->id, objid, obj_entry);
+		break;
+	}
+
+done:
+	return 0;
+};
+
+static void add_browse_sets(void *_set, void *_data)
+{
+	struct browse_add_param *param = _data;
+	struct kddm_set *set = _set;
+
+	BUG_ON(set->def_owner < 0);
+	BUG_ON(set->def_owner > KDDM_MAX_DEF_OWNER);
+
+	switch (set->def_owner) {
+	case KDDM_RR_DEF_OWNER:
+	case KDDM_CUSTOM_DEF_OWNER:
+		param->set = set;
+		__for_each_kddm_object(set, add_browse_objects, _data);
+		break;
+
+	case KDDM_UNIQUE_ID_DEF_OWNER:
+		/* The unique_id default owners are hard-coded depending on
+		 * object ids. Adding a node doesn't change anything. */
+	default:
+		/* The default owner is hard coded to a given node.
+		 * Adding a node doesn't change anything for these cases.
+		 */
+		break;
+	};
+
+};
+
+static void set_add(krgnodemask_t * vector)
+{
+	struct browse_add_param param;
+        kerrighed_node_t node;
+
+	if(__krgnode_isset(kerrighed_node_id, vector))
+		rpc_enable(KDDM_CHANGE_PROB_OWNER);
+
+	krgnodes_copy(param.new_nodes_map, krgnode_online_map);
+
+	param.new_nb_nodes = kerrighed_nb_nodes;
+	__for_each_krgnode_mask(node, vector) {
+		if (!krgnode_online(node)) {
+			krgnode_set(node, param.new_nodes_map);
+			param.new_nb_nodes++;
+		}
+	};
+
+	freeze_kddm();
+
+	cluster_barrier(kddm_barrier, &param.new_nodes_map,
+			__first_krgnode(&param.new_nodes_map));
+
+	__hashtable_foreach_data(kddm_def_ns->kddm_set_table,
+				 add_browse_sets, &param);
+
+	cluster_barrier(kddm_barrier, &param.new_nodes_map,
+			__first_krgnode(&param.new_nodes_map));
+
+	kddm_nb_nodes = param.new_nb_nodes;
+	krgnodes_copy(krgnode_kddm_map, param.new_nodes_map);
+
+	unfreeze_kddm();
+
+	if(!__krgnode_isset(kerrighed_node_id, vector))
+		return;
+
+	rpc_enable(REQ_OBJECT_COPY);
+	rpc_enable(REQ_OBJECT_REMOVE);
+	rpc_enable(REQ_OBJECT_REMOVE_TO_MGR);
+	rpc_enable(SEND_BACK_FIRST_TOUCH);
+	rpc_enable(REQ_OBJECT_INVALID);
+	rpc_enable(INVALIDATION_ACK);
+	rpc_enable(REMOVE_ACK);
+	rpc_enable(REMOVE_ACK2);
+	rpc_enable(REMOVE_DONE);
+	rpc_enable(SEND_OWNERSHIP);
+	rpc_enable(CHANGE_OWNERSHIP_ACK);
+	rpc_enable(OBJECT_SEND);
+	rpc_enable(SEND_WRITE_ACCESS);
+	rpc_enable(NO_OBJECT_SEND);
+}
+
+/**
+ *
+ * Remove related part
+ *
+ **/
+
+static int browse_remove(unsigned long objid, void *_obj_entry,
+			 void *_data)
+{
+	struct kddm_obj *obj_entry = _obj_entry;
+	struct kddm_set *kddm_set = (struct kddm_set *)_data;
+
+	might_sleep();
+	switch (OBJ_STATE(obj_entry)) {
+	case READ_OWNER:
+		up (&kddm_def_ns->table_sem);
+		_kddm_flush_object(kddm_set, objid,
+				   krgnode_next_online_in_ring(kerrighed_node_id));
+		down (&kddm_def_ns->table_sem);
+		return -1;
+		break;
+
+	case READ_COPY:
+	case WRITE_GHOST:
+	case WRITE_OWNER:
+		// we have to flush this object
+		_kddm_flush_object(kddm_set, objid,
+				   krgnode_next_online_in_ring(kerrighed_node_id));
+		break;
+
+	case WAIT_ACK_INV:
+	case WAIT_OBJ_RM_ACK:
+		printk ("kddm_set_remove_cb: WAIT_ACK_INV: todo\n");
+		break;
+
+	case WAIT_ACK_WRITE:
+	case WAIT_CHG_OWN_ACK:
+	case WAIT_OBJ_READ:
+	case WAIT_OBJ_WRITE:
+		// here we have to check if there are some pending process...
+		// and may be we kill them
+		if (waitqueue_active
+		    (&obj_entry->waiting_tsk))
+			printk("we have some pending process in %lu %s (%x)\n",
+			       objid,
+			       STATE_NAME
+			       (OBJ_STATE(obj_entry)),
+			       OBJ_STATE(obj_entry));
+
+	case INV_OWNER:
+	case INV_COPY:
+		break;
+
+	case WAIT_OBJ_RM_DONE:
+		PANIC ("Case not yet managed\n");
+
+	default:
+		STATE_MACHINE_ERROR
+			(kddm_set->id,
+			 objid, obj_entry);
+		break;
+	}
+
+	return 0;
+};
+
+static void kddm_set_remove_cb(void *_kddm_set, void *_data)
+{
+	struct kddm_set *kddm_set = _kddm_set;
+
+	__for_each_kddm_object(kddm_set, browse_remove, kddm_set);
+
+};
+
+static void set_remove(krgnodemask_t * vector)
+{
+
+	printk("set_remove...\n");
+	return;
+
+	down (&kddm_def_ns->table_sem);
+	__hashtable_foreach_data(kddm_def_ns->kddm_set_table,
+				 kddm_set_remove_cb, vector);
+	up (&kddm_def_ns->table_sem);
+};
+
+/**
+ *
+ * Failure related part
+ *
+ **/
+
+#if 0
+
+extern krgnodemask_t failure_vector;
+
+/*
+ * comm transact: set_copyset
+ */
+
+static void handle_set_copyset(struct rpc_desc *desc)
+{
+	struct kddm_set *set;
+	struct rpc_desc *new_desc;
+	struct kddm_obj *obj_entry;
+	kddm_set_id_t set_id;
+	objid_t objid;
+	krgnodemask_t map;
+	kerrighed_node_t true_owner;
+	kerrighed_node_t potential_owner;
+
+	rpc_unpack_type(desc, set_id);
+	rpc_unpack_type(desc, objid);
+	rpc_unpack_type(desc, map);
+	rpc_unpack_type(desc, true_owner);
+	rpc_unpack_type(desc, potential_owner);
+
+	// check if the object is available on this node
+	// check if we are in the received copyset
+	// yes: a new copyset has beed built (set myself as owner)
+	// no: we are building a new copyset (update the copyset and forward the rq to the next node)
+	// we have to look our probeOwner in order to decide if we relay the rq (and take care) or not
+
+	// Is it my request ?
+	if (krgnode_isset(kerrighed_node_id, map)) {
+		// Yes it is (since I'm already in the copyset)
+
+		obj_entry = _get_kddm_obj_entry(kddm_def_ns, set_id,
+						objid, &set);
+
+		// the object should be frozen... so we expect to be in READ_COPY
+		BUG_ON(OBJ_STATE(obj_entry) != READ_COPY);
+
+		// Is the true owner still alive ?
+		if (true_owner == -1) {
+			//No, it is dead: I migth have to become the new owner
+
+			// May be there are some other applicant to this position...
+			if (potential_owner == kerrighed_node_id) {
+				// I'm the new owner
+				kddm_change_obj_state(set, obj_entry, objid, READ_OWNER);
+				change_prob_owner(obj_entry, kerrighed_node_id);
+
+				krgnodes_copy(obj_entry->master_obj.copyset, map);
+
+				/* TODO: have to check if the set is HARDLINKED */
+				if (nth_online_krgnode(objid % kerrighed_nb_nodes) !=
+				    kerrighed_node_id) {
+					struct rpc_desc *desc;
+
+					desc = rpc_begin(KDDM_ADVERTISE_OWNER,
+							 nth_online_krgnode(objid % kerrighed_nb_nodes));
+					rpc_pack_type(desc, set_id);
+					rpc_pack_type(desc, objid);
+					rpc_end(desc, 0);
+				};
+
+			} else {
+				// There is another (better) applicant
+				change_prob_owner(obj_entry, potential_owner);
+			};
+		} else {
+			// Yes, It is: just update my prob_owner
+			change_prob_owner(obj_entry, true_owner);
+		};
+
+		object_clear_frozen(obj_entry, set);
+		put_kddm_obj_entry(set, obj_entry, objid);
+	} else {
+
+		set = _local_get_kddm_set(kddm_def_ns, set_id);
+
+		// is this set availaible on this node ?
+		if (!set)
+			goto forward_rq;
+
+		obj_entry = __get_kddm_obj_entry(set, objid);
+
+		// is this object available on this node ?
+		if (!obj_entry)
+			goto put_set_forward_rq;
+
+		if (I_AM_OWNER(obj_entry)) {
+			// I'm the owner of this object: let everyboy knows about that
+			true_owner = kerrighed_node_id;
+			goto unlock_forward_rq;
+		};
+
+		// I'm not the owner
+		if (true_owner != -1) {
+			// The owner is known... just update my probOwner
+			change_prob_owner(obj_entry, true_owner);
+			goto unlock_forward_rq;
+		};
+
+		// The owner is unknown (until now), may be I could become the next one
+		if (OBJ_STATE(obj_entry) == READ_COPY) {
+			krgnode_set(kerrighed_node_id, map);
+
+			// update our probOwner to potentiel owner
+			change_prob_owner(obj_entry, potential_owner);
+
+			// Check if local node is node a better potential owner
+			// Here the potential owner is the one with the lowest id
+			if (potential_owner < kerrighed_node_id) {
+				potential_owner = kerrighed_node_id;
+			};
+
+			goto unlock_forward_rq;
+		};
+
+		// By default, if probOwner failed: update to potential_owner
+		if(krgnode_isset(get_prob_owner(obj_entry), failure_vector))
+			change_prob_owner(obj_entry, potential_owner);
+
+	unlock_forward_rq:
+		put_kddm_obj_entry(set, obj_entry, objid);
+
+	put_set_forward_rq:
+		put_kddm_set(set);
+
+	forward_rq:
+		// Forward the request
+		new_desc = rpc_begin(krgnode_next_online_in_ring(kerrighed_node_id),
+				     KDDM_COPYSET);
+		rpc_pack_type(new_desc, set_id);
+		rpc_pack_type(new_desc, objid);
+		rpc_pack_type(new_desc, map);
+		rpc_pack_type(new_desc, true_owner);
+		rpc_pack_type(new_desc, potential_owner);
+		rpc_end(new_desc, 0);
+	};
+
+};
+
+/*
+ * comm transact: advertise_owner
+ */
+static krgnodemask_t select_sync;
+
+static void handle_select_owner(struct rpc_desc *desc)
+{
+	struct kddm_set *set;
+	struct kddm_obj *obj_entry;
+	kddm_set_id_t set_id;
+	objid_t objid;
+	krgnodemask_t copyset;
+	kerrighed_node_t sender;
+	int sync;
+
+	rpc_unpack_type(desc, sender);
+	rpc_unpack_type(desc, set_id);
+	rpc_unpack_type(desc, objid);
+	rpc_unpack_type(desc, copyset);
+	rpc_unpack_type(desc, sync);
+
+	if (sync != KERRIGHED_NODE_ID_NONE) {
+		krgnode_set(sync, select_sync);
+
+		// forward to the next node ?
+		if (krgnode_next_online_in_ring(kerrighed_node_id) != sender)
+			rpc_forward(desc, krgnode_next_online_in_ring(kerrighed_node_id));
+
+		if (!krgnodes_equal(select_sync, krgnode_online_map)) {
+			// We have to wait for another sync
+			return;
+		} else {
+			// We received all the sync... we can continue the recovery mechanism
+			down (&kddm_def_ns->table_sem);
+			__hashtable_foreach_data(kddm_def_ns->kddm_set_table,
+						 kddm_set_failure_cb,
+						 &failure_vector);
+			up (&kddm_def_ns->table_sem);
+
+			return;
+		};
+	};
+
+	// the struct kddm_setable lock is already held by the fct: kddm_set_failure
+	set = __hashtable_find(kddm_def_ns->kddm_set_table, set_id);
+
+	// is this set availaible on this node ?
+	if (set != NULL) {
+
+		obj_entry = __get_kddm_obj_entry(set, objid);
+
+		// is this object available on this node ?
+		if (obj_entry != NULL) {
+			CLEAR_FAILURE_FLAG(obj_entry);
+			change_prob_owner(obj_entry, sender);
+			put_kddm_obj_entry(set, obj_entry, objid);
+		};
+
+	};
+
+	// TODO: optimisation: ici on peut envoyer au suivant du copyset
+	if (krgnode_next_online_in_ring(kerrighed_node_id) != sender) {
+		struct rpc_desc *new_desc;
+
+		new_desc = rpc_begin(KDDM_SELECT_OWNER,
+				     krgnode_next_online_in_ring(kerrighed_node_id));
+		rpc_pack_type(new_desc, sender);
+		rpc_pack_type(new_desc, set_id);
+		rpc_pack_type(new_desc, objid);
+		rpc_pack_type(new_desc, copyset);
+		rpc_pack_type(new_desc, sync);
+		rpc_end(new_desc, 0);
+
+	};
+
+};
+
+/**
+ **
+ ** Per kddm_set callback
+ **
+ **/
+static int browse_clean_failure(unsigned long objid, void *_obj_entry,
+				void *_data)
+{
+	struct kddm_obj *obj_entry = (struct kddm_obj *)_obj_entry;
+	BUG_ON(!obj_entry);
+	CLEAR_FAILURE_FLAG(obj_entry);
+	return 0;
+};
+
+static void kddm_set_clean_failure_cb(void *_set, void *_data)
+{
+	struct kddm_set *set = _set;
+
+	__for_each_kddm_object(set, browse_clean_failure, NULL);
+};
+
+static int browse_failure(unsigned long objid, void *_obj_entry,
+			  void *_data)
+{
+	int correct_prob_owner = 0;
+	struct kddm_set * set = (struct kddm_set *)_data;
+	struct kddm_obj *obj_entry = (struct kddm_obj *)_obj_entry;
+
+	if(TEST_FAILURE_FLAG(obj_entry))
+		goto exit;
+
+	if (kddm_ft_linked(set)) {
+		if (!krgnode_online(get_prob_owner(obj_entry))){
+			printk("browse_failure: TODO: set %ld is FT Linked\n",
+			       set->id);
+			change_prob_owner(obj_entry, kerrighed_node_id);
+		}
+	} else {
+		if (krgnode_online(get_prob_owner(obj_entry)))
+			correct_prob_owner = 1;
+
+		if (!krgnode_online(get_prob_owner(obj_entry))
+		    || get_prob_owner(obj_entry) == kerrighed_node_id)
+			change_prob_owner(obj_entry,
+			  nth_online_krgnode(objid % kerrighed_nb_nodes));
+
+	};
+
+	switch (OBJ_STATE(obj_entry)) {
+	case READ_COPY:{
+		struct rpc_desc *desc;
+		krgnodemask_t copyset;
+		kerrighed_node_t unknown = -1;
+
+		// Does our prob-chain still valid ?
+		if (correct_prob_owner)
+			break;
+
+		// No, it does not. We might have to choose a new owner
+
+		// since we might be the new owner... freeze the object
+		set_object_frozen(obj_entry, set);
+
+		// start a ring-request in order to compute the new copyset
+		krgnodes_clear(copyset);
+		krgnode_set(kerrighed_node_id, copyset);
+
+		desc = rpc_begin(KDDM_COPYSET,
+				 krgnode_next_online_in_ring(kerrighed_node_id));
+		rpc_pack_type(desc, set->id);
+		rpc_pack_type(desc, objid);
+		rpc_pack_type(desc, copyset);
+		rpc_pack_type(desc, unknown);
+		rpc_pack_type(desc, kerrighed_node_id);
+		rpc_end(desc, 0);
+
+		break;
+	};
+
+	case INV_OWNER:
+	case READ_OWNER:
+	case WRITE_GHOST:
+	case WRITE_OWNER:{
+		/* We just have to update the default probeOwner */
+
+		if (get_prob_owner(obj_entry) != kerrighed_node_id) {
+			struct rpc_desc *desc;
+
+			desc = rpc_begin(KDDM_ADVERTISE_OWNER,
+					 get_prob_owner(obj_entry));
+			rpc_pack_type(desc, set->id);
+			rpc_pack_type(desc, objid);
+			rpc_end(desc, 0);
+		};
+
+		break;
+	};
+
+	case INV_COPY:
+		break;
+
+	case WAIT_ACK_INV:
+		printk("kddm_set_failure_cb: WAIT_ACK_INV: todo\n");
+		// we are waiting for the ack of an invalidation.
+		// if the dest is a fail-node, we can just discard this rq silently
+		break;
+
+	case WAIT_OBJ_RM_ACK:
+		printk("kddm_set_failure_cb: WAIT_OBJ_RM_ACK: todo\n");
+		break;
+
+	case WAIT_OBJ_RM_ACK2:
+		printk("kddm_set_failure_cb: WAIT_OBJ_RM_ACK2: todo\n");
+		break;
+
+	case WAIT_ACK_WRITE:
+	case WAIT_CHG_OWN_ACK:
+	case WAIT_OBJ_READ:
+	case WAIT_OBJ_WRITE:
+		// we don't have the object and no one claim for it... destroy
+		SET_FAILURE_FLAG(obj_entry);
+
+		// we have some stuff waiting for unreachable object... destroy
+		if (waitqueue_active(&obj_entry->waiting_tsk)){
+			wait_queue_t *wait;
+
+			wait = list_entry(obj_entry->waiting_tsk.task_list.next,
+					  wait_queue_t, task_list);
+			printk("we have some pending processes in %lu (%ld) "
+			       "%s (%x)\n",
+			       objid, set->id,
+			       STATE_NAME(OBJ_STATE(obj_entry)),
+			       OBJ_STATE(obj_entry));
+
+			wake_up(&obj_entry->waiting_tsk);
+		};
+		break;
+
+	default:
+		STATE_MACHINE_ERROR(set->id, objid, obj_entry);
+		break;
+	};
+
+exit:
+	return 0;
+};
+
+static void kddm_set_failure_cb(void *_set, void *_data)
+{
+	struct kddm_set *set = _set;
+
+	__for_each_kddm_object(set, browse_failure, set);
+};
+
+static int browse_select_owner(objid_t objid, void *_obj_entry,
+			       void *_data)
+{
+	krgnodemask_t *vector_fail = _data;
+	struct kddm_obj * obj_entry = (struct kddm_obj *)_obj_entry;
+
+	BUG_ON(!vector_fail);
+
+	if (I_AM_OWNER(obj_entry)) {
+		kerrighed_node_t node;
+
+		__for_each_krgnode_mask(node, vector_fail){
+			REMOVE_FROM_SET(COPYSET(obj_entry), node);
+		};
+
+	};
+
+	return 0;
+};
+
+static void kddm_set_select_owner_cb(void *_set, void *_data)
+{
+	struct kddm_set *set = _set;
+
+	__for_each_kddm_object(set, browse_select_owner, _data);
+};
+
+/* set_failure
+ * Handle the kddm recovery mechanism
+ * 1. Stop ownership/copyset management of the object
+ * 2. Clean the recovery information (may be from a previous recovery) of each object
+ * 3. Prepare to receive ownership advertisement
+ * 4. Advertise to every nodes each object we own
+ * 5. Global synchro (in order to be sure that every node had sent its ownership)
+ * 6. Restart ownership management
+ * 7. Clean the object that loose the owner
+ *    (try to elect a new ownership, destroy non correctible entries, SEGFAULT corresponding processes)
+ */
+static void set_failure(krgnodemask_t * vector)
+{
+	struct rpc_desc *desc;
+	objid_t objid = 0;
+	kddm_set_id_t set_id = 0;
+	int sync = kerrighed_node_id;
+	krgnodemask_t v;
+
+	down (&kddm_def_ns->table_sem);
+
+	__hashtable_foreach_data(kddm_def_ns->kddm_set_table,
+				 kddm_set_clean_failure_cb, vector);
+
+	krgnode_set(kerrighed_node_id, select_sync);
+
+	__hashtable_foreach_data(kddm_def_ns->kddm_set_table,
+				 kddm_set_select_owner_cb, vector);
+
+	printk("TODO: we MUST lock creation/destruction of kddm_set during"
+	       "the recovery step and we should use read/write lock\n");
+	up (&kddm_def_ns->table_sem);
+
+	desc = rpc_begin(KDDM_SELECT_OWNER,
+			 krgnode_next_online_in_ring(kerrighed_node_id));
+	rpc_pack_type(desc, kerrighed_node_id);
+	rpc_pack_type(desc, set_id);
+	rpc_pack_type(desc, objid);
+	rpc_pack_type(desc, v);
+	rpc_pack_type(desc, sync);
+	rpc_end(desc, 0);
+
+};
+
+#endif
+
+/**
+ *
+ * Notifier related part
+ *
+ */
+
+static int kddm_notification(struct notifier_block *nb, hotplug_event_t event,
+			     void *data){
+	struct hotplug_context *ctx;
+	struct hotplug_node_set *node_set;
+
+	switch(event){
+	case HOTPLUG_NOTIFY_ADD:
+		ctx = data;
+		set_add(&ctx->node_set.v);
+		break;
+	case HOTPLUG_NOTIFY_REMOVE:
+		node_set = data;
+		set_remove(&node_set->v);
+		break;
+	case HOTPLUG_NOTIFY_FAIL:
+		node_set = data;
+//		set_failure(&node_set->v);
+		break;
+	default:
+		break;
+	}
+
+	return NOTIFY_OK;
+};
+
+int kddm_hotplug_init(void){
+	kddm_barrier = alloc_cluster_barrier(KDDM_HOTPLUG_BARRIER);
+	BUG_ON (IS_ERR(kddm_barrier));
+
+//	rpc_register(KDDM_COPYSET, handle_set_copyset, 0);
+//	rpc_register(KDDM_SELECT_OWNER, handle_select_owner, 0);
+
+	register_hotplug_notifier(kddm_notification, HOTPLUG_PRIO_KDDM);
+	return 0;
+};
+
+void kddm_hotplug_cleanup(void){
+};
diff -ruN linux-2.6.29/kddm/kddm_put_object.c android_cluster/linux-2.6.29/kddm/kddm_put_object.c
--- linux-2.6.29/kddm/kddm_put_object.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm_put_object.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,63 @@
+/** KDDM put object
+ *  @file kddm_put_object.c
+ *
+ *  Implementation of KDDM put object function.
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/module.h>
+
+#include <kddm/kddm.h>
+#include "protocol_action.h"
+
+
+/** Release an object which has been acquired by a get, grab or find.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set        KDDM set hosting the object.
+ *  @param obj_entry  Object entry of the object to put.
+ *  @param objid      Identifier of the object to put.
+ *
+ *  @return           Pointer to the object if it is present in memory,
+ *                    NULL otherwise.
+ */
+void _kddm_put_object(struct kddm_set *set,
+		      objid_t objid)
+{
+	struct kddm_obj *obj_entry;
+	int pending = 0;
+
+	obj_entry = __get_kddm_obj_entry(set, objid);
+	if (!obj_entry)
+		return;
+
+	/* The object is not frozen, nothing to do */
+	if (atomic_read(&obj_entry->frozen_count) == 0)
+		goto exit;
+
+	kddm_io_put_object(obj_entry, set, objid);
+	object_clear_frozen(obj_entry, set);
+	if (TEST_OBJECT_PENDING(obj_entry)) {
+		CLEAR_OBJECT_PENDING(obj_entry);
+		pending = 1;
+	}
+
+exit:
+	put_kddm_obj_entry(set, obj_entry, objid);
+	if (pending)
+		flush_kddm_event(set, objid);
+}
+EXPORT_SYMBOL(_kddm_put_object);
+
+
+
+void kddm_put_object(struct kddm_ns *ns, kddm_set_id_t set_id, objid_t objid)
+{
+	struct kddm_set *set;
+
+	set = _find_get_kddm_set (ns, set_id);
+	_kddm_put_object(set, objid);
+	put_kddm_set(set);
+}
+EXPORT_SYMBOL(kddm_put_object);
diff -ruN linux-2.6.29/kddm/kddm_remove_object.c android_cluster/linux-2.6.29/kddm/kddm_remove_object.c
--- linux-2.6.29/kddm/kddm_remove_object.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm_remove_object.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,167 @@
+/** KDDM remove object
+ *  @file kddm_remove_object.c
+ *
+ *  Implementation of KDDM remove object function.
+ *
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/module.h>
+
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+#include "protocol_action.h"
+
+
+static inline void wait_copies_remove_done(struct kddm_set *set,
+					   struct kddm_obj *obj_entry,
+					   objid_t objid)
+{
+	kddm_change_obj_state(set, obj_entry, objid,
+			      WAIT_OBJ_RM_ACK);
+sleep_again:
+	__sleep_on_kddm_obj(set, obj_entry, objid, 0);
+	/* We can be woken up by a put  */
+	if (!SET_IS_EMPTY(RMSET(obj_entry)))
+		goto sleep_again;
+}
+
+
+
+/** Remove an object cluster wide.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set        KDDM set hosting the object.
+ *  @param obj_entry  Object entry of the object to remove.
+ *  @param objid      Identifier of the object to remove.
+ *  @param frozen     Set to 1 if we remove a frozen object.
+ *
+ *  @return              0 if everything ok
+ *                       Negative value if error.
+ */
+int generic_kddm_remove_object(struct kddm_set *set,
+			       objid_t objid,
+			       int remove_frozen)
+{
+	struct kddm_obj *obj_entry;
+	int res = 0, need_wait;
+
+	inc_remove_object_counter(set);
+
+	obj_entry = __get_kddm_obj_entry(set, objid);
+	if (likely(obj_entry != NULL))
+		goto try_again;
+
+	if (I_AM_DEFAULT_OWNER(set, objid))
+		return 0;
+
+	BUG_ON(remove_frozen);
+
+	obj_entry = __get_alloc_kddm_obj_entry(set, objid);
+
+try_again:
+	switch (OBJ_STATE(obj_entry)) {
+	case WAIT_OBJ_RM_ACK:
+	case WAIT_OBJ_RM_ACK2:
+	case WAIT_OBJ_RM_DONE:
+		res = -EALREADY;
+		break;
+
+	case READ_COPY:
+		if (object_frozen(obj_entry, set)) {
+			__sleep_on_kddm_obj(set, obj_entry, objid, 0);
+			goto try_again;
+		}
+		/* Fall through */
+
+	case INV_COPY:
+		kddm_change_obj_state(set, obj_entry, objid, WAIT_OBJ_RM_DONE);
+
+		request_objects_remove_to_mgr(set, obj_entry, objid);
+		CLEAR_SET (RMSET(obj_entry));
+
+		/* Wait for remove ACK from object manager */
+		__sleep_on_kddm_obj(set, obj_entry, objid, 0);
+
+		destroy_kddm_obj_entry(set, obj_entry, objid, 1);
+		goto exit_no_unlock;
+
+	case WAIT_OBJ_READ:
+	case WAIT_ACK_WRITE:
+	case WAIT_OBJ_WRITE:
+	case WAIT_CHG_OWN_ACK:
+	case WAIT_ACK_INV:
+	case INV_FILLING:
+		__sleep_on_kddm_obj(set, obj_entry, objid, 0);
+		goto try_again;
+
+	case INV_OWNER:
+	case READ_OWNER:
+	case WRITE_OWNER:
+	case WRITE_GHOST:
+		if (remove_frozen &&
+		    (atomic_read(&obj_entry->frozen_count) == 1) ) {
+			object_clear_frozen(obj_entry, set);
+			remove_frozen = 0;
+		}
+		if (object_frozen(obj_entry, set)) {
+			__sleep_on_kddm_obj(set, obj_entry, objid, 0);
+			goto try_again;
+		}
+
+		need_wait = request_copies_remove(set, obj_entry, objid,
+						  kerrighed_node_id);
+		if (need_wait)
+			wait_copies_remove_done(set, obj_entry, objid);
+
+		destroy_kddm_obj_entry(set, obj_entry, objid, 1);
+		goto exit_no_unlock;
+
+	default:
+		STATE_MACHINE_ERROR(set->id, objid, obj_entry);
+		break;
+	}
+
+	put_kddm_obj_entry(set, obj_entry, objid);
+exit_no_unlock:
+
+	return res;
+}
+
+int _kddm_remove_object(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_remove_object(set, objid, 0);
+}
+EXPORT_SYMBOL(_kddm_remove_object);
+
+int kddm_remove_object(struct kddm_ns *ns, kddm_set_id_t set_id, objid_t objid)
+{
+	struct kddm_set *set;
+	int res;
+
+	set = _find_get_kddm_set (ns, set_id);
+	res = generic_kddm_remove_object(set, objid, 0);
+	put_kddm_set(set);
+
+	return res;
+}
+EXPORT_SYMBOL(kddm_remove_object);
+
+int _kddm_remove_frozen_object(struct kddm_set *set, objid_t objid)
+{
+	return generic_kddm_remove_object(set, objid, 1);
+}
+EXPORT_SYMBOL(_kddm_remove_frozen_object);
+
+int kddm_remove_frozen_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+			      objid_t objid)
+{
+	struct kddm_set *set;
+	int res;
+
+	set = _find_get_kddm_set (ns, set_id);
+	res = generic_kddm_remove_object(set, objid, 1);
+	put_kddm_set(set);
+
+	return res;
+}
+EXPORT_SYMBOL(kddm_remove_frozen_object);
diff -ruN linux-2.6.29/kddm/kddm_set.c android_cluster/linux-2.6.29/kddm/kddm_set.c
--- linux-2.6.29/kddm/kddm_set.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm_set.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,742 @@
+/** KDDM set interface.
+ *  @file kddm_set.c
+ *
+ *  Implementation of KDDM set manipulation functions.
+ *
+ *  Copyright (C) 2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/spinlock.h>
+#include <linux/wait.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+#include <linux/hashtable.h>
+#include <linux/unique_id.h>
+
+#include "process.h"
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+
+#include <kddm/kddm.h>
+#include <kddm/kddm_set.h>
+#include <kddm/name_space.h>
+#include <kddm/kddm_tree.h>
+#include "protocol_action.h"
+#include "procfs.h"
+
+struct kmem_cache *kddm_set_cachep;
+extern struct kmem_cache *kddm_tree_cachep;
+extern struct kmem_cache *kddm_tree_lvl_cachep;
+
+static struct lock_class_key obj_lock_key[NR_OBJ_ENTRY_LOCKS];
+
+
+/** Alloc a new KDDM set id.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ns     Name space to create the set id in.
+ *
+ *  @return   A newly allocated KDDM set id.
+ */
+static inline kddm_set_id_t alloc_new_kddm_set_id (struct kddm_ns *ns)
+{
+	return get_unique_id (&ns->kddm_set_unique_id_root);
+}
+
+
+
+/** Alloc a new KDDM set structure.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ns     Name space to create the set in.
+ *
+ *  @return   A newly allocated KDDM set structure.
+ */
+static inline struct kddm_set *alloc_kddm_set_struct (struct kddm_ns *ns,
+						      kddm_set_id_t set_id,
+						      int init_state)
+{
+	struct kddm_set *kddm_set;
+
+	kddm_set = kmem_cache_alloc (kddm_set_cachep, GFP_ATOMIC);
+	if (kddm_set == NULL) {
+		kddm_set = ERR_PTR(-ENOMEM);
+		goto err;
+	}
+
+	/* Make minimal initialisation */
+
+	memset (kddm_set, 0, sizeof(struct kddm_set));
+	kddm_set->state = init_state;
+	kddm_set->id = set_id;
+	kddm_set->ns = ns;
+	kddm_set->flags = 0;
+	init_waitqueue_head (&kddm_set->create_wq);
+	init_waitqueue_head (&kddm_set->frozen_wq);
+	spin_lock_init(&kddm_set->lock);
+	atomic_set(&kddm_set->count, 1);
+	INIT_LIST_HEAD(&kddm_set->event_list);
+	spin_lock_init(&kddm_set->event_lock);
+
+err:
+	return kddm_set;
+}
+
+
+
+/** Make full kddm set initialization
+ *  @author Renaud Lottiaux
+ */
+int init_kddm_set (struct kddm_set *set,
+		   kddm_set_id_t set_id,
+		   struct kddm_set_ops *set_ops,
+		   void *tree_init_data,
+		   unsigned long flags,
+		   kerrighed_node_t def_owner,
+		   int obj_size)
+{
+	int i, err = -ENOMEM;
+
+	set->ops = set_ops;
+
+	spin_lock_init(&set->table_lock);
+
+	for (i = 0; i < NR_OBJ_ENTRY_LOCKS; i++) {
+		spin_lock_init(&set->obj_lock[i]);
+		lockdep_set_class(&set->obj_lock[i], &obj_lock_key[i]);
+	}
+
+	set->id = set_id;
+	set->obj_size = obj_size;
+	set->flags |= flags;
+	set->def_owner = def_owner;
+	set->ra_window_size = DEFAULT_READAHEAD_WINDOW_SIZE;
+	set->state = KDDM_SET_LOCKED;
+	atomic_set (&set->nr_objects, 0);
+	atomic_set (&set->nr_masters, 0);
+	atomic_set (&set->nr_copies, 0);
+	atomic_set (&set->nr_entries, 0);
+	set->get_object_counter = 0;
+	set->grab_object_counter = 0;
+	set->remove_object_counter = 0;
+	set->flush_object_counter = 0;
+	set->private = NULL;
+
+	set->obj_set = set->ops->obj_set_alloc(set, tree_init_data);
+	if (!set->obj_set)
+		goto exit;
+
+	/* create /proc/kerrighed/kddm_set entry. */
+	set->procfs_entry = create_kddm_proc(set->id);
+
+	err = 0;
+exit:
+	return err;
+}
+
+
+static int __free_kddm_obj_entry(unsigned long index,
+				 void *data,
+				 void *priv_data)
+{
+	put_obj_entry_count((struct kddm_set *)priv_data,
+			      (struct kddm_obj *)data, index);
+
+	return 0;
+}
+
+/** Free a kddm set structure. */
+
+void free_kddm_set_struct(struct kddm_set * kddm_set)
+{
+	{   /// JUST FOR DEBUGGING: BEGIN
+		struct kddm_set *_kddm_set;
+		_kddm_set = _local_get_kddm_set(kddm_set->ns,
+						kddm_set->id);
+		BUG_ON (_kddm_set != NULL);
+	}   /// JUST FOR DEBUGGING: END
+
+	/*** Free object struct and objects content ***/
+
+	kddm_set->ops->obj_set_free(kddm_set->obj_set, __free_kddm_obj_entry,
+				    kddm_set);
+
+	/*** Get rid of the IO linker ***/
+
+	kddm_io_uninstantiate(kddm_set, 0);
+
+	if (kddm_set->procfs_entry)
+		remove_kddm_proc(kddm_set->procfs_entry);
+
+	/*** Finally, we are done... The kddm set is free ***/
+
+	kmem_cache_free(kddm_set_cachep, kddm_set);
+}
+
+
+
+void put_kddm_set(struct kddm_set *set)
+{
+	if (atomic_dec_and_test(&set->count))
+		free_kddm_set_struct(set);
+}
+EXPORT_SYMBOL(put_kddm_set);
+
+
+/** Find a KDDM set structure from its id.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ns            Name space to search the set in.
+ *  @param set_id        Identifier of the requested kddm set.
+ *  @param init_state    Initial state of the set.
+ *  @param flags         Identify extra actions to cary out on the look-up.
+ *
+ *  @return  Structure of the requested KDDM set.
+ *           NULL if the set does not exist.
+ */
+struct kddm_set *_generic_local_get_kddm_set(struct kddm_ns *ns,
+					     kddm_set_id_t set_id,
+					     int init_state,
+					     int flags)
+{
+	struct kddm_set *kddm_set;
+
+	if (!(flags & KDDM_LOCK_FREE))
+		down (&ns->table_sem);
+	kddm_set = __hashtable_find (ns->kddm_set_table, set_id);
+
+	if ( (kddm_set != NULL) && (flags & KDDM_CHECK_UNIQUE)) {
+		kddm_set = ERR_PTR(-EEXIST);
+		goto found;
+	}
+
+	if ( (kddm_set == NULL) && (flags & KDDM_ALLOC_STRUCT)) {
+		kddm_set = alloc_kddm_set_struct(ns, set_id, init_state);
+		__hashtable_add (ns->kddm_set_table, set_id, kddm_set);
+	}
+
+	if (likely(kddm_set != NULL))
+		atomic_inc(&kddm_set->count);
+
+found:
+	if (!(flags & KDDM_LOCK_FREE))
+		up (&ns->table_sem);
+
+	return kddm_set;
+}
+
+
+
+/** Find a KDDM set structure from its id.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ns_id         Name space id to search the set in.
+ *  @param set_id        Identifier of the requested kddm set.
+ *  @param flags         Identify extra actions to cary out on the look-up.
+ *
+ *  @return  Structure of the requested KDDM set.
+ *           NULL if the set does not exist.
+ */
+struct kddm_set *generic_local_get_kddm_set(int ns_id,
+					    kddm_set_id_t set_id,
+					    int init_state,
+					    int flags)
+{
+	struct kddm_ns *ns;
+	struct kddm_set *kddm_set;
+
+	ns = kddm_ns_get (ns_id);
+	if (ns == NULL)
+		return ERR_PTR(-EINVAL);
+	kddm_set = _generic_local_get_kddm_set(ns , set_id, init_state, flags);
+	kddm_ns_put (ns);
+
+	return kddm_set;
+}
+
+
+
+/** Try to find the given set on a remote node and create a local instance
+ *  @author Renaud Lottiaux
+ *
+ *  @param kddm_set   Struct of the kddm set to lookup.
+ *
+ *  @return  Structure of the requested kddm set or NULL if not found.
+ */
+int find_kddm_set_remotely(struct kddm_set *kddm_set)
+{
+	kerrighed_node_t kddm_set_mgr_node_id ;
+	kddm_id_msg_t kddm_id;
+	msg_kddm_set_t *msg;
+	int msg_size;
+	int err = -ENOMEM;
+	struct rpc_desc* desc;
+	struct kddm_set_ops *set_ops = NULL;
+	void *tree_init_data = NULL;
+	int free_init_data = 1;
+
+	kddm_set_mgr_node_id = KDDM_SET_MGR(kddm_set);
+
+	kddm_id.set_id = kddm_set->id;
+	kddm_id.ns_id = kddm_set->ns->id;
+
+	desc = rpc_begin(REQ_KDDM_SET_LOOKUP, kddm_set_mgr_node_id);
+	rpc_pack_type(desc, kddm_id);
+
+	msg_size = sizeof(msg_kddm_set_t) + MAX_PRIVATE_DATA_SIZE;
+
+	msg = kmalloc(msg_size, GFP_KERNEL);
+	if (msg == NULL)
+		OOM;
+
+	rpc_unpack(desc, 0, msg, msg_size);
+
+	if (msg->kddm_set_id != KDDM_SET_UNUSED) {
+		set_ops = krgsyms_import (msg->set_ops);
+	tree_init_data = set_ops->import(desc, &free_init_data);
+	}
+
+	rpc_end(desc, 0);
+
+	if (msg->kddm_set_id == KDDM_SET_UNUSED) {
+		err = -ENOENT;
+		goto check_err;
+	}
+
+	BUG_ON(msg->kddm_set_id != kddm_set->id);
+
+	err = init_kddm_set(kddm_set, kddm_set->id, set_ops, tree_init_data,
+			    msg->flags, msg->link, msg->obj_size);
+
+	if (tree_init_data && free_init_data)
+		kfree(tree_init_data);
+
+	if (err != 0)
+		goto check_err;
+
+	err = kddm_io_instantiate(kddm_set, msg->link, msg->linker_id,
+				  msg->private_data, msg->data_size, 0);
+
+check_err:
+	kfree(msg);
+
+	spin_lock(&kddm_set->lock);
+
+	if (err == 0)
+		kddm_set->state = KDDM_SET_READY;
+	else
+		kddm_set->state = KDDM_SET_INVALID;
+
+	wake_up(&kddm_set->create_wq);
+
+	spin_unlock(&kddm_set->lock);
+
+	return err;
+}
+
+
+
+/** Return a pointer to the given kddm_set. */
+
+struct kddm_set *__find_get_kddm_set(struct kddm_ns *ns,
+				     kddm_set_id_t set_id,
+				     int flags)
+{
+	struct kddm_set *kddm_set;
+
+	flags |= KDDM_ALLOC_STRUCT;
+	kddm_set = _generic_local_get_kddm_set(ns, set_id,
+					       KDDM_SET_NEED_LOOKUP, flags);
+	if (unlikely(IS_ERR(kddm_set)))
+		return kddm_set;
+
+	/* If the kddm set has been found INVALID earlier, we have to check if
+	 * it is still invalid... So, we force a new remote kddm set lookup.
+	 */
+	spin_lock(&kddm_set->lock);
+
+	if (kddm_set->state == KDDM_SET_INVALID)
+		kddm_set->state = KDDM_SET_NEED_LOOKUP;
+
+	goto check_no_lock;
+
+recheck_state:
+	spin_lock(&kddm_set->lock);
+
+check_no_lock:
+	/* If KDDM frozen, sleep until it is no more frozen */
+	if (!(flags & KDDM_LOCK_FREE) && kddm_frozen(kddm_set)) {
+		spin_unlock(&kddm_set->lock);
+		wait_event (kddm_set->frozen_wq, (!kddm_frozen(kddm_set)));
+		goto recheck_state;
+	}
+	/* Make sure we only use bypass_frozen when KDDM are frozen (i.e.
+	   hotplug cases) */
+	BUG_ON ((flags & KDDM_LOCK_FREE) && !kddm_frozen(kddm_set));
+
+	switch (kddm_set->state) {
+	  case KDDM_SET_READY:
+		  spin_unlock(&kddm_set->lock);
+		  break;
+
+	  case KDDM_SET_NEED_LOOKUP:
+		  /* The kddm set structure has just been allocated or
+		   * a remote lookup has been forced.
+		   */
+		  kddm_set->state = KDDM_SET_LOCKED;
+		  spin_unlock(&kddm_set->lock);
+		  find_kddm_set_remotely(kddm_set);
+		  goto recheck_state;
+
+	  case KDDM_SET_UNINITIALIZED:
+	  case KDDM_SET_INVALID:
+		  spin_unlock(&kddm_set->lock);
+		  kddm_set = NULL;
+		  break;
+
+	  case KDDM_SET_LOCKED:
+		  sleep_on_and_spin_unlock(&kddm_set->create_wq,
+					   &kddm_set->lock);
+		  goto recheck_state;
+
+	  default:
+		  BUG();
+	}
+
+	return kddm_set;
+}
+EXPORT_SYMBOL(_find_get_kddm_set);
+
+
+struct kddm_set *find_get_kddm_set(int ns_id,
+				   kddm_set_id_t set_id)
+{
+	struct kddm_ns *ns;
+	struct kddm_set *kddm_set;
+
+	ns = kddm_ns_get (ns_id);
+
+	kddm_set = _find_get_kddm_set(ns, set_id);
+
+	kddm_ns_put(ns);
+
+	return kddm_set;
+}
+EXPORT_SYMBOL(find_get_kddm_set);
+
+
+
+/** High level function to create a new kddm set.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ns             Name space to create a new set in.
+ *  @param set_id         Id of the kddm set to create. 0 -> auto attribution.
+ *  @param order          Order of the number of objects storable in the set.
+ *  @param linker_id      Id of the IO linker to link the kddm set with.
+ *  @param def_owner      Default owner node.
+ *  @param obj_size       Size of objects stored in the kddm set.
+ *  @param private_data   Data used by the instantiator.
+ *  @param data_size      Size of private data.
+ *  @param flags          Kddm set flags.
+ *
+ *  @return      A newly created kddm set if everything is ok.
+ *               Negative value otherwise
+ */
+struct kddm_set *__create_new_kddm_set(struct kddm_ns *ns,
+				       kddm_set_id_t set_id,
+				       struct kddm_set_ops *set_ops,
+				       void *tree_init_data,
+				       iolinker_id_t linker_id,
+				       kerrighed_node_t def_owner,
+				       int obj_size,
+				       void *private_data,
+				       unsigned long data_size,
+				       unsigned long flags)
+{
+	struct kddm_set *kddm_set;
+	int err = -EINVAL;
+
+	if (data_size > MAX_PRIVATE_DATA_SIZE)
+		goto error;
+
+	if (set_id == 0)
+		set_id = alloc_new_kddm_set_id(ns);
+
+	kddm_set = _local_get_alloc_unique_kddm_set(ns, set_id,
+						    KDDM_SET_UNINITIALIZED);
+	if (IS_ERR(kddm_set))
+		goto error;
+
+	err = init_kddm_set(kddm_set, set_id, set_ops, tree_init_data,
+			    flags, def_owner, obj_size);
+	if (err)
+		goto error;
+
+	err = kddm_io_instantiate(kddm_set, def_owner, linker_id,
+				  private_data, data_size, 1);
+	if (err)
+		goto error;
+
+	spin_lock(&kddm_set->lock);
+
+	kddm_set->state = KDDM_SET_READY;
+	wake_up(&kddm_set->create_wq);
+
+	spin_unlock(&kddm_set->lock);
+
+	put_kddm_set(kddm_set);
+
+	goto exit;
+
+error:
+	kddm_set = ERR_PTR(err);
+exit:
+	return kddm_set;
+}
+EXPORT_SYMBOL(__create_new_kddm_set);
+
+
+static void do_freeze_kddm_set(void *_set, void *_data)
+{
+	struct kddm_set *set = _set;
+
+	spin_lock(&set->lock);
+
+	BUG_ON (kddm_frozen(set));
+	set_kddm_frozen(set);
+	freeze_kddm_event(set);
+
+	spin_unlock(&set->lock);
+}
+
+static void do_unfreeze_kddm_set(void *_set, void *_data)
+{
+	struct kddm_set *set = _set;
+
+	spin_lock(&set->lock);
+
+	BUG_ON (!kddm_frozen(set));
+
+	unfreeze_kddm_event(set);
+	clear_kddm_frozen(set);
+
+	wake_up(&set->frozen_wq);
+
+	spin_unlock(&set->lock);
+}
+
+void freeze_kddm(void)
+{
+	down (&kddm_def_ns->table_sem);
+	__hashtable_foreach_data(kddm_def_ns->kddm_set_table,
+				 do_freeze_kddm_set, NULL);
+}
+
+void unfreeze_kddm(void)
+{
+	__hashtable_foreach_data(kddm_def_ns->kddm_set_table,
+				 do_unfreeze_kddm_set, NULL);
+	up (&kddm_def_ns->table_sem);
+}
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              REQUEST HANDLERS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+/** kddm set lookup handler.
+ *  @author Renaud Lottiaux
+ *
+ *  @param sender    Identifier of the remote requesting machine.
+ *  @param msg       Identifier of the kddm set to lookup for.
+ */
+int handle_req_kddm_set_lookup(struct rpc_desc* desc,
+			       void *_msg, size_t size)
+{
+	kddm_id_msg_t kddm_id = *((kddm_id_msg_t *) _msg);
+	struct kddm_set *kddm_set;
+	msg_kddm_set_t *msg;
+	int msg_size = sizeof(msg_kddm_set_t);
+
+	BUG_ON(!krgnode_online(rpc_desc_get_client(desc)));
+
+	kddm_set = local_get_kddm_set(kddm_id.ns_id, kddm_id.set_id);
+
+	if (kddm_set)
+		msg_size += kddm_set->private_data_size;
+
+	/* Prepare the kddm set creation message */
+
+	msg = kmalloc(msg_size, GFP_KERNEL);
+	if (msg == NULL)
+		OOM;
+
+	if (kddm_set == NULL || kddm_set->state != KDDM_SET_READY) {
+		msg->kddm_set_id = KDDM_SET_UNUSED;
+		goto done;
+	}
+
+	msg->kddm_set_id = kddm_id.set_id;
+	msg->linker_id = kddm_set->iolinker->linker_id;
+	msg->flags = kddm_set->flags;
+	msg->link = kddm_set->def_owner;
+	msg->obj_size = kddm_set->obj_size;
+	msg->data_size = kddm_set->private_data_size;
+	msg->set_ops = krgsyms_export (kddm_set->ops);
+	memcpy(msg->private_data, kddm_set->private_data, kddm_set->private_data_size);
+
+done:
+	rpc_pack(desc, 0, msg, msg_size);
+	if (msg->kddm_set_id != KDDM_SET_UNUSED)
+		kddm_set->ops->export(desc, kddm_set);
+
+	kfree(msg);
+
+	if (kddm_set)
+		put_kddm_set(kddm_set);
+
+	return 0;
+}
+
+
+
+/** kddm set destroy handler.
+ *  @author Renaud Lottiaux
+ *
+ *  @param sender    Identifier of the remote requesting machine.
+ *  @param msg       Identifier of the kddm set to destroy.
+ */
+static inline
+int __handle_req_kddm_set_destroy(kerrighed_node_t sender,
+				void *msg)
+{
+	kddm_id_msg_t kddm_id = *((kddm_id_msg_t *) msg);
+	struct kddm_ns *ns;
+	struct kddm_set *kddm_set;
+
+	BUG_ON(!krgnode_online(sender));
+
+	/* Remove the kddm set from the name space */
+
+	ns = kddm_ns_get (kddm_id.ns_id);
+	if (ns == NULL)
+		return -EINVAL;
+
+	down (&ns->table_sem);
+	kddm_set = __hashtable_remove(ns->kddm_set_table, kddm_id.set_id);
+	up (&ns->table_sem);
+
+	kddm_ns_put (ns);
+
+	if (kddm_set == NULL)
+		return -EINVAL;
+
+	/* Free the kddm set structure */
+
+	put_kddm_set(kddm_set);
+
+	return 0;
+}
+
+int handle_req_kddm_set_destroy(struct rpc_desc* desc,
+				void *msg, size_t size){
+	return __handle_req_kddm_set_destroy(rpc_desc_get_client(desc), msg);
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                INTERFACE FUNCTIONS FOR DISTRIBUTED ACTIONS                */
+/*                                                                           */
+/*****************************************************************************/
+
+
+/* High level function to destroy a kddm set. */
+
+int _destroy_kddm_set(struct kddm_set * kddm_set)
+{
+	kddm_id_msg_t kddm_id;
+
+	kddm_id.set_id = kddm_set->id;
+	kddm_id.ns_id = kddm_set->ns->id;
+
+	rpc_async_m(REQ_KDDM_SET_DESTROY, &krgnode_online_map,
+		    &kddm_id, sizeof(kddm_id_msg_t));
+	return 0;
+}
+EXPORT_SYMBOL(_destroy_kddm_set);
+
+int destroy_kddm_set(struct kddm_ns *ns, kddm_set_id_t set_id)
+{
+	struct kddm_set * kddm_set;
+	int r;
+
+	kddm_set = _find_get_kddm_set(ns, set_id);
+	if (kddm_set == NULL)
+		return -EINVAL;
+	r = _destroy_kddm_set(kddm_set);
+
+	put_kddm_set(kddm_set);
+	return r;
+}
+EXPORT_SYMBOL(destroy_kddm_set);
+
+/*****************************************************************************/
+/*                                                                           */
+/*                               INIT / FINALIZE                             */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+void __kddm_set_destroy(void *_kddm_set,
+			void *dummy)
+{
+	struct kddm_set *kddm_set = _kddm_set;
+	kddm_id_msg_t kddm_id;
+
+	kddm_id.ns_id = kddm_set->ns->id;
+	kddm_id.set_id = kddm_set->id;
+
+	handle_req_kddm_set_destroy(0, &kddm_id, sizeof(kddm_id));
+}
+
+
+
+/* KDDM set mecanisms initialisation.*/
+
+void kddm_set_init()
+{
+	struct rpc_synchro* kddm_server;
+
+	printk ("KDDM set init\n");
+
+	kddm_server = rpc_synchro_new(1, "kddm server", 0);
+
+	kddm_set_cachep = KMEM_CACHE(kddm_set, SLAB_PANIC);
+
+	kddm_tree_cachep = KMEM_CACHE(kddm_tree, SLAB_PANIC);
+
+	kddm_tree_lvl_cachep = KMEM_CACHE(kddm_tree_lvl, SLAB_PANIC);
+
+	__rpc_register(REQ_KDDM_SET_LOOKUP,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       kddm_server, handle_req_kddm_set_lookup, 0);
+
+	__rpc_register(REQ_KDDM_SET_DESTROY,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       kddm_server, handle_req_kddm_set_destroy, 0);
+
+	printk ("KDDM set init : done\n");
+}
+
+
+
+void kddm_set_finalize()
+{
+}
diff -ruN linux-2.6.29/kddm/kddm_set_object.c android_cluster/linux-2.6.29/kddm/kddm_set_object.c
--- linux-2.6.29/kddm/kddm_set_object.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm_set_object.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,91 @@
+/** KDDM set object
+ *  @file kddm_set_object.c
+ *
+ *  Implementation of KDDM set object function.
+ *
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/module.h>
+
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+#include "protocol_action.h"
+
+
+/** Set the initial value of an object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set        KDDM set hosting the object.
+ *  @param obj_entry  Object entry of the object to set.
+ *  @param objid      Identifier of the object to set.
+ *  @param object     Object to store in the kddm set entry.
+ *
+ *  This function assumes that a call to kddm_*_object_manual_ft has been done
+ *  before. A kddm_put_object must be done after.
+ *
+ *  @return        0 if everything OK, -1 otherwise.
+ */
+int _kddm_set_object_state(struct kddm_set *set,
+			    objid_t objid,
+			    void *object,
+			    kddm_obj_state_t state)
+{
+	struct kddm_obj *obj_entry;
+
+retry:
+	obj_entry = __get_kddm_obj_entry(set, objid);
+
+	BUG_ON(OBJ_STATE(obj_entry) != INV_OWNER);
+	BUG_ON(!object_frozen(obj_entry, set));
+
+	if (obj_entry->object != NULL) {
+		kddm_io_remove_object_and_unlock(obj_entry, set, objid);
+		printk ("Humf.... Can do really better !\n");
+		goto retry;
+	}
+
+	obj_entry->object = object;
+	atomic_inc(&set->nr_objects);
+	ADD_TO_SET (COPYSET(obj_entry), kerrighed_node_id);
+	kddm_insert_object (set, objid, obj_entry, state);
+	put_kddm_obj_entry(set, obj_entry, objid);
+
+	return 0;
+}
+
+
+
+int kddm_set_object_state(struct kddm_ns *ns, kddm_set_id_t set_id,
+			  objid_t objid, void *object, kddm_obj_state_t state)
+{
+	struct kddm_set *set;
+	int res;
+
+	set = _find_get_kddm_set (ns, set_id);
+	res = _kddm_set_object_state(set, objid, object, state);
+	put_kddm_set(set);
+
+	return res;
+}
+
+
+
+int _kddm_set_object(struct kddm_set *set, objid_t objid, void *object)
+{
+	return _kddm_set_object_state(set, objid, object, WRITE_OWNER);
+}
+EXPORT_SYMBOL(_kddm_set_object);
+
+int kddm_set_object(struct kddm_ns *ns, kddm_set_id_t set_id, objid_t objid,
+		     void *object)
+{
+	struct kddm_set *set;
+	int res;
+
+	set = _find_get_kddm_set (ns, set_id);
+	res = _kddm_set_object_state(set, objid, object, WRITE_OWNER);
+	put_kddm_set(set);
+
+	return res;
+}
+EXPORT_SYMBOL(kddm_set_object);
diff -ruN linux-2.6.29/kddm/kddm_sync_object.c android_cluster/linux-2.6.29/kddm/kddm_sync_object.c
--- linux-2.6.29/kddm/kddm_sync_object.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm_sync_object.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,78 @@
+/** KDDM sync object
+ *  @file kddm_sync_object.c
+ *
+ *  Implementation of KDDM sync object function.
+ *
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/module.h>
+
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+#include "protocol_action.h"
+
+
+/** Synchronize an object with its attached physical device.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set        KDDM set hosting the object.
+ *  @param obj_entry  Object entry of the object to sync.
+ *  @param objid      Identifier of the object to sync.
+ *  @param dest       Identifier of the node to send object to if needed.
+ *  @return           0 if everything OK, -1 otherwise.
+ */
+int _kddm_sync_frozen_object(struct kddm_set *set,
+			     objid_t objid)
+{
+	kddm_obj_state_t new_state = INV_COPY;
+	struct kddm_obj *obj_entry;
+	int res = -1;
+
+	BUG_ON(!kddm_ft_linked(set));
+
+	obj_entry = __get_kddm_obj_entry(set, objid);
+	if (obj_entry == NULL)
+		return -ENOENT;
+
+	BUG_ON(!object_frozen(obj_entry, set));
+
+	switch (OBJ_STATE(obj_entry)) {
+	case WRITE_OWNER:
+		new_state = READ_OWNER;
+		break;
+
+	case READ_OWNER:
+	case READ_COPY:
+		new_state = OBJ_STATE(obj_entry);
+		break;
+
+	default:
+		STATE_MACHINE_ERROR(set->id, objid, obj_entry);
+		break;
+	}
+
+	if (I_AM_DEFAULT_OWNER(set, objid)) {
+		put_kddm_obj_entry(set, obj_entry, objid);
+		res = kddm_io_sync_object(obj_entry, set, objid);
+	}
+	else
+		request_sync_object_and_unlock(set, obj_entry, objid,
+					       new_state);
+
+	return res;
+}
+EXPORT_SYMBOL(_kddm_sync_frozen_object);
+
+int kddm_sync_frozen_object(struct kddm_ns *ns, kddm_set_id_t set_id,
+			    objid_t objid)
+{
+	struct kddm_set *set;
+	int res;
+
+	set = _find_get_kddm_set (ns, set_id);
+	res = _kddm_sync_frozen_object(set, objid);
+	put_kddm_set(set);
+
+	return res;
+}
+EXPORT_SYMBOL(kddm_sync_frozen_object);
diff -ruN linux-2.6.29/kddm/kddm_tree.c android_cluster/linux-2.6.29/kddm/kddm_tree.c
--- linux-2.6.29/kddm/kddm_tree.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/kddm_tree.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,434 @@
+/** KDDM tree management.
+ *  @file kddm_tree.c
+ *
+ *  Copyright (C) 2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/mm.h>
+
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm_tree.h>
+#include <kddm/kddm_types.h>
+#include <kddm/object.h>
+
+struct kmem_cache *kddm_tree_cachep;
+struct kmem_cache *kddm_tree_lvl_cachep;
+
+int _2levels_kddm_tree = _2LEVELS_KDDM_TREE;
+int _nlevels_kddm_tree = _NLEVELS_KDDM_TREE;
+void *_2levels_kddm_tree_init_data = (void*)&_2levels_kddm_tree;
+void *_nlevels_kddm_tree_init_data = (void*)&_nlevels_kddm_tree;
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              HELPER FUNCTIONS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+static inline int lvl_bits(struct kddm_tree *tree, int level)
+{
+	if (level < tree->nr_level - 1 || !tree->bit_size_last)
+		return tree->bit_size;
+	return tree->bit_size_last;
+}
+
+static inline int lvl_shift(struct kddm_tree *tree, int level)
+{
+	return (tree->nr_level - 1 - level) * tree->bit_size;
+}
+
+static inline unsigned long
+lvl_sub_index(struct kddm_tree *tree, int level, unsigned long index)
+{
+	int bits = lvl_bits(tree, level);
+
+	return (index >> lvl_shift(tree, level)) & ((1UL << bits) - 1UL);
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                               CORE TREE CODE                              */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+/** Lookup for a data in a kddm tree.
+ *  @param tree      The tree to lookup in.
+ *  @param index     The index of the data to lookup.
+ *
+ *  @return The data if found.
+ *          NULL if the data is not found.
+ */
+static void *kddm_tree_lookup (struct kddm_tree *tree,
+			       unsigned long index)
+{
+	struct kddm_tree_lvl *cur_level;
+	int sub_index, i;
+
+	BUG_ON (tree == NULL);
+
+	if (tree->lvl1 == NULL)
+		return NULL;
+
+	cur_level = tree->lvl1;
+
+	for (i = 0; i < tree->nr_level; i++) {
+		sub_index = lvl_sub_index(tree, i, index);
+		cur_level = cur_level->sub_lvl[sub_index];
+		if (cur_level == NULL)
+			break;
+	}
+
+	return cur_level;
+}
+
+
+
+/** Lookup for a data in a kddm tree and allocated slots if needed.
+ *  @param tree      The tree to lookup in.
+ *  @param index     The index of the data to lookup.
+ *
+ *  @return The address of the slot hosting the data.
+ *          If the data does not exist, an empty slot is allocated.
+ */
+static void **kddm_tree_lookup_slot (struct kddm_tree *tree,
+				     unsigned long index,
+				     int flags)
+{
+	struct kddm_tree_lvl *cur_level, *prev_level;
+	int sub_index, nr_entries, i;
+	void *data;
+
+	BUG_ON (tree == NULL);
+
+	prev_level = NULL;
+	cur_level = tree->lvl1;
+	sub_index = 0;
+
+	for (i = 0; i < tree->nr_level; i++) {
+		if (cur_level == NULL) {
+			nr_entries = 1UL << lvl_bits(tree, i);
+
+			cur_level = kmem_cache_alloc (kddm_tree_lvl_cachep,
+						      GFP_ATOMIC);
+			cur_level->sub_lvl = kmalloc(
+				sizeof(struct kddm_tree_lvl *) * nr_entries,
+				GFP_ATOMIC);
+			cur_level->nr_obj = 0;
+
+			memset(cur_level->sub_lvl, 0,
+			       sizeof(struct kddm_tree_lvl *) * nr_entries);
+
+			if (i == 0)
+				tree->lvl1 = cur_level;
+			else {
+			        prev_level->sub_lvl[sub_index] = cur_level;
+				prev_level->nr_obj++;
+			}
+		}
+		sub_index = lvl_sub_index(tree, i, index);
+
+		prev_level = cur_level;
+		cur_level = cur_level->sub_lvl[sub_index];
+	}
+
+	data = prev_level->sub_lvl[sub_index];
+	if ((flags & KDDM_TREE_ADD_ENTRY) && (data == NULL))
+		prev_level->nr_obj++;
+
+	return (void **)&(prev_level->sub_lvl[sub_index]);
+}
+
+
+
+/** Lookup for a data in a kddm tree and allocated slots if needed.
+ *  @param tree      The tree to lookup in.
+ *  @param index     The index of the data to lookup.
+ *
+ *  @return The address of the slot hosting the data.
+ *          If the data does not exist, an empty slot is allocated.
+ */
+static void *__kddm_tree_remove (struct kddm_tree *tree,
+				 struct kddm_tree_lvl *cur_level,
+				 int level,
+				 unsigned long index,
+				 int *_sub_level_freed)
+{
+	struct kddm_tree_lvl *sub_level;
+	int sub_index, sub_level_freed = 0;
+	void *data;
+
+	sub_index = lvl_sub_index(tree, level, index);
+
+	if ((level + 1) == tree->nr_level) {
+		data = cur_level->sub_lvl[sub_index];
+		goto free_sub_level_slot;
+	}
+
+	sub_level = cur_level->sub_lvl[sub_index];
+	if (sub_level == NULL)
+		return NULL;
+
+	data = __kddm_tree_remove(tree, sub_level, level+1, index,
+				  &sub_level_freed);
+	if (sub_level_freed)
+		goto free_sub_level_slot;
+	return data;
+
+free_sub_level_slot:
+	cur_level->sub_lvl[sub_index] = NULL;
+	cur_level->nr_obj--;
+	if (cur_level->nr_obj == 0) {
+		kfree (cur_level->sub_lvl);
+		kmem_cache_free(kddm_tree_lvl_cachep, cur_level);
+		*_sub_level_freed = 1;
+	}
+	return data;
+}
+
+
+
+/** Remove a data from a kddm tree.
+ *  @param tree      The tree to lookup in.
+ *  @param index     The index of the data to remove.
+ */
+static void *kddm_tree_remove(struct kddm_tree *tree,
+			      unsigned long index)
+{
+	void *data;
+	int sub_level_freed = 0;
+
+	data = __kddm_tree_remove(tree, tree->lvl1, 0, index,
+				  &sub_level_freed);
+
+	if (sub_level_freed)
+		tree->lvl1 = NULL;
+
+	return data;
+}
+
+
+
+static void __kddm_tree_for_each_level(struct kddm_tree *tree,
+				       struct kddm_tree_lvl *cur_level,
+				       int level,
+				       int index,
+				       int free,
+				       int(*f)(unsigned long, void*, void*),
+				       void *priv)
+{
+	int i;
+	struct kddm_tree_lvl *sub_level;
+	unsigned long index_gap = 1UL << lvl_shift(tree, level);
+
+	for (i = 0; i < (1UL << lvl_bits(tree, level)); i++) {
+		sub_level = cur_level->sub_lvl[i];
+		if (sub_level != NULL) {
+			if ((level + 1) == tree->nr_level)
+				f(index, sub_level, priv);
+			else
+				__kddm_tree_for_each_level(tree, sub_level,
+							   level+1, index,
+							   free, f, priv);
+		}
+		index += index_gap ;
+	}
+	if (free) {
+		kfree (cur_level->sub_lvl);
+		kmem_cache_free(kddm_tree_lvl_cachep, cur_level);
+	}
+}
+
+static inline void __kddm_tree_for_each(struct kddm_tree *tree,
+					int free,
+					int(*f)(unsigned long, void*, void*),
+					void *priv)
+{
+	if (tree->lvl1 == NULL)
+		return;
+	__kddm_tree_for_each_level(tree, tree->lvl1, 0, 0, free, f, priv);
+}
+
+
+
+/** Executes a function for each data in a tree.
+ *  @param tree      The tree.
+ *  @param f         The function to execute for each data.
+ *  @param priv      Private data passed to the function.
+ */
+static void kddm_tree_for_each(struct kddm_tree *tree,
+			       int(*f)(unsigned long, void*, void*),
+			       void *priv)
+{
+	__kddm_tree_for_each(tree, 0, f, priv);
+}
+
+
+
+/** Alloc a KDDM tree.
+ *  @param tree_type   The tree type :)
+ *
+ *  @return   A newly allocated tree.
+ */
+static void *kddm_tree_alloc (struct kddm_set *set, void *data)
+{
+	struct kddm_tree *tree;
+	int width, bit_size;
+	int tree_type = *((int*)data);
+
+	tree = kmem_cache_alloc (kddm_tree_cachep, GFP_KERNEL);
+	if (tree == NULL)
+		return NULL;
+
+	tree->lvl1 = NULL;
+	tree->tree_type = tree_type;
+
+	switch (tree_type) {
+	case _2LEVELS_KDDM_TREE:
+		width = 20;
+		bit_size = 10;
+		break;
+
+	case _NLEVELS_KDDM_TREE:
+		width = BITS_PER_LONG;
+		bit_size = 8;
+		break;
+
+	default:
+		  printk ("Unknown KDDM tree type %d\n", tree_type);
+		  BUG();
+	}
+	tree->bit_width = width;
+	tree->max_data = (-1UL) >> (BITS_PER_LONG - width);
+	tree->bit_size = bit_size;
+	tree->nr_level = width / bit_size;
+	if (width % bit_size) {
+		tree->bit_size_last = width % bit_size;
+		tree->nr_level++;
+	} else
+		tree->bit_size_last = 0;
+
+	return tree;
+}
+
+
+
+/** Delete a KDDM tree.
+ *  @param tree_type   The tree to delete.
+ *  @param f           A function to call on each found data.
+ *  @param priv        Private data passed to the function.
+ */
+static void kddm_tree_free (void *tree,
+			    int (*f)(unsigned long, void *data, void *priv),
+			    void *priv)
+{
+	__kddm_tree_for_each(tree, 1, f, priv);
+
+	kmem_cache_free(kddm_tree_cachep, tree);
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                             KDDM SET OPERATIONS                           */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+static struct kddm_obj *kddm_tree_lookup_obj_entry (struct kddm_set *set,
+						    objid_t objid)
+{
+	struct kddm_obj *obj_entry;
+
+	spin_lock (&set->table_lock);
+	obj_entry = kddm_tree_lookup(set->obj_set, objid);
+	spin_unlock (&set->table_lock);
+
+	return obj_entry;
+}
+
+
+
+static struct kddm_obj *kddm_tree_get_obj_entry (struct kddm_set *set,
+						 objid_t objid,
+						 struct kddm_obj *new_obj)
+{
+	struct kddm_obj **obj_ptr, *obj_entry;
+
+	spin_lock (&set->table_lock);
+
+	obj_ptr = (struct kddm_obj **)kddm_tree_lookup_slot(set->obj_set,
+					    objid, KDDM_TREE_ADD_ENTRY);
+
+	if (*obj_ptr == NULL)
+		*obj_ptr = new_obj;
+
+	obj_entry = *obj_ptr;
+	spin_unlock (&set->table_lock);
+	return obj_entry;
+}
+
+
+
+static void kddm_tree_remove_obj_entry (struct kddm_set *set,
+					objid_t objid)
+{
+	spin_lock (&set->table_lock);
+	kddm_tree_remove (set->obj_set, objid);
+	spin_unlock (&set->table_lock);
+}
+
+
+
+static void kddm_tree_for_each_obj_entry(struct kddm_set *set,
+					 int(*f)(unsigned long, void *, void*),
+					 void *data)
+{
+	spin_lock (&set->table_lock);
+	kddm_tree_for_each(set->obj_set, f, data);
+	spin_unlock (&set->table_lock);
+}
+
+
+
+static void kddm_tree_export (struct rpc_desc* desc, struct kddm_set *set)
+{
+	struct kddm_tree *tree = set->obj_set;
+
+	rpc_pack_type(desc, tree->tree_type);
+}
+
+
+
+static void *kddm_tree_import (struct rpc_desc* desc, int *free_data)
+{
+	int *tree_type;
+
+	tree_type = kmalloc (sizeof (int), GFP_KERNEL);
+	*free_data = 1;
+
+	rpc_unpack(desc, 0, tree_type, sizeof (int));
+	return tree_type;
+}
+
+
+
+struct kddm_set_ops kddm_tree_set_ops = {
+	obj_set_alloc:       kddm_tree_alloc,
+	obj_set_free:        kddm_tree_free,
+	lookup_obj_entry:    kddm_tree_lookup_obj_entry,
+	get_obj_entry:       kddm_tree_get_obj_entry,
+	remove_obj_entry:    kddm_tree_remove_obj_entry,
+	for_each_obj_entry:  kddm_tree_for_each_obj_entry,
+	export:              kddm_tree_export,
+	import:              kddm_tree_import,
+};
diff -ruN linux-2.6.29/kddm/Makefile android_cluster/linux-2.6.29/kddm/Makefile
--- linux-2.6.29/kddm/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/Makefile	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,15 @@
+#
+# Makefile for the Kernel Distributed Data Management system
+#
+
+obj-$(CONFIG_KRG_KDDM) := kddm.o object.o procfs.o \
+	io_linker.o object_server.o protocol_action.o \
+	kddm_tree.o \
+	name_space.o kddm_set.o \
+	kddm_find_object.o kddm_put_object.o kddm_get_object.o \
+	kddm_grab_object.o kddm_set_object.o kddm_flush_object.o \
+	kddm_remove_object.o kddm_sync_object.o kddm_bench.o kddm_hotplug.o
+
+obj-$(CONFIG_KRG_EPM) += mobility.o
+
+EXTRA_CFLAGS += -Wall -Werror
diff -ruN linux-2.6.29/kddm/mobility.c android_cluster/linux-2.6.29/kddm/mobility.c
--- linux-2.6.29/kddm/mobility.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/mobility.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,132 @@
+/** Implementation of KDDM mobility mechanisms.
+ *  @file mobility.c
+ *
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ *
+ *  Implementation of functions used to migrate, duplicate and checkpoint
+ *  process KDDM related structures.
+ */
+
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/errno.h>
+#include <kddm/kddm_types.h>
+
+#include <kerrighed/ghost.h>
+#include <kerrighed/action.h>
+
+
+int initialize_kddm_info_struct (struct task_struct *task);
+extern struct kmem_cache *kddm_info_cachep;
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              EXPORT FUNCTIONS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+/** Export a KDDM info structure
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost    Ghost where data should be stored.
+ *  @param tsk      The task to ghost the KDDM info struct for.
+ *
+ *  @return  0 if everything was OK.
+ *           Negative value otherwise.
+ */
+int export_kddm_info_struct (struct epm_action *action,
+			     ghost_t *ghost,
+			     struct task_struct *tsk)
+{
+	int r = 0;
+
+	BUG_ON (tsk->kddm_info == NULL);
+
+	switch (action->type) {
+	  case EPM_REMOTE_CLONE:
+		  /* */
+		  break;
+
+	  case EPM_CHECKPOINT:
+	  case EPM_MIGRATE:
+		  r = ghost_write (ghost, tsk->kddm_info,
+				   sizeof(struct kddm_info_struct));
+		  break;
+
+	  default:
+		  break;
+	}
+
+	return r;
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              IMPORT FUNCTIONS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+int import_kddm_info_struct (struct epm_action *action,
+			     ghost_t *ghost,
+			     struct task_struct *tsk)
+{
+	struct kddm_info_struct *kddm_info;
+	int r;
+
+	switch (action->type) {
+	  case EPM_REMOTE_CLONE:
+		  r = initialize_kddm_info_struct (tsk);
+		  break;
+
+	  case EPM_CHECKPOINT:
+	  case EPM_MIGRATE:
+		  r = -ENOMEM;
+		  kddm_info = kmem_cache_alloc(kddm_info_cachep,
+					       GFP_KERNEL);
+
+		  if (!kddm_info)
+			break;
+
+		  r = ghost_read (ghost, kddm_info,
+				  sizeof(struct kddm_info_struct));
+		  if (r) {
+			kmem_cache_free(kddm_info_cachep, kddm_info);
+			break;
+		  }
+
+		  kddm_info->wait_obj = NULL;
+
+		  tsk->kddm_info = kddm_info;
+
+		  break;
+
+	  default:
+		  BUG();
+		  r = -EINVAL;
+	}
+
+	return r;
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                            UNIMPORT FUNCTIONS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+void unimport_kddm_info_struct (struct task_struct *tsk)
+{
+	kmem_cache_free (kddm_info_cachep, tsk->kddm_info);
+}
diff -ruN linux-2.6.29/kddm/name_space.c android_cluster/linux-2.6.29/kddm/name_space.c
--- linux-2.6.29/kddm/name_space.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/name_space.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,145 @@
+/** KDDM name space interface.
+ *  @file name_space.c
+ *
+ *  Implementation of KDDM name space manipulation functions.
+ *
+ *  Copyright (C) 2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/hashtable.h>
+#include <linux/module.h>
+
+#include <kddm/kddm.h>
+#include <kddm/name_space.h>
+
+struct kddm_ns *kddm_def_ns;
+EXPORT_SYMBOL(kddm_def_ns);
+
+struct radix_tree_root kddm_ns_tree;
+static DEFINE_RWLOCK(ns_tree_lock);
+struct kmem_cache *kddm_ns_cachep;
+
+
+
+static inline void free_kddm_ns_entry(struct kddm_ns *ns)
+{
+	{   /// JUST FOR DEBUGGING: BEGIN
+		struct kddm_ns *_ns;
+
+		read_lock_irq(&ns_tree_lock);
+		_ns = radix_tree_lookup(&kddm_ns_tree, ns->id);
+		read_unlock_irq(&ns_tree_lock);
+
+		BUG_ON (_ns != NULL);
+	}   /// JUST FOR DEBUGGING: END
+
+	hashtable_free(ns->kddm_set_table);
+	kmem_cache_free(kddm_ns_cachep, ns);
+}
+
+
+
+void kddm_ns_put(struct kddm_ns *ns)
+{
+	if (atomic_dec_and_test(&ns->count))
+		free_kddm_ns_entry(ns);
+}
+
+
+
+struct kddm_ns * create_kddm_ns(int ns_id,
+				void *private,
+				struct kddm_ns_ops *ops)
+
+{
+	struct kddm_ns *ns;
+	int error;
+
+	ns = kmem_cache_alloc (kddm_ns_cachep, GFP_KERNEL);
+	if (ns == NULL)
+		return NULL;
+
+	ns->private = private;
+	ns->ops = ops;
+	ns->id = ns_id;
+	init_MUTEX(&ns->table_sem);
+	ns->kddm_set_table = hashtable_new(KDDM_SET_HASH_TABLE_SIZE);
+	init_and_set_unique_id_root(&ns->kddm_set_unique_id_root, MIN_KDDM_ID);
+	atomic_set(&ns->count, 1);
+
+	error = radix_tree_preload(GFP_KERNEL);
+	if (likely(error == 0)) {
+		write_lock_irq(&ns_tree_lock);
+		error = radix_tree_insert(&kddm_ns_tree, ns_id, ns);
+		if (unlikely(error))
+			free_kddm_ns_entry(ns);
+
+		write_unlock_irq(&ns_tree_lock);
+		radix_tree_preload_end();
+	}
+
+	if (error)
+		ns = ERR_PTR(error);
+
+	return ns;
+}
+
+
+
+int remove_kddm_ns(int ns_id)
+{
+	struct kddm_ns *ns;
+
+	write_lock_irq(&ns_tree_lock);
+	ns = radix_tree_delete(&kddm_ns_tree, ns_id);
+	write_unlock_irq(&ns_tree_lock);
+
+	if (ns == NULL)
+		return -EINVAL;
+
+	kddm_ns_put (ns);
+
+	return 0;
+}
+
+
+
+struct kddm_ns *kddm_ns_get(int ns_id)
+{
+	struct kddm_ns *ns;
+
+	read_lock_irq(&ns_tree_lock);
+	ns = radix_tree_lookup(&kddm_ns_tree, ns_id);
+	if (ns)
+		atomic_inc(&ns->count);
+	read_unlock_irq(&ns_tree_lock);
+
+	return ns;
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                               INIT / FINALIZE                             */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+void kddm_ns_init(void)
+{
+	kddm_ns_cachep = KMEM_CACHE(kddm_ns, SLAB_PANIC);
+
+	INIT_RADIX_TREE(&kddm_ns_tree, GFP_ATOMIC);
+
+	kddm_def_ns = create_kddm_ns (KDDM_DEF_NS_ID, NULL, NULL);
+
+	BUG_ON(IS_ERR(kddm_def_ns));
+}
+
+
+
+void kddm_ns_finalize(void)
+{
+}
diff -ruN linux-2.6.29/kddm/object.c android_cluster/linux-2.6.29/kddm/object.c
--- linux-2.6.29/kddm/object.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/object.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,613 @@
+/** Management of KDDM objects.
+ *  @file object.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/proc_fs.h>
+#include <linux/swap.h>
+#include <linux/slab.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+
+#include <kddm/kddm.h>
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/object_server.h>
+#include <kddm/object.h>
+#include <kddm/io_linker.h>
+
+atomic_t nr_master_objects = ATOMIC_INIT(0);
+atomic_t nr_copy_objects = ATOMIC_INIT(0);
+atomic_t nr_OBJ_STATE[NB_OBJ_STATE];
+
+#define STATE_DEF(state) [OBJ_STATE_INDEX(state)] = #state
+
+const char *state_name[NB_OBJ_STATE] = {
+	STATE_DEF(INV_COPY),
+	STATE_DEF(READ_COPY),
+	STATE_DEF(INV_OWNER),
+	STATE_DEF(READ_OWNER),
+	STATE_DEF(WRITE_OWNER),
+	STATE_DEF(WRITE_GHOST),
+	STATE_DEF(WAIT_ACK_INV),
+	STATE_DEF(WAIT_ACK_WRITE),
+	STATE_DEF(WAIT_CHG_OWN_ACK),
+	STATE_DEF(WAIT_OBJ_READ),
+	STATE_DEF(WAIT_OBJ_WRITE),
+	STATE_DEF(WAIT_OBJ_RM_DONE),
+	STATE_DEF(WAIT_OBJ_RM_ACK),
+	STATE_DEF(WAIT_OBJ_RM_ACK2),
+	STATE_DEF(INV_FILLING),
+};
+EXPORT_SYMBOL(state_name);
+
+struct kmem_cache *kddm_obj_cachep;
+
+
+
+/** Init an object state.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry  Entry of the object to set the state.
+ *  @param state      State to set the object with.
+ */
+static inline void set_object_state(struct kddm_set * set,
+				    struct kddm_obj * obj_entry,
+				    kddm_obj_state_t state)
+{
+	INC_STATE_COUNTER(state);
+
+	obj_entry->flags = state |
+		(obj_entry->flags & ~OBJECT_STATE_MASK);
+
+	if (I_AM_OWNER(obj_entry)) {
+		atomic_inc (&nr_master_objects);
+		atomic_inc (&set->nr_masters);
+	} else {
+		atomic_inc (&nr_copy_objects);
+		atomic_inc (&set->nr_copies);
+	}
+}
+
+
+
+/** Modify an object dsm state.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry   Entry of the object to modify the dsm state.
+ *  @param new_state   New state to set the object with.
+ */
+static void change_object_state (struct kddm_set *set,
+				 struct kddm_obj * obj_entry,
+				 objid_t objid,
+				 kddm_obj_state_t new_state)
+{
+	DEC_STATE_COUNTER (OBJ_STATE(obj_entry));
+	INC_STATE_COUNTER (new_state);
+
+	if (I_AM_OWNER (obj_entry)) {
+		atomic_dec(&nr_master_objects);
+		atomic_dec(&set->nr_masters);
+	}
+	else {
+		atomic_dec(&nr_copy_objects);
+		atomic_dec(&set->nr_copies);
+	}
+
+	obj_entry->flags = new_state |
+		(obj_entry->flags & ~OBJECT_STATE_MASK);
+
+	if (I_AM_OWNER (obj_entry)) {
+		atomic_inc(&nr_master_objects);
+		atomic_inc(&set->nr_masters);
+	}
+	else {
+		atomic_inc(&nr_copy_objects);
+		atomic_inc(&set->nr_copies);
+	}
+
+	if (new_state & KDDM_OWNER_OBJ)
+		change_prob_owner(obj_entry, kerrighed_node_id);
+}
+
+
+
+/** Change an object state.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set        kddm set the object is hosted by.
+ *  @param obj_entry  Entry of the object to change the state.
+ *  @param objid      Id of the object to change the state.
+ *  @param state      State to set the object with.
+ */
+void kddm_change_obj_state(struct kddm_set * set,
+			   struct kddm_obj *obj_entry,
+			   objid_t objid,
+			   kddm_obj_state_t newState)
+{
+	if (!obj_entry)
+		return;
+
+	if (OBJ_STATE(obj_entry) != newState) {
+		change_object_state(set, obj_entry, objid, newState);
+
+		kddm_io_change_state(obj_entry, set, objid, newState);
+	}
+}
+
+
+
+/** Alloc and init a kddm object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set     Set to allocate the object in.
+ *  @param objid   Id of the object to allocate.
+ *
+ *  @return  The newly allocated object.
+ */
+struct kddm_obj *alloc_kddm_obj_entry(struct kddm_set *set,
+				      objid_t objid)
+{
+	struct kddm_obj * obj_entry ;
+
+	obj_entry = kmem_cache_alloc(kddm_obj_cachep, GFP_KERNEL);
+	if (obj_entry == NULL) {
+		OOM;
+		return NULL;
+	}
+
+	obj_entry->flags = 0;
+	obj_entry->object = NULL;
+	atomic_set(&obj_entry->count, 1);
+
+	BUG_ON(set->def_owner < 0 ||
+	       set->def_owner > KDDM_MAX_DEF_OWNER);
+
+	change_prob_owner(obj_entry, kddm_io_default_owner(set, objid));
+
+	if (get_prob_owner(obj_entry) == kerrighed_node_id)
+		set_object_state(set, obj_entry, INV_OWNER);
+	else
+		set_object_state(set, obj_entry, INV_COPY);
+
+	atomic_set(&obj_entry->frozen_count, 0);
+	atomic_set(&obj_entry->sleeper_count, 0);
+
+	atomic_inc (&set->nr_entries);
+
+	CLEAR_SET(COPYSET(obj_entry));
+	CLEAR_SET(RMSET(obj_entry));
+
+	init_waitqueue_head(&obj_entry->waiting_tsk);
+
+	return obj_entry;
+}
+
+struct kddm_obj *dup_kddm_obj_entry(struct kddm_obj *src_obj)
+{
+	struct kddm_obj * obj_entry;
+
+	BUG_ON(atomic_read(&src_obj->frozen_count) != 0);
+
+	obj_entry = kmem_cache_alloc(kddm_obj_cachep, GFP_ATOMIC);
+	if (obj_entry == NULL) {
+		OOM;
+		return NULL;
+	}
+
+	*obj_entry = *src_obj;
+
+	atomic_set(&obj_entry->count, 1);
+	CLEAR_OBJECT_PINNED(obj_entry);
+	atomic_set(&obj_entry->sleeper_count, 0);
+	init_waitqueue_head(&obj_entry->waiting_tsk);
+
+	return obj_entry;
+}
+
+/** Remove a local object frame from a kddm set
+ */
+void free_kddm_obj_entry(struct kddm_set *set,
+			 struct kddm_obj *obj_entry,
+			 objid_t objid)
+{
+	BUG_ON(atomic_read(&obj_entry->frozen_count) != 0);
+	BUG_ON(obj_entry_count(obj_entry) != 0);
+	BUG_ON(TEST_OBJECT_LOCKED(obj_entry));
+
+	/* Ask the IO linker to remove the object */
+	if (obj_entry->object != NULL)
+		kddm_io_remove_object(obj_entry->object, set, objid);
+
+	atomic_dec(&set->nr_entries);
+
+	kmem_cache_free(kddm_obj_cachep, obj_entry);
+}
+
+
+
+/*** Remove an object entry from a kddm set object table. ***/
+
+int destroy_kddm_obj_entry (struct kddm_set *set,
+			    struct kddm_obj *obj_entry,
+			    objid_t objid,
+			    int cluster_wide_remove)
+{
+	kerrighed_node_t default_owner = kddm_io_default_owner(set, objid);
+	BUG_ON (object_frozen(obj_entry, set));
+
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	/* Check if we are in a flush case i.e. cluster_wide_remove == 0
+	 * or if we have a pending request on the object. In both cases, can
+	 * cannot remove the object entry.
+	 */
+	if ((!cluster_wide_remove) ||
+	    atomic_read (&obj_entry->sleeper_count)) {
+
+		if (cluster_wide_remove && (default_owner == kerrighed_node_id))
+			kddm_change_obj_state(set, obj_entry, objid, INV_OWNER);
+		else {
+			kddm_change_obj_state(set, obj_entry, objid, INV_COPY);
+			if (cluster_wide_remove)
+				change_prob_owner(obj_entry, default_owner);
+		}
+
+		wake_up (&obj_entry->waiting_tsk);
+		kddm_io_remove_object_and_unlock (obj_entry, set, objid);
+		goto exit;
+	}
+
+	if (I_AM_OWNER(obj_entry)) {
+		atomic_dec(&nr_master_objects);
+		atomic_dec(&set->nr_masters);
+	} else {
+		atomic_dec(&nr_copy_objects);
+		atomic_dec(&set->nr_copies);
+	}
+
+	set->ops->remove_obj_entry(set, objid);
+
+	put_kddm_obj_entry(set, obj_entry, objid);
+
+	put_obj_entry_count(set, obj_entry, objid);
+exit:
+	return 0;
+}
+
+
+
+/*** Get an object entry from a kddm set. ***/
+
+struct kddm_obj *__get_kddm_obj_entry (struct kddm_set *set,
+				       objid_t objid)
+{
+	struct kddm_obj *obj_entry;
+
+retry:
+	kddm_obj_path_lock(set, objid);
+
+	obj_entry = set->ops->lookup_obj_entry(set, objid);
+	if (obj_entry) {
+		if (TEST_AND_SET_OBJECT_LOCKED (obj_entry)) {
+			kddm_obj_path_unlock (set, objid);
+			while (TEST_OBJECT_LOCKED (obj_entry))
+				cpu_relax();
+			goto retry;
+		}
+	}
+	else
+		kddm_obj_path_unlock(set, objid);
+
+	return obj_entry;
+}
+
+
+
+/*** Get or alloc an object entry from a kddm set. ***/
+
+struct kddm_obj *__get_alloc_kddm_obj_entry (struct kddm_set *set,
+					     objid_t objid)
+{
+	struct kddm_obj *obj_entry, *new_obj;
+
+	/* Since we cannot allocate in a lock section, we need to
+	 * pre-allocate an obj_entry and free it after the lock section if an
+	 * object was already present in the table. Can do better with a cache
+	 * of new objects (see radix tree code for instance).
+	 */
+retry:
+	new_obj = alloc_kddm_obj_entry(set, objid);
+
+	kddm_obj_path_lock(set, objid);
+
+	obj_entry = set->ops->get_obj_entry(set, objid, new_obj);
+	if (obj_entry != new_obj)
+		put_obj_entry_count(set, new_obj, objid);
+
+	if (TEST_AND_SET_OBJECT_LOCKED (obj_entry)) {
+		kddm_obj_path_unlock(set, objid);
+		while (TEST_OBJECT_LOCKED (obj_entry))
+			cpu_relax();
+		goto retry;
+	}
+
+	return obj_entry;
+}
+
+
+
+/*** Insert a new object in a kddm set ***/
+
+void kddm_insert_object(struct kddm_set * set,
+			objid_t objid,
+			struct kddm_obj * obj_entry,
+			kddm_obj_state_t objectState)
+{
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	put_kddm_obj_entry(set, obj_entry, objid);
+
+	if (set->ops->insert_object)
+		set->ops->insert_object (set, objid, obj_entry);
+
+	kddm_io_insert_object(obj_entry, set, objid);
+
+	kddm_obj_path_lock(set, objid);
+
+	kddm_change_obj_state(set, obj_entry, objid, objectState);
+
+	if (objectState & KDDM_OWNER_OBJ) {
+		CLEAR_SET(COPYSET(obj_entry));
+		ADD_TO_SET(COPYSET(obj_entry), kerrighed_node_id);
+		ADD_TO_SET(RMSET(obj_entry), kerrighed_node_id);
+	}
+	if (OBJ_STATE(obj_entry) != WAIT_ACK_INV)
+		wake_up_on_wait_object(obj_entry, set);
+}
+
+
+
+/*** Invalidate a local kddm object ***/
+
+void kddm_invalidate_local_object_and_unlock(struct kddm_obj * obj_entry,
+					     struct kddm_set * set,
+					     objid_t objid,
+					     kddm_obj_state_t state)
+{
+	BUG_ON(obj_entry->object == NULL);
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	obj_entry = kddm_break_cow_object (set, obj_entry,objid,
+					   KDDM_BREAK_COW_INV);
+
+	if (!obj_entry)
+		goto done;
+
+	/* Inform interface linkers to invalidate the object */
+	kddm_change_obj_state(set, obj_entry, objid, INV_COPY);
+	if (state != INV_COPY)
+		kddm_change_obj_state(set, obj_entry, objid, state);
+
+	CLEAR_SET(COPYSET(obj_entry));
+
+	/* Ask the IO linker to invalidate the object */
+	kddm_io_invalidate_object(obj_entry, set, objid);
+
+done:
+	put_kddm_obj_entry(set, obj_entry, objid);
+}
+
+/* Unlock, and make a process sleep until the corresponding
+ * object is received.
+ */
+void __sleep_on_kddm_obj(struct kddm_set * set,
+			 struct kddm_obj * obj_entry,
+			 objid_t objid,
+			 int flags)
+{
+	struct kddm_info_struct *kddm_info = current->kddm_info;
+	wait_queue_t wait;
+#ifdef CONFIG_KRG_EPM
+	struct task_struct *krg_cur = krg_current;
+#endif
+
+#ifdef CONFIG_KRG_EPM
+	if (krg_cur)
+		krg_current = NULL;
+#endif
+
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	/* Increase sleeper count and enqueue the task in the obj wait queue */
+	atomic_inc(&obj_entry->sleeper_count);
+	CLEAR_OBJECT_PINNED(obj_entry);
+
+	init_waitqueue_entry(&wait, current);
+
+	add_wait_queue(&obj_entry->waiting_tsk, &wait);
+
+	set_current_state(TASK_UNINTERRUPTIBLE);
+
+	put_kddm_obj_entry(set, obj_entry, objid);
+
+	if (kddm_info) {
+		kddm_info->wait_obj = obj_entry;
+		kddm_info->ns_id = set->ns->id;
+		kddm_info->set_id = set->id;
+		kddm_info->obj_id = objid;
+	}
+
+	schedule();
+
+retry:
+	kddm_obj_path_lock(set, objid);
+	if (TEST_AND_SET_OBJECT_LOCKED (obj_entry)) {
+		kddm_obj_path_unlock (set, objid);
+		while (TEST_OBJECT_LOCKED (obj_entry))
+			cpu_relax();
+		goto retry;
+	}
+
+	if( (TEST_FAILURE_FLAG(obj_entry)) &&
+	    !(flags & KDDM_DONT_KILL) ){
+		printk("sleep_on_object_and...:Should kill current: %d %s "
+		       "(%ld:%ld)\n", current->pid, current->comm,
+		       set->id, objid);
+		do_exit(SIGSEGV);
+		BUG();
+	};
+
+	if (kddm_info)
+		kddm_info->wait_obj = NULL;
+
+	remove_wait_queue(&obj_entry->waiting_tsk, &wait);
+
+	/* If all tasks waiting for the object have been woken-up we can
+	   release the object */
+	if (atomic_dec_and_test(&obj_entry->sleeper_count))
+		CLEAR_OBJECT_PINNED(obj_entry);
+
+#ifdef CONFIG_KRG_EPM
+	if (krg_cur)
+		krg_current = krg_cur;
+#endif
+}
+
+
+
+/* Check if we need to sleep on a local exclusive set.
+ */
+int check_sleep_on_local_exclusive (struct kddm_set * set,
+				    struct kddm_obj * obj_entry,
+				    objid_t objid,
+				    int flags)
+{
+	int res = 0;
+
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	if (object_frozen(obj_entry, set) &&
+	    (kddm_local_exclusive(set))) {
+		sleep_on_kddm_obj(set, obj_entry, objid, flags);
+		res = 1;
+	}
+	return res;
+}
+
+
+
+/** Indicate if an object is frozen, ie if it should not be modified.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry  Entry of the object to test.
+ */
+int object_frozen(struct kddm_obj * obj_entry,
+		  struct kddm_set * set)
+{
+	return (atomic_read(&obj_entry->frozen_count) != 0);
+}
+
+
+
+/** Indicate if an object is frozen, ie if it should not be modified.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry  Entry of the object to test.
+ */
+int object_frozen_or_pinned(struct kddm_obj * obj_entry,
+			    struct kddm_set * set)
+{
+	return ((atomic_read(&obj_entry->frozen_count) != 0) ||
+		TEST_OBJECT_PINNED(obj_entry));
+}
+
+
+
+/** Freeze the given object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry  Entry of the object to freeze.
+ */
+void set_object_frozen(struct kddm_obj * obj_entry,
+		       struct kddm_set * set)
+{
+	atomic_inc(&obj_entry->frozen_count);
+}
+
+
+
+/** Object clear Frozen.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry  Entry of the object to warm.
+ */
+void object_clear_frozen(struct kddm_obj * obj_entry,
+			 struct kddm_set * set)
+{
+	atomic_dec(&obj_entry->frozen_count);
+
+	BUG_ON(atomic_read(&obj_entry->frozen_count) < 0);
+
+	wake_up_on_wait_object(obj_entry, set);
+}
+
+
+
+void __for_each_kddm_object(struct kddm_set *set,
+			    int(*f)(unsigned long, void *, void*),
+			    void *data)
+{
+	int i;
+
+	for (i = 0; i < NR_OBJ_ENTRY_LOCKS; i++)
+		spin_lock(&(set->obj_lock[i]));
+
+	set->ops->for_each_obj_entry(set, f, data);
+
+	for (i = 0; i < NR_OBJ_ENTRY_LOCKS; i++)
+		spin_unlock(&(set->obj_lock[i]));
+}
+
+
+
+
+void for_each_kddm_object(int ns_id,
+			  kddm_set_id_t set_id,
+			  int(*f)(unsigned long, void*, void*),
+			  void *data)
+{
+	struct kddm_set *set;
+
+	set = find_get_kddm_set (ns_id, set_id);
+	if(!set)
+		return;
+
+	__for_each_kddm_object(set, f, data);
+
+	put_kddm_set(set);
+}
+EXPORT_SYMBOL(for_each_kddm_object);
+
+
+int init_kddm_objects (void)
+{
+	unsigned long cache_flags = SLAB_PANIC;
+	int i ;
+
+#ifdef CONFIG_DEBUG_SLAB
+	cache_flags |= SLAB_POISON;
+#endif
+	kddm_obj_cachep = kmem_cache_create("kddm_obj", sizeof(struct kddm_obj),
+					    16, cache_flags, NULL);
+
+	for (i = 0; i < NB_OBJ_STATE; i++) {
+		atomic_set(&nr_OBJ_STATE[i], 0);
+	}
+
+	return 0;
+}
diff -ruN linux-2.6.29/kddm/object_server.c android_cluster/linux-2.6.29/kddm/object_server.c
--- linux-2.6.29/kddm/object_server.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/object_server.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,1307 @@
+/** Object Server.
+ *  @file object_server.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/kernel.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+#include "protocol_action.h"
+
+
+/** Forward a message to the supposed correct prob Owner.
+ *  @author Renaud Lottiaux
+ */
+static inline void forward_object_server_msg (struct kddm_obj * obj_entry,
+					      struct kddm_set *set,
+					      enum rpcid msg_type,
+					      void *_msg)
+{
+	msg_server_t *msg = (msg_server_t *)_msg;
+	kerrighed_node_t prob_owner;
+
+	if (obj_entry == NULL)
+		prob_owner = kddm_io_default_owner(set, msg->objid);
+	else
+		prob_owner = get_prob_owner(obj_entry);
+
+	BUG_ON(prob_owner == kerrighed_node_id);
+
+	msg->req_id = 0;
+	rpc_async(msg_type, prob_owner, _msg, sizeof(msg_server_t));
+}
+
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              REQUEST HANDLERS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+/** Handle an invalidation ack receive.
+ *  @author Renaud Lottiaux
+ *
+ *  @param msg  Message received from the requesting node.
+ */
+static inline int __handle_invalidation_ack (kerrighed_node_t sender,
+					     void *_msg)
+{
+	msg_server_t *msg = _msg;
+	struct kddm_obj *obj_entry;
+	struct kddm_set *set;
+	kerrighed_node_t dest;
+
+	BUG_ON (sender < 0 || sender > KERRIGHED_MAX_NODES);
+
+	obj_entry = get_kddm_obj_entry (msg->ns_id, msg->set_id, msg->objid,
+					&set);
+
+	/* Managing this message on a frozen object could lead to some bad
+	 * behaviors. For instance, sending a write copy since we actually
+	 * cannot do it because of the frozen state.
+	 */
+	if (object_frozen_or_pinned (obj_entry, set)) {
+		queue_event (__handle_invalidation_ack, sender, set, obj_entry,
+			     msg->objid, msg, sizeof (msg_server_t));
+
+		goto exit;
+	}
+
+	switch (OBJ_STATE(obj_entry)) {
+	  case INV_FILLING:
+		  if (get_prob_owner(obj_entry) == kerrighed_node_id)
+			  goto handle_ack;
+		  /* else fall through */
+	  case INV_COPY:
+	  case READ_COPY:
+	  case WAIT_OBJ_READ:
+		  forward_object_server_msg (obj_entry, set, INVALIDATION_ACK,
+					     msg);
+		  break;
+
+	  case WAIT_OBJ_WRITE:
+		  ADD_TO_SET (COPYSET(obj_entry), msg->reply_node);
+		  break;
+
+	  case READ_OWNER:
+		  REMOVE_FROM_SET (COPYSET(obj_entry), msg->reply_node);
+
+		  /* If the local object is the last one in the cluster
+		     and it is not used locally, we just remote it ! */
+		  break;
+
+	  case WAIT_CHG_OWN_ACK:
+		  REMOVE_FROM_SET (COPYSET(obj_entry), msg->reply_node);
+
+		  BUG_ON(SET_IS_EMPTY (COPYSET(obj_entry)));
+
+		  if (OBJ_EXCLUSIVE (obj_entry)) {
+			  dest = choose_injection_node ();
+
+			  BUG_ON (dest == 1);
+
+			  obj_entry = send_copy_on_write_and_inv (
+				  set, obj_entry, msg->objid, dest,
+				  KDDM_IO_FLUSH);
+
+			  /* Wake up the set_flush function */
+
+			  wake_up_on_wait_object (obj_entry, set);
+
+			  goto exit_no_unlock;
+		  }
+		  else {
+			  dest = choose_injection_node_in_copyset (obj_entry);
+
+			  BUG_ON (dest == 1);
+
+			  send_change_ownership_req (set, obj_entry,
+						     msg->objid, dest,
+						     &obj_entry->master_obj);
+		  }
+		  break;
+
+	  case WAIT_ACK_INV:
+handle_ack:
+		  if (!NODE_IN_SET (COPYSET(obj_entry), sender))
+			  printk ("Problem with object (%ld;%ld)\n",
+				  set->id, msg->objid);
+		  BUG_ON(!NODE_IN_SET (COPYSET(obj_entry), sender));
+		  REMOVE_FROM_SET (COPYSET(obj_entry), sender);
+		  BUG_ON(SET_IS_EMPTY (COPYSET(obj_entry)));
+
+		  if (OBJ_EXCLUSIVE (obj_entry) &&
+		      (OBJ_STATE(obj_entry) != INV_FILLING)) {
+			  kddm_insert_object (set, msg->objid, obj_entry,
+					      WRITE_OWNER);
+		  }
+
+		  break;
+
+	  case WAIT_ACK_WRITE:
+		  BUG_ON(!NODE_IN_SET (COPYSET(obj_entry), sender));
+		  REMOVE_FROM_SET (COPYSET(obj_entry), sender);
+		  BUG_ON(SET_IS_EMPTY (COPYSET(obj_entry)));
+
+		  if (OBJ_EXCLUSIVE (obj_entry)) {
+			  kddm_change_obj_state (set, obj_entry, msg->objid,
+						 WRITE_OWNER);
+			  wake_up_on_wait_object (obj_entry, set);
+		  }
+		  break;
+
+	  case WAIT_OBJ_RM_ACK:
+		  /* Local remove is in competition with a remote flush.
+		   * Just ignore this message and continue to wait for the
+		   * remove ACK.
+		   */
+		  break;
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  break;
+	}
+
+exit:
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+
+exit_no_unlock:
+	return 0;
+}
+
+void handle_invalidation_ack (struct rpc_desc* desc,
+			     void *_msg, size_t size){
+	__handle_invalidation_ack(desc->client, _msg);
+}
+
+
+
+/** Handle a remove ack receive.
+ *  @author Renaud Lottiaux
+ *
+ *  @param msg  Message received from the requesting node.
+ */
+void handle_remove_ack (struct rpc_desc* desc,
+		       void *_msg, size_t size)
+{
+	msg_server_t *msg = _msg;
+	struct kddm_obj *obj_entry;
+	struct kddm_set *set;
+
+	BUG_ON (desc->client < 0 || desc->client > KERRIGHED_MAX_NODES);
+
+	obj_entry = get_kddm_obj_entry (msg->ns_id, msg->set_id, msg->objid,
+					&set);
+
+	switch (OBJ_STATE(obj_entry)) {
+	  case WAIT_OBJ_RM_DONE:
+		  ADD_TO_SET (RMSET(obj_entry), desc->client);
+		  break;
+
+	  case WAIT_OBJ_RM_ACK:
+		  BUG_ON(!NODE_IN_SET (RMSET(obj_entry), desc->client));
+
+		  REMOVE_FROM_SET (COPYSET(obj_entry), desc->client);
+		  REMOVE_FROM_SET (RMSET(obj_entry), desc->client);
+
+		  if (msg->flags & KDDM_NEED_OBJ_RM_ACK2)
+			  SET_OBJECT_RM_ACK2(obj_entry);
+
+		  if (SET_IS_EMPTY (RMSET(obj_entry))) {
+			  BUG_ON (!SET_IS_EMPTY (COPYSET(obj_entry)));
+			  if (TEST_OBJECT_RM_ACK2(obj_entry)) {
+				  send_remove_ack2 (set, msg->objid,
+						    kddm_io_default_owner(set,
+									  msg->objid));
+				  CLEAR_OBJECT_RM_ACK2(obj_entry);
+			  }
+			  wake_up_on_wait_object (obj_entry, set);
+		  }
+		  break;
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  break;
+	}
+
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+
+	return;
+}
+
+
+/** Handle a remove ack .
+ *  @author Renaud Lottiaux
+ *
+ *  @param msg  Message received from the requesting node.
+ */
+void handle_remove_ack2 (struct rpc_desc* desc,
+		       void *_msg, size_t size)
+{
+	msg_server_t *msg = _msg;
+	struct kddm_obj *obj_entry;
+	struct kddm_set *set;
+
+	BUG_ON (desc->client < 0 || desc->client > KERRIGHED_MAX_NODES);
+
+	obj_entry = get_kddm_obj_entry (msg->ns_id, msg->set_id, msg->objid,
+					&set);
+
+	switch (OBJ_STATE(obj_entry)) {
+	  case WAIT_OBJ_WRITE:
+	  case WAIT_OBJ_READ:
+		  kddm_change_obj_state (set, obj_entry, msg->objid,
+					 INV_OWNER);
+		  wake_up_on_wait_object (obj_entry, set);
+
+		  /* Fall through */
+	  case INV_OWNER:
+		  break;
+
+	  case WAIT_OBJ_RM_ACK2:
+		  destroy_kddm_obj_entry(set, obj_entry, msg->objid, 1);
+		  goto exit_no_unlock;
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  break;
+	}
+
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+exit_no_unlock:
+	return;
+}
+
+
+
+/** Handle a global remove ack receive.
+ *  @author Renaud Lottiaux
+ *
+ *  @param msg  Message received from the requesting node.
+ */
+void handle_remove_done (struct rpc_desc* desc,
+			void *_msg, size_t size)
+{
+	rm_done_msg_server_t *msg = _msg;
+	struct kddm_obj *obj_entry;
+	struct kddm_set *set;
+
+	BUG_ON (desc->client < 0 || desc->client > KERRIGHED_MAX_NODES);
+
+	obj_entry = get_kddm_obj_entry (msg->ns_id, msg->set_id, msg->objid,
+					&set);
+
+	switch (OBJ_STATE(obj_entry)) {
+	  case WAIT_OBJ_RM_DONE:
+		  merge_ack_set(RMSET(obj_entry), &msg->rmset);
+		  if (SET_IS_EMPTY(RMSET(obj_entry)))
+			  wake_up_on_wait_object (obj_entry, set);
+		  else
+			  kddm_change_obj_state(set, obj_entry, msg->objid,
+						WAIT_OBJ_RM_ACK);
+		  break;
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  break;
+	}
+
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+
+	return;
+}
+
+
+
+/** Handle an object invalidation request.
+ *  @author Renaud Lottiaux
+ *
+ *  @param msg  Message received from the requesting node.
+ */
+static inline
+int __handle_object_invalidation (kerrighed_node_t sender,
+				  void *_msg)
+{
+	msg_server_t *msg = _msg;
+	struct kddm_obj *obj_entry;
+	struct kddm_set *set;
+
+	BUG_ON (sender < 0 || sender > KERRIGHED_MAX_NODES);
+
+	obj_entry = get_kddm_obj_entry (msg->ns_id, msg->set_id, msg->objid,
+					&set);
+
+	if (object_frozen_or_pinned (obj_entry, set)) {
+		queue_event (__handle_object_invalidation, sender, set,
+			     obj_entry, msg->objid, msg,
+			     sizeof (msg_server_t));
+
+		goto exit;
+	}
+
+	switch (OBJ_STATE(obj_entry)) {
+	  case INV_COPY:
+		  /* Nothing to do... */
+		  break;
+
+	  case READ_COPY:
+		  kddm_invalidate_local_object_and_unlock (obj_entry, set,
+							   msg->objid,
+							   INV_COPY);
+
+		  change_prob_owner (obj_entry, msg->reply_node);
+		  send_invalidation_ack (set, msg->objid, msg->reply_node);
+		  goto exit_no_unlock;
+
+	  case WAIT_OBJ_READ:
+	  case INV_FILLING:
+		  queue_event (__handle_object_invalidation, sender, set,
+			       obj_entry, msg->objid, msg,
+			       sizeof (msg_server_t));
+		  break;
+
+	  case WAIT_OBJ_WRITE:
+		  kddm_invalidate_local_object_and_unlock (obj_entry, set,
+							   msg->objid,
+							   WAIT_OBJ_WRITE);
+
+		  change_prob_owner (obj_entry, msg->reply_node);
+		  send_invalidation_ack (set, msg->objid, msg->reply_node);
+		  goto exit_no_unlock;
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  break;
+	}
+
+exit:
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+exit_no_unlock:
+
+	return 0;
+}
+
+void handle_object_invalidation (struct rpc_desc* desc,
+                                void *_msg, size_t size)
+{
+	__handle_object_invalidation(desc->client, _msg);
+};
+
+
+
+/** Handle an object remove request.
+ *  @author Renaud Lottiaux
+ *
+ *  @param msg  Message received from the requesting node.
+ */
+static inline int __handle_object_remove_req (kerrighed_node_t sender,
+					      void *_msg)
+{
+	msg_server_t *msg = _msg;
+	struct kddm_obj *obj_entry;
+	struct kddm_set *set;
+	int flag = 0;
+
+	BUG_ON (sender < 0 || sender > KERRIGHED_MAX_NODES);
+
+	obj_entry = get_kddm_obj_entry (msg->ns_id, msg->set_id, msg->objid,
+					&set);
+
+	if (obj_entry == NULL) {
+		send_remove_ack (set, msg->objid, msg->reply_node, 0);
+		goto exit_no_unlock;
+	}
+
+	if (object_frozen_or_pinned (obj_entry, set)) {
+
+		queue_event (__handle_object_remove_req, sender, set,
+			     obj_entry, msg->objid, msg,
+			     sizeof (msg_server_t));
+
+		goto exit;
+	}
+
+	if (kddm_io_default_owner(set, msg->objid) == kerrighed_node_id)
+		flag = KDDM_NEED_OBJ_RM_ACK2;
+
+	switch (OBJ_STATE(obj_entry)) {
+	  case INV_COPY:
+	  case READ_COPY:
+		  if (flag) {
+			  kddm_change_obj_state (set, obj_entry, msg->objid,
+						 WAIT_OBJ_RM_ACK2);
+
+			  kddm_io_remove_object_and_unlock(obj_entry, set,
+							   msg->objid);
+
+			  send_remove_ack (set, msg->objid, msg->reply_node,
+					   flag);
+
+			  goto exit_no_unlock;
+		  }
+		  destroy_kddm_obj_entry(set, obj_entry, msg->objid, 1);
+
+		  send_remove_ack (set, msg->objid, msg->reply_node, flag);
+		  goto exit_no_unlock;
+
+	  case WAIT_OBJ_WRITE:
+	  case WAIT_OBJ_READ:
+		  BUG_ON(TEST_OBJECT_PINNED(obj_entry));
+
+		  kddm_io_remove_object_and_unlock (obj_entry, set,
+						    msg->objid);
+
+		  send_remove_ack (set, msg->objid, msg->reply_node, flag);
+		  goto exit_no_unlock;
+
+	  case INV_FILLING:
+		  queue_event (__handle_object_remove_req, sender, set,
+			       obj_entry, msg->objid, msg,
+			       sizeof (msg_server_t));
+		  break;
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  break;
+	}
+
+exit:
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+exit_no_unlock:
+
+	return 0;
+}
+
+void handle_object_remove_req (struct rpc_desc* desc,
+			      void *_msg, size_t size)
+{
+	__handle_object_remove_req(desc->client, _msg);
+};
+
+
+
+/** Handle an object ownership modification request.
+ *  @author Renaud Lottiaux
+ *
+ *  @param sender  Node sending the ownership.
+ *  @param msg     Message received from the requesting node.
+ */
+static inline int __handle_send_ownership_req (kerrighed_node_t sender,
+					       void *_msg)
+{
+	msg_injection_t *msg = _msg;
+	struct kddm_obj *obj_entry;
+	struct kddm_set *set;
+
+	BUG_ON (sender < 0 || sender > KERRIGHED_MAX_NODES);
+
+	obj_entry = get_kddm_obj_entry (msg->ns_id, msg->set_id, msg->objid,
+					&set);
+
+	switch (OBJ_STATE(obj_entry)) {
+	  case INV_COPY:
+		  /* Nothing to do */
+		  break;
+
+	  case READ_COPY:
+		  ack_change_object_owner (set, obj_entry, msg->objid,
+					   msg->reply_node, &msg->owner_info);
+		  break;
+
+	  case WAIT_OBJ_WRITE:
+		  send_invalidation_ack (set, msg->objid, msg->reply_node);
+		  break;
+
+	  case INV_FILLING:
+		  queue_event (__handle_send_ownership_req, sender, set,
+			       obj_entry, msg->objid, msg,
+			       sizeof (msg_injection_t));
+		  break;
+
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  break;
+	}
+
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+
+	return 0;
+}
+
+void handle_send_ownership_req (struct rpc_desc* desc,
+                               void *_msg, size_t size){
+	__handle_send_ownership_req(desc->client, _msg);
+};
+
+
+/** Handle an object ownership modification request.
+ *  @author Renaud Lottiaux
+ *
+ *  @param sender  Node sending the ownership.
+ *  @param msg     Message received from the requesting node.
+ */
+void handle_change_ownership_ack (struct rpc_desc* desc,
+                                 void *_msg, size_t size)
+{
+	msg_server_t *msg = _msg;
+	struct kddm_obj *obj_entry;
+	struct kddm_set *set;
+
+	BUG_ON (desc->client < 0 || desc->client > KERRIGHED_MAX_NODES);
+
+	obj_entry = get_kddm_obj_entry (msg->ns_id, msg->set_id, msg->objid,
+					&set);
+
+	if (OBJ_STATE(obj_entry) == WAIT_CHG_OWN_ACK)
+	{
+		change_prob_owner(obj_entry, msg->new_owner);
+
+		/* Wake up the set_flush_object function */
+		wake_up_on_wait_object (obj_entry, set);
+	}
+	else
+		STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+
+	return ;
+}
+
+
+
+/** Handle an object receive.
+ *  @author Renaud Lottiaux
+ *
+ *  @param sender  Node sending the object.
+ *  @param msg     Message received from the requesting node.
+ */
+void handle_object_receive (struct rpc_desc* desc,
+                           void *_msg, size_t size)
+{
+	msg_object_receiver_t *msg = _msg;
+	kddm_obj_state_t obj_state = 0;
+	struct kddm_obj *obj_entry;
+	struct kddm_set *set;
+	masterObj_t master_info;
+	int res, dont_insert = 0 ;
+
+	BUG_ON (desc->client < 0 || desc->client > KERRIGHED_MAX_NODES);
+
+	if (msg->object_state & KDDM_OWNER_OBJ) {
+		res = rpc_unpack(desc, 0, &master_info, sizeof(masterObj_t));
+		if (res)
+			return;
+	}
+
+	obj_entry = get_alloc_kddm_obj_entry (msg->ns_id, msg->set_id,
+					      msg->objid, &set);
+
+	if (!obj_entry) {
+		if (msg->flags & KDDM_SYNC_OBJECT) {
+			res = -EINVAL;
+			rpc_pack_type(desc, res);
+		}
+		return;
+	}
+
+	if (msg->object_state & KDDM_OWNER_OBJ) {
+		DUP2_SET(&master_info.rmset, &obj_entry->master_obj.rmset);
+		ADD_TO_SET(RMSET(obj_entry), desc->client);
+		ADD_TO_SET(RMSET(obj_entry), kerrighed_node_id);
+	}
+
+	switch (OBJ_STATE(obj_entry)) {
+	  case INV_COPY:
+		  if (msg->flags & KDDM_SYNC_OBJECT)
+			  obj_state = READ_COPY;
+		  else {
+			  change_prob_owner(obj_entry, kerrighed_node_id);
+			  obj_state = WRITE_GHOST;
+		  }
+		  break;
+
+	  case WAIT_OBJ_WRITE:
+		  change_prob_owner(obj_entry, kerrighed_node_id);
+		  ADD_TO_SET(COPYSET(obj_entry), desc->client);
+		  merge_ack_set(COPYSET(obj_entry), &master_info.copyset);
+		  ADD_TO_SET(COPYSET(obj_entry), kerrighed_node_id);
+		  if (OBJ_EXCLUSIVE2 (COPYSET(obj_entry)))
+			  obj_state = msg->object_state;
+		  else
+			  obj_state = WAIT_ACK_INV;
+
+		  break;
+
+	  case WAIT_OBJ_READ:
+		  change_prob_owner(obj_entry, desc->client);
+		  obj_state = msg->object_state;
+		  break;
+
+	  case READ_OWNER:
+	  case READ_COPY:
+		  obj_state = OBJ_STATE(obj_entry);
+		  if (!(msg->flags & KDDM_NO_DATA))
+			  dont_insert = 1;
+		  break;
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  kddm_obj_path_unlock(set, msg->objid);
+		  BUG();
+	}
+
+	if (!(msg->flags & KDDM_NO_DATA)) {
+		kddm_change_obj_state (set, obj_entry, msg->objid,
+				       INV_FILLING);
+		put_kddm_obj_entry(set, obj_entry, msg->objid);
+
+		res = kddm_io_alloc_object (obj_entry, set, msg->objid);
+		BUG_ON(res != 0);
+
+		kddm_io_import_object (desc, set, obj_entry, msg->objid,
+				       msg->flags);
+
+		kddm_obj_path_lock(set, msg->objid);
+
+		if (obj_state == WAIT_ACK_INV) {
+			if (OBJ_EXCLUSIVE (obj_entry))
+				/* Missing ACKs has been received during the
+				 * import
+				 */
+				obj_state = WRITE_OWNER;
+			else
+				/* We are still waiting for ACKs... Don't
+				 * insert the object until all ACKs has been
+				 * received. Insert is done in the ACK receive
+				 * function.
+				 */
+				dont_insert = 1;
+		}
+
+		if (dont_insert) {
+			kddm_change_obj_state (set, obj_entry, msg->objid,
+					       obj_state);
+		}
+		else
+			kddm_insert_object (set, msg->objid, obj_entry,
+					    obj_state);
+	}
+
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+
+	if (msg->flags & KDDM_SYNC_OBJECT) {
+		res = kddm_io_sync_object(obj_entry, set, msg->objid);
+		rpc_pack_type(desc, res);
+	}
+
+	return;
+}
+
+
+
+/** Handle no object request.
+ *  @author Renaud Lottiaux
+ *
+ *  @param sender  Node sending the request.
+ *  @param msg     Message received from the requesting node.
+ */
+int __handle_no_object (kerrighed_node_t sender,
+			void *_msg)
+{
+	msg_server_t *msg = _msg;
+	struct kddm_obj *obj_entry;
+	struct kddm_set *set;
+
+	BUG_ON (sender < 0 || sender > KERRIGHED_MAX_NODES);
+
+	obj_entry = get_alloc_kddm_obj_entry (msg->ns_id, msg->set_id,
+					      msg->objid, &set);
+
+	if (object_frozen (obj_entry, set)) {
+		queue_event (__handle_no_object, sender, set, obj_entry,
+			     msg->objid, msg, sizeof (msg_server_t));
+
+		goto exit;
+	}
+
+	ADD_TO_SET(RMSET(obj_entry), sender);
+
+	switch (OBJ_STATE(obj_entry)) {
+	  case WAIT_OBJ_READ:
+	  case WAIT_OBJ_WRITE:
+		  if (msg->flags ||
+		      (kddm_io_default_owner(set, msg->objid) ==
+		       kerrighed_node_id))
+			  kddm_change_obj_state (set, obj_entry, msg->objid,
+						 INV_OWNER);
+		  else
+			  kddm_change_obj_state (set, obj_entry, msg->objid,
+						 INV_COPY);
+
+		  wake_up_on_wait_object (obj_entry, set);
+
+		  kddm_io_remove_object_and_unlock (obj_entry, set,
+						    msg->objid);
+		  goto exit_no_unlock;
+
+	  case INV_OWNER:
+		  wake_up_on_wait_object (obj_entry, set);
+		  break;
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  break;
+	}
+
+exit:
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+
+exit_no_unlock:
+	return 0;
+}
+
+void handle_no_object (struct rpc_desc* desc,
+		      void *_msg, size_t size){
+	__handle_no_object(desc->client, _msg);
+};
+
+
+/** Handle a write access receive.
+ *  @author Renaud Lottiaux
+ *
+ *  @param sender  Node sending the write access.
+ *  @param msg     Message received.
+ */
+void handle_receive_write_access (struct rpc_desc* desc,
+				  void *_msg, size_t size)
+{
+	msg_injection_t *msg = _msg;
+	struct kddm_obj *obj_entry;
+	struct kddm_set *set;
+
+	BUG_ON (desc->client < 0 || desc->client > KERRIGHED_MAX_NODES);
+
+	obj_entry = get_alloc_kddm_obj_entry (msg->ns_id, msg->set_id,
+					      msg->objid, &set);
+
+	krgnodes_copy(obj_entry->master_obj.rmset, msg->owner_info.rmset);
+
+	switch (OBJ_STATE(obj_entry)) {
+	  case WAIT_OBJ_WRITE:
+		  BUG_ON (obj_entry->object == NULL);
+		  ADD_TO_SET(COPYSET(obj_entry), desc->client);
+		  merge_ack_set(COPYSET(obj_entry), &msg->owner_info.copyset);
+		  ADD_TO_SET(COPYSET(obj_entry), kerrighed_node_id);
+		  if (OBJ_EXCLUSIVE2 (COPYSET(obj_entry))) {
+			  kddm_change_obj_state (set, obj_entry, msg->objid,
+						 WRITE_OWNER);
+			  wake_up_on_wait_object (obj_entry, set);
+		  }
+		  else
+			  kddm_change_obj_state (set, obj_entry, msg->objid,
+						 WAIT_ACK_WRITE);
+		  break;
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  break;
+	}
+
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+
+	return;
+}
+
+
+
+/** Handle an object copy request.
+ *  @author Renaud Lottiaux
+ *
+ *  @param sender    Node sending the request.
+ *  @param msg       Message received from the requesting node.
+ */
+static inline int __handle_object_copy_req (kerrighed_node_t sender,
+					    void *_msg)
+{
+	msg_server_t *msg = _msg;
+	struct kddm_obj *obj_entry;
+	struct kddm_set *set;
+	int request_type = msg->flags & KDDM_SET_REQ_TYPE;
+	int send_ownership = msg->flags & KDDM_SEND_OWNERSHIP;
+	int r;
+
+	BUG_ON (sender < 0 || sender > KERRIGHED_MAX_NODES);
+
+	obj_entry = get_kddm_obj_entry (msg->ns_id, msg->set_id,
+					msg->objid, &set);
+
+	/* First, check for NULL obj_entry case */
+	if (obj_entry == NULL) {
+		if (!I_AM_DEFAULT_OWNER(set, msg->objid)) {
+			forward_object_server_msg (obj_entry, set,
+						   REQ_OBJECT_COPY, msg);
+			goto exit_no_unlock;
+		}
+
+		if ((msg->flags & KDDM_NO_FT_REQ) && !send_ownership) {
+			send_no_object (set, obj_entry, msg->objid,
+					msg->reply_node, send_ownership);
+			goto exit_no_unlock;
+		}
+
+		obj_entry = get_alloc_kddm_obj_entry (msg->ns_id, msg->set_id,
+						      msg->objid, &set);
+	}
+
+	if (object_frozen_or_pinned (obj_entry, set)) {
+		if (msg->flags & KDDM_TRY_GRAB)
+			send_no_object (set, obj_entry, msg->objid,
+					msg->reply_node, send_ownership);
+		else
+			queue_event (__handle_object_copy_req, sender, set,
+				     obj_entry, msg->objid, msg,
+				     sizeof (msg_server_t));
+		goto exit;
+	}
+
+	/* First checks if we are in a loop-back request. Some of them can be
+	 * valid requests due to some corner cases in the protocol.
+	 */
+	if (msg->reply_node != kerrighed_node_id)
+		goto regular_case;
+
+	switch (OBJ_STATE(obj_entry)) {
+	  case WAIT_OBJ_READ:
+	  case WAIT_OBJ_WRITE:
+		  /* The following test prevents from the answer of a node
+		   * which has just removed its copy and now believes we are
+		   * the new owner. And this is true, we are the new owner
+		   * of a new object we have to create.
+		   */
+		  if (msg->flags & KDDM_NO_FT_REQ) {
+			  kddm_change_obj_state (set, obj_entry, msg->objid,
+						 INV_OWNER);
+			  wake_up_on_wait_object (obj_entry, set);
+			  break;
+		  }
+
+		  r = object_first_touch (set, obj_entry, msg->objid,
+					  WRITE_OWNER, msg->flags);
+		  if (r)
+			  goto first_touch_error;
+		  break;
+
+	  case INV_COPY:
+	  case READ_COPY:
+	  case READ_OWNER:
+	  case WRITE_OWNER:
+	  case WRITE_GHOST:
+	  case WAIT_ACK_INV:
+	  case WAIT_OBJ_RM_ACK:
+	  case WAIT_OBJ_RM_ACK2:
+	  case WAIT_CHG_OWN_ACK:
+	  case WAIT_OBJ_RM_DONE:
+	  case WAIT_ACK_WRITE:
+	  case INV_OWNER:
+	  case INV_FILLING:
+		  /* Here, we receive a copy request following a flush on the
+		   * sending node. Our copy request has been served by the
+		   * flush. We can ignore this message.
+		   */
+		  break;
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  break;
+	}
+
+	goto exit;
+
+	/* Now, the regular cases : no loop back request */
+
+regular_case:
+	switch (OBJ_STATE(obj_entry)) {
+	  case WAIT_OBJ_READ:
+	  case INV_COPY:
+	  case READ_COPY:
+		  /* Shorten the prob owner chain on a write request */
+/*		  if (request_type == KDDM_OBJ_COPY_ON_WRITE) */
+/*			  change_prob_owner(obj_entry, msg->new_owner); */
+
+		  forward_object_server_msg (obj_entry, set,
+					     REQ_OBJECT_COPY, msg);
+		  break;
+
+	  case INV_OWNER:
+		  if (msg->flags & KDDM_NO_FT_REQ) {
+			  send_no_object (set, obj_entry, msg->objid,
+					  msg->reply_node, send_ownership);
+			  break;
+		  }
+
+		  if (!kddm_ft_linked (set))
+			  /* The object can be created on the faulting node */
+			  send_back_object_first_touch (set, obj_entry,
+							msg->objid,
+							msg->reply_node,
+							msg->flags,
+							SEND_BACK_FIRST_TOUCH);
+		  else {
+			  /* The object can be created on the local node  */
+			  if (request_type == KDDM_OBJ_COPY_ON_WRITE) {
+				  /* The object can be created on the local node */
+				  r = object_first_touch_no_wakeup (
+					  set, obj_entry, msg->objid,
+					  WRITE_OWNER, msg->flags);
+				  if (r)
+					  goto first_touch_error;
+				  obj_entry = send_copy_on_write_and_inv(
+					  set, obj_entry, msg->objid,
+					  msg->reply_node, 0);
+
+				  goto exit_no_unlock;
+			  }
+			  else {
+				  r = object_first_touch (
+					  set, obj_entry, msg->objid,
+					  READ_OWNER, msg->flags);
+				  if (r)
+					  goto first_touch_error;
+				  goto send_copy;
+			  }
+		  }
+		  break;
+
+	  case WRITE_GHOST:
+		  obj_entry = send_copy_on_write_and_inv(
+			  set, obj_entry, msg->objid,
+			  msg->reply_node, 0);
+
+		  goto exit_no_unlock;
+
+	  case WRITE_OWNER:
+		  if (request_type == KDDM_OBJ_COPY_ON_READ) {
+			  kddm_change_obj_state (set, obj_entry, msg->objid,
+						 READ_OWNER);
+			  goto send_copy;
+		  }
+		  else {
+			  obj_entry = send_copy_on_write_and_inv(
+				  set, obj_entry, msg->objid,
+				  msg->reply_node, 0);
+
+			  goto exit_no_unlock;
+		  }
+
+	  case READ_OWNER:
+
+		  if (request_type == KDDM_OBJ_COPY_ON_READ) {
+send_copy:
+			  /* Read copy request */
+
+			  send_copy_on_read (set, obj_entry, msg->objid,
+					     msg->reply_node, 0);
+			  break;
+		  }
+
+		  /* Write copy request */
+
+		  change_prob_owner (obj_entry, msg->reply_node);
+		  request_copies_invalidation (set, obj_entry,
+					       msg->objid,
+					       msg->reply_node);
+
+		  if (NODE_IN_SET (COPYSET(obj_entry), msg->reply_node))
+			  transfer_write_access_and_unlock (
+				  set, obj_entry, msg->objid,
+				  msg->reply_node,
+				  &obj_entry->master_obj);
+		  else {
+			  obj_entry = send_copy_on_write_and_inv(
+				  set, obj_entry, msg->objid,
+				  msg->reply_node, 0);
+		  }
+
+		  goto exit_no_unlock;
+
+	  case WAIT_OBJ_RM_ACK:
+	  case WAIT_OBJ_RM_ACK2:
+	  case WAIT_OBJ_RM_DONE:
+	  case WAIT_CHG_OWN_ACK:
+	  case WAIT_ACK_WRITE:
+	  case WAIT_OBJ_WRITE:
+	  case WAIT_ACK_INV:
+	  case INV_FILLING:
+		  queue_event (__handle_object_copy_req, sender, set,
+			       obj_entry, msg->objid, msg,
+			       sizeof (msg_server_t));
+		  break;
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  break;
+	}
+
+exit:
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+
+exit_no_unlock:
+	return 0;
+
+first_touch_error:
+	BUG_ON (r != ENODATA);
+
+	BUG_ON (msg->reply_node == kerrighed_node_id);
+	send_no_object (set, obj_entry, msg->objid, msg->reply_node,
+			0 /* send ownership */);
+	goto exit;
+}
+
+void handle_object_copy_req (struct rpc_desc* desc,
+			    void *_msg, size_t size){
+	__handle_object_copy_req(desc->client, _msg);
+}
+
+/** Handle an object remove request on the manager node.
+ *  @author Renaud Lottiaux
+ *
+ *  @param sender    Node sending the request.
+ *  @param msg       Message received from the requesting node.
+ */
+static inline
+int __handle_object_remove_to_mgr_req (kerrighed_node_t sender,
+				       void *_msg)
+{
+	msg_server_t *msg = _msg;
+	struct kddm_set *set;
+	struct kddm_obj *obj_entry;
+	int err = 0;
+
+	obj_entry = get_alloc_kddm_obj_entry (msg->ns_id, msg->set_id,
+					      msg->objid, &set);
+
+	if (object_frozen_or_pinned(obj_entry, set)) {
+		queue_event (__handle_object_remove_to_mgr_req, sender, set,
+			     obj_entry, msg->objid, msg,
+			     sizeof (msg_server_t));
+		goto exit;
+	}
+
+	switch (OBJ_STATE(obj_entry)) {
+	  case INV_COPY:
+	  case READ_COPY:
+	  case WAIT_OBJ_READ:
+	  case WAIT_OBJ_WRITE:
+		  forward_object_server_msg (obj_entry, set,
+					     REQ_OBJECT_REMOVE_TO_MGR, msg);
+		  break;
+
+	  case WAIT_OBJ_RM_ACK:
+	  case WAIT_OBJ_RM_ACK2:
+	  case WAIT_OBJ_RM_DONE:
+		  err = -EALREADY;
+		  break;
+
+	  case WAIT_ACK_WRITE:
+	  case WAIT_CHG_OWN_ACK:
+	  case WAIT_ACK_INV:
+	  case INV_FILLING:
+		  queue_event (__handle_object_remove_to_mgr_req, sender, set,
+			       obj_entry, msg->objid, msg,
+			       sizeof (msg_server_t));
+		  break;
+
+	  case INV_OWNER:
+	  case READ_OWNER:
+	  case WRITE_OWNER:
+	  case WRITE_GHOST:
+		  request_copies_remove(set, obj_entry,
+					msg->objid,
+					msg->reply_node);
+		  send_remove_object_done(set, msg->objid,
+					  msg->reply_node, RMSET(obj_entry));
+
+		  destroy_kddm_obj_entry(set, obj_entry,
+					 msg->objid, 1);
+		  goto exit_no_unlock;
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  break;
+	}
+
+exit:
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+
+exit_no_unlock:
+	return err;
+}
+
+void handle_object_remove_to_mgr_req (struct rpc_desc* desc,
+				     void *_msg, size_t size){
+	__handle_object_remove_to_mgr_req(desc->client, _msg);
+};
+
+
+/** Handle an object request response which is : make a local first touch.
+ *  @author Renaud Lottiaux
+ *
+ *  @param msg  Message received from the requesting node.
+ */
+void handle_send_back_first_touch_req (struct rpc_desc* desc,
+				       void *_msg, size_t size)
+{
+	msg_server_t *msg = _msg;
+	struct kddm_set *set;
+	struct kddm_obj *obj_entry;
+
+	BUG_ON (desc->client < 0 || desc->client > KERRIGHED_MAX_NODES);
+
+	obj_entry = get_kddm_obj_entry (msg->ns_id, msg->set_id,
+					msg->objid, &set);
+
+	switch (OBJ_STATE(obj_entry)) {
+	  case WAIT_OBJ_READ:
+	  case WAIT_OBJ_WRITE:
+		  BUG_ON (msg->reply_node == kerrighed_node_id);
+
+		  ADD_TO_SET(RMSET(obj_entry), desc->client);
+		  if (object_first_touch(set, obj_entry, msg->objid,
+					 WRITE_OWNER, msg->flags) != 0)
+			  BUG();
+		  break;
+
+	  default:
+		  STATE_MACHINE_ERROR (msg->set_id, msg->objid, obj_entry);
+		  break;
+	}
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+
+	return;
+}
+
+
+/** Handle the change of an object default owner.
+ *  @author Renaud Lottiaux
+ *
+ *  @param msg  Message received from the requesting node.
+ */
+static int handle_change_prob_owner_req(struct rpc_desc* desc,
+					void *_msg, size_t size)
+{
+	msg_server_t *msg = _msg;
+	struct kddm_obj *obj_entry;
+	struct kddm_set *set;
+	struct kddm_ns *ns;
+
+	BUG_ON (desc->client < 0 || desc->client > KERRIGHED_MAX_NODES);
+
+	ns = kddm_ns_get (msg->ns_id);
+	set = __find_get_kddm_set(ns, msg->set_id, KDDM_LOCK_FREE);
+
+	obj_entry = __get_alloc_kddm_obj_entry (set, msg->objid);
+
+	put_kddm_set(set);
+	kddm_ns_put(ns);
+
+	change_prob_owner(obj_entry, msg->new_owner);
+
+	if (OBJ_STATE(obj_entry) == INV_OWNER)
+		kddm_change_obj_state(set, obj_entry, msg->objid, INV_COPY);
+
+	put_kddm_obj_entry(set, obj_entry, msg->objid);
+
+	return 0;
+};
+
+/* Object Server Initialisation */
+
+void object_server_init ()
+{
+        struct rpc_synchro* object_server;
+	struct rpc_synchro* object_server_may_block;
+
+	object_server = rpc_synchro_new(1, "object server", 1);
+	object_server_may_block = rpc_synchro_new(1, "object srv may block", 1);
+
+	/***  Init the object serveur  ***/
+
+	__rpc_register(REQ_OBJECT_COPY,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server, handle_object_copy_req, 0);
+
+	__rpc_register(REQ_OBJECT_REMOVE,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server, handle_object_remove_req, 0);
+
+	__rpc_register(REQ_OBJECT_REMOVE_TO_MGR,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server, handle_object_remove_to_mgr_req, 0);
+
+	__rpc_register(SEND_BACK_FIRST_TOUCH,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server_may_block, handle_send_back_first_touch_req, 0);
+
+	__rpc_register(REQ_OBJECT_INVALID,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server, handle_object_invalidation, 0);
+
+	__rpc_register(INVALIDATION_ACK,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server, handle_invalidation_ack, 0);
+
+	__rpc_register(REMOVE_ACK,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server, handle_remove_ack, 0);
+
+	__rpc_register(REMOVE_ACK2,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server_may_block, handle_remove_ack2, 0);
+
+	__rpc_register(REMOVE_DONE,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server, handle_remove_done, 0);
+
+	__rpc_register(SEND_OWNERSHIP,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server, handle_send_ownership_req, 0);
+
+	__rpc_register(CHANGE_OWNERSHIP_ACK,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server, handle_change_ownership_ack, 0);
+
+	__rpc_register(OBJECT_SEND,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server, handle_object_receive, 0);
+
+	__rpc_register(SEND_WRITE_ACCESS,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server, handle_receive_write_access, 0);
+
+	__rpc_register(NO_OBJECT_SEND,
+		       RPC_TARGET_NODE, RPC_HANDLER_KTHREAD_VOID,
+		       object_server, handle_no_object, 0);
+
+	rpc_register_int(KDDM_CHANGE_PROB_OWNER, handle_change_prob_owner_req,
+			 0);
+}
+
+
+
+/* Object Server Finalization */
+
+void object_server_finalize ()
+{
+}
diff -ruN linux-2.6.29/kddm/process.h android_cluster/linux-2.6.29/kddm/process.h
--- linux-2.6.29/kddm/process.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/process.h	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,298 @@
+#ifndef __TOOLS_PROCESS__
+#define __TOOLS_PROCESS__
+
+#define task_on_runqueue(t) (t->run_list.next != LIST_POISON1)
+
+
+/** Make a process sleep
+ *
+ *  @param task  Task struct of the process
+ */
+static inline void sleep_on_task(struct task_struct *task)
+{
+	set_task_state(task, TASK_UNINTERRUPTIBLE);
+#ifdef SLEEP_ON_GENERIC_TASK
+	if (task != current && task_on_runqueue(task)) {
+		spin_lock_irq(&runqueue_lock);
+		del_from_runqueue(task);
+		spin_unlock_irq(&runqueue_lock);
+	}
+#else
+	if (task != current) {
+		printk("%s: task != current\n", __PRETTY_FUNCTION__);
+		while (1)
+			schedule();
+	}
+#endif
+	schedule();
+}
+
+
+/** Make a process sleep
+ *
+ *  @param task  Task struct of the process
+ */
+static inline void interruptible_sleep_on_task(struct task_struct *task)
+{
+	set_task_state(task, TASK_INTERRUPTIBLE);
+#ifdef SLEEP_ON_GENERIC_TASK
+	if (task != current && task_on_runqueue(task)) {
+		spin_lock_irq(&runqueue_lock);
+		del_from_runqueue(task);
+		spin_unlock_irq(&runqueue_lock);
+	}
+#else
+	if (task != current) {
+		printk("%s: task != current\n", __PRETTY_FUNCTION__);
+		while (1)
+			schedule();
+	}
+#endif
+	schedule();
+}
+
+
+/** Make a process sleep, unlock and restore IRQs.
+ *
+ *  @param task   Task struct of the process
+ *  @param mutex  Mutex to unlock
+ *  @param flafs  Flags of the IRQ to restore.
+ */
+static inline void
+sleep_on_task_and_spin_unlock_irqrestore(struct task_struct *task,
+					 spinlock_t *mutex,
+					 unsigned long flags)
+{
+	set_task_state(task, TASK_UNINTERRUPTIBLE);
+#ifdef SLEEP_ON_GENERIC_TASK
+	if (task != current && task_on_runqueue(task)) {
+		spin_lock(&runqueue_lock);
+		del_from_runqueue(task);
+		spin_unlock(&runqueue_lock);
+	}
+#else
+	if (task != current) {
+		printk("%s: task != current\n", __PRETTY_FUNCTION__);
+		while (1)
+			schedule();
+	}
+#endif
+
+	spin_unlock_irqrestore(mutex, flags);
+	schedule();
+}
+
+
+/** Make a process sleep, unlock and restore soft IRQs.
+ *
+ *  @param task   Task struct of the process
+ *  @param mutex  Mutex to unlock
+ */
+static inline void sleep_on_task_and_spin_unlock_bh(struct task_struct *task,
+						    spinlock_t * mutex)
+{
+	set_task_state(task, TASK_UNINTERRUPTIBLE);
+#ifdef SLEEP_ON_GENERIC_TASK
+	if (task != current && task_on_runqueue(task)) {
+		spin_lock(&runqueue_lock);
+		del_from_runqueue(task);
+		spin_unlock(&runqueue_lock);
+	}
+#else
+	if (task != current) {
+		printk("%s: task != current\n", __PRETTY_FUNCTION__);
+		while (1)
+			schedule();
+	}
+#endif
+
+	spin_unlock_bh(mutex);
+	schedule();
+}
+
+
+/** Make a process sleep, unlock and restore soft IRQs.
+ *
+ *  @param task   Task struct of the process
+ *  @param mutex  Mutex to unlock
+ */
+static inline void
+interruptible_sleep_on_task_and_spin_unlock_bh(struct task_struct *task,
+					       spinlock_t *mutex)
+{
+	set_task_state(task, TASK_INTERRUPTIBLE);
+#ifdef SLEEP_ON_GENERIC_TASK
+	if (task != current && task_on_runqueue(task)) {
+		spin_lock(&runqueue_lock);
+		del_from_runqueue(task);
+		spin_unlock(&runqueue_lock);
+	}
+#else
+	if (task != current) {
+		printk("%s: task != current\n", __PRETTY_FUNCTION__);
+		while (1)
+			schedule();
+	}
+#endif
+
+	spin_unlock_bh(mutex);
+	schedule();
+}
+
+
+/** Make a process sleep for a given amount of time on an wait queue,
+ *  unlock and restore soft IRQs.
+ *
+ *  @param wqh      Head of the wait queue to make the process sleep on.
+ *  @param tsk      Task struct of the process to sleep on.
+ *  @param timeout  Maximum amount of time to sleep (in jiffies).
+ *  @param mutex    Mutex to unlock
+ */
+static inline long sleep_on_timeout_and_spin_unlock_bh(wait_queue_head_t *wqh,
+						       struct task_struct *task,
+						       unsigned long timeout,
+						       spinlock_t *mutex)
+{
+	wait_queue_t wait;
+
+	init_waitqueue_entry(&wait, task);
+
+#ifdef CONFIG_PREEMPT
+	/* The following code is not preempt save. Process can be preempted
+	 * between the insertion in the wait queue and the schedule. */
+	BUG();
+#endif
+
+	add_wait_queue(wqh, &wait);
+
+	set_task_state(task, TASK_UNINTERRUPTIBLE);
+#ifdef SLEEP_ON_GENERIC_TASK
+	if (task != current && task_on_runqueue(task)) {
+		spin_lock(&runqueue_lock);
+		del_from_runqueue(task);
+		spin_unlock(&runqueue_lock);
+	}
+#else
+	if (task != current) {
+		printk("%s: task != current\n", __PRETTY_FUNCTION__);
+		while (1)
+			schedule();
+	}
+#endif
+
+	spin_unlock_bh(mutex);
+
+	timeout = schedule_timeout(timeout);
+
+	remove_wait_queue(wqh, &wait);
+
+	return timeout;
+}
+
+
+/** Make a process sleep, unlock and restore soft IRQs.
+ *
+ *  @param q        Head of the wait queue to make the process sleep on.
+ *  @param mutex    Mutex to unlock
+ */
+static inline void sleep_on_and_spin_unlock(wait_queue_head_t * wqh,
+					    spinlock_t * mutex)
+{
+	wait_queue_t wait;
+
+	init_waitqueue_entry(&wait, current);
+
+	current->state = TASK_UNINTERRUPTIBLE;
+
+#ifdef CONFIG_PREEMPT
+	// The following code is not preempt save. Process can be preempted between
+	// the insertion in the wait queue and the schedule.
+	BUG();
+#endif
+
+	add_wait_queue(wqh, &wait);
+
+	spin_unlock(mutex);
+
+	schedule();
+
+	remove_wait_queue(wqh, &wait);
+}
+
+
+/** Make a process sleep on an exclusive wait queue,
+ *  unlock and restore soft IRQs.
+ *
+ *  @param wqh    Head of the wait queue to make the process sleep on.
+ *  @param tsk    Task struct of the process to sleep on.
+ *  @param mutex  Mutex to unlock
+ */
+static inline void sleep_on_exclusive_and_spin_unlock_bh(wait_queue_head_t *
+							 wqh,
+							 struct task_struct
+							 *tsk,
+							 spinlock_t * mutex)
+{
+	wait_queue_t wait;
+
+	init_waitqueue_entry(&wait, tsk);
+
+	add_wait_queue_exclusive(wqh, &wait);
+
+	sleep_on_task_and_spin_unlock_bh(tsk, mutex);
+
+	remove_wait_queue(wqh, &wait);
+}
+
+/** Make a process sleep on an exclusive wait queue,
+ *  unlock and restore soft IRQs.
+ *
+ *  @param wqh    Head of the wait queue to make the process sleep on.
+ *  @param tsk    Task struct of the process to sleep on.
+ *  @param mutex  Mutex to unlock
+ */
+static inline void
+interruptible_sleep_on_exclusive_and_spin_unlock_bh(wait_queue_head_t * wqh,
+						    struct task_struct *tsk,
+						    spinlock_t * mutex)
+{
+	wait_queue_t wait;
+
+	init_waitqueue_entry(&wait, tsk);
+
+	add_wait_queue_exclusive(wqh, &wait);
+
+	interruptible_sleep_on_task_and_spin_unlock_bh(tsk, mutex);
+
+	remove_wait_queue(wqh, &wait);
+}
+
+/** Make a process sleep for a given amount of time on an exclusive wait
+ *  queue, unlock and restore soft IRQs.
+ *
+ *  @param wqh      Head of the wait queue to make the process sleep on.
+ *  @param tsk      Task struct of the process to sleep on.
+ *  @param timeout  Maximum amount of time to sleep (in jiffies).
+ *  @param mutex    Mutex to unlock
+ */
+static inline long
+sleep_on_timeout_exclusive_and_spin_unlock_bh(wait_queue_head_t * wqh,
+					      struct task_struct *tsk,
+					      unsigned long timeout,
+					      spinlock_t * mutex)
+{
+	return sleep_on_timeout_and_spin_unlock_bh(wqh, tsk, timeout, mutex);
+}
+
+
+/** Make the current process sleep for a given number of seconds.
+ *
+ *  @param seconds   The number of seconds the process should sleep
+ */
+static inline void sleep(int seconds)
+{
+	set_current_state(TASK_INTERRUPTIBLE);
+	schedule_timeout(seconds * HZ);
+}
+
+#endif
diff -ruN linux-2.6.29/kddm/procfs.c android_cluster/linux-2.6.29/kddm/procfs.c
--- linux-2.6.29/kddm/procfs.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/procfs.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,700 @@
+/** /proc manager
+ *  @file procfs.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/mmzone.h>
+#include <linux/proc_fs.h>
+#include <asm/uaccess.h>
+#include <linux/swap.h>
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/list.h>
+#include <linux/seq_file.h>
+#include <linux/hashtable.h>
+
+#include <kerrighed/procfs.h>
+#include <kddm/kddm.h>
+#include "kddm_bench.h"
+
+/*  /proc/kerrighed/kddm          */
+static struct proc_dir_entry *procfs_kddm;
+
+/*  /proc/kerrighed/kddm/meminfo  */
+static struct proc_dir_entry *procfs_meminfo;
+
+/*  /proc/kerrighed/kddm/kddmstat */
+static struct proc_dir_entry *procfs_setstat;
+
+/*  /proc/kerrighed/kddm/bench */
+static struct proc_dir_entry *procfs_bench;
+
+
+
+void check_pages(void)
+{
+        struct page *page;
+        pg_data_t *pgdat;
+        unsigned long i, pb;
+	unsigned long flags;
+
+        for_each_online_pgdat(pgdat) {
+                pgdat_resize_lock(pgdat, &flags);
+                for (i = 0; i < pgdat->node_spanned_pages; ++i) {
+			pb = 0;
+                        page = pgdat_page_nr(pgdat, i);
+			if (page_count(page) < 0) {
+				printk ("Negative count\n");
+				pb = 1;
+			}
+			if (page_mapcount(page) < 0) {
+				printk ("Negative mapcount\n");
+				pb = 1;
+			}
+			if (page_mapcount(page) > page_count(page)) {
+				printk ("Count problem\n");
+				pb = 1;
+			}
+			if (page_count(page) == 0) {
+				if (page_mapcount(page) != 0) {
+					printk ("Non null map count\n");
+					pb = 1;
+				}
+				if (page->mapping != NULL) {
+					printk ("Non null mapping\n");
+					pb = 1;
+				}
+			}
+			else {
+				if (page_mapcount(page)) {
+					if (page->mapping == NULL) {
+						printk ("Null mapping\n");
+						pb = 1;
+					}
+				}
+				if (page->mapping) {
+					if ((page_mapcount(page) == 0) &&
+					    (PageAnon(page))) {
+						printk ("Null mapcount\n");
+						pb = 1;
+					}
+				}
+			}
+			if (pb)
+				printk ("Page %p - count %d - map count %d - "
+					"mapping %p - flags 0x%08lx\n", page,
+					page_count(page),
+					page_mapcount(page),
+					page->mapping,
+					page->flags);
+                }
+		printk ("%ld pages checked\n", i);
+                pgdat_resize_unlock(pgdat, &flags);
+        }
+}
+
+
+
+/****************************************************************************/
+/*                                                                          */
+/*                         /proc/kddminfo Management                        */
+/*                                                                          */
+/****************************************************************************/
+
+
+
+static void *s_start(struct seq_file *m, loff_t *pos)
+{
+	struct kddm_set *set;
+	unsigned long found;
+
+	if (*pos == 0)
+		seq_printf (m, "       Set ID           nr entries       nr objects     obj size      Set size     \n");
+
+	/* Assumption: KDDM set id 0 is never used. */
+
+	down (&kddm_def_ns->table_sem);
+	set = __hashtable_find_next (kddm_def_ns->kddm_set_table, *pos,
+				     &found);
+	up (&kddm_def_ns->table_sem);
+	*pos = found;
+	return set;
+}
+
+static void *s_next(struct seq_file *m, void *p, loff_t *pos)
+{
+	return s_start(m, pos);
+}
+
+static void s_stop(struct seq_file *m, void *p)
+{
+}
+
+static int s_show(struct seq_file *m, void *p)
+{
+	struct kddm_set *set = p;
+
+	seq_printf (m, "%20ld %16d %16d %10d %16d\n", set->id,
+		    atomic_read(&set->nr_entries),
+		    atomic_read(&set->nr_objects), set->obj_size,
+		    atomic_read(&set->nr_objects) * set->obj_size);
+
+        return 0;
+}
+
+
+
+const struct seq_operations kddminfo_op = {
+        .start = s_start,
+        .next = s_next,
+        .stop = s_stop,
+        .show = s_show,
+};
+
+
+
+static int kddminfo_open(struct inode *inode, struct file *file)
+{
+        return seq_open(file, &kddminfo_op);
+}
+
+
+
+static struct file_operations proc_kddminfo_operations = {
+        .open           = kddminfo_open,
+        .read           = seq_read,
+        .llseek         = seq_lseek,
+        .release        = seq_release,
+};
+
+
+
+/****************************************************************************/
+/*                                                                          */
+/*                     /proc/kerrighed/kddm  Management                     */
+/*                                                                          */
+/****************************************************************************/
+
+
+
+/** Read function for /proc/kerrighed/kddm/meminfo entry.
+ *  @author Renaud Lottiaux
+ *
+ *  @param buffer           Buffer to write data to.
+ *  @param buffer_location  Alternative buffer to return...
+ *  @param offset           Offset of the first byte to write in the buffer.
+ *  @param buffer_length    Length of given buffer
+ *
+ *  @return  Number of bytes written.
+ */
+int read_meminfo (char *buffer,
+                  char **start, off_t offset, int count, int *eof, void *data)
+{
+  static char mybuffer[80 * (5 + NB_OBJ_STATE)];
+  static int len;
+  int i;
+
+
+
+  if (offset == 0)
+    {
+      len = sprintf (mybuffer,
+                     "Free Pages:         %lu\n"
+                     "Kddm objects:       %d\n"
+                     "Master objetcs:     %d\n"
+                     "Copy objects:       %d\n",
+                     nr_free_pages (),
+                     atomic_read(&nr_master_objects) +
+		     atomic_read(&nr_copy_objects),
+                     atomic_read(&nr_master_objects),
+		     atomic_read(&nr_copy_objects));
+
+      for (i = 0; i < NB_OBJ_STATE; i++)
+        {
+          len += sprintf (mybuffer + len,
+                          "%s: \t %d\n", STATE_NAME (i),
+			  atomic_read(&nr_OBJ_STATE[i]));
+        }
+      check_pages();
+    }
+
+  if (offset + count >= len)
+    {
+      count = len - offset;
+      if (count < 0)
+        count = 0;
+      *eof = 1;
+    }
+
+  memcpy (buffer, &mybuffer[offset], count);
+
+  return count;
+}
+
+
+
+/** Read function for /proc/kerrighed/kddm/setstat entry.
+ *  @author Renaud Lottiaux
+ *
+ *  @param buffer           Buffer to write data to.
+ *  @param buffer_location  Alternative buffer to return...
+ *  @param offset           Offset of the first byte to write in the buffer.
+ *  @param buffer_length    Length of given buffer
+ *
+ *  @return  Number of bytes written.
+ */
+int read_setstat (char *buffer,
+                   char **start, off_t offset, int count, int *eof, void *data)
+{
+  static char mybuffer[80 * 5];
+  static int len;
+
+  if (offset == 0)
+    {
+      len = 0;
+
+      len += sprintf (mybuffer + len, "Get Object:          %ld\n",
+                      total_get_object_counter);
+
+      len += sprintf (mybuffer + len, "Grab Object:         %ld\n",
+                      total_grab_object_counter);
+
+      len += sprintf (mybuffer + len, "Remove Object:       %ld\n",
+                      total_remove_object_counter);
+
+      len += sprintf (mybuffer + len, "Flush Object:        %ld\n",
+                      total_flush_object_counter);
+    }
+
+  if (offset + count >= len)
+    {
+      count = len - offset;
+      if (count < 0)
+        count = 0;
+      *eof = 1;
+    }
+
+  memcpy (buffer, &mybuffer[offset], count);
+
+  return count;
+}
+
+
+
+/** Read function for /proc/kerrighed/kddm/bench entry.
+ *  @author Renaud Lottiaux
+ *
+ *  @param buffer           Buffer to write data to.
+ *  @param buffer_location  Alternative buffer to return...
+ *  @param offset           Offset of the first byte to write in the buffer.
+ *  @param buffer_length    Length of given buffer
+ *
+ *  @return  Number of bytes written.
+ */
+int read_bench (char *buffer,
+		char **start, off_t offset, int count, int *eof, void *data)
+{
+	static char *mybuffer = NULL;
+	static int len, size = 80*50;
+
+	if (mybuffer == NULL)
+		mybuffer = kmalloc (size, GFP_KERNEL);
+
+	if (offset == 0)
+		len = kddm_bench(mybuffer, size);
+
+	if (offset + count >= len)
+	{
+		count = 1 + len - offset;
+		if (count < 0)
+			count = 0;
+		*eof = 1;
+	}
+
+	snprintf (buffer, count, "%s", &mybuffer[offset]);
+
+	*start = buffer;
+
+	return count;
+}
+
+
+
+/** Create the /proc/kerrighed/kddm directory and sub-directories.
+ *  @author Renaud Lottiaux
+ */
+void create_kddm_proc_dir (void)
+{
+  /* Create the /proc/kerrighed/kddm entry */
+
+  BUG_ON (proc_kerrighed == NULL);
+
+  procfs_kddm = create_proc_entry ("kddm", S_IFDIR | S_IRUGO | S_IWUGO |
+                                   S_IXUGO, proc_kerrighed);
+
+  if (procfs_kddm == NULL)
+    {
+      printk ("Cannot create /proc/kerrighed/kddm\n");
+      return;
+    }
+
+  /* Create the /proc/kerrighed/kddm/meminfo entry */
+
+  procfs_meminfo = create_proc_entry ("meminfo", S_IRUGO, procfs_kddm);
+
+  if (procfs_meminfo == NULL)
+    {
+      printk ("Cannot create /proc/kerrighed/kddm/memfinfo\n");
+      return;
+    }
+
+  procfs_meminfo->read_proc = read_meminfo;
+
+  /* Create the /proc/kerrighed/kddm/setstat entry */
+
+  procfs_setstat = create_proc_entry ("setstat", S_IRUGO, procfs_kddm);
+  if (procfs_setstat == NULL)
+    {
+      printk ("Cannot create /proc/kerrighed/kddm/setstat\n");
+      return;
+    }
+
+  procfs_setstat->read_proc = read_setstat;
+
+  /* Create the /proc/kerrighed/kddm/bench entry */
+
+  procfs_bench = create_proc_entry ("bench", S_IRUGO, procfs_kddm);
+  if (procfs_bench == NULL) {
+	  printk ("Cannot create /proc/kerrighed/kddm/bench\n");
+	  return;
+  }
+
+  procfs_bench->read_proc = read_bench;
+
+  /* Create the /proc/kddminfo entry */
+
+  proc_create("kddminfo", S_IRUGO, NULL, &proc_kddminfo_operations);
+}
+
+
+
+/** Delete the /proc/kerrighed/kddm directory and sub-directories.
+ *  @author Renaud Lottiaux
+ */
+void remove_kddm_proc_dir (void)
+{
+  procfs_deltree (procfs_kddm);
+}
+
+
+
+/****************************************************************************/
+/*                                                                          */
+/*               /proc/kerrighed/kddm/<set_id>  Management                */
+/*                                                                          */
+/****************************************************************************/
+
+
+
+
+/** Read function for /proc/kerrighed/kddm/<set_id>/setstat entry.
+ *  @author Renaud Lottiaux
+ *
+ *  @param buffer           Buffer to write data to.
+ *  @param buffer_location  Alternative buffer to return...
+ *  @param offset           Offset of the first byte to write in the buffer.
+ *  @param buffer_length    Length of given buffer
+ *
+ *  @return  Number of bytes written.
+ */
+int read_set_id_setstat (char *buffer,
+                          char **start,
+                          off_t offset, int count, int *eof, void *data)
+{
+	struct kddm_set *set = NULL;
+	static char mybuffer[80 * 5];
+	static int len;
+
+	if (offset == 0) {
+		set = _find_get_kddm_set (kddm_def_ns, (kddm_set_id_t) data);
+		BUG_ON (!set);
+
+		len = 0;
+
+		len += sprintf (mybuffer + len, "Get Object:          %ld\n",
+				set->get_object_counter);
+
+		len += sprintf (mybuffer + len, "Grab Object:         %ld\n",
+				set->grab_object_counter);
+
+		len += sprintf (mybuffer + len, "Remove Object:       %ld\n",
+				set->remove_object_counter);
+
+		len += sprintf (mybuffer + len, "Flush Object:        %ld\n",
+				set->flush_object_counter);
+
+		put_kddm_set(set);
+	}
+
+  if (offset + count >= len)
+    {
+      count = len - offset;
+      if (count < 0)
+        count = 0;
+      *eof = 1;
+    }
+
+  memcpy (buffer, &mybuffer[offset], count);
+
+  return count;
+}
+
+
+
+/** Read function for /proc/kerrighed/kddm/<set_id>/setinfo entry.
+ *  @author Renaud Lottiaux
+ *
+ *  @param buffer           Buffer to write data to.
+ *  @param buffer_location  Alternative buffer to return...
+ *  @param offset           Offset of the first byte to write in the buffer.
+ *  @param buffer_length    Length of given buffer
+ *
+ *  @return  Number of bytes written.
+ */
+int read_set_id_setinfo (char *buffer,
+                          char **start,
+                          off_t offset, int count, int *eof, void *data)
+{
+	struct kddm_set *set = NULL;
+	static char mybuffer[80 * 20];
+
+	static int len;
+
+	if (offset == 0) {
+		len = 0;
+
+		set = _find_get_kddm_set (kddm_def_ns, (kddm_set_id_t) data);
+		BUG_ON (!set);
+
+		if (set->iolinker == NULL)
+			len += sprintf (mybuffer + len,
+					"Type:         No IO Linker\n");
+		else
+			len += sprintf (mybuffer + len, "Type:         %s\n",
+					set->iolinker->linker_name);
+
+		len += sprintf (mybuffer + len, "Manager            : %d\n",
+				KDDM_SET_MGR (set));
+
+		len += sprintf (mybuffer + len, "Nr Objects         : %d\n",
+				atomic_read(&set->nr_objects));
+
+		len += sprintf (mybuffer + len, "Nr Entries         : %d\n",
+				atomic_read(&set->nr_entries));
+
+		len += sprintf (mybuffer + len, "Flags              : "
+				"0x%08lx\n", set->flags);
+
+		len += sprintf (mybuffer + len, "State              : %d\n",
+				set->state);
+
+		len += sprintf (mybuffer + len, "- Master entries   : %d\n",
+				atomic_read(&set->nr_masters));
+
+		len += sprintf (mybuffer + len, "- Copy entries     : %d\n",
+				atomic_read(&set->nr_copies));
+
+		len += sprintf (mybuffer + len, "Usage count  : %d\n",
+				atomic_read(&set->count) - 1);
+
+		switch (set->def_owner) {
+		  case KDDM_RR_DEF_OWNER:
+			  len += sprintf (mybuffer + len,
+					  "Default owner    : Round Robin\n");
+			  break;
+
+		  case KDDM_CUSTOM_DEF_OWNER:
+			  len += sprintf (mybuffer + len,
+					  "Default owner    : Custom\n");
+			  break;
+
+		  default:
+			  len += sprintf (mybuffer + len, "Default owner    : %d\n",
+					  set->def_owner);
+		}
+
+		put_kddm_set(set);
+	}
+
+	if (offset + count >= len) {
+		count = len - offset;
+		if (count < 0)
+			count = 0;
+		*eof = 1;
+	}
+
+	memcpy (buffer, &mybuffer[offset], count);
+
+	return count;
+}
+
+
+
+/** Read function for /proc/kerrighed/kddm/<set_id>/objectstates entry.
+ *  @author Gael Utard
+ */
+int read_set_id_objectstates (char *buffer,
+                              char **start,
+                              off_t offset, int count, int *eof, void *data)
+{
+	int i, size = 0;
+	struct kddm_set *set;
+
+	if (offset >= size) {
+		*eof = 1;
+		return 0;
+	}
+
+	for (i = 0; offset + i < size && i < count; i++) {
+		struct kddm_obj *obj_entry;
+
+		obj_entry = _get_kddm_obj_entry(kddm_def_ns,
+						(kddm_set_id_t) data,
+						offset + i, &set);
+
+		if (obj_entry != NULL) {
+			switch (OBJ_STATE(obj_entry)) {
+			case READ_COPY:
+				buffer[i] = 'R';
+				break;
+
+			case WAIT_CHG_OWN_ACK:
+			case WAIT_ACK_WRITE:
+			case WAIT_ACK_INV:
+			case READ_OWNER:
+				buffer[i] = 'O';
+				break;
+
+			case WRITE_GHOST:
+			case WRITE_OWNER:
+				buffer[i] = 'W';
+				break;
+
+			case INV_COPY:
+			case INV_OWNER:
+			case INV_FILLING:
+			case WAIT_OBJ_READ:
+			case WAIT_OBJ_WRITE:
+				buffer[i] = 'I';
+				break;
+
+			default:
+				buffer[i] = '?';
+			}
+			put_kddm_obj_entry(set, obj_entry, offset + i);
+		}
+		else
+			buffer[i] = 'I';
+	}
+
+	return i;
+}
+
+
+
+/* Create a /proc/kerrighed/kddm/<set_id> directory and sub-directories. */
+
+struct proc_dir_entry *create_kddm_proc (kddm_set_id_t set_id)
+{
+	struct proc_dir_entry *entry, *objectstates, *stat, *info;
+	char buffer[24];
+
+	BUG_ON (procfs_kddm == NULL);
+
+	/* Create the /proc/kerrighed/kddm/<set_id> entry */
+
+	snprintf (buffer, 24, "%ld", set_id);
+	entry = create_proc_entry (buffer, S_IFDIR|S_IRUGO|S_IWUGO|S_IXUGO,
+				   procfs_kddm);
+
+	if (entry == NULL)
+		return NULL;
+
+	/* Create the /proc/kerrighed/kddm/<set_id>/objectstates entry */
+
+	objectstates = create_proc_entry ("objectstates", S_IRUGO, entry);
+
+	if (objectstates == NULL)
+		return NULL;
+
+	objectstates->data = (void *) set_id;
+	objectstates->read_proc = read_set_id_objectstates;
+
+	/* Create the /proc/kerrighed/kddm/<set_id>/setstat entry */
+
+	stat = create_proc_entry ("setstat", S_IRUGO, entry);
+	if (stat == NULL) {
+		printk ("Cannot create proc entry for %ld/setstat\n",
+			set_id);
+		return NULL;
+	}
+
+	stat->data = (void *) set_id;
+	stat->read_proc = read_set_id_setstat;
+
+	/* Create the /proc/kerrighed/kddm/<set_id>/setinfo entry */
+
+	info = create_proc_entry ("setinfo", S_IRUGO, entry);
+	if (info == NULL)
+		return NULL;
+
+	info->data = (void *) set_id;
+	info->read_proc = read_set_id_setinfo;
+
+	return entry;
+}
+
+
+
+/* Remove a /proc/kerrighed/kddm/<set_id> directory and sub-directories. */
+
+
+void remove_kddm_proc (struct proc_dir_entry *proc_entry)
+{
+  if (proc_entry != NULL)
+    procfs_deltree (proc_entry);
+}
+
+
+
+/***********************************************************************/
+/*                                                                     */
+/*         Define Kddm services in the /proc/kerrighed/services        */
+/*                                                                     */
+/***********************************************************************/
+
+
+
+/** Init Kddm proc stuffs.
+ *  @author Renaud Lottiaux
+ */
+int procfs_kddm_init (void)
+{
+	create_kddm_proc_dir ();
+
+	return 0;
+};
+
+
+
+/** Finalize Kddm proc stuffs.
+ *  @author Renaud Lottiaux
+ */
+int procfs_kddm_finalize (void)
+{
+	remove_kddm_proc_dir ();
+
+	return 0;
+};
diff -ruN linux-2.6.29/kddm/procfs.h android_cluster/linux-2.6.29/kddm/procfs.h
--- linux-2.6.29/kddm/procfs.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/procfs.h	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,65 @@
+#ifndef KDDM_PROC_H
+
+#define KDDM_PROC_H
+
+#ifdef __KERNEL__
+
+#include <linux/proc_fs.h>
+#include <kerrighed/krg_services.h>
+#include <kddm/kddm_types.h>
+
+#endif // __KERNEL__
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                 MACROS                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+#ifdef __KERNEL__
+
+int procfs_kddm_init (void);
+int procfs_kddm_finalize (void);
+
+
+/** Create a /proc/kerrighed/kddm/<set_id> directory and sub-directories.
+ *  @author Gael Utard, Renaud Lottiaux
+ *
+ *  @param set_id   Id of the kddm set to create a proc entry for.
+ *
+ *  @return proc_fs entry created.
+ */
+struct proc_dir_entry *create_kddm_proc (kddm_set_id_t set_id);
+
+
+
+/** Remove a /proc/kerrighed/kddm/<set_id> directory and sub-directories.
+ *  @author Renaud Lottiaux
+ *
+ *  @param proc_entry    Struct of the proc entry to destroy.
+ */
+void remove_kddm_proc (struct proc_dir_entry *proc_entry);
+
+
+#endif /* __KERNEL__ */
+
+#endif /* KDDM_PROC_H */
diff -ruN linux-2.6.29/kddm/protocol_action.c android_cluster/linux-2.6.29/kddm/protocol_action.c
--- linux-2.6.29/kddm/protocol_action.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/protocol_action.c	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,949 @@
+/** Basic coherence protocol actions.
+ *  @file protocol_action.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ *
+ *  The file implements the basic operations used by the KDDM coherence
+ *  protocol.
+ */
+
+#include <linux/slab.h>
+#include <linux/workqueue.h>
+#include <linux/spinlock.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/krginit.h>
+
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+#include "protocol_action.h"
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+
+int delayed_transfer_write_access (kerrighed_node_t dest_node, void *msg);
+
+struct kmem_cache *kddm_da_cachep;
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              HELPER FUNCTIONS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+/** Wrapper to send a message to the object server handler.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  dest        Destination node.
+ *  @param  type        Type of the message to send.
+ *  @param  set_id      Id of the concerned set.
+ *  @param  objid       Id of the concerned object.
+ */
+static inline void send_msg_to_object_server(kerrighed_node_t dest,
+					     enum rpcid type,
+					     int ns_id,
+					     kddm_set_id_t set_id,
+					     objid_t objid,
+					     int flags,
+					     kerrighed_node_t new_owner,
+					     long req_id)
+{
+	msg_server_t msg_to_server;
+
+	BUG_ON(dest < 0 || dest > KERRIGHED_MAX_NODES);
+
+	msg_to_server.ns_id = ns_id;
+	msg_to_server.set_id = set_id;
+	msg_to_server.objid = objid;
+	msg_to_server.flags = flags;
+	msg_to_server.new_owner = new_owner;
+	msg_to_server.reply_node = kerrighed_node_id;
+
+	rpc_async(type, dest, &msg_to_server, sizeof(msg_server_t));
+}
+
+
+
+/** Wrapper to send a message to the object receiver handler.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  dest          Destination node.
+ *  @param  set           Set hosting the object.
+ *  @param  objid         Id of the concerned object.
+ *  @param  obj_entry     Structure of the concerned object.
+ *  @param  object_state  State of the concerned object.
+ */
+static inline int send_msg_to_object_receiver(kerrighed_node_t dest,
+					      struct kddm_set *set,
+					      objid_t objid,
+					      struct kddm_obj *obj_entry,
+					      kddm_obj_state_t object_state,
+					      int flags,
+					      long req_id)
+{
+	msg_object_receiver_t object_send_msg;
+	struct rpc_desc *desc;
+	int err = 0;
+
+	BUG_ON(dest < 0 || dest > KERRIGHED_MAX_NODES);
+
+	object_send_msg.ns_id = set->ns->id;
+	object_send_msg.set_id = set->id;
+	object_send_msg.objid = objid;
+	object_send_msg.req_id = req_id;
+	object_send_msg.object_state = object_state;
+	object_send_msg.flags = flags;
+
+	desc = rpc_begin(OBJECT_SEND, dest);
+	if (!desc)
+		OOM;
+
+	err = rpc_pack_type(desc, object_send_msg);
+	if (err)
+		goto err_cancel;
+
+	if (object_state & KDDM_OWNER_OBJ) {
+		err = rpc_pack(desc, 0, &obj_entry->master_obj,
+			       sizeof(masterObj_t));
+		if (err)
+			goto err_cancel;
+	}
+
+	if (!(flags & KDDM_NO_DATA)) {
+		err = kddm_io_export_object(desc, set, obj_entry, objid, flags);
+		if (err)
+			goto err_cancel;
+	}
+
+	if (flags & KDDM_SYNC_OBJECT)
+		rpc_unpack_type (desc, err);
+
+	rpc_end(desc, 0);
+
+	if (flags & KDDM_REMOVE_ON_ACK)
+		destroy_kddm_obj_entry(set, obj_entry, objid, 0);
+
+out:
+	return err;
+
+err_cancel:
+	rpc_cancel(desc);
+	goto out;
+}
+
+
+
+/** Request to synchronize the given object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  set        Struct of the concerned set.
+ *  @param  obj_entry  Structure of the concerned object.
+ *  @param  objid      Id of the concerned object.
+ */
+int request_sync_object_and_unlock(struct kddm_set * set,
+				   struct kddm_obj *obj_entry,
+				   objid_t objid,
+				   kddm_obj_state_t new_state)
+{
+	kerrighed_node_t dest;
+	int err = 0, flags = KDDM_SYNC_OBJECT;
+
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	dest = kddm_io_default_owner (set, objid);
+	BUG_ON (dest == kerrighed_node_id);
+
+	if ((OBJ_STATE(obj_entry) == READ_OWNER) &&
+	    NODE_IN_SET(COPYSET(obj_entry), dest))
+		flags |= KDDM_NO_DATA;
+
+	kddm_change_obj_state (set, obj_entry, objid, new_state);
+
+	put_kddm_obj_entry(set, obj_entry, objid);
+	err = send_copy_on_read(set, obj_entry, objid, dest, flags);
+
+	return err;
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                         COHERENCE PROTOCOL ACTIONS                        */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+/** Send object invalidation requests.
+ *  @author Renaud Lottiaux
+ *
+ *  DO NOT invalidate the local copy and the sender copy.
+ *
+ *  @param set        Set the object to invalidate belong to.
+ *  @param objid      Id of the object to invalidate.
+ *  @param sender     The node which initiated the object invalidation
+ *                    (i.e. the node who want a write copy of the object)
+ */
+void request_copies_invalidation(struct kddm_set * set,
+				 struct kddm_obj *obj_entry,
+				 objid_t objid,
+				 kerrighed_node_t sender)
+{
+	msg_server_t msgToServer;
+	krgnodemask_t nodes;
+
+	BUG_ON(sender < 0 || sender > KERRIGHED_MAX_NODES);
+
+	msgToServer.ns_id = set->ns->id;
+	msgToServer.set_id = set->id;
+	msgToServer.objid = objid;
+	msgToServer.reply_node = sender;
+
+	DUP2_SET(COPYSET(obj_entry), &nodes);
+	krgnode_clear(kerrighed_node_id, nodes);
+	krgnode_clear(sender, nodes);
+
+	rpc_async_m(REQ_OBJECT_INVALID, &nodes,
+		    &msgToServer, sizeof(msg_server_t));
+
+	return;
+}
+
+
+
+/** Send object remove requests.
+ *  @author Renaud Lottiaux
+ *
+ *  DO NOT invalidate the local copy and the sender copy.
+ *
+ *  @param set        Set the object to invalidate belong to.
+ *  @param objid      Id of the object to invalidate.
+ *  @param sender     The node which initiated the object invalidation
+ *                    (i.e. the node who want a write copy of the object)
+ */
+int request_copies_remove(struct kddm_set * set,
+			  struct kddm_obj *obj_entry,
+			  objid_t objid,
+			  kerrighed_node_t sender)
+{
+	int need_wait = 0;
+	msg_server_t msgToServer;
+
+	BUG_ON(sender < 0 || sender > KERRIGHED_MAX_NODES);
+
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	REMOVE_FROM_SET(COPYSET(obj_entry), kerrighed_node_id);
+	REMOVE_FROM_SET(RMSET(obj_entry), kerrighed_node_id);
+	if (SET_IS_EMPTY(RMSET(obj_entry))) {
+		BUG_ON (!SET_IS_EMPTY(COPYSET(obj_entry)));
+		goto exit;
+	}
+
+	REMOVE_FROM_SET(COPYSET(obj_entry), sender);
+	REMOVE_FROM_SET(RMSET(obj_entry), sender);
+
+	if (!SET_IS_EMPTY(RMSET(obj_entry))) {
+		msgToServer.ns_id = set->ns->id;
+		msgToServer.set_id = set->id;
+		msgToServer.objid = objid;
+		msgToServer.reply_node = sender;
+
+		rpc_async_m(REQ_OBJECT_REMOVE, RMSET(obj_entry),
+			    &msgToServer, sizeof(msg_server_t));
+
+		need_wait = 1;
+	}
+
+	change_prob_owner (obj_entry, sender);
+
+exit:
+	return need_wait;
+}
+
+
+
+/** Send an object write request for the given object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry   Object entry of the object.
+ *  @param set         Set the object belong to.
+ *  @param objid       Id of the object.
+ *  @param flags       Sync / Async request, FT or not, etc...
+ */
+void request_object_on_write(struct kddm_set * set,
+			     struct kddm_obj * obj_entry,
+			     objid_t objid,
+			     int flags)
+{
+	send_msg_to_object_server(get_prob_owner(obj_entry), REQ_OBJECT_COPY,
+				  set->ns->id, set->id, objid,
+				  flags | KDDM_OBJ_COPY_ON_WRITE,
+				  kerrighed_node_id, 0);
+}
+
+
+
+/** Send an object read request for the given object.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry   Object entry of the object.
+ *  @param set         Set the object belong to.
+ *  @param objid       Id of the object.
+ *  @param flags       Sync / Async request, FT or not, etc...
+ */
+void request_object_on_read(struct kddm_set * set,
+			    struct kddm_obj * obj_entry,
+			    objid_t objid,
+			    int flags)
+{
+	send_msg_to_object_server(get_prob_owner(obj_entry), REQ_OBJECT_COPY,
+				  set->ns->id, set->id, objid,
+				  flags | KDDM_OBJ_COPY_ON_READ, 0, 0);
+}
+
+
+
+/** Send an object remove request for the given object the the object manager.
+ *  @author Renaud Lottiaux
+ *
+ *  @param obj_entry   Object entry of the object.
+ *  @param set         Set the object belong to.
+ *  @param objid       Id of the object.
+ */
+void request_objects_remove_to_mgr(struct kddm_set * set,
+				   struct kddm_obj * obj_entry,
+				   objid_t objid)
+{
+	send_msg_to_object_server(get_prob_owner(obj_entry),
+				  REQ_OBJECT_REMOVE_TO_MGR,
+				  set->ns->id, set->id, objid, 0, 0, 0);
+}
+
+
+
+/** Send an object write copy to the given node.
+ *  @author Renaud Lottiaux
+ *
+ *  @param dest_node  Node to send the write copy to.
+ *  @param set        Set the object belong to.
+ *  @param objid      Id of the object to send.
+ *  @param obj_entry  Object entry of the object to send.
+ */
+void send_copy_on_write(struct kddm_set * set,
+			struct kddm_obj * obj_entry,
+			objid_t objid,
+			kerrighed_node_t dest_node,
+			int flags)
+{
+	kddm_obj_state_t state = WRITE_OWNER;
+
+	BUG_ON (!TEST_OBJECT_LOCKED(obj_entry));
+
+	BUG_ON(dest_node < 0 || dest_node > KERRIGHED_MAX_NODES);
+	BUG_ON(object_frozen_or_pinned(obj_entry, set));
+
+	kddm_change_obj_state(set, obj_entry, objid, READ_OWNER);
+
+	change_prob_owner(obj_entry, dest_node);
+
+	send_msg_to_object_receiver(dest_node, set, objid, obj_entry, state,
+				    flags, 0);
+}
+
+struct kddm_obj *send_copy_on_write_and_inv(struct kddm_set *set,
+					    struct kddm_obj *obj_entry,
+					    objid_t objid,
+					    kerrighed_node_t dest,
+					    int flags)
+{
+	/* Unlock the path to allow sleeping function un send_copy_on_write
+	 * The object itself if still locked.
+	 */
+	kddm_obj_path_unlock(set, objid);
+
+	send_copy_on_write (set, obj_entry, objid, dest, 0);
+
+	kddm_obj_path_lock(set, objid);
+
+	kddm_invalidate_local_object_and_unlock(obj_entry, set, objid,
+						INV_COPY);
+
+	return obj_entry;
+}
+
+/** Send an object read copy to the given node.
+ *  @author Renaud Lottiaux
+ *
+ *  @param dest_node  Node to send the read copy to.
+ *  @param set        Set the object belong to.
+ *  @param objid      Id of the object to send.
+ *  @param obj_entry  Object entry of the object to send.
+ */
+int send_copy_on_read(struct kddm_set * set,
+		      struct kddm_obj * obj_entry,
+		      objid_t objid,
+		      kerrighed_node_t dest_node,
+		      int flags)
+{
+	int r ;
+	BUG_ON(dest_node < 0 || dest_node > KERRIGHED_MAX_NODES);
+
+	r = send_msg_to_object_receiver(dest_node, set, objid,
+					obj_entry, READ_COPY, flags, 0);
+
+	ADD_TO_SET(COPYSET(obj_entry), dest_node);
+	ADD_TO_SET(RMSET(obj_entry), dest_node);
+
+	return r;
+}
+
+
+
+/** Send an "no object" answer.
+ *  @author Renaud Lottiaux
+ *
+ *  @param dest_node  Node to send the message to.
+ *  @param set        Set the object belong to.
+ *  @param objid      Id of the object to send.
+ *  @param obj_entry  Object entry of the object to send.
+ */
+void send_no_object(struct kddm_set * set,
+		    struct kddm_obj * obj_entry,
+		    objid_t objid,
+		    kerrighed_node_t dest_node,
+		    int send_ownership)
+{
+	int r = 0;
+	BUG_ON(dest_node < 0 || dest_node > KERRIGHED_MAX_NODES);
+
+	if (send_ownership) {
+		r = change_prob_owner (obj_entry, dest_node);
+		kddm_change_obj_state(set, obj_entry, objid, INV_COPY);
+	}
+
+	send_msg_to_object_server(dest_node, NO_OBJECT_SEND, set->ns->id,
+				  set->id, objid, send_ownership,
+				  kerrighed_node_id, 0);
+}
+
+
+
+/** Send object write access to the given node.
+ *  @author Renaud Lottiaux
+ *
+ *  @param dest_node  Node to give object write access to.
+ *  @param set        Set the object belong to.
+ *  @param objid      Id of the object to send write access.
+ *  @param obj_entry  Object entry of the object to send the write access.
+ */
+void transfer_write_access_and_unlock(struct kddm_set * set,
+				      struct kddm_obj * obj_entry,
+				      objid_t objid,
+				      kerrighed_node_t dest_node,
+				      masterObj_t * master_info)
+{
+	msg_injection_t msg;
+
+	BUG_ON(dest_node < 0 || dest_node > KERRIGHED_MAX_NODES);
+
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	msg.ns_id = set->ns->id;
+	msg.set_id = set->id;
+	msg.objid = objid;
+	msg.req_id = 0;
+	msg.owner_info = *master_info;
+
+	if (object_frozen_or_pinned(obj_entry, set)) {
+		queue_event(delayed_transfer_write_access, dest_node, set,
+			    obj_entry, objid, &msg, sizeof(msg_injection_t));
+		put_kddm_obj_entry(set, obj_entry, objid);
+
+		return;
+	}
+
+	rpc_async(SEND_WRITE_ACCESS, dest_node, &msg, sizeof(msg_injection_t));
+
+	kddm_invalidate_local_object_and_unlock(obj_entry, set, objid,
+						INV_COPY);
+}
+
+
+
+int delayed_transfer_write_access(kerrighed_node_t dest_node, void *_msg)
+{
+	msg_injection_t *msg = _msg;
+	struct kddm_set *set;
+	struct kddm_obj *obj_entry;
+
+	BUG_ON(dest_node < 0 || dest_node > KERRIGHED_MAX_NODES);
+
+	obj_entry = get_kddm_obj_entry(msg->ns_id, msg->set_id, msg->objid,
+				       &set);
+	if (obj_entry == NULL)
+		return -EINVAL;
+
+	transfer_write_access_and_unlock(set, obj_entry, msg->objid, dest_node,
+					 &msg->owner_info);
+
+	return 0;
+}
+
+
+
+void merge_ack_set(krgnodemask_t *obj_set,
+		   krgnodemask_t *recv_set)
+{
+	krgnodemask_t v;
+
+	__krgnodes_xor(&v, obj_set, recv_set, KERRIGHED_MAX_NODES);
+	__krgnodes_and(obj_set, &v, recv_set, KERRIGHED_MAX_NODES);
+}
+
+
+
+/** Send an object invalidation ack to the given node.
+ *  @author Renaud Lottiaux
+ *
+ *  @param dest_node  Node to send the invalidation ack to.
+ *  @param set        Set the object belongs to.
+ *  @param objid      Id of the object.
+ */
+void send_invalidation_ack(struct kddm_set * set,
+			   objid_t objid,
+			   kerrighed_node_t dest_node)
+{
+	BUG_ON(dest_node < 0 || dest_node > KERRIGHED_MAX_NODES);
+
+	send_msg_to_object_server(dest_node, INVALIDATION_ACK, set->ns->id,
+				  set->id, objid, 0, kerrighed_node_id,
+				  0);
+}
+
+
+
+/** Send an object remove ack to the given node.
+ *  @author Renaud Lottiaux
+ *
+ *  @param dest_node  Node to send the remove ack to.
+ *  @param set        Set the object belongs to.
+ *  @param objid      Id of the object.
+ */
+void send_remove_ack(struct kddm_set * set,
+		     objid_t objid,
+		     kerrighed_node_t dest_node,
+		     int flags)
+{
+	BUG_ON(dest_node < 0 || dest_node > KERRIGHED_MAX_NODES);
+
+	send_msg_to_object_server(dest_node, REMOVE_ACK, set->ns->id, set->id,
+				  objid, flags, 0, 0);
+}
+
+
+
+/** Send an object remove ack2 to the given node.
+ *  @author Renaud Lottiaux
+ *
+ *  @param dest_node  Node to send the remove ack to.
+ *  @param set        Set the object belongs to.
+ *  @param objid      Id of the object.
+ */
+void send_remove_ack2(struct kddm_set * set,
+		      objid_t objid,
+		      kerrighed_node_t dest_node)
+{
+	msg_server_t msg_to_server;
+
+	BUG_ON(dest_node < 0 || dest_node > KERRIGHED_MAX_NODES);
+
+	msg_to_server.ns_id = set->ns->id;
+	msg_to_server.set_id = set->id;
+	msg_to_server.objid = objid;
+	msg_to_server.req_id = 0;
+
+	rpc_async(REMOVE_ACK2, dest_node,
+		  &msg_to_server, sizeof(msg_server_t));
+}
+
+
+
+/** Send a global objects remove ack from the manager node to the given node.
+ *  @author Renaud Lottiaux
+ *
+ *  @param dest_node  Node to send the removes ack to.
+ *  @param set        Set the object belongs to.
+ *  @param objid      Id of the object.
+ */
+void send_remove_object_done(struct kddm_set * set,
+			     objid_t objid,
+			     kerrighed_node_t dest_node,
+			     krgnodemask_t *rmset)
+{
+	rm_done_msg_server_t msg;
+
+	BUG_ON(dest_node < 0 || dest_node > KERRIGHED_MAX_NODES);
+
+	msg.ns_id = set->ns->id;
+	msg.set_id = set->id;
+	msg.objid = objid;
+	msg.req_id = 0;
+
+	DUP2_SET(rmset, &msg.rmset);
+
+	rpc_async(REMOVE_DONE, dest_node, &msg, sizeof(rm_done_msg_server_t));
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                         OBJECT FIRST TOUCH ACTIONS                        */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+/** Do an object first touch.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set       Set the object belong to.
+ *  @param objid     Id of the object to first touch.
+ */
+int object_first_touch_no_wakeup(struct kddm_set * set,
+				 struct kddm_obj * obj_entry,
+				 objid_t objid,
+				 kddm_obj_state_t object_state,
+				 int flags)
+{
+	int res;
+
+	kddm_change_obj_state (set, obj_entry, objid, INV_FILLING);
+	put_kddm_obj_entry(set, obj_entry, objid);
+
+	res = kddm_io_first_touch_object(obj_entry, set, objid, flags);
+
+	kddm_obj_path_lock(set, objid);
+
+	if (res)
+		return res;
+
+	if (object_state != INV_FILLING) {
+		kddm_change_obj_state(set, obj_entry, objid, object_state);
+
+		if (object_state & KDDM_OWNER_OBJ) {
+			CLEAR_SET(COPYSET(obj_entry));
+			ADD_TO_SET(COPYSET(obj_entry), kerrighed_node_id);
+			ADD_TO_SET(RMSET(obj_entry), kerrighed_node_id);
+		}
+	}
+
+	return res;
+}
+
+
+
+/** Do an object first touch.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set       Set the object belong to.
+ *  @param objid     Id of the object to first touch.
+ */
+int object_first_touch(struct kddm_set * set,
+		       struct kddm_obj * obj_entry,
+		       objid_t objid,
+		       kddm_obj_state_t object_state,
+		       int flags)
+{
+	int res;
+
+	BUG_ON(kddm_ft_linked(set) && !I_AM_DEFAULT_OWNER(set, objid));
+
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	res = object_first_touch_no_wakeup(set, obj_entry, objid, INV_FILLING,
+					   flags);
+	if (res < 0)
+		return res;
+
+	kddm_insert_object (set, objid, obj_entry, object_state);
+
+	return res;
+}
+
+
+
+/** Send back an object first touch request to the faulting node.
+ *  @author Renaud Lottiaux
+ *
+ *  @param dest_node  Node to send the request to.
+ *  @param set        Set the object belong to.
+ *  @param objid      Id of the object to send a first touch request for.
+ *  @param req_type   Type of the first touch request (read or write).
+ *  @param obj_entry  Object entry of the object.
+ */
+void send_back_object_first_touch(struct kddm_set * set,
+				  struct kddm_obj * obj_entry,
+				  objid_t objid,
+				  kerrighed_node_t dest_node,
+				  int flags,
+				  int req_type)
+{
+	msg_server_t msgToServer;
+
+	BUG_ON(dest_node < 0 || dest_node > KERRIGHED_MAX_NODES);
+
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	msgToServer.ns_id = set->ns->id;
+	msgToServer.set_id = set->id;
+	msgToServer.objid = objid;
+	msgToServer.req_id = 0;
+	msgToServer.reply_node = kerrighed_node_id;
+	msgToServer.flags = flags;
+
+	rpc_async(req_type, dest_node,
+		  &msgToServer, sizeof(msg_server_t));
+
+	change_prob_owner(obj_entry, dest_node);
+	kddm_change_obj_state(set, obj_entry, objid, INV_COPY);
+}
+
+
+/** Change the probable owner on a given node.
+ *  @author Renaud Lottiaux
+ *
+ *  @param set        Set the object belong to.
+ *  @param objid      Id of the object to change the owner
+ *  @param dest_node  Node to send the request to.
+ *  @param new_owner  The new default owner.
+ */
+void request_change_prob_owner(struct kddm_set * set,
+			       objid_t objid,
+			       kerrighed_node_t dest_node,
+			       kerrighed_node_t new_owner)
+{
+	msg_server_t msg_to_server;
+
+	msg_to_server.ns_id = set->ns->id;
+	msg_to_server.set_id = set->id;
+	msg_to_server.objid = objid;
+	msg_to_server.new_owner = new_owner;
+
+	rpc_sync(KDDM_CHANGE_PROB_OWNER, dest_node, &msg_to_server,
+		 sizeof(msg_server_t));
+}
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                          OBJECT INJECTION ACTIONS                         */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+void send_change_ownership_req(struct kddm_set * set,
+			       struct kddm_obj * obj_entry,
+			       objid_t objid,
+			       kerrighed_node_t dest_node,
+			       masterObj_t * master_info)
+{
+	msg_injection_t changeOwnerMsg;
+
+	BUG_ON(dest_node < 0 || dest_node > KERRIGHED_MAX_NODES);
+
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	changeOwnerMsg.ns_id = set->ns->id;
+	changeOwnerMsg.set_id = set->id;
+	changeOwnerMsg.objid = objid;
+	changeOwnerMsg.reply_node = kerrighed_node_id;
+	changeOwnerMsg.owner_info = *master_info;
+
+	rpc_async(SEND_OWNERSHIP, dest_node,
+		  &changeOwnerMsg, sizeof(msg_injection_t));
+
+	kddm_change_obj_state(set, obj_entry, objid, WAIT_CHG_OWN_ACK);
+}
+
+
+
+void ack_change_object_owner(struct kddm_set * set,
+			     struct kddm_obj * obj_entry,
+			     objid_t objid,
+			     kerrighed_node_t dest_node,
+			     masterObj_t * master_info)
+{
+	msg_server_t msgToServer;
+
+	BUG_ON(dest_node < 0 || dest_node > KERRIGHED_MAX_NODES);
+
+	ASSERT_OBJ_PATH_LOCKED(set, objid);
+
+	msgToServer.ns_id = set->ns->id;
+	msgToServer.set_id = set->id;
+	msgToServer.objid = objid;
+	msgToServer.reply_node = kerrighed_node_id;
+	msgToServer.new_owner = kerrighed_node_id;
+
+	obj_entry->master_obj = *master_info;
+	kddm_change_obj_state(set, obj_entry, objid, READ_OWNER);
+
+	rpc_async(CHANGE_OWNERSHIP_ACK, dest_node,
+		  &msgToServer, sizeof (msg_server_t));
+}
+
+
+
+kerrighed_node_t choose_injection_node()
+{
+	int res = -1;
+
+	return res;
+}
+
+
+
+kerrighed_node_t choose_injection_node_in_copyset(struct kddm_obj * object)
+{
+	int i = 0, res = -1;
+
+	while (i < KERRIGHED_MAX_NODES && res == -1) {
+		if (krgnode_online(i)
+		    && i != kerrighed_node_id
+		    && NODE_IN_SET(COPYSET(object), i)) {
+			res = i;
+			break;
+		}
+		i++;
+	}
+	return res;
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                       EVENT QUEUE ACTIONS AND MANAGEMENT                  */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+typedef struct kddm_delayed_action {
+	struct list_head list;
+	struct delayed_work work;
+	queue_event_handler_t fn;
+	kerrighed_node_t sender;
+	struct kddm_set *set;
+	objid_t objid;
+	void *data;
+} kddm_delayed_action_t;
+
+static struct workqueue_struct *kddm_wq;
+
+void kddm_workqueue_handler(struct work_struct *work)
+{
+	struct kddm_delayed_action *action;
+
+	action = container_of(work, struct kddm_delayed_action, work.work);
+
+	spin_lock(&action->set->event_lock);
+	list_del(&action->list);
+	spin_unlock(&action->set->event_lock);
+
+	action->fn(action->sender, action->data);
+	kfree(action->data);
+	kmem_cache_free(kddm_da_cachep, action);
+}
+
+void flush_kddm_event(struct kddm_set *set,
+		      objid_t objid)
+{
+	struct kddm_delayed_action *action;
+
+	spin_lock(&set->event_lock);
+	list_for_each_entry(action, &set->event_list, list) {
+		if (action->objid == objid) {
+			if (cancel_delayed_work (&action->work))
+				queue_delayed_work(kddm_wq, &action->work, 0);
+		}
+	}
+	spin_unlock(&set->event_lock);
+}
+
+void freeze_kddm_event(struct kddm_set *set)
+{
+	struct kddm_delayed_action *action;
+
+	spin_lock(&set->event_lock);
+	list_for_each_entry(action, &set->event_list, list)
+		cancel_delayed_work (&action->work);
+	spin_unlock(&set->event_lock);
+}
+
+void unfreeze_kddm_event(struct kddm_set *set)
+{
+	struct kddm_delayed_action *action;
+	int delay = 1;
+
+	spin_lock(&set->event_lock);
+	list_for_each_entry(action, &set->event_list, list)
+		queue_delayed_work(kddm_wq, &action->work, delay);
+	spin_unlock(&set->event_lock);
+}
+
+void queue_event(queue_event_handler_t fn,
+		 kerrighed_node_t sender,
+		 struct kddm_set *set,
+		 struct kddm_obj * obj_entry,
+		 objid_t objid,
+		 void *dataIn,
+		 size_t data_size)
+{
+	struct kddm_delayed_action *action;
+	int delay = 1;
+	void *data;
+
+	action = kmem_cache_alloc(kddm_da_cachep, GFP_ATOMIC);
+	data = kmalloc(data_size, GFP_ATOMIC);
+	memcpy(data, dataIn, data_size);
+	action->fn = fn;
+	action->sender = sender;
+	action->set = set;
+	action->objid = objid;
+	action->data = data;
+
+	INIT_DELAYED_WORK(&action->work, kddm_workqueue_handler);
+
+	spin_lock(&set->event_lock);
+	list_add_tail(&action->list, &set->event_list);
+	spin_unlock(&set->event_lock);
+
+	SET_OBJECT_PENDING(obj_entry);
+
+	queue_delayed_work(kddm_wq, &action->work, delay);
+}
+
+
+
+void start_run_queue_thread()
+{
+	kddm_da_cachep = KMEM_CACHE(kddm_delayed_action, SLAB_PANIC);
+
+	kddm_wq = create_singlethread_workqueue("kddm");
+}
+
+
+
+void stop_run_queue_thread()
+{
+	destroy_workqueue (kddm_wq);
+}
diff -ruN linux-2.6.29/kddm/protocol_action.h android_cluster/linux-2.6.29/kddm/protocol_action.h
--- linux-2.6.29/kddm/protocol_action.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kddm/protocol_action.h	2014-05-27 23:04:10.426027924 -0700
@@ -0,0 +1,167 @@
+/** Basic coherence protocol actions.
+ *  @file protocol_action.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __PROTOCOL_ACTION__
+#define __PROTOCOL_ACTION__
+
+typedef int (*queue_event_handler_t) (kerrighed_node_t sender, void* msg);
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Send object invalidation requests.
+ *  @author Renaud Lottiaux
+ */
+void request_copies_invalidation (struct kddm_set *set,
+				  struct kddm_obj *obj_entry, objid_t objid,
+                                  kerrighed_node_t sender);
+
+/** Send object remove requests.
+ *  @author Renaud Lottiaux
+ */
+int request_copies_remove (struct kddm_set * set, struct kddm_obj *obj_entry,
+			   objid_t objid, kerrighed_node_t sender);
+
+/** Send object remove request to the object manager.
+ *  @author Renaud Lottiaux
+ */
+void request_objects_remove_to_mgr (struct kddm_set * set,
+				    struct kddm_obj * obj_entry,
+				    objid_t objid);
+
+/** Send an object write request for the given object.
+ *  @author Renaud Lottiaux
+ */
+void request_object_on_write (struct kddm_set * set,
+			      struct kddm_obj *obj_entry,
+			      objid_t objid, int flags);
+
+/** Send an object read request for the given object.
+ *  @author Renaud Lottiaux
+ */
+void request_object_on_read (struct kddm_set * set, struct kddm_obj *obj_entry,
+			     objid_t objid, int flags);
+
+/** Send an object write copy to the given node.
+ *  @author Renaud Lottiaux
+ */
+void send_copy_on_write (struct kddm_set *set, struct kddm_obj *obj_entry,
+			 objid_t objid, kerrighed_node_t dest_node, int flags);
+
+struct kddm_obj *send_copy_on_write_and_inv (struct kddm_set *set,
+					     struct kddm_obj *obj_entry,
+					     objid_t objid,
+					     kerrighed_node_t dest_node,
+					     int flags);
+
+
+/** Send an object read copy to the given node.
+ *  @author Renaud Lottiaux
+ */
+int send_copy_on_read (struct kddm_set *set, struct kddm_obj *obj_entry,
+		       objid_t objid, kerrighed_node_t dest_node, int flags);
+
+/** Send a "no object" anwser to the given node.
+ *  @author Renaud Lottiaux
+ */
+void send_no_object (struct kddm_set * set, struct kddm_obj *obj_entry,
+		     objid_t objid, kerrighed_node_t dest_node,
+		     int send_ownership);
+
+/** Send object write access to the given node.
+ *  @author Renaud Lottiaux
+ */
+void transfer_write_access_and_unlock (struct kddm_set *set,
+                                       struct kddm_obj *obj_entry,
+				       objid_t objid,
+				       kerrighed_node_t dest_node,
+				       masterObj_t * master_info);
+
+void merge_ack_set(krgnodemask_t *obj_set, krgnodemask_t *recv_set);
+
+/** Send an object invalidation ack to the given node.
+ *  @author Renaud Lottiaux
+ */
+void send_invalidation_ack (struct kddm_set *set, objid_t objid,
+			    kerrighed_node_t dest_node);
+
+/** Send an object remove ack to the given node.
+ *  @author Renaud Lottiaux
+ */
+void send_remove_ack (struct kddm_set *set, objid_t objid,
+		      kerrighed_node_t dest_node, int flags);
+void send_remove_ack2 (struct kddm_set *set, objid_t objid,
+		       kerrighed_node_t dest_node);
+
+/** Send a global objects remove ack from the manager node to the given node.
+ *  @author Renaud Lottiaux
+ */
+void send_remove_object_done (struct kddm_set *set, objid_t objid,
+			      kerrighed_node_t dest_node,
+			      krgnodemask_t *rmset);
+
+
+/** Do an object first touch.
+ *  @author Renaud Lottiaux
+ */
+int object_first_touch (struct kddm_set *set, struct kddm_obj *obj_entry,
+			objid_t objid, kddm_obj_state_t objectState,
+			int flags);
+int object_first_touch_no_wakeup (struct kddm_set *set,
+				  struct kddm_obj *obj_entry,objid_t objid,
+                                  kddm_obj_state_t objectState, int flags);
+
+
+/** Send back an object first touch request to the faulting node.
+ *  @author Renaud Lottiaux
+ */
+void send_back_object_first_touch (struct kddm_set *set,
+				   struct kddm_obj * obj_entry,
+                                   objid_t objid, kerrighed_node_t dest_node,
+                                   int flags, int req_type);
+
+void send_change_ownership_req (struct kddm_set * set,
+				struct kddm_obj *obj_entry, objid_t objid,
+				kerrighed_node_t dest_node,
+                                masterObj_t * master_info);
+
+void ack_change_object_owner (struct kddm_set * set,
+                              struct kddm_obj * obj_entry, objid_t objid,
+			      kerrighed_node_t dest_node,
+			      masterObj_t * master_info);
+
+void queue_event (queue_event_handler_t event, kerrighed_node_t sender,
+		  struct kddm_set *set, struct kddm_obj * obj_entry,
+		  objid_t objid, void *dataIn, size_t data_size);
+
+void flush_kddm_event(struct kddm_set *set, objid_t objid);
+void freeze_kddm_event(struct kddm_set *set);
+void unfreeze_kddm_event(struct kddm_set *set);
+
+kerrighed_node_t choose_injection_node_in_copyset (struct kddm_obj * object);
+kerrighed_node_t choose_injection_node (void);
+
+
+int request_sync_object_and_unlock (struct kddm_set * set,
+				    struct kddm_obj *obj_entry, objid_t objid,
+				    kddm_obj_state_t new_state);
+
+
+void request_change_prob_owner(struct kddm_set * set, objid_t objid,
+			       kerrighed_node_t dest_node,
+			       kerrighed_node_t new_owner);
+
+void start_run_queue_thread (void);
+void stop_run_queue_thread (void);
+
+#endif // __PROTOCOL_ACTION__
diff -ruN linux-2.6.29/kernel/acct.c android_cluster/linux-2.6.29/kernel/acct.c
--- linux-2.6.29/kernel/acct.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/acct.c	2014-05-27 23:04:10.426027924 -0700
@@ -215,6 +215,7 @@
 static int acct_on(char *name)
 {
 	struct file *file;
+	struct vfsmount *mnt;
 	int error;
 	struct pid_namespace *ns;
 	struct bsd_acct_struct *acct = NULL;
@@ -256,11 +257,12 @@
 		acct = NULL;
 	}
 
-	mnt_pin(file->f_path.mnt);
+	mnt = file->f_path.mnt;
+	mnt_pin(mnt);
 	acct_file_reopen(ns->bacct, file, ns);
 	spin_unlock(&acct_lock);
 
-	mntput(file->f_path.mnt); /* it's pinned, now give up active reference */
+	mntput(mnt); /* it's pinned, now give up active reference */
 	kfree(acct);
 
 	return 0;
diff -ruN linux-2.6.29/kernel/auditsc.c android_cluster/linux-2.6.29/kernel/auditsc.c
--- linux-2.6.29/kernel/auditsc.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/auditsc.c	2014-05-27 23:04:10.430027841 -0700
@@ -66,6 +66,10 @@
 #include <linux/syscalls.h>
 #include <linux/inotify.h>
 #include <linux/capability.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/action.h>
+#include <kerrighed/ghost.h>
+#endif
 
 #include "audit.h"
 
@@ -1665,6 +1669,75 @@
 	p->current_state = ctx->current_state;
 }
 
+#ifdef CONFIG_KRG_EPM
+int export_audit_context(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task)
+{
+	struct audit_context *ctx = task->audit_context;
+	int err = 0;
+
+	if (ctx)
+		err = ghost_write(ghost, ctx, sizeof(*ctx));
+
+	return err;
+}
+
+int import_audit_context(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task)
+{
+	struct audit_context ctx;
+	struct audit_context *p;
+	int err = 0;
+
+	if (!task->audit_context)
+		goto out;
+
+	err = ghost_read(ghost, &ctx, sizeof(ctx));
+	if (err)
+		goto out;
+
+	task->audit_context = NULL;
+	err = audit_alloc(task);
+	if (err)
+		goto out;
+
+	p = task->audit_context;
+	if (!p)
+		goto out;
+
+	p->arch = ctx.arch;
+	p->major = ctx.major;
+	memcpy(p->argv, ctx.argv, sizeof(ctx.argv));
+	p->ctime = ctx.ctime;
+	p->dummy = ctx.dummy;
+	p->in_syscall = ctx.in_syscall;
+	/* Keep filterkey as assigned by audit_alloc() */
+	/* Keep RPC handler's pid as ppid */
+	p->ppid = current->pid;
+	p->prio = ctx.prio;
+	p->current_state = ctx.current_state;
+
+out:
+	return err;
+}
+
+void unimport_audit_context(struct task_struct *task)
+{
+	struct audit_context *ctx = task->audit_context;
+
+	if (ctx)
+		audit_free_context(ctx);
+}
+
+void free_ghost_audit_context(struct task_struct *task)
+{
+	struct audit_context *ctx = task->audit_context;
+
+	if (ctx)
+		audit_free_context(ctx);
+}
+#endif /* CONFIG_KRG_EPM */
+
 /**
  * audit_syscall_exit - deallocate audit context after a system call
  * @valid: success/failure flag
diff -ruN linux-2.6.29/kernel/cgroup.c android_cluster/linux-2.6.29/kernel/cgroup.c
--- linux-2.6.29/kernel/cgroup.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/cgroup.c	2014-06-09 18:19:41.512374049 -0700
@@ -46,6 +46,10 @@
 #include <linux/cgroupstats.h>
 #include <linux/hash.h>
 #include <linux/namei.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/ghost.h>
+#include <kerrighed/action.h>
+#endif
 
 #include <asm/atomic.h>
 
@@ -3240,3 +3244,40 @@
 	return 1;
 }
 __setup("cgroup_disable=", cgroup_disable);
+
+#ifdef CONFIG_KRG_EPM
+int export_cgroups(struct epm_action *action,
+		   ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+
+	/* TODO */
+	if (task->cgroups != &init_css_set)
+		err = -EBUSY;
+
+	return err;
+}
+
+int import_cgroups(struct epm_action *action,
+		   ghost_t *ghost, struct task_struct *task)
+{
+	/* TODO */
+	get_css_set(&init_css_set);
+	task->cgroups = &init_css_set;
+	INIT_LIST_HEAD(&task->cg_list);
+
+	return 0;
+}
+
+void unimport_cgroups(struct task_struct *task)
+{
+	/* TODO */
+	cgroup_exit(task, 0);
+}
+
+void free_ghost_cgroups(struct task_struct *ghost)
+{
+	/* TODO */
+	cgroup_exit(ghost, 0);
+}
+#endif /* CONFIG_KRG_EPM */
diff -ruN linux-2.6.29/kernel/compat.c android_cluster/linux-2.6.29/kernel/compat.c
--- linux-2.6.29/kernel/compat.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/compat.c	2014-05-27 23:04:10.430027841 -0700
@@ -25,6 +25,9 @@
 #include <linux/posix-timers.h>
 #include <linux/times.h>
 #include <linux/ptrace.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krgsyms.h>
+#endif
 
 #include <asm/uaccess.h>
 
@@ -702,6 +705,34 @@
 	return err;
 }
 
+#ifdef CONFIG_KRG_EPM
+int compat_krgsyms_register(void)
+{
+	int err;
+
+	err = krgsyms_register(KRGSYMS_COMPAT_NANOSLEEP_RESTART,
+			compat_nanosleep_restart);
+	if (err)
+		return err;
+	err = krgsyms_register(KRGSYMS_COMPAT_CLOCK_NANOSLEEP_RESTART,
+			compat_clock_nanosleep_restart);
+	if (err)
+		krgsyms_unregister(KRGSYMS_COMPAT_NANOSLEEP_RESTART);
+
+	return err;
+}
+
+int compat_krgsyms_unregister(void)
+{
+	int err;
+
+	err = krgsyms_unregister(KRGSYMS_COMPAT_CLOCK_NANOSLEEP_RESTART);
+	err = err ? err : krgsyms_unregister(KRGSYMS_COMPAT_NANOSLEEP_RESTART);
+
+	return err;
+}
+#endif /* CONFIG_KRG_EPM */
+
 /*
  * We currently only need the following fields from the sigevent
  * structure: sigev_value, sigev_signo, sig_notify and (sometimes
diff -ruN linux-2.6.29/kernel/exit.c android_cluster/linux-2.6.29/kernel/exit.c
--- linux-2.6.29/kernel/exit.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/exit.c	2014-05-27 23:04:10.434027758 -0700
@@ -48,6 +48,24 @@
 #include <linux/tracehook.h>
 #include <linux/init_task.h>
 #include <trace/sched.h>
+#ifdef CONFIG_KRG_KDDM
+#include <kddm/kddm_info.h>
+#endif
+#ifdef CONFIG_KRG_HOTPLUG
+#include <kerrighed/namespace.h>
+#endif
+#ifdef CONFIG_KRG_PROC
+#include <kerrighed/task.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/krg_exit.h>
+#endif
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/signal.h>
+#include <kerrighed/children.h>
+#endif
+#ifdef CONFIG_KRG_SCHED
+#include <kerrighed/scheduler/info.h>
+#endif
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -59,12 +77,11 @@
 DEFINE_TRACE(sched_process_exit);
 DEFINE_TRACE(sched_process_wait);
 
-static void exit_mm(struct task_struct * tsk);
+#ifndef CONFIG_KRG_MM
+static
+#endif
+void exit_mm(struct task_struct * tsk);
 
-static inline int task_detached(struct task_struct *p)
-{
-	return p->exit_signal == -1;
-}
 
 static void __unhash_process(struct task_struct *p)
 {
@@ -96,6 +113,13 @@
 	spin_lock(&sighand->siglock);
 
 	posix_cpu_timers_exit(tsk);
+#ifdef CONFIG_KRG_EPM
+	if (tsk->exit_state == EXIT_MIGRATION) {
+		BUG_ON(atomic_read(&sig->count) > 1);
+		posix_cpu_timers_exit_group(tsk);
+		sig->curr_target = NULL;
+	} else
+#endif
 	if (atomic_dec_and_test(&sig->count))
 		posix_cpu_timers_exit_group(tsk);
 	else {
@@ -144,16 +168,30 @@
 	tsk->sighand = NULL;
 	spin_unlock(&sighand->siglock);
 
+#ifdef CONFIG_KRG_EPM
+	if (tsk->exit_state == EXIT_MIGRATION)
+		krg_sighand_unpin(sighand);
+	else
+#endif
 	__cleanup_sighand(sighand);
 	clear_tsk_thread_flag(tsk,TIF_SIGPENDING);
 	if (sig) {
 		flush_sigqueue(&sig->shared_pending);
+#ifdef CONFIG_KRG_EPM
+		if (tsk->exit_state != EXIT_MIGRATION)
+#endif
 		taskstats_tgid_free(sig);
 		/*
 		 * Make sure ->signal can't go away under rq->lock,
 		 * see account_group_exec_runtime().
 		 */
 		task_rq_unlock_wait(tsk);
+#ifdef CONFIG_KRG_EPM
+		if (tsk->exit_state == EXIT_MIGRATION) {
+			krg_signal_unpin(sig);
+			return;
+		}
+#endif
 		__cleanup_signal(sig);
 	}
 }
@@ -171,7 +209,36 @@
 {
 	struct task_struct *leader;
 	int zap_leader;
+#ifdef CONFIG_KRG_EPM
+	struct signal_struct *locked_sig;
+	unsigned long locked_sighand_id;
+	int delay_notify_parent = 0;
+
+	/*
+	 * Because we may have to release the group leader at the same time and
+	 * because with KRG_EPM this may need to do blocking operations in the
+	 * context of an unhashed task (current thread), we make sure that the
+	 * task that will do the job will remain a plain task during the whole
+	 * operation.
+	 */
+	if (krg_delay_release_task(p))
+		return;
+#endif /* CONFIG_KRG_EPM */
 repeat:
+#ifdef CONFIG_KRG_SCHED
+	krg_sched_info_free(p);
+#endif
+#ifdef CONFIG_KRG_PROC
+	krg_release_task(p);
+#endif /* CONFIG_KRG_PROC */
+#ifdef CONFIG_KRG_EPM
+	locked_sig = NULL;
+	locked_sighand_id = 0;
+	if (p->exit_state != EXIT_MIGRATION) {
+		locked_sig = krg_signal_exit(p->signal);
+		locked_sighand_id = krg_sighand_exit(p->sighand);
+	}
+#endif /* CONFIG_KRG_EPM */
 	tracehook_prepare_release_task(p);
 	/* don't need to get the RCU readlock here - the process is dead and
 	 * can't be modifying its own credentials */
@@ -191,6 +258,13 @@
 	leader = p->group_leader;
 	if (leader != p && thread_group_empty(leader) && leader->exit_state == EXIT_ZOMBIE) {
 		BUG_ON(task_detached(leader));
+#ifdef CONFIG_KRG_EPM
+		if (leader->parent_children_obj) {
+			delay_notify_parent = 1;
+			leader->flags |= PF_DELAY_NOTIFY;
+			goto unlock;
+		}
+#endif
 		do_notify_parent(leader, leader->exit_signal);
 		/*
 		 * If we were the last child thread and the leader has
@@ -210,10 +284,27 @@
 			leader->exit_state = EXIT_DEAD;
 	}
 
+#ifdef CONFIG_KRG_EPM
+unlock:
+#endif
 	write_unlock_irq(&tasklist_lock);
 	release_thread(p);
+#ifdef CONFIG_KRG_EPM
+	krg_children_cleanup(p);
+	if (locked_sighand_id)
+		krg_sighand_unlock(locked_sighand_id);
+	krg_signal_unlock(locked_sig);
+#endif
 	call_rcu(&p->rcu, delayed_put_task_struct);
 
+#ifdef CONFIG_KRG_EPM
+	if (delay_notify_parent) {
+		BUG_ON(p == current);
+		delay_notify_parent = 0;
+
+		zap_leader = krg_delayed_notify_parent(leader);
+	}
+#endif
 	p = leader;
 	if (unlikely(zap_leader))
 		goto repeat;
@@ -254,6 +345,9 @@
 
 	do_each_pid_task(pgrp, PIDTYPE_PGID, p) {
 		if ((p == ignored_task) ||
+#ifdef CONFIG_KRG_EPM
+		    (p->real_parent == baby_sitter) ||
+#endif
 		    (p->exit_state && thread_group_empty(p)) ||
 		    is_global_init(p->real_parent))
 			continue;
@@ -312,6 +406,11 @@
 		 * we are, and it was the only connection outside.
 		 */
 		ignored_task = NULL;
+#ifdef CONFIG_KRG_EPM
+	if (parent == baby_sitter)
+		/* TODO: check for orphaned pgrp with remote real_parent */
+		return;
+#endif
 
 	if (task_pgrp(parent) != pgrp &&
 	    task_session(parent) == task_session(tsk) &&
@@ -336,6 +435,17 @@
  */
 static void reparent_to_kthreadd(void)
 {
+#ifdef CONFIG_KRG_EPM
+	struct children_kddm_object *parent_children_obj = NULL;
+	pid_t parent_tgid;
+
+	down_read(&kerrighed_init_sem);
+
+	if (rcu_dereference(current->parent_children_obj))
+		parent_children_obj = krg_parent_children_writelock(
+					current,
+					&parent_tgid);
+#endif /* CONFIG_KRG_EPM */
 	write_lock_irq(&tasklist_lock);
 
 	ptrace_unlink(current);
@@ -357,6 +467,15 @@
 	atomic_inc(&init_cred.usage);
 	commit_creds(&init_cred);
 	write_unlock_irq(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+	if (parent_children_obj) {
+		krg_set_child_ptraced(parent_children_obj, current, 0);
+		krg_remove_child(parent_children_obj, current);
+		krg_children_unlock(parent_children_obj);
+	}
+
+	up_read(&kerrighed_init_sem);
+#endif /* CONFIG_KRG_EPM */
 }
 
 void __set_special_pids(struct pid *pid)
@@ -678,7 +797,10 @@
  * Turn us into a lazy TLB process if we
  * aren't already..
  */
-static void exit_mm(struct task_struct * tsk)
+#ifndef CONFIG_KRG_MM
+static
+#endif
+void exit_mm(struct task_struct * tsk)
 {
 	struct mm_struct *mm = tsk->mm;
 	struct core_state *core_state;
@@ -886,8 +1008,15 @@
 static void forget_original_parent(struct task_struct *father)
 {
 	struct task_struct *p, *n, *reaper;
+#ifdef CONFIG_KRG_EPM
+	struct children_kddm_object *children_obj = NULL;
+#endif
 	LIST_HEAD(ptrace_dead);
 
+#ifdef CONFIG_KRG_EPM
+	if (rcu_dereference(father->children_obj))
+		children_obj = __krg_children_writelock(father);
+#endif
 	write_lock_irq(&tasklist_lock);
 	reaper = find_new_reaper(father);
 	/*
@@ -901,10 +1030,24 @@
 			BUG_ON(p->ptrace);
 			p->parent = p->real_parent;
 		}
+#ifdef CONFIG_KRG_EPM
+		else {
+			BUG_ON(!p->ptrace);
+			krg_ptrace_reparent_ptraced(father, p);
+		}
+#endif
 		reparent_thread(p, father);
 	}
 
 	write_unlock_irq(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+	if (children_obj) {
+		/* Reparent remote children */
+		krg_forget_original_remote_parent(father, reaper);
+		krg_children_exit(father);
+	}
+#endif
+
 	BUG_ON(!list_empty(&father->children));
 
 	ptrace_exit_finish(father, &ptrace_dead);
@@ -918,6 +1061,12 @@
 {
 	int signal;
 	void *cookie;
+#ifdef CONFIG_KRG_PROC
+	void *krg_cookie;
+#endif
+#ifdef CONFIG_KRG_EPM
+	u32 real_parent_self_exec_id;
+#endif
 
 	/*
 	 * This does two things:
@@ -930,6 +1079,9 @@
 	forget_original_parent(tsk);
 	exit_task_namespaces(tsk);
 
+#ifdef CONFIG_KRG_PROC
+	krg_cookie = krg_prepare_exit_notify(tsk);
+#endif /* CONFIG_KRG_PROC */
 	write_lock_irq(&tasklist_lock);
 	if (group_dead)
 		kill_orphaned_pgrp(tsk->group_leader, NULL);
@@ -948,11 +1100,22 @@
 	 * we have changed execution domain as these two values started
 	 * the same after a fork.
 	 */
+#ifdef CONFIG_KRG_EPM
+	/* remote parent aware version of vanilla linux check (below) */
+	real_parent_self_exec_id = krg_get_real_parent_self_exec_id(tsk,
+								    krg_cookie);
+	if (tsk->exit_signal != SIGCHLD && !task_detached(tsk) &&
+	    (tsk->parent_exec_id != real_parent_self_exec_id ||
+	     tsk->self_exec_id != tsk->parent_exec_id) &&
+	    !capable(CAP_KILL))
+		tsk->exit_signal = SIGCHLD;
+#else
 	if (tsk->exit_signal != SIGCHLD && !task_detached(tsk) &&
 	    (tsk->parent_exec_id != tsk->real_parent->self_exec_id ||
 	     tsk->self_exec_id != tsk->parent_exec_id) &&
 	    !capable(CAP_KILL))
 		tsk->exit_signal = SIGCHLD;
+#endif
 
 	signal = tracehook_notify_death(tsk, &cookie, group_dead);
 	if (signal >= 0)
@@ -967,6 +1130,14 @@
 		wake_up_process(tsk->signal->group_exit_task);
 
 	write_unlock_irq(&tasklist_lock);
+#ifdef CONFIG_KRG_PROC
+	krg_finish_exit_notify(tsk, signal, krg_cookie);
+	/*
+	 * No kerrighed structure should be accessed after this point,
+	 * since the task may have already been released by its reaper.
+	 * The exception of course is the case in which the task self-reaps.
+	 */
+#endif /* CONFIG_KRG_PROC */
 
 	tracehook_report_death(tsk, signal, cookie, group_dead);
 
@@ -1003,11 +1174,38 @@
 static inline void check_stack_usage(void) {}
 #endif
 
+#ifdef CONFIG_KRG_EPM
+static void exit_migration(struct task_struct *tsk)
+{
+	/* Not a real exit... just a migration. */
+	exit_task_namespaces(tsk);
+
+	write_lock_irq(&tasklist_lock);
+	BUG_ON(!list_empty(&tsk->children));
+	BUG_ON(!list_empty(&tsk->ptraced));
+	BUG_ON(!list_empty(&tsk->ptrace_entry));
+
+	BUG_ON(tsk->parent != baby_sitter);
+	BUG_ON(tsk->real_parent != baby_sitter);
+
+	tsk->exit_state = EXIT_MIGRATION;
+	write_unlock_irq(&tasklist_lock);
+
+	release_task(tsk);
+}
+#endif /* CONFIG_KRG_EPM */
+
+#ifdef CONFIG_KRG_EPM
+static NORET_TYPE void __do_exit(long code, bool notify)
+#else
 NORET_TYPE void do_exit(long code)
+#endif
 {
 	struct task_struct *tsk = current;
 	int group_dead;
-
+#ifdef CONFIG_KRG_MM
+	struct mm_struct *mm = NULL;
+#endif
 	profile_task_exit(tsk);
 
 	WARN_ON(atomic_read(&tsk->fs_excl));
@@ -1040,6 +1238,19 @@
 		schedule();
 	}
 
+#ifdef CONFIG_KRG_HOTPLUG
+	if (tsk->nsproxy->krg_ns && tsk == tsk->nsproxy->krg_ns->root_task) {
+		krg_ns_root_exit(tsk);
+		printk(KERN_WARNING
+		       "kerrighed: Root task exiting! Leaking zombies.\n");
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		schedule();
+	}
+#endif
+
+#ifdef CONFIG_KRG_PROC
+	down_read_non_owner(&kerrighed_init_sem);
+#endif
 	exit_signals(tsk);  /* sets PF_EXITING */
 	/*
 	 * tsk->flags are checked in the futex code to protect against
@@ -1068,9 +1279,15 @@
 
 	tsk->exit_code = code;
 	taskstats_exit(tsk, group_dead);
-
+#ifdef CONFIG_KRG_MM
+	if (tsk->mm && tsk->mm->mm_id)
+		mm = tsk->mm;
+#endif
 	exit_mm(tsk);
-
+#ifdef CONFIG_KRG_MM
+	if (mm)
+		KRGFCT(kh_mm_release)(mm, notify);
+#endif
 	if (group_dead)
 		acct_process();
 	trace_sched_process_exit(tsk);
@@ -1082,6 +1299,10 @@
 	exit_thread();
 	cgroup_exit(tsk, 1);
 
+#ifdef CONFIG_KRG_EPM
+	/* Do not kill the session when session leader only migrates */
+	if (notify)
+#endif
 	if (group_dead && tsk->signal->leader)
 		disassociate_ctty(1);
 
@@ -1090,11 +1311,20 @@
 		module_put(tsk->binfmt->module);
 
 	proc_exit_connector(tsk);
+#ifdef CONFIG_KRG_EPM
+	if (!notify)
+		exit_migration(tsk);
+	else
+#endif
 	exit_notify(tsk, group_dead);
 #ifdef CONFIG_NUMA
 	mpol_put(tsk->mempolicy);
 	tsk->mempolicy = NULL;
 #endif
+#ifdef CONFIG_KRG_KDDM
+	if (tsk->kddm_info)
+		kmem_cache_free(kddm_info_cachep, tsk->kddm_info);
+#endif
 #ifdef CONFIG_FUTEX
 	/*
 	 * This must happen late, after the PID is not
@@ -1125,6 +1355,9 @@
 	preempt_disable();
 	/* causes final put_task_struct in finish_task_switch(). */
 	tsk->state = TASK_DEAD;
+#ifdef CONFIG_KRG_PROC
+	up_read_non_owner(&kerrighed_init_sem);
+#endif
 	schedule();
 	BUG();
 	/* Avoid "noreturn function does return".  */
@@ -1132,6 +1365,22 @@
 		cpu_relax();	/* For when BUG is null */
 }
 
+#ifdef CONFIG_KRG_EPM
+NORET_TYPE void do_exit(long code)
+{
+	__do_exit(code, true);
+	/* Avoid "noreturn function does return".  */
+	for (;;);
+}
+
+NORET_TYPE void do_exit_wo_notify(long code)
+{
+	__do_exit(code, false);
+	/* Avoid "noreturn function does return".  */
+	for (;;);
+}
+#endif /* CONFIG_KRG_EPM */
+
 EXPORT_SYMBOL_GPL(do_exit);
 
 NORET_TYPE void complete_and_exit(struct completion *comp, long code)
@@ -1259,7 +1508,10 @@
  * the lock and this task is uninteresting.  If we return nonzero, we have
  * released the lock and the system call should return.
  */
-static int wait_task_zombie(struct task_struct *p, int options,
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+int wait_task_zombie(struct task_struct *p, int options,
 			    struct siginfo __user *infop,
 			    int __user *stat_addr, struct rusage __user *ru)
 {
@@ -1277,6 +1529,11 @@
 
 		get_task_struct(p);
 		read_unlock(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+		/* If caller is remote, current has no children object. */
+		if (current->children_obj)
+			krg_children_unlock(current->children_obj);
+#endif
 		if ((exit_code & 0x7f) == 0) {
 			why = CLD_EXITED;
 			status = exit_code >> 8;
@@ -1288,6 +1545,11 @@
 					   status, infop, ru);
 	}
 
+#ifdef CONFIG_KRG_EPM
+	/* Do not reap it yet, krg_delayed_notify_parent() has not finished. */
+	if (p->flags & PF_DELAY_NOTIFY)
+		return 0;
+#endif
 	/*
 	 * Try to move the task's state to DEAD
 	 * only one thread is allowed to do this:
@@ -1300,6 +1562,10 @@
 
 	traced = ptrace_reparented(p);
 
+#ifdef CONFIG_KRG_EPM
+	/* remote call iff p->parent == baby_sitter */
+	if (p->parent != baby_sitter)
+#endif
 	if (likely(!traced)) {
 		struct signal_struct *psig;
 		struct signal_struct *sig;
@@ -1365,6 +1631,10 @@
 	 * thread can reap it because we set its state to EXIT_DEAD.
 	 */
 	read_unlock(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+	if (current->children_obj)
+		krg_children_unlock(current->children_obj);
+#endif
 
 	retval = ru ? getrusage(p, RUSAGE_BOTH, ru) : 0;
 	status = (p->signal->flags & SIGNAL_GROUP_EXIT)
@@ -1397,6 +1667,17 @@
 		retval = pid;
 
 	if (traced) {
+#ifdef CONFIG_KRG_EPM
+		struct children_kddm_object *parent_children_obj = NULL;
+		pid_t real_parent_tgid;
+		/* p may be set to NULL while we still need it */
+		struct task_struct *saved_p = p;
+
+		if (rcu_dereference(saved_p->parent_children_obj))
+			parent_children_obj =
+				krg_parent_children_writelock(saved_p,
+							      &real_parent_tgid);
+#endif
 		write_lock_irq(&tasklist_lock);
 		/* We dropped tasklist, ptracer could die and untrace */
 		ptrace_unlink(p);
@@ -1413,6 +1694,13 @@
 			}
 		}
 		write_unlock_irq(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+		if (parent_children_obj) {
+			krg_set_child_ptraced(parent_children_obj, saved_p, 0);
+			krg_set_child_exit_signal(parent_children_obj, saved_p);
+			krg_children_unlock(parent_children_obj);
+		}
+#endif /* CONFIG_KRG_EPM */
 	}
 	if (p != NULL)
 		release_task(p);
@@ -1475,6 +1763,10 @@
 	pid = task_pid_vnr(p);
 	why = ptrace ? CLD_TRAPPED : CLD_STOPPED;
 	read_unlock(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+	if (current->children_obj)
+		krg_children_unlock(current->children_obj);
+#endif
 
 	if (unlikely(options & WNOWAIT))
 		return wait_noreap_copyout(p, pid, uid,
@@ -1538,6 +1830,10 @@
 	pid = task_pid_vnr(p);
 	get_task_struct(p);
 	read_unlock(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+	if (current->children_obj)
+		krg_children_unlock(current->children_obj);
+#endif
 
 	if (!infop) {
 		retval = ru ? getrusage(p, RUSAGE_BOTH, ru) : 0;
@@ -1673,9 +1969,16 @@
 	return 0;
 }
 
+#ifdef CONFIG_KRG_EPM
+static
+long do_wait(enum pid_type type, struct pid *pid, pid_t upid, int options,
+	     struct siginfo __user *infop, int __user *stat_addr,
+	     struct rusage __user *ru)
+#else
 static long do_wait(enum pid_type type, struct pid *pid, int options,
 		    struct siginfo __user *infop, int __user *stat_addr,
 		    struct rusage __user *ru)
+#endif
 {
 	DECLARE_WAITQUEUE(wait, current);
 	struct task_struct *tsk;
@@ -1683,6 +1986,9 @@
 
 	trace_sched_process_wait(pid);
 
+#ifdef CONFIG_KRG_PROC
+	down_read(&kerrighed_init_sem);
+#endif
 	add_wait_queue(&current->signal->wait_chldexit,&wait);
 repeat:
 	/*
@@ -1691,9 +1997,16 @@
 	 * match our criteria, even if we are not able to reap it yet.
 	 */
 	retval = -ECHILD;
+#ifdef CONFIG_KRG_EPM
+	if (!current->children_obj)
+#endif
 	if ((type < PIDTYPE_MAX) && (!pid || hlist_empty(&pid->tasks[type])))
 		goto end;
 
+#ifdef CONFIG_KRG_EPM
+	if (current->children_obj)
+		__krg_children_readlock(current);
+#endif
 	current->state = TASK_INTERRUPTIBLE;
 	read_lock(&tasklist_lock);
 	tsk = current;
@@ -1719,11 +2032,28 @@
 		BUG_ON(tsk->signal != current->signal);
 	} while (tsk != current);
 	read_unlock(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+	if (current->children_obj) {
+		/* Try all children, even remote ones but don't wait yet */
+		/* Releases children lock */
+		int tsk_result = krg_do_wait(current->children_obj, &retval,
+					     type, upid, options,
+					     infop, stat_addr, ru);
+		if (tsk_result)
+			retval = tsk_result;
+	}
+#endif
 
 	if (!retval && !(options & WNOHANG)) {
 		retval = -ERESTARTSYS;
 		if (!signal_pending(current)) {
+#ifdef CONFIG_KRG_PROC
+			up_read(&kerrighed_init_sem);
+#endif
 			schedule();
+#ifdef CONFIG_KRG_PROC
+			down_read(&kerrighed_init_sem);
+#endif
 			goto repeat;
 		}
 	}
@@ -1731,6 +2061,9 @@
 end:
 	current->state = TASK_RUNNING;
 	remove_wait_queue(&current->signal->wait_chldexit,&wait);
+#ifdef CONFIG_KRG_PROC
+	up_read(&kerrighed_init_sem);
+#endif
 	if (infop) {
 		if (retval > 0)
 			retval = 0;
@@ -1789,7 +2122,11 @@
 
 	if (type < PIDTYPE_MAX)
 		pid = find_get_pid(upid);
+#ifdef CONFIG_KRG_EPM
+	ret = do_wait(type, pid, upid, options, infop, NULL, ru);
+#else
 	ret = do_wait(type, pid, options, infop, NULL, ru);
+#endif
 	put_pid(pid);
 
 	/* avoid REGPARM breakage on x86: */
@@ -1821,7 +2158,17 @@
 		pid = find_get_pid(upid);
 	}
 
+#ifdef CONFIG_KRG_EPM
+	if (type == PIDTYPE_PGID) {
+		if (upid == 0)
+			upid = pid_vnr(pid);
+		else /* upid < 0 */
+			upid = -upid;
+	}
+	ret = do_wait(type, pid, upid, options | WEXITED, NULL, stat_addr, ru);
+#else
 	ret = do_wait(type, pid, options | WEXITED, NULL, stat_addr, ru);
+#endif
 	put_pid(pid);
 
 	/* avoid REGPARM breakage on x86: */
diff -ruN linux-2.6.29/kernel/fork.c android_cluster/linux-2.6.29/kernel/fork.c
--- linux-2.6.29/kernel/fork.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/fork.c	2014-05-27 23:04:10.434027758 -0700
@@ -61,6 +61,24 @@
 #include <linux/proc_fs.h>
 #include <linux/blkdev.h>
 #include <trace/sched.h>
+#ifdef CONFIG_KRG_KDDM
+#include <kddm/kddm_info.h>
+#endif
+#ifdef CONFIG_KRG_HOTPLUG
+#include <kerrighed/namespace.h>
+#endif
+#ifdef CONFIG_KRG_PROC
+#include <kerrighed/task.h>
+#include <kerrighed/krginit.h>
+#endif
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/signal.h>
+#include <kerrighed/children.h>
+#include <kerrighed/application.h>
+#endif
+#ifdef CONFIG_KRG_SCHED
+#include <kerrighed/scheduler/info.h>
+#endif
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -97,7 +115,10 @@
 #ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR
 # define alloc_task_struct()	kmem_cache_alloc(task_struct_cachep, GFP_KERNEL)
 # define free_task_struct(tsk)	kmem_cache_free(task_struct_cachep, (tsk))
-static struct kmem_cache *task_struct_cachep;
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+struct kmem_cache *task_struct_cachep;
 #endif
 
 #ifndef __HAVE_ARCH_THREAD_INFO_ALLOCATOR
@@ -118,7 +139,10 @@
 #endif
 
 /* SLAB cache for signal_struct structures (tsk->signal) */
-static struct kmem_cache *signal_cachep;
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+struct kmem_cache *signal_cachep;
 
 /* SLAB cache for sighand_struct structures (tsk->sighand) */
 struct kmem_cache *sighand_cachep;
@@ -133,7 +157,10 @@
 struct kmem_cache *vm_area_cachep;
 
 /* SLAB cache for mm_struct structures (tsk->mm) */
-static struct kmem_cache *mm_cachep;
+#ifndef CONFIG_KRG_MM
+static
+#endif
+struct kmem_cache *mm_cachep;
 
 void free_task(struct task_struct *tsk)
 {
@@ -214,6 +241,9 @@
 	struct thread_info *ti;
 	int err;
 
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current)
+#endif
 	prepare_to_copy(orig);
 
 	tsk = alloc_task_struct();
@@ -258,7 +288,11 @@
 }
 
 #ifdef CONFIG_MMU
+#ifdef CONFIG_KRG_MM
+int __dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm, int anon_only)
+#else
 static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
+#endif
 {
 	struct vm_area_struct *mpnt, *tmp, **pprev;
 	struct rb_node **rb_link, *rb_parent;
@@ -288,7 +322,12 @@
 	for (mpnt = oldmm->mmap; mpnt; mpnt = mpnt->vm_next) {
 		struct file *file;
 
+#ifdef CONFIG_KRG_MM
+		if ((mpnt->vm_flags & VM_DONTCOPY)
+		    || (anon_only && !anon_vma(mpnt))) {
+#else
 		if (mpnt->vm_flags & VM_DONTCOPY) {
+#endif
 			long pages = vma_pages(mpnt);
 			mm->total_vm -= pages;
 			vm_stat_account(mm, mpnt->vm_flags, mpnt->vm_file,
@@ -353,7 +392,11 @@
 		rb_parent = &tmp->vm_rb;
 
 		mm->map_count++;
+#ifdef CONFIG_KRG_MM
+		retval = copy_page_range(mm, oldmm, mpnt, anon_only);
+#else
 		retval = copy_page_range(mm, oldmm, mpnt);
+#endif
 
 		if (tmp->vm_ops && tmp->vm_ops->open)
 			tmp->vm_ops->open(tmp);
@@ -377,6 +420,14 @@
 	goto out;
 }
 
+#ifdef CONFIG_KRG_MM
+static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
+{
+       return __dup_mmap(mm, oldmm, 0);
+}
+#endif
+
+
 static inline int mm_alloc_pgd(struct mm_struct * mm)
 {
 	mm->pgd = pgd_alloc(mm);
@@ -397,8 +448,10 @@
 
 __cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);
 
+#ifndef CONFIG_KRG_MM
 #define allocate_mm()	(kmem_cache_alloc(mm_cachep, GFP_KERNEL))
 #define free_mm(mm)	(kmem_cache_free(mm_cachep, (mm)))
+#endif
 
 static unsigned long default_dump_filter = MMF_DUMP_FILTER_DEFAULT;
 
@@ -414,8 +467,18 @@
 
 #include <linux/init_task.h>
 
-static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
+#ifndef CONFIG_KRG_MM
+static
+#endif
+struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 {
+#ifdef CONFIG_KRG_MM
+	atomic_set(&mm->mm_tasks, 1);
+#endif
+#ifdef CONFIG_KRG_EPM
+	atomic_set(&mm->mm_ltasks, 1);
+	init_rwsem(&mm->remove_sem);
+#endif
 	atomic_set(&mm->mm_users, 1);
 	atomic_set(&mm->mm_count, 1);
 	init_rwsem(&mm->mmap_sem);
@@ -423,6 +486,9 @@
 	mm->flags = (current->mm) ? current->mm->flags : default_dump_filter;
 	mm->core_state = NULL;
 	mm->nr_ptes = 0;
+#ifdef CONFIG_KRG_MM
+	mm->mm_id = 0;
+#endif
 	set_mm_counter(mm, file_rss, 0);
 	set_mm_counter(mm, anon_rss, 0);
 	spin_lock_init(&mm->page_table_lock);
@@ -489,6 +555,9 @@
 			spin_unlock(&mmlist_lock);
 		}
 		put_swap_token(mm);
+#ifdef CONFIG_KRG_EPM
+		BUG_ON(atomic_read(&mm->mm_ltasks) != 0);
+#endif
 		mmdrop(mm);
 	}
 }
@@ -549,10 +618,19 @@
 
 	/* Get rid of any cached register state */
 	deactivate_mm(tsk, mm);
+#ifdef CONFIG_KRG_EPM
+	if (mm)
+		atomic_dec(&mm->mm_ltasks);
+#endif
 
 	/* notify parent sleeping on vfork() */
 	if (vfork_done) {
 		tsk->vfork_done = NULL;
+#ifdef CONFIG_KRG_EPM
+		if (tsk->remote_vfork_done)
+			krg_vfork_done(vfork_done);
+		else
+#endif
 		complete(vfork_done);
 	}
 
@@ -617,6 +695,9 @@
 	return mm;
 
 free_pt:
+#ifdef CONFIG_KRG_EPM
+	atomic_dec(&mm->mm_ltasks);
+#endif
 	mmput(mm);
 
 fail_nomem:
@@ -653,12 +734,28 @@
 		return 0;
 
 	if (clone_flags & CLONE_VM) {
+#ifdef CONFIG_KRG_EPM
+		atomic_inc(&oldmm->mm_ltasks);
+#endif
 		atomic_inc(&oldmm->mm_users);
+#ifdef CONFIG_KRG_MM
+#ifdef CONFIG_KRG_EPM
+		/* Forking the ghost do not create a real new task. No need
+		 * to inc the mm_task counter */
+		if (!krg_current)
+#endif
+			KRGFCT(kh_mm_get)(oldmm);
+#endif
 		mm = oldmm;
 		goto good_mm;
 	}
 
 	retval = -ENOMEM;
+#ifdef CONFIG_KRG_MM
+	if (kh_copy_mm)
+		mm = kh_copy_mm(tsk, oldmm, clone_flags);
+	else
+#endif
 	mm = dup_mm(tsk);
 	if (!mm)
 		goto fail_nomem;
@@ -769,8 +866,29 @@
 {
 	struct sighand_struct *sig;
 
+#ifdef CONFIG_KRG_EPM
+	if (krg_current && !in_krg_do_fork())
+		/*
+		 * This is a process migration or restart: sighand_struct is
+		 * already setup.
+		 */
+		return 0;
+
+	if (!krg_current)
+#endif
 	if (clone_flags & CLONE_SIGHAND) {
+#ifdef CONFIG_KRG_EPM
+		sig = current->sighand;
+		if (sig->kddm_obj)
+			krg_sighand_writelock(sig->krg_objid);
+#endif
 		atomic_inc(&current->sighand->count);
+#ifdef CONFIG_KRG_EPM
+		if (sig->kddm_obj) {
+			krg_sighand_share(current);
+			krg_sighand_unlock(sig->krg_objid);
+		}
+#endif
 		return 0;
 	}
 	sig = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);
@@ -779,6 +897,14 @@
 		return -ENOMEM;
 	atomic_set(&sig->count, 1);
 	memcpy(sig->action, current->sighand->action, sizeof(sig->action));
+#ifdef CONFIG_KRG_EPM
+	/*
+	 * Too early to allocate the KDDM object, will do it once we know the
+	 * pid.
+	 */
+	sig->krg_objid = 0;
+	sig->kddm_obj = NULL;
+#endif
 	return 0;
 }
 
@@ -792,7 +918,10 @@
 /*
  * Initialize POSIX timer handling for a thread group.
  */
-static void posix_cpu_timers_init_group(struct signal_struct *sig)
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+void posix_cpu_timers_init_group(struct signal_struct *sig)
 {
 	/* Thread group counters. */
 	thread_group_cputime_init(sig);
@@ -818,9 +947,31 @@
 {
 	struct signal_struct *sig;
 
+#ifdef CONFIG_KRG_EPM
+	if (krg_current && !in_krg_do_fork()) {
+		/*
+		 * This is a process migration or restart: signal_struct is
+		 * already setup.
+		 */
+		tsk->signal->curr_target = tsk;
+		return 0;
+	}
+
+	if (!krg_current)
+#endif
 	if (clone_flags & CLONE_THREAD) {
+#ifdef CONFIG_KRG_EPM
+		if (current->signal->kddm_obj)
+			krg_signal_writelock(current->signal);
+#endif
 		atomic_inc(&current->signal->count);
 		atomic_inc(&current->signal->live);
+#ifdef CONFIG_KRG_EPM
+		if (current->signal->kddm_obj) {
+			krg_signal_share(current->signal);
+			krg_signal_unlock(current->signal);
+		}
+#endif
 		return 0;
 	}
 	sig = kmem_cache_alloc(signal_cachep, GFP_KERNEL);
@@ -861,14 +1012,28 @@
 	sig->sum_sched_runtime = 0;
 	taskstats_tgid_init(sig);
 
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current)
+#endif
 	task_lock(current->group_leader);
 	memcpy(sig->rlim, current->signal->rlim, sizeof sig->rlim);
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current)
+#endif
 	task_unlock(current->group_leader);
 
 	acct_init_pacct(&sig->pacct);
 
 	tty_audit_fork(sig);
 
+#ifdef CONFIG_KRG_EPM
+	/*
+	 * Too early to allocate the KDDM object, will do it once the tgid is
+	 * known.
+	 */
+	sig->krg_objid = 0;
+	sig->kddm_obj = NULL;
+#endif
 	return 0;
 }
 
@@ -882,11 +1047,20 @@
 static void cleanup_signal(struct task_struct *tsk)
 {
 	struct signal_struct *sig = tsk->signal;
+#ifdef CONFIG_KRG_EPM
+	struct signal_struct *locked_sig;
+#endif
 
+#ifdef CONFIG_KRG_EPM
+	locked_sig = krg_signal_exit(sig);
+#endif
 	atomic_dec(&sig->live);
 
 	if (atomic_dec_and_test(&sig->count))
 		__cleanup_signal(sig);
+#ifdef CONFIG_KRG_EPM
+	krg_signal_unlock(locked_sig);
+#endif
 }
 
 static void copy_flags(unsigned long clone_flags, struct task_struct *p)
@@ -944,7 +1118,10 @@
  * parts of the process environment (as per the clone
  * flags). The actual kick-off is left to the caller.
  */
-static struct task_struct *copy_process(unsigned long clone_flags,
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+struct task_struct *copy_process(unsigned long clone_flags,
 					unsigned long stack_start,
 					struct pt_regs *regs,
 					unsigned long stack_size,
@@ -952,6 +1129,9 @@
 					struct pid *pid,
 					int trace)
 {
+#ifdef CONFIG_KRG_HOTPLUG
+	int saved_create_krg_ns;
+#endif
 	int retval;
 	struct task_struct *p;
 	int cgroup_callbacks_done = 0;
@@ -974,6 +1154,11 @@
 	if ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))
 		return ERR_PTR(-EINVAL);
 
+#ifdef CONFIG_KRG_HOTPLUG
+	saved_create_krg_ns = current->create_krg_ns;
+	current->create_krg_ns = can_create_krg_ns(clone_flags);
+#endif
+
 	retval = security_task_create(clone_flags);
 	if (retval)
 		goto fork_out;
@@ -983,6 +1168,10 @@
 	if (!p)
 		goto fork_out;
 
+#ifdef CONFIG_KRG_HOTPLUG
+	p->create_krg_ns = 0;
+#endif
+
 	rt_mutex_init_task(p);
 
 #ifdef CONFIG_PROVE_LOCKING
@@ -1017,6 +1206,9 @@
 		goto bad_fork_cleanup_put_domain;
 
 	p->did_exec = 0;
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current)
+#endif
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
 	copy_flags(clone_flags, p);
 	INIT_LIST_HEAD(&p->children);
@@ -1025,6 +1217,9 @@
 	p->rcu_read_lock_nesting = 0;
 	p->rcu_flipctr_idx = 0;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current)
+#endif
 	p->vfork_done = NULL;
 	spin_lock_init(&p->alloc_lock);
 
@@ -1046,8 +1241,14 @@
 	p->last_switch_timestamp = 0;
 #endif
 
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current || in_krg_do_fork()) {
+#endif
 	task_io_accounting_init(&p->ioac);
 	acct_clear_integrals(p);
+#ifdef CONFIG_KRG_EPM
+	}
+#endif
 
 	posix_cpu_timers_init(p);
 
@@ -1101,8 +1302,23 @@
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	sched_fork(p, clone_flags);
 
+#ifdef CONFIG_KRG_CAP
+	krg_cap_fork(p, clone_flags);
+#endif /* CONFIG_KRG_CAP */
+
+#ifdef CONFIG_KRG_KDDM
+	if (!kh_copy_kddm_info)
+		p->kddm_info = NULL;
+	else if ((retval = kh_copy_kddm_info(clone_flags, p)))
+		goto bad_fork_cleanup_policy;
+#endif /* CONFIG_KRG_KDDM */
+
 	if ((retval = audit_alloc(p)))
+#ifdef CONFIG_KRG_KDDM
+		goto bad_fork_cleanup_kddm_info;
+#else
 		goto bad_fork_cleanup_policy;
+#endif /* CONFIG_KRG_KDDM */
 	/* copy all the process information */
 	if ((retval = copy_semundo(clone_flags, p)))
 		goto bad_fork_cleanup_audit;
@@ -1124,6 +1340,9 @@
 	if (retval)
 		goto bad_fork_cleanup_io;
 
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current)
+#endif
 	if (pid != &init_struct_pid) {
 		retval = -ENOMEM;
 		pid = alloc_pid(p->nsproxy->pid_ns);
@@ -1154,6 +1373,9 @@
 	/*
 	 * Clear TID on mm_release()?
 	 */
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current || in_krg_do_fork())
+#endif
 	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr: NULL;
 #ifdef CONFIG_FUTEX
 	p->robust_list = NULL;
@@ -1166,6 +1388,9 @@
 	/*
 	 * sigaltstack should be cleared when sharing the same VM
 	 */
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current)
+#endif
 	if ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)
 		p->sas_ss_sp = p->sas_ss_size = 0;
 
@@ -1180,9 +1405,22 @@
 	clear_all_latency_tracing(p);
 
 	/* ok, now we should be set up.. */
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current) {
+#endif
 	p->exit_signal = (clone_flags & CLONE_THREAD) ? -1 : (clone_flags & CSIGNAL);
 	p->pdeath_signal = 0;
 	p->exit_state = 0;
+#ifdef CONFIG_KRG_EPM
+	} else {
+		p->exit_signal = clone_flags & CSIGNAL;
+		if (in_krg_do_fork()) {
+			/* Remote clone */
+			p->pdeath_signal = 0;
+			p->exit_state = 0;
+		}
+	}
+#endif
 
 	/*
 	 * Ok, make it visible to the rest of the system.
@@ -1197,6 +1435,39 @@
 	cgroup_fork_callbacks(p);
 	cgroup_callbacks_done = 1;
 
+#ifdef CONFIG_KRG_EPM
+	krg_sighand_alloc(p, clone_flags);
+	krg_signal_alloc(p, pid, clone_flags);
+
+	if (!krg_current) {
+		retval = krg_copy_application(p);
+		if (retval)
+			goto bad_fork_free_graph;
+	}
+
+	retval = krg_children_prepare_fork(p, pid, clone_flags);
+	if (retval)
+		goto bad_fork_cleanup_application;
+#endif
+#ifdef CONFIG_KRG_PROC
+	retval = krg_task_alloc(p, pid);
+	if (retval)
+#ifdef CONFIG_KRG_EPM
+		goto bad_fork_cleanup_children;
+#else
+		goto bad_fork_free_graph;
+#endif
+#endif
+#ifdef CONFIG_KRG_SCHED
+	retval = krg_sched_info_copy(p);
+	if (retval)
+#ifdef CONFIG_KRG_PROC
+		goto bad_fork_free_krg_task;
+#else
+		goto bad_fork_free_graph;
+#endif
+#endif /* CONFIG_KRG_SCHED */
+
 	/* Need tasklist lock for parent etc handling! */
 	write_lock_irq(&tasklist_lock);
 
@@ -1223,6 +1494,17 @@
 		p->real_parent = current;
 		p->parent_exec_id = current->self_exec_id;
 	}
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current
+	    && p->real_parent == baby_sitter && !p->parent_children_obj)
+		/*
+		 * parent died (remotely) and current is still attached to
+		 * baby_sitter. p having no parent_children_obj pointer, p must
+		 * be attached to a real local process. Fortunately this can
+		 * only be the local child reaper.
+		 */
+		p->real_parent = task_active_pid_ns(current)->child_reaper;
+#endif /* CONFIG_KRG_EPM */
 
 	spin_lock(&current->sighand->siglock);
 
@@ -1235,20 +1517,51 @@
 	 * thread can't slip out of an OOM kill (or normal SIGKILL).
  	 */
 	recalc_sigpending();
+#ifdef CONFIG_KRG_EPM
+	/* Only check if inside a remote clone() */
+	if (!krg_current || in_krg_do_fork())
+#endif
 	if (signal_pending(current)) {
 		spin_unlock(&current->sighand->siglock);
 		write_unlock_irq(&tasklist_lock);
 		retval = -ERESTARTNOINTR;
+#if defined(CONFIG_KRG_SCHED)
+		goto bad_fork_free_krg_sched;
+#elif defined(CONFIG_KRG_PROC)
+		goto bad_fork_free_krg_task;
+#else
 		goto bad_fork_free_graph;
+#endif
 	}
 
+#ifdef CONFIG_KRG_EPM
+	retval = krg_children_fork(p, pid, clone_flags);
+	if (retval) {
+		spin_unlock(&current->sighand->siglock);
+		write_unlock_irq(&tasklist_lock);
+#ifdef CONFIG_KRG_SCHED
+		goto bad_fork_free_krg_sched;
+#else
+		goto bad_fork_free_krg_task;
+#endif
+	}
+#endif /* CONFIG_KRG_EPM */
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current || !thread_group_leader(krg_current))
+#endif
 	if (clone_flags & CLONE_THREAD) {
 		p->group_leader = current->group_leader;
 		list_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);
 	}
 
 	if (likely(p->pid)) {
+#ifdef CONFIG_KRG_EPM
+		if (p->real_parent != baby_sitter)
+#endif
 		list_add_tail(&p->sibling, &p->real_parent->children);
+#ifdef CONFIG_KRG_EPM
+		attach_pid(p, PIDTYPE_PID, pid);
+#endif
 		tracehook_finish_clone(p, clone_flags, trace);
 
 		if (thread_group_leader(p)) {
@@ -1265,20 +1578,52 @@
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
 			__get_cpu_var(process_counts)++;
 		}
+#ifndef CONFIG_KRG_EPM
 		attach_pid(p, PIDTYPE_PID, pid);
+#endif
 		nr_threads++;
 	}
+#ifdef CONFIG_KRG_PROC
+	krg_task_fill(p, clone_flags);
+#endif
 
 	total_forks++;
 	spin_unlock(&current->sighand->siglock);
 	write_unlock_irq(&tasklist_lock);
+#ifdef CONFIG_KRG_PROC
+	krg_task_commit(p);
+#endif
+#ifdef CONFIG_KRG_EPM
+	krg_children_commit_fork(p);
+#endif
 	proc_fork_connector(p);
 	cgroup_post_fork(p);
+#ifdef CONFIG_KRG_HOTPLUG
+	current->create_krg_ns = saved_create_krg_ns;
+#endif
 	return p;
 
+#ifdef CONFIG_KRG_SCHED
+bad_fork_free_krg_sched:
+	krg_sched_info_free(p);
+#endif
+#ifdef CONFIG_KRG_PROC
+bad_fork_free_krg_task:
+	krg_task_abort(p);
+#endif
+#ifdef CONFIG_KRG_EPM
+bad_fork_cleanup_children:
+	krg_children_abort_fork(p);
+bad_fork_cleanup_application:
+	if (!krg_current)
+		krg_exit_application(p);
+#endif
 bad_fork_free_graph:
 	ftrace_graph_exit_task(p);
 bad_fork_free_pid:
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current)
+#endif
 	if (pid != &init_struct_pid)
 		free_pid(pid);
 bad_fork_cleanup_io:
@@ -1286,12 +1631,31 @@
 bad_fork_cleanup_namespaces:
 	exit_task_namespaces(p);
 bad_fork_cleanup_mm:
+#ifdef CONFIG_KRG_MM
+	if (p->mm && p->mm->mm_id && (clone_flags & CLONE_VM))
+#ifdef CONFIG_KRG_EPM
+		if (!krg_current)
+#endif
+			KRGFCT(kh_mm_release)(p->mm, 1);
+#endif
+#ifdef CONFIG_KRG_EPM
+	if (p->mm)
+		atomic_dec(&p->mm->mm_ltasks);
+#endif
 	if (p->mm)
 		mmput(p->mm);
 bad_fork_cleanup_signal:
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current || in_krg_do_fork())
+#endif
 	cleanup_signal(p);
 bad_fork_cleanup_sighand:
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current || in_krg_do_fork())
+		krg_sighand_cleanup(p->sighand);
+#else
 	__cleanup_sighand(p->sighand);
+#endif /* CONFIG_KRG_EPM */
 bad_fork_cleanup_fs:
 	exit_fs(p); /* blocking */
 bad_fork_cleanup_files:
@@ -1300,12 +1664,20 @@
 	exit_sem(p);
 bad_fork_cleanup_audit:
 	audit_free(p);
+#ifdef CONFIG_KRG_KDDM
+bad_fork_cleanup_kddm_info:
+	if (p->kddm_info)
+		kmem_cache_free(kddm_info_cachep, p->kddm_info);
+#endif
 bad_fork_cleanup_policy:
 #ifdef CONFIG_NUMA
 	mpol_put(p->mempolicy);
 bad_fork_cleanup_cgroup:
 #endif
 	cgroup_exit(p, cgroup_callbacks_done);
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current)
+#endif
 	delayacct_tsk_free(p);
 	if (p->binfmt)
 		module_put(p->binfmt->module);
@@ -1318,6 +1690,9 @@
 bad_fork_free:
 	free_task(p);
 fork_out:
+#ifdef CONFIG_KRG_HOTPLUG
+	current->create_krg_ns = saved_create_krg_ns;
+#endif
 	return ERR_PTR(retval);
 }
 
@@ -1395,6 +1770,20 @@
 	if (likely(user_mode(regs)))
 		trace = tracehook_prepare_clone(clone_flags);
 
+#ifdef CONFIG_KRG_EPM
+#ifdef CONFIG_KRG_CAP
+	nr = 0;
+	if (can_use_krg_cap(current, CAP_DISTANT_FORK))
+#endif
+		nr = krg_do_fork(clone_flags, stack_start, regs, stack_size,
+				 parent_tidptr, child_tidptr, trace);
+	if (nr > 0)
+		return nr;
+	/* Give a chance to local fork */
+#endif /* CONFIG_KRG_EPM */
+#ifdef CONFIG_KRG_PROC
+	down_read(&kerrighed_init_sem);
+#endif
 	p = copy_process(clone_flags, stack_start, regs, stack_size,
 			 child_tidptr, NULL, trace);
 	/*
@@ -1413,6 +1802,9 @@
 
 		if (clone_flags & CLONE_VFORK) {
 			p->vfork_done = &vfork;
+#ifdef CONFIG_KRG_EPM
+			p->remote_vfork_done = 0;
+#endif
 			init_completion(&vfork);
 		}
 
@@ -1450,6 +1842,9 @@
 	} else {
 		nr = PTR_ERR(p);
 	}
+#ifdef CONFIG_KRG_PROC
+	up_read(&kerrighed_init_sem);
+#endif
 	return nr;
 }
 
@@ -1611,9 +2006,16 @@
 	struct files_struct *fd, *new_fd = NULL;
 	struct nsproxy *new_nsproxy = NULL;
 	int do_sysvsem = 0;
+#ifdef CONFIG_KRG_HOTPLUG
+	int saved_create_krg_ns;
+#endif
 
 	check_unshare_flags(&unshare_flags);
 
+#ifdef CONFIG_KRG_HOTPLUG
+	saved_create_krg_ns = current->create_krg_ns;
+	current->create_krg_ns = 0;
+#endif
 	/* Return -EINVAL for all unsupported flags */
 	err = -EINVAL;
 	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
@@ -1703,6 +2105,9 @@
 
 bad_unshare_cleanup_thread:
 bad_unshare_out:
+#ifdef CONFIG_KRG_HOTPLUG
+	current->create_krg_ns = saved_create_krg_ns;
+#endif
 	return err;
 }
 
diff -ruN linux-2.6.29/kernel/futex.c android_cluster/linux-2.6.29/kernel/futex.c
--- linux-2.6.29/kernel/futex.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/futex.c	2014-06-09 18:19:41.628362446 -0700
@@ -55,6 +55,9 @@
 #include <linux/magic.h>
 #include <linux/pid.h>
 #include <linux/nsproxy.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krgsyms.h>
+#endif
 
 #include <asm/futex.h>
 
@@ -2012,6 +2015,18 @@
 	return do_futex(uaddr, op, val, tp, uaddr2, val2, val3);
 }
 
+#ifdef CONFIG_KRG_EPM
+int futex_krgsyms_register(void)
+{
+	return krgsyms_register(KRGSYMS_FUTEX_WAIT_RESTART, futex_wait_restart);
+}
+
+int futex_krgsyms_unregister(void)
+{
+	return krgsyms_unregister(KRGSYMS_FUTEX_WAIT_RESTART);
+}
+#endif
+
 static int __init futex_init(void)
 {
 	u32 curval;
diff -ruN linux-2.6.29/kernel/hrtimer.c android_cluster/linux-2.6.29/kernel/hrtimer.c
--- linux-2.6.29/kernel/hrtimer.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/hrtimer.c	2014-05-27 23:04:10.434027758 -0700
@@ -43,6 +43,9 @@
 #include <linux/seq_file.h>
 #include <linux/err.h>
 #include <linux/debugobjects.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krgsyms.h>
+#endif
 
 #include <asm/uaccess.h>
 
@@ -1518,6 +1521,19 @@
 	return hrtimer_nanosleep(&tu, rmtp, HRTIMER_MODE_REL, CLOCK_MONOTONIC);
 }
 
+#ifdef CONFIG_KRG_EPM
+int hrtimer_krgsyms_register(void)
+{
+	return krgsyms_register(KRGSYMS_HRTIMER_NANOSLEEP_RESTART,
+				hrtimer_nanosleep_restart);
+}
+
+int hrtimer_krgsyms_unregister(void)
+{
+	return krgsyms_unregister(KRGSYMS_HRTIMER_NANOSLEEP_RESTART);
+}
+#endif
+
 /*
  * Functions related to boot-time initialization:
  */
diff -ruN linux-2.6.29/kernel/nsproxy.c android_cluster/linux-2.6.29/kernel/nsproxy.c
--- linux-2.6.29/kernel/nsproxy.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/nsproxy.c	2014-05-27 23:04:10.442027592 -0700
@@ -21,8 +21,14 @@
 #include <linux/pid_namespace.h>
 #include <net/net_namespace.h>
 #include <linux/ipc_namespace.h>
-
-static struct kmem_cache *nsproxy_cachep;
+#ifdef CONFIG_KRG_HOTPLUG
+#include <kerrighed/namespace.h>
+#endif
+
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+struct kmem_cache *nsproxy_cachep;
 
 struct nsproxy init_nsproxy = INIT_NSPROXY(init_nsproxy);
 
@@ -86,8 +92,19 @@
 		goto out_net;
 	}
 
+#ifdef CONFIG_KRG_HOTPLUG
+	err = copy_krg_ns(tsk, new_nsp);
+	if (err)
+		goto out_krg;
+#endif
+
 	return new_nsp;
 
+#ifdef CONFIG_KRG_HOTPLUG
+out_krg:
+	if (new_nsp->net_ns)
+		put_net(new_nsp->net_ns);
+#endif
 out_net:
 	if (new_nsp->pid_ns)
 		put_pid_ns(new_nsp->pid_ns);
@@ -156,6 +173,10 @@
 
 void free_nsproxy(struct nsproxy *ns)
 {
+#ifdef CONFIG_KRG_HOTPLUG
+	if (ns->krg_ns)
+		put_krg_ns(ns->krg_ns);
+#endif
 	if (ns->mnt_ns)
 		put_mnt_ns(ns->mnt_ns);
 	if (ns->uts_ns)
diff -ruN linux-2.6.29/kernel/pid.c android_cluster/linux-2.6.29/kernel/pid.c
--- linux-2.6.29/kernel/pid.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/pid.c	2014-05-27 23:04:10.442027592 -0700
@@ -36,6 +36,10 @@
 #include <linux/pid_namespace.h>
 #include <linux/init_task.h>
 #include <linux/syscalls.h>
+#ifdef CONFIG_KRG_PROC
+#include <kerrighed/pid.h>
+#include <kerrighed/krginit.h>
+#endif
 
 #define pid_hashfn(nr, ns)	\
 	hash_long((unsigned long)nr + (unsigned long)ns, pidhash_shift)
@@ -78,6 +82,10 @@
 	.last_pid = 0,
 	.level = 0,
 	.child_reaper = &init_task,
+#ifdef CONFIG_KRG_PROC
+	.krg_ns_root = NULL,
+	.global = 0,
+#endif
 };
 EXPORT_SYMBOL_GPL(init_pid_ns);
 
@@ -112,9 +120,17 @@
 
 static  __cacheline_aligned_in_smp DEFINE_SPINLOCK(pidmap_lock);
 
+#ifdef CONFIG_KRG_EPM
+void __free_pidmap(struct upid *upid)
+#else
 static void free_pidmap(struct upid *upid)
+#endif
 {
+#ifndef CONFIG_KRG_PROC
 	int nr = upid->nr;
+#else
+	int nr = SHORT_PID(upid->nr);
+#endif
 	struct pidmap *map = upid->ns->pidmap + nr / BITS_PER_PAGE;
 	int offset = nr & BITS_PER_PAGE_MASK;
 
@@ -122,6 +138,35 @@
 	atomic_inc(&map->nr_free);
 }
 
+#ifdef CONFIG_KRG_EPM
+static void free_pidmap(struct upid *upid)
+{
+	if ((upid->nr & GLOBAL_PID_MASK)
+	    && ORIG_NODE(upid->nr) != kerrighed_node_id)
+		krg_free_pidmap(upid);
+	else
+		__free_pidmap(upid);
+}
+
+int alloc_pidmap_page(struct pidmap *map)
+{
+	void *page = kzalloc(PAGE_SIZE, GFP_KERNEL);
+	/*
+	 * Free the page if someone raced with us
+	 * installing it:
+	 */
+	spin_lock_irq(&pidmap_lock);
+	if (map->page)
+		kfree(page);
+	else
+		map->page = page;
+	spin_unlock_irq(&pidmap_lock);
+	if (unlikely(!map->page))
+		return -ENOMEM;
+	return 0;
+}
+#endif /* CONFIG_KRG_EPM */
+
 static int alloc_pidmap(struct pid_namespace *pid_ns)
 {
 	int i, offset, max_scan, pid, last = pid_ns->last_pid;
@@ -135,6 +180,9 @@
 	max_scan = (pid_max + BITS_PER_PAGE - 1)/BITS_PER_PAGE - !offset;
 	for (i = 0; i <= max_scan; ++i) {
 		if (unlikely(!map->page)) {
+#ifdef CONFIG_KRG_EPM
+			if (alloc_pidmap_page(map))
+#else
 			void *page = kzalloc(PAGE_SIZE, GFP_KERNEL);
 			/*
 			 * Free the page if someone raced with us
@@ -147,6 +195,7 @@
 				map->page = page;
 			spin_unlock_irq(&pidmap_lock);
 			if (unlikely(!map->page))
+#endif
 				break;
 		}
 		if (likely(atomic_read(&map->nr_free))) {
@@ -182,6 +231,36 @@
 	return -1;
 }
 
+#ifdef CONFIG_KRG_EPM
+int reserve_pidmap(struct pid_namespace *pid_ns, int pid)
+{
+	int offset;
+	struct pidmap *map;
+
+	pid = SHORT_PID(pid);
+	if (pid >= pid_max)
+		return -EINVAL;
+
+	offset = pid & BITS_PER_PAGE_MASK;
+	map = &pid_ns->pidmap[pid/BITS_PER_PAGE];
+	if (!map->page) {
+		/* next_pidmap() is safe if intermediate pages are missing */
+		int err = alloc_pidmap_page(map);
+		if (err)
+			return err;
+	}
+
+	/* Reserve pid in the page */
+	BUG_ON(pid != mk_pid(pid_ns, map, offset));
+	if (!test_and_set_bit(offset, map->page)) {
+		atomic_dec(&map->nr_free);
+		return 0;
+	}
+
+	return -EBUSY;
+}
+#endif /* CONFIG_KRG_EPM */
+
 int next_pidmap(struct pid_namespace *pid_ns, int last)
 {
 	int offset;
@@ -239,7 +318,11 @@
 	call_rcu(&pid->rcu, delayed_put_pid);
 }
 
+#ifdef CONFIG_KRG_EPM
+struct pid *__alloc_pid(struct pid_namespace *ns, const int *req_nr)
+#else
 struct pid *alloc_pid(struct pid_namespace *ns)
+#endif
 {
 	struct pid *pid;
 	enum pid_type type;
@@ -250,12 +333,28 @@
 	pid = kmem_cache_alloc(ns->pid_cachep, GFP_KERNEL);
 	if (!pid)
 		goto out;
+#ifdef CONFIG_KRG_EPM
+	pid->kddm_obj = NULL;
+	BUG_ON(req_nr && !is_krg_pid_ns_root(ns));
+#endif
 
 	tmp = ns;
 	for (i = ns->level; i >= 0; i--) {
+#ifdef CONFIG_KRG_EPM
+		if (req_nr && tmp == ns) {
+			nr = req_nr[i - tmp->level];
+		} else {
+#endif
 		nr = alloc_pidmap(tmp);
 		if (nr < 0)
 			goto out_free;
+#ifdef CONFIG_KRG_PROC
+		if (tmp->global && nr != 1)
+			nr = GLOBAL_PID(nr);
+#endif
+#ifdef CONFIG_KRG_EPM
+		}
+#endif
 
 		pid->numbers[i].nr = nr;
 		pid->numbers[i].ns = tmp;
@@ -267,6 +366,10 @@
 	atomic_set(&pid->count, 1);
 	for (type = 0; type < PIDTYPE_MAX; ++type)
 		INIT_HLIST_HEAD(&pid->tasks[type]);
+#ifdef CONFIG_KRG_SCHED
+	for (type = 0; type < PIDTYPE_MAX; ++type)
+		INIT_HLIST_HEAD(&pid->process_sets[type]);
+#endif
 
 	spin_lock_irq(&pidmap_lock);
 	for (i = ns->level; i >= 0; i--) {
@@ -280,6 +383,9 @@
 	return pid;
 
 out_free:
+#ifdef CONFIG_KRG_EPM
+	BUG_ON(req_nr);
+#endif
 	while (++i <= ns->level)
 		free_pidmap(pid->numbers + i);
 
@@ -305,7 +411,7 @@
 
 struct pid *find_vpid(int nr)
 {
-	return find_pid_ns(nr, current->nsproxy->pid_ns);
+	return find_pid_ns(nr, task_active_pid_ns(current));
 }
 EXPORT_SYMBOL_GPL(find_vpid);
 
@@ -339,7 +445,11 @@
 		if (!hlist_empty(&pid->tasks[tmp]))
 			return;
 
+#ifdef CONFIG_KRG_EPM
+	krg_put_pid(pid);
+#else
 	free_pid(pid);
+#endif
 }
 
 void detach_pid(struct task_struct *task, enum pid_type type)
@@ -389,7 +499,7 @@
 struct task_struct *find_task_by_vpid(pid_t vnr)
 {
 	return find_task_by_pid_type_ns(PIDTYPE_PID, vnr,
-			current->nsproxy->pid_ns);
+					task_active_pid_ns(current));
 }
 EXPORT_SYMBOL(find_task_by_vpid);
 
@@ -443,10 +553,13 @@
 	}
 	return nr;
 }
+#ifdef CONFIG_KRG_PROC
+EXPORT_SYMBOL(pid_nr_ns);
+#endif
 
 pid_t pid_vnr(struct pid *pid)
 {
-	return pid_nr_ns(pid, current->nsproxy->pid_ns);
+	return pid_nr_ns(pid, task_active_pid_ns(current));
 }
 EXPORT_SYMBOL_GPL(pid_vnr);
 
@@ -488,16 +601,57 @@
 struct pid *find_ge_pid(int nr, struct pid_namespace *ns)
 {
 	struct pid *pid;
+#ifdef CONFIG_KRG_PROC
+	int global = (nr & GLOBAL_PID_MASK) && ns->global;
+#endif
 
 	do {
+#ifdef CONFIG_KRG_PROC
+		if (global && !(nr & GLOBAL_PID_MASK))
+			nr = GLOBAL_PID(nr);
+#endif
 		pid = find_pid_ns(nr, ns);
 		if (pid)
 			break;
+#ifdef CONFIG_KRG_PROC
+		if (global) {
+			if (ORIG_NODE(nr) != kerrighed_node_id)
+				break;
+			nr = SHORT_PID(nr);
+		}
+#endif
 		nr = next_pidmap(ns, nr);
 	} while (nr > 0);
+#ifdef CONFIG_KRG_PROC
+	if (nr <= 0 && !global && ns->global)
+		return find_ge_pid(GLOBAL_PID(0), ns);
+#endif
+
+	return pid;
+}
+
+#ifdef CONFIG_KRG_PROC
+struct pid *krg_find_ge_pid(int nr, struct pid_namespace *pid_ns,
+			    struct pid_namespace *pidmap_ns)
+{
+	kerrighed_node_t node = ORIG_NODE(nr);
+	struct pid *pid;
+
+	BUG_ON(!pid_ns->global);
+	BUG_ON(!(nr & GLOBAL_PID_MASK));
+
+	do {
+		pid = find_pid_ns(nr, pid_ns);
+		if (pid)
+			break;
+		nr = next_pidmap(pidmap_ns, SHORT_PID(nr));
+		if (nr > 0)
+			nr = GLOBAL_PID_NODE(nr, node);
+	} while (nr > 0);
 
 	return pid;
 }
+#endif /* CONFIG_KRG_PROC */
 
 /*
  * The pid hash table is scaled according to the amount of memory in the
diff -ruN linux-2.6.29/kernel/pid_namespace.c android_cluster/linux-2.6.29/kernel/pid_namespace.c
--- linux-2.6.29/kernel/pid_namespace.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/pid_namespace.c	2014-05-27 23:04:10.442027592 -0700
@@ -13,6 +13,10 @@
 #include <linux/syscalls.h>
 #include <linux/err.h>
 #include <linux/acct.h>
+#ifdef CONFIG_KRG_PROC
+#include <linux/module.h>
+#include <kerrighed/namespace.h>
+#endif
 
 #define BITS_PER_PAGE		(PAGE_SIZE*8)
 
@@ -67,7 +71,10 @@
 	return NULL;
 }
 
-static struct pid_namespace *create_pid_namespace(unsigned int level)
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+struct pid_namespace *create_pid_namespace(unsigned int level)
 {
 	struct pid_namespace *ns;
 	int i;
@@ -107,6 +114,10 @@
 {
 	int i;
 
+#ifdef CONFIG_KRG_PROC
+	if (ns->krg_ns_root && ns->krg_ns_root != ns)
+		put_pid_ns(ns->krg_ns_root);
+#endif
 	for (i = 0; i < PIDMAP_ENTRIES; i++)
 		kfree(ns->pidmap[i].page);
 	kmem_cache_free(pid_ns_cachep, ns);
@@ -127,7 +138,18 @@
 
 	new_ns = create_pid_namespace(old_ns->level + 1);
 	if (!IS_ERR(new_ns))
+#ifdef CONFIG_KRG_PROC
+	{
+		new_ns->global = old_ns->global;
+		new_ns->global |= current->create_krg_ns;
+		if (old_ns->krg_ns_root)
+			get_pid_ns(old_ns->krg_ns_root);
+		new_ns->krg_ns_root = old_ns->krg_ns_root;
 		new_ns->parent = get_pid_ns(old_ns);
+	}
+#else
+		new_ns->parent = get_pid_ns(old_ns);
+#endif
 
 out_put:
 	put_pid_ns(old_ns);
@@ -147,6 +169,9 @@
 	if (parent != NULL)
 		put_pid_ns(parent);
 }
+#ifdef CONFIG_KRG_PROC
+EXPORT_SYMBOL(free_pid_ns);
+#endif
 
 void zap_pid_ns_processes(struct pid_namespace *pid_ns)
 {
@@ -183,6 +208,17 @@
 	return;
 }
 
+#ifdef CONFIG_KRG_PROC
+struct pid_namespace *find_get_krg_pid_ns(void)
+{
+	struct krg_namespace *krg_ns = find_get_krg_ns();
+	struct pid_namespace *ns = get_pid_ns(krg_ns->root_nsproxy.pid_ns);
+	put_krg_ns(krg_ns);
+	return ns;
+}
+EXPORT_SYMBOL(find_get_krg_pid_ns);
+#endif
+
 static __init int pid_namespaces_init(void)
 {
 	pid_ns_cachep = KMEM_CACHE(pid_namespace, SLAB_PANIC);
diff -ruN linux-2.6.29/kernel/posix-cpu-timers.c android_cluster/linux-2.6.29/kernel/posix-cpu-timers.c
--- linux-2.6.29/kernel/posix-cpu-timers.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/posix-cpu-timers.c	2014-06-09 18:19:41.644360846 -0700
@@ -8,6 +8,9 @@
 #include <linux/math64.h>
 #include <asm/uaccess.h>
 #include <linux/kernel_stat.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krgsyms.h>
+#endif
 
 /*
  * Called after updating RLIMIT_CPU to set timer expiration if necessary.
@@ -1682,6 +1685,19 @@
 	return -EINVAL;
 }
 
+#ifdef CONFIG_KRG_EPM
+int posix_cpu_timers_krgsyms_register(void)
+{
+	return krgsyms_register(KRGSYMS_POSIX_CPU_NSLEEP_RESTART,
+				posix_cpu_nsleep_restart);
+}
+
+int posix_cpu_timers_krgsyms_unregister(void)
+{
+	return krgsyms_unregister(KRGSYMS_POSIX_CPU_NSLEEP_RESTART);
+}
+#endif /* CONFIG_KRG_EPM */
+
 static __init int init_posix_cpu_timers(void)
 {
 	struct k_clock process = {
diff -ruN linux-2.6.29/kernel/ptrace.c android_cluster/linux-2.6.29/kernel/ptrace.c
--- linux-2.6.29/kernel/ptrace.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/ptrace.c	2014-06-09 18:19:41.704354846 -0700
@@ -25,6 +25,89 @@
 #include <asm/pgtable.h>
 #include <asm/uaccess.h>
 
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/action.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/children.h>
+#include <kerrighed/krg_exit.h>
+#endif
+
+
+#ifdef CONFIG_KRG_EPM
+/* Helpers to make ptrace and migration mutually exclusive */
+
+int krg_ptrace_link(struct task_struct *task, struct task_struct *tracer)
+{
+	struct task_struct *parent;
+	int retval;
+
+	/* Lock to-be-ptraced task on this node */
+	retval = krg_action_disable(task, EPM_MIGRATE, 0);
+	if (retval)
+		goto bad_task;
+	/* Lock tracer on this node */
+	retval = krg_action_disable(tracer, EPM_MIGRATE, 0);
+	if (retval)
+		goto bad_tracer;
+	/* Lock parent on this node */
+	retval = -EPERM;
+	parent = task->parent;
+	if (parent == baby_sitter)
+		goto bad_parent;
+	if (!is_container_init(parent) && parent != tracer) {
+		retval = krg_action_disable(parent, EPM_MIGRATE, 0);
+		if (retval)
+			goto bad_parent;
+	}
+
+	return 0;
+
+bad_parent:
+	krg_action_enable(tracer, EPM_MIGRATE, 0);
+bad_tracer:
+	krg_action_enable(task, EPM_MIGRATE, 0);
+bad_task:
+	return retval;
+}
+
+/* Assumes at least read_lock on tasklist */
+/* Called with write_lock_irq on tasklist */
+void krg_ptrace_unlink(struct task_struct *task)
+{
+	BUG_ON(task->real_parent == baby_sitter);
+	if (!is_container_init(task->real_parent)
+	    && task->real_parent != task->parent)
+		krg_action_enable(task->real_parent, EPM_MIGRATE, 0);
+	BUG_ON(task->parent == baby_sitter);
+	krg_action_enable(task->parent, EPM_MIGRATE, 0);
+	krg_action_enable(task, EPM_MIGRATE, 0);
+}
+
+/* Assumes at least read_lock on tasklist */
+/* Called with write_lock_irq on tasklist */
+void krg_ptrace_reparent_ptraced(struct task_struct *real_parent,
+				 struct task_struct *task)
+{
+	/*
+	 * We do not support that the new real parent can migrate at
+	 * all. This will not induce new limitations as long as threads can not
+	 * migrate.
+	 */
+
+	/* Not really needed as long as zombies do not migrate... */
+	krg_action_enable(real_parent, EPM_MIGRATE, 0);
+	/* new real_parent has already been assigned. */
+	BUG_ON(task->real_parent == baby_sitter);
+	if (!is_container_init(task->real_parent)
+	    && task->real_parent != task->parent) {
+		int retval;
+
+		retval = krg_action_disable(task->real_parent, EPM_MIGRATE, 0);
+		BUG_ON(retval);
+	}
+}
+ 
+#endif /* CONFIG_KRG_EPM */
 
 /*
  * Initialize a new task whose father had been ptraced.
@@ -79,6 +162,9 @@
 {
 	BUG_ON(!child->ptrace);
 
+#ifdef CONFIG_KRG_EPM
+	krg_ptrace_unlink(child);
+#endif
 	child->ptrace = 0;
 	child->parent = child->real_parent;
 	list_del_init(&child->ptrace_entry);
@@ -174,6 +260,10 @@
 
 int ptrace_attach(struct task_struct *task)
 {
+#ifdef CONFIG_KRG_EPM
+	struct children_kddm_object *parent_children_obj;
+	pid_t real_parent_tgid;
+#endif
 	int retval;
 	unsigned long flags;
 
@@ -189,6 +279,13 @@
 	retval = mutex_lock_interruptible(&current->cred_exec_mutex);
 	if (retval  < 0)
 		goto out;
+#ifdef CONFIG_KRG_EPM
+	down_read(&kerrighed_init_sem);
+	parent_children_obj = rcu_dereference(task->parent_children_obj);
+	if (parent_children_obj)
+		parent_children_obj =
+			krg_parent_children_writelock(task, &real_parent_tgid);
+#endif /* CONFIG_KRG_EPM */
 
 	retval = -EPERM;
 repeat:
@@ -218,6 +315,16 @@
 	retval = __ptrace_may_access(task, PTRACE_MODE_ATTACH);
 	if (retval)
 		goto bad;
+#ifdef CONFIG_KRG_EPM
+	retval = krg_set_child_ptraced(parent_children_obj, task, 1);
+	if (retval)
+		goto bad;
+	retval = krg_ptrace_link(task, current);
+	if (retval) {
+		krg_set_child_ptraced(parent_children_obj, task, 0);
+		goto bad;
+	}
+#endif /* CONFIG_KRG_EPM */
 
 	/* Go */
 	task->ptrace |= PT_PTRACED;
@@ -230,9 +337,15 @@
 bad:
 	write_unlock_irqrestore(&tasklist_lock, flags);
 	task_unlock(task);
+#ifdef CONFIG_KRG_EPM
+	if (parent_children_obj)
+		krg_children_unlock(parent_children_obj);
+	up_read(&kerrighed_init_sem);
+#endif /* CONFIG_KRG_EPM */
 	mutex_unlock(&current->cred_exec_mutex);
 out:
 	return retval;
+
 }
 
 static inline void __ptrace_detach(struct task_struct *child, unsigned int data)
@@ -247,6 +360,10 @@
 
 int ptrace_detach(struct task_struct *child, unsigned int data)
 {
+#ifdef CONFIG_KRG_EPM
+	struct children_kddm_object *parent_children_obj;
+	pid_t real_parent_tgid;
+#endif
 	if (!valid_signal(data))
 		return -EIO;
 
@@ -254,11 +371,30 @@
 	ptrace_disable(child);
 	clear_tsk_thread_flag(child, TIF_SYSCALL_TRACE);
 
+#ifdef CONFIG_KRG_EPM
+	down_read(&kerrighed_init_sem);
+	parent_children_obj = rcu_dereference(child->parent_children_obj);
+	if (parent_children_obj)
+		parent_children_obj =
+			krg_parent_children_writelock(child, &real_parent_tgid);
+#endif /* CONFIG_KRG_EPM */
 	write_lock_irq(&tasklist_lock);
 	/* protect against de_thread()->release_task() */
-	if (child->ptrace)
+	if (child->ptrace) {
 		__ptrace_detach(child, data);
+#ifdef CONFIG_KRG_EPM
+		krg_set_child_ptraced(parent_children_obj, child, 0);
+#endif	
+	}
 	write_unlock_irq(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+	if (parent_children_obj)
+		krg_children_unlock(parent_children_obj);
+#endif /* CONFIG_KRG_EPM */
+
+#ifdef CONFIG_KRG_EPM
+	up_read(&kerrighed_init_sem);
+#endif
 
 	return 0;
 }
@@ -508,8 +644,19 @@
  */
 int ptrace_traceme(void)
 {
+#ifdef CONFIG_KRG_EPM
+	struct children_kddm_object *parent_children_obj;
+	pid_t real_parent_tgid;
+#endif /* CONFIG_KRG_EPM */
 	int ret = -EPERM;
 
+#ifdef CONFIG_KRG_EPM
+	down_read(&kerrighed_init_sem);
+	parent_children_obj = rcu_dereference(current->parent_children_obj);
+	if (parent_children_obj)
+		parent_children_obj =
+			krg_parent_children_writelock(current, &real_parent_tgid);
+#endif /* CONFIG_KRG_EPM */
 	/*
 	 * Are we already being traced?
 	 */
@@ -528,7 +675,23 @@
 			goto repeat;
 		}
 
+#ifdef CONFIG_KRG_EPM
+		if (current->parent == baby_sitter)
+			ret = -EPERM;
+		else
+#endif
 		ret = security_ptrace_traceme(current->parent);
+#ifdef CONFIG_KRG_EPM
+		if (!ret)
+			ret = krg_set_child_ptraced(parent_children_obj,
+						    current, 1);
+		if (!ret) {
+			ret = krg_ptrace_link(current, current->parent);
+			if (ret)
+				krg_set_child_ptraced(parent_children_obj,
+						      current, 0);
+		}
+#endif /* CONFIG_KRG_EPM */
 
 		/*
 		 * Set the ptrace bit in the process ptrace flags.
@@ -538,10 +701,26 @@
 			current->ptrace |= PT_PTRACED;
 			__ptrace_link(current, current->real_parent);
 		}
+#ifdef CONFIG_KRG_EPM
+		else if (!ret) {
+			/*
+			 * Since tracer should have been real_parent, it's ok
+			 * to call krg_ptrace_unlink() without having called
+			 * __ptrace_link() before.
+			 */
+			krg_ptrace_unlink(current);
+			krg_set_child_ptraced(parent_children_obj, current, 0);
+		}
+#endif /* CONFIG_KRG_EPM */
 
 		write_unlock_irqrestore(&tasklist_lock, flags);
 	}
 	task_unlock(current);
+#ifdef CONFIG_KRG_EPM
+	if (parent_children_obj)
+		krg_children_unlock(parent_children_obj);
+	up_read(&kerrighed_init_sem);
+#endif /* CONFIG_KRG_EPM */
 	return ret;
 }
 
diff -ruN linux-2.6.29/kernel/sched.c android_cluster/linux-2.6.29/kernel/sched.c
--- linux-2.6.29/kernel/sched.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/sched.c	2014-06-09 18:19:41.848340444 -0700
@@ -73,6 +73,17 @@
 #include <linux/ctype.h>
 #include <linux/ftrace.h>
 #include <trace/sched.h>
+#ifdef CONFIG_KRG_PROC
+#include <net/krgrpc/rpc.h>
+#include <net/krgrpc/rpcid.h>
+#include <kerrighed/remote_syscall.h>
+#endif
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/ghost.h>
+#endif
+#ifdef CONFIG_KRG_SCHED
+#include <kerrighed/scheduler/hooks.h>
+#endif
 
 #include <asm/tlb.h>
 #include <asm/irq_regs.h>
@@ -2242,6 +2253,13 @@
 
 #endif /* CONFIG_SMP */
 
+#if defined(CONFIG_KRG_SCHED) && defined(CONFIG_MODULE_HOOK)
+struct module_hook_desc kmh_process_on;
+EXPORT_SYMBOL(kmh_process_on);
+struct module_hook_desc kmh_process_off;
+EXPORT_SYMBOL(kmh_process_off);
+#endif
+
 /***
  * try_to_wake_up - wake up a thread
  * @p: the to-be-woken-up thread
@@ -2344,6 +2362,9 @@
 		schedstat_inc(p, se.nr_wakeups_remote);
 	activate_task(rq, p, 1);
 	success = 1;
+#if defined(CONFIG_KRG_SCHED) && defined(CONFIG_MODULE_HOOK)
+	module_hook_call(&kmh_process_on, (unsigned long)p);
+#endif
 
 out_running:
 	trace_sched_wakeup(rq, p, success);
@@ -2381,6 +2402,9 @@
  */
 static void __sched_fork(struct task_struct *p)
 {
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current || in_krg_do_fork()) {
+#endif
 	p->se.exec_start		= 0;
 	p->se.sum_exec_runtime		= 0;
 	p->se.prev_sum_exec_runtime	= 0;
@@ -2398,6 +2422,9 @@
 	p->se.slice_max			= 0;
 	p->se.wait_max			= 0;
 #endif
+#ifdef CONFIG_KRG_EPM
+	}
+#endif
 
 	INIT_LIST_HEAD(&p->rt.run_list);
 	p->se.on_rq = 0;
@@ -2438,6 +2465,9 @@
 		p->sched_class = &fair_sched_class;
 
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
+#ifdef CONFIG_KRG_EPM
+	if (!krg_current || in_krg_do_fork())
+#endif
 	if (likely(sched_info_on()))
 		memset(&p->sched_info, 0, sizeof(p->sched_info));
 #endif
@@ -2479,6 +2509,9 @@
 		p->sched_class->task_new(rq, p);
 		inc_nr_running(rq);
 	}
+#if defined(CONFIG_KRG_SCHED) && defined(CONFIG_MODULE_HOOK)
+	module_hook_call(&kmh_process_on, (unsigned long)p);
+#endif
 	trace_sched_wakeup_new(rq, p, 1);
 	check_preempt_curr(rq, p, 0);
 #ifdef CONFIG_SMP
@@ -2711,6 +2744,9 @@
 
 	return sum;
 }
+#ifdef CONFIG_KRG_SCHED
+EXPORT_SYMBOL(nr_running);
+#endif
 
 unsigned long nr_uninterruptible(void)
 {
@@ -4549,6 +4585,14 @@
 	unsigned long *switch_count;
 	struct rq *rq;
 	int cpu;
+#ifdef CONFIG_KRG_EPM
+	struct task_struct *krg_cur;
+#endif
+ 
+#ifdef CONFIG_KRG_EPM
+	krg_cur = krg_current;
+	krg_current = NULL;
+#endif
 
 need_resched:
 	preempt_disable();
@@ -4574,7 +4618,14 @@
 		if (unlikely(signal_pending_state(prev->state, prev)))
 			prev->state = TASK_RUNNING;
 		else
+#if defined(CONFIG_KRG_SCHED) && defined(CONFIG_MODULE_HOOK)
+		{
+			module_hook_call(&kmh_process_off, (unsigned long)prev);
+#endif
 			deactivate_task(rq, prev, 1);
+#if defined(CONFIG_KRG_SCHED) && defined(CONFIG_MODULE_HOOK)
+		}
+#endif
 		switch_count = &prev->nvcsw;
 	}
 
@@ -4609,6 +4660,10 @@
 	if (unlikely(reacquire_kernel_lock(current) < 0))
 		goto need_resched_nonpreemptible;
 
+#ifdef CONFIG_KRG_EPM
+	krg_current = krg_cur;
+#endif
+
 	preempt_enable_no_resched();
 	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
 		goto need_resched;
@@ -5410,6 +5465,52 @@
 	return __sched_setscheduler(p, policy, param, false);
 }
 
+#ifdef CONFIG_KRG_PROC
+struct setscheduler_msg {
+	int policy;
+	struct sched_param param;
+};
+
+static
+int handle_sched_setscheduler(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	struct setscheduler_msg msg;
+	struct pid *pid;
+	const struct cred *old_cred;
+	struct task_struct *p;
+	int retval;
+
+	pid = krg_handle_remote_syscall_begin(desc, _msg, size,
+					      &msg, &old_cred);
+	if (IS_ERR(pid)) {
+		retval = PTR_ERR(pid);
+		goto out;
+	}
+
+	rcu_read_lock();
+	p = pid_task(pid, PIDTYPE_PID);
+	BUG_ON(!p);
+	retval = sched_setscheduler(p, msg.policy, &msg.param);
+	rcu_read_unlock();
+
+	krg_handle_remote_syscall_end(pid, old_cred);
+
+out:
+	return retval;
+}
+
+static
+int krg_sched_setscheduler(pid_t pid, int policy, struct sched_param *param)
+{
+	struct setscheduler_msg msg;
+
+	msg.policy = policy;
+	msg.param = *param;
+	return krg_remote_syscall_simple(PROC_SCHED_SETSCHEDULER, pid,
+					 &msg, sizeof(msg));
+}
+#endif /* CONFIG_KRG_PROC */
+
 static int
 do_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)
 {
@@ -5428,6 +5529,10 @@
 	if (p != NULL)
 		retval = sched_setscheduler(p, policy, &lparam);
 	rcu_read_unlock();
+#ifdef CONFIG_KRG_PROC
+	if (!p)
+		retval = krg_sched_setscheduler(pid, policy, &lparam);
+#endif
 
 	return retval;
 }
@@ -5458,6 +5563,36 @@
 	return do_sched_setscheduler(pid, -1, param);
 }
 
+#ifdef CONFIG_KRG_PROC
+static
+int handle_sched_getscheduler(struct rpc_desc *desc, void *msg, size_t size)
+{
+	struct pid *pid;
+	const struct cred *old_cred;
+	int retval;
+
+	pid = krg_handle_remote_syscall_begin(desc, msg, size,
+					      NULL, &old_cred);
+	if (IS_ERR(pid)) {
+		retval = PTR_ERR(pid);
+		goto out;
+	}
+
+	retval = sys_sched_getscheduler(pid_vnr(pid));
+
+	krg_handle_remote_syscall_end(pid, old_cred);
+
+out:
+	return retval;
+}
+
+static int krg_sched_getscheduler(pid_t pid)
+{
+	return krg_remote_syscall_simple(PROC_SCHED_GETSCHEDULER, pid,
+					 NULL, 0);
+}
+#endif /* CONFIG_KRG_PROC */
+
 /**
  * sys_sched_getscheduler - get the policy (scheduling class) of a thread
  * @pid: the pid in question.
@@ -5479,9 +5614,81 @@
 			retval = p->policy;
 	}
 	read_unlock(&tasklist_lock);
+#ifdef CONFIG_KRG_PROC
+	if (!p)
+		retval = krg_sched_getscheduler(pid);
+#endif
 	return retval;
 }
 
+#ifdef CONFIG_KRG_PROC
+static
+int handle_sched_getparam(struct rpc_desc *desc, void *msg, size_t size)
+{
+	struct pid *pid;
+	struct sched_param param;
+	const struct cred *old_cred;
+	int retval, err;
+
+	pid = krg_handle_remote_syscall_begin(desc, msg, size,
+					      NULL, &old_cred);
+	if (IS_ERR(pid)) {
+		retval = PTR_ERR(pid);
+		goto out;
+	}
+
+	retval = sys_sched_getparam(pid_vnr(pid), &param);
+	if (retval)
+		goto out_end;
+
+	err = rpc_pack_type(desc, param);
+	if (err) {
+		rpc_cancel(desc);
+		retval = err;
+	}
+
+out_end:
+	krg_handle_remote_syscall_end(pid, old_cred);
+
+out:
+	return retval;
+}
+
+static int krg_sched_getparam(pid_t pid, struct sched_param *param)
+{
+	struct rpc_desc *desc;
+	int res, r;
+
+	desc = krg_remote_syscall_begin(PROC_SCHED_GETPARAM, pid, NULL, 0);
+	if (IS_ERR(desc)) {
+		r = PTR_ERR(desc);
+		goto out;
+	}
+
+	r = rpc_unpack_type(desc, res);
+	if (r)
+		goto err_cancel;
+	r = res;
+	if (r)
+		goto out_end;
+	r = rpc_unpack_type(desc, *param);
+	if (r)
+		goto err_cancel;
+
+out_end:
+	krg_remote_syscall_end(desc, pid);
+
+out:
+	return r;
+
+err_cancel:
+	if (r > 0)
+		r = -EPIPE;
+	rpc_cancel(desc);
+	goto out_end;
+}
+#endif /* CONFIG_KRG_PROC */
+
 /**
  * sys_sched_getscheduler - get the RT priority of a thread
  * @pid: the pid in question.
@@ -5498,6 +5705,15 @@
 
 	read_lock(&tasklist_lock);
 	p = find_process_by_pid(pid);
+#ifdef CONFIG_KRG_PROC
+	if (!p) {
+		read_unlock(&tasklist_lock);
+		retval = krg_sched_getparam(pid, &lp);
+		if (retval)
+			goto out_nounlock;
+		goto copy;
+	}
+#endif
 	retval = -ESRCH;
 	if (!p)
 		goto out_unlock;
@@ -5509,11 +5725,17 @@
 	lp.sched_priority = p->rt_priority;
 	read_unlock(&tasklist_lock);
 
+#ifdef CONFIG_KRG_PROC
+copy:
+#endif
 	/*
 	 * This one might sleep, we cannot do it with a spinlock held ...
 	 */
 	retval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;
 
+#ifdef CONFIG_KRG_PROC
+out_nounlock:
+#endif
 	return retval;
 
 out_unlock:
@@ -5780,8 +6002,15 @@
  */
 void __sched yield(void)
 {
+#ifdef CONFIG_KRG_EPM
+	struct task_struct *krg_cur = krg_current;
+	krg_current = NULL;
+#endif
 	set_current_state(TASK_RUNNING);
 	sys_sched_yield();
+#ifdef CONFIG_KRG_EPM
+	krg_current = krg_cur;
+#endif
 }
 EXPORT_SYMBOL(yield);
 
@@ -5922,6 +6151,88 @@
 	return retval;
 }
 
+#ifdef CONFIG_KRG_EPM
+
+struct epm_action;
+
+struct task_sched_params {
+	int policy, rt_prio, static_prio;
+};
+
+int export_sched(struct epm_action *action,
+		 ghost_t *ghost, struct task_struct *task)
+{
+	struct task_sched_params params;
+	unsigned long flags;
+	struct rq *rq;
+	int err = 0;
+
+	rq = task_rq_lock(task, &flags);
+	params.policy = task->policy;
+	params.rt_prio = task->rt_priority;
+	params.static_prio = task->static_prio;
+	/* Group scheduling is not supported yet */
+#if defined(CONFIG_GROUP_SCHED) && !defined(CONFIG_USER_SCHED)
+	if (task_group(task) != &init_task_group)
+		err = -EPERM;
+#endif
+	task_rq_unlock(rq, &flags);
+
+	if (!err)
+		err = ghost_write(ghost, &params, sizeof(params));
+
+	return err;
+}
+
+int import_sched(struct epm_action *action,
+		 ghost_t *ghost, struct task_struct *task)
+{
+	struct task_sched_params params;
+	int cpu;
+	int err;
+
+	err = ghost_read(ghost, &params, sizeof(params));
+	if (err)
+		goto out;
+
+	/* Mostly bug-catchers inits */
+	INIT_LIST_HEAD(&task->rt.run_list);
+	task->rt.back = NULL;
+#ifdef CONFIG_RT_GROUP_SCHED
+	task->rt.parent = NULL;
+	task->rt.rt_rq = NULL;
+	task->rt.my_q = NULL;
+#endif
+	/* Checked by __setscheduler() */
+	task->se.on_rq = 0;
+	INIT_LIST_HEAD(&task->se.group_node);
+#ifdef CONFIG_FAIR_GROUP_SCHED
+	task->se.parent = NULL;
+	task->se.cfs_rq = NULL;
+	task->se.my_q = NULL;
+#endif
+	cpu = get_cpu();
+	__set_task_cpu(task, cpu);
+	put_cpu();
+
+	task->static_prio = params.static_prio;
+	__setscheduler(NULL, task, params.policy, params.rt_prio);
+
+out:
+	return err;
+}
+
+#endif /* CONFIG_KRG_EPM */
+
+#ifdef CONFIG_KRG_PROC
+void remote_sched_init(void)
+{
+	rpc_register_int(PROC_SCHED_SETSCHEDULER, handle_sched_setscheduler, 0);
+	rpc_register_int(PROC_SCHED_GETPARAM, handle_sched_getparam, 0);
+	rpc_register_int(PROC_SCHED_GETSCHEDULER, handle_sched_getscheduler, 0);
+}
+#endif /* CONFIG_KRG_PROC */
+
 static const char stat_nam[] = TASK_STATE_TO_CHAR_STR;
 
 void sched_show_task(struct task_struct *p)
@@ -6369,6 +6680,9 @@
 
 	update_rq_clock(rq);
 	activate_task(rq, p, 0);
+#if defined(CONFIG_KRG_SCHED) && defined(CONFIG_MODULE_HOOK)
+	module_hook_call(&kmh_process_on, (unsigned long)p);
+#endif
 
 	spin_unlock_irqrestore(&rq->lock, flags);
 }
diff -ruN linux-2.6.29/kernel/signal.c android_cluster/linux-2.6.29/kernel/signal.c
--- linux-2.6.29/kernel/signal.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/signal.c	2014-05-27 23:04:10.454027342 -0700
@@ -28,6 +28,21 @@
 #include <linux/pid_namespace.h>
 #include <linux/nsproxy.h>
 #include <trace/sched.h>
+#ifdef CONFIG_KRG_PROC
+#include <net/krgrpc/rpc.h>
+#include <net/krgrpc/rpcid.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/remote_cred.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/remote_syscall.h>
+#endif
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krg_exit.h>
+#include <kerrighed/action.h>
+#include <kerrighed/kerrighed_signal.h>
+#include <kerrighed/signal.h>
+#endif
 
 #include <asm/param.h>
 #include <asm/uaccess.h>
@@ -109,7 +124,10 @@
 
 #define PENDING(p,b) has_pending_signals(&(p)->signal, (b))
 
-static int recalc_sigpending_tsk(struct task_struct *t)
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+int recalc_sigpending_tsk(struct task_struct *t)
 {
 	if (t->signal->group_stop_count > 0 ||
 	    PENDING(&t->pending, &t->blocked) ||
@@ -184,7 +202,10 @@
  * - this may be called without locks if and only if t == current, otherwise an
  *   appopriate lock must be held to stop the target task from exiting
  */
-static struct sigqueue *__sigqueue_alloc(struct task_struct *t, gfp_t flags,
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+struct sigqueue *__sigqueue_alloc(struct task_struct *t, gfp_t flags,
 					 int override_rlimit)
 {
 	struct sigqueue *q = NULL;
@@ -214,7 +235,10 @@
 	return q;
 }
 
-static void __sigqueue_free(struct sigqueue *q)
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+void __sigqueue_free(struct sigqueue *q)
 {
 	if (q->flags & SIGQUEUE_PREALLOC)
 		return;
@@ -574,8 +598,14 @@
  * Bad permissions for sending the signal
  * - the caller must hold at least the RCU read lock
  */
+#ifdef CONFIG_KRG_PROC
+static int __check_kill_permission(int sig, struct siginfo *info,
+				   struct task_struct *t,
+				   struct pid_namespace *ns, pid_t session)
+#else
 static int check_kill_permission(int sig, struct siginfo *info,
 				 struct task_struct *t)
+#endif
 {
 	const struct cred *cred = current_cred(), *tcred;
 	struct pid *sid;
@@ -604,7 +634,11 @@
 			 * We don't return the error if sid == NULL. The
 			 * task was unhashed, the caller must notice this.
 			 */
+#ifdef CONFIG_KRG_PROC
+			if (!sid || pid_nr_ns(sid, ns) == session)
+#else
 			if (!sid || sid == task_session(current))
+#endif
 				break;
 		default:
 			return -EPERM;
@@ -614,6 +648,16 @@
 	return security_task_kill(t, info, sig, 0);
 }
 
+#ifdef CONFIG_KRG_PROC
+static int check_kill_permission(int sig, struct siginfo *info,
+				 struct task_struct *t)
+{
+	return __check_kill_permission(sig, info, t, &init_pid_ns,
+				       task_session_nr_ns(current,
+							  &init_pid_ns));
+}
+#endif
+
 /*
  * Handle magic process-wide effects of stop/continue signals. Unlike
  * the signal actions, these happen immediately at signal-generation
@@ -1092,13 +1136,138 @@
 	return error;
 }
 
+#ifdef CONFIG_KRG_PROC
+/* Caller guarantees that info->si_pid is 0 if from an ancestor namespace. */
+int __krg_group_send_sig_info(int sig, struct siginfo *info,
+			      struct task_struct *p)
+{
+//	return __send_signal(sig, info, p, 1, 0);
+	return send_signal(sig, info, p, 1);
+}
+
+/* Caller guarantees that p remains hashed. */
+int krg_group_send_sig_info(int sig, struct siginfo *info,
+			    struct task_struct *p,
+			    pid_t session)
+{
+	unsigned long flags;
+	int ret;
+
+	ret = __check_kill_permission(sig, info, p,
+				      task_active_pid_ns(p)->krg_ns_root,
+				      session);
+
+	if (!ret && sig) {
+		ret = -ESRCH;
+		if (lock_task_sighand(p, &flags)) {
+			ret = __krg_group_send_sig_info(sig, info, p);
+			unlock_task_sighand(p, &flags);
+		}
+	}
+
+	return ret;
+}
+
+struct kill_info_msg {
+	int sig;
+	struct siginfo info;
+	pid_t pid;
+	pid_t session;
+};
+
+static int handle_kill_proc_info(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	struct kill_info_msg msg;
+	struct pid *pid;
+	const struct cred *old_cred;
+	int retval;
+
+	pid = krg_handle_remote_syscall_begin(desc, _msg, size,
+					      &msg, &old_cred);
+	if (IS_ERR(pid)) {
+		retval = PTR_ERR(pid);
+		goto out;
+	}
+
+	rcu_read_lock();
+	retval = krg_group_send_sig_info(msg.sig, &msg.info,
+					 pid_task(pid, PIDTYPE_PID),
+					 msg.session);
+	rcu_read_unlock();
+
+	krg_handle_remote_syscall_end(pid, old_cred);
+
+out:
+	return retval;
+}
+
+static void make_kill_info_msg(struct kill_info_msg *msg, int sig,
+			       struct siginfo *info, pid_t pid)
+{
+	msg->sig = sig;
+	msg->pid = pid;
+	msg->session = task_session_knr(current);
+
+	switch ((unsigned long)info) {
+	case (unsigned long)SEND_SIG_NOINFO:
+		msg->info.si_signo = sig;
+		msg->info.si_errno = 0;
+		msg->info.si_code = SI_USER;
+		msg->info.si_pid = task_tgid_knr(current);
+		msg->info.si_uid = current_uid();
+		break;
+	case (unsigned long)SEND_SIG_PRIV:
+		msg->info.si_signo = sig;
+		msg->info.si_errno = 0;
+		msg->info.si_code = SI_KERNEL;
+		msg->info.si_pid = 0;
+		msg->info.si_uid = 0;
+		break;
+	case (unsigned long)SEND_SIG_FORCED:
+		BUG();
+	default:
+		copy_siginfo(&msg->info, info);
+		break;
+	}
+}
+
+static int krg_kill_proc_info(int sig, struct siginfo *info, pid_t pid)
+{
+	struct kill_info_msg msg;
+
+	make_kill_info_msg(&msg, sig, info, pid);
+	return krg_remote_syscall_simple(PROC_KILL_PROC_INFO, pid,
+					 &msg, sizeof(msg));
+}
+#endif /* CONFIG_KRG_PROC */
+
 int
 kill_proc_info(int sig, struct siginfo *info, pid_t pid)
 {
+#ifdef CONFIG_KRG_EPM
+	struct pid *p;
+	struct task_struct *t;
+#endif /* CONFIG_KRG_EPM */
 	int error;
 	rcu_read_lock();
+#ifdef CONFIG_KRG_EPM
+	p = find_vpid(pid);
+	t = pid_task(p, PIDTYPE_PID);
+	if (t && krg_action_block_any(t)) {
+		error = kill_pid_info(sig, info, p);
+		krg_action_unblock_any(t);
+	} else {
+		/* Try a remote syscall */
+		error = -ESRCH;
+	}
+#else /* CONFIG_KRG_EPM */
 	error = kill_pid_info(sig, info, find_vpid(pid));
+#endif /* CONFIG_KRG_EPM */
 	rcu_read_unlock();
+#ifdef CONFIG_KRG_PROC
+	if (error == -ESRCH)
+		error = krg_kill_proc_info(sig, info, pid);
+#endif /* CONFIG_KRG_PROC */
 	return error;
 }
 
@@ -1142,6 +1311,104 @@
 }
 EXPORT_SYMBOL_GPL(kill_pid_info_as_uid);
 
+#ifdef CONFIG_KRG_PROC
+static int handle_kill_pg_info(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	struct kill_info_msg *msg = _msg;
+	const struct cred *old_cred;
+	struct pid *pgrp;
+	struct task_struct *p;
+	int retval, err, success;
+
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred))
+		goto err_cancel;
+
+	read_lock(&tasklist_lock);
+
+	retval = -ESRCH;
+	pgrp = find_kpid(msg->pid);
+
+	success = 0;
+	do_each_pid_task(pgrp, PIDTYPE_PGID, p) {
+		err = krg_group_send_sig_info(msg->sig, &msg->info, p,
+					      msg->session);
+		success |= !err;
+		retval = err;
+	} while_each_pid_task(pgrp, PIDTYPE_PGID, p);
+	retval = success ? 0 : retval;
+
+	read_unlock(&tasklist_lock);
+
+	revert_creds(old_cred);
+
+	return retval;
+
+err_cancel:
+	rpc_cancel(desc);
+	return -EPIPE;
+}
+
+static int krg_kill_pg_info(int sig, struct siginfo *info, pid_t pgid)
+{
+	struct kill_info_msg msg;
+	struct rpc_desc *desc;
+	krgnodemask_t nodes;
+	kerrighed_node_t node;
+	int retval = -ESRCH;
+
+	if (!current->nsproxy->krg_ns)
+		goto out;
+
+	if (!is_krg_pid_ns_root(task_active_pid_ns(current)))
+		goto out;
+
+	if (!(pgid & GLOBAL_PID_MASK))
+		goto out;
+
+	krgnodes_copy(nodes, krgnode_online_map);
+	krgnode_clear(kerrighed_node_id, nodes);
+	if (krgnodes_empty(nodes))
+		goto out;
+
+	desc = rpc_begin_m(PROC_KILL_PG_INFO, &nodes);
+
+	make_kill_info_msg(&msg, sig, info, pgid);
+	retval = rpc_pack_type(desc, msg);
+	if (retval)
+		goto err_cancel;
+	retval = pack_creds(desc, current_cred());
+	if (retval)
+		goto err_cancel;
+
+	retval = -ESRCH;
+	for_each_krgnode_mask(node, nodes) {
+		retval = rpc_wait_return_from(desc, node);
+		if (!retval)
+			break;
+	}
+
+out_end:
+	rpc_end(desc, 0);
+
+out:
+	return retval;
+
+err_cancel:
+	rpc_cancel(desc);
+	goto out_end;
+}
+
+static void krg_kill_all(int sig, struct siginfo *info, int *count, int *retval)
+{
+	if (!current->nsproxy->krg_ns)
+		return;
+
+	if (!is_krg_pid_ns_root(task_active_pid_ns(current)))
+		return;
+}
+#endif /* CONFIG_KRG_PROC */
+
 /*
  * kill_something_info() interprets pid in interesting ways just like kill(2).
  *
@@ -1157,6 +1424,10 @@
 		rcu_read_lock();
 		ret = kill_pid_info(sig, info, find_vpid(pid));
 		rcu_read_unlock();
+#ifdef CONFIG_KRG_PROC
+		if (ret == -ESRCH)
+			ret = krg_kill_proc_info(sig, info, pid);
+#endif /* CONFIG_KRG_PROC */
 		return ret;
 	}
 
@@ -1164,6 +1435,13 @@
 	if (pid != -1) {
 		ret = __kill_pgrp_info(sig, info,
 				pid ? find_vpid(-pid) : task_pgrp(current));
+#ifdef CONFIG_KRG_PROC
+		read_unlock(&tasklist_lock);
+		if (!pid)
+			pid = -task_pgrp_vnr(current);
+		ret = krg_kill_pg_info(sig, info, -pid) ? ret : 0;
+		return ret;
+#endif /* CONFIG_KRG_PROC */
 	} else {
 		int retval = 0, count = 0;
 		struct task_struct * p;
@@ -1177,7 +1455,14 @@
 					retval = err;
 			}
 		}
+#ifdef CONFIG_KRG_PROC
+		read_unlock(&tasklist_lock);
+		krg_kill_all(sig, info, &count, &retval);
+#endif /* CONFIG_KRG_PROC */
 		ret = count ? retval : -ESRCH;
+#ifdef CONFIG_KRG_PROC
+		return ret;
+#endif
 	}
 	read_unlock(&tasklist_lock);
 
@@ -1346,6 +1631,60 @@
 	return ret;
 }
 
+#ifdef CONFIG_KRG_EPM
+int send_kerrighed_signal(int sig, struct siginfo *info, struct task_struct *t)
+{
+	struct sigqueue *q;
+	unsigned long flags;
+
+	/*
+	 * To bypass blocked/ignored signals masks as well as user tracing stuff
+	 * we open-code the core of send_signal().
+	 */
+
+	q = __sigqueue_alloc(t, GFP_ATOMIC, 1);
+	if (!q)
+		return -ENOMEM;
+
+	printk("send_kerrighed_signal: %d (%s) -> %d (%s)\n",
+	       current->pid, current->comm, t->pid, t->comm);
+
+	info->si_signo = sig;
+	info->si_code = SI_KERRIGHED;
+
+	if (!lock_task_sighand(t, &flags))
+		BUG();
+
+	list_add_tail(&q->list, &t->pending.list);
+	copy_siginfo(&q->info, info);
+
+	sigaddset(&t->pending.signal, sig);
+	signal_wake_up(t, 0);
+
+	unlock_task_sighand(t, &flags);
+
+	return 0;
+}
+
+kerrighed_handler_t *krg_handler[_NSIG];
+
+static int handle_kerrighed_signal(int sig, struct siginfo *info,
+				   struct pt_regs *regs)
+{
+	kerrighed_handler_t *kh = krg_handler[sig];
+	int released = 0;
+
+	if (kh) {
+		spin_unlock_irq(&current->sighand->siglock);
+		released = 1;
+
+		(*kh)(sig, info, regs);
+	}
+
+	return released;
+}
+#endif /* CONFIG_KRG_EPM */
+
 /*
  * Wake up any threads in the parent blocked in wait* syscalls.
  */
@@ -1383,16 +1722,20 @@
 	 * we are under tasklist_lock here so our parent is tied to
 	 * us and cannot exit and release its namespace.
 	 *
-	 * the only it can is to switch its nsproxy with sys_unshare,
-	 * bu uncharing pid namespaces is not allowed, so we'll always
-	 * see relevant namespace
+	 * The only it can is to switch its nsproxy with sys_unshare,
+	 * but we use the pid_namespace for task_pid which never changes.
 	 *
 	 * write_lock() currently calls preempt_disable() which is the
 	 * same as rcu_read_lock(), but according to Oleg, this is not
 	 * correct to rely on this
 	 */
 	rcu_read_lock();
-	info.si_pid = task_pid_nr_ns(tsk, tsk->parent->nsproxy->pid_ns);
+#ifdef CONFIG_KRG_EPM
+	if (tsk->parent == baby_sitter)
+		info.si_pid = task_pid_knr(tsk);
+	else
+#endif
+	info.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(tsk->parent));
 	info.si_uid = __task_cred(tsk)->uid;
 	rcu_read_unlock();
 
@@ -1411,6 +1754,22 @@
 		info.si_status = tsk->exit_code >> 8;
 	}
 
+#ifdef CONFIG_KRG_EPM
+	if (tsk->parent == baby_sitter) {
+		/*
+		 * Current users hold write_lock_irq(&tasklist_lock).
+		 * Ok to temporarily release the lock with use cases having
+		 * remote parent (that is neither __ptrace_detach() nor
+		 * reparent_thread()).
+		 */
+		write_unlock_irq(&tasklist_lock);
+		ret = krg_do_notify_parent(tsk, &info);
+		write_lock_irq(&tasklist_lock);
+		if (ret < 0)
+			tsk->exit_signal = -1;
+		return ret;
+	}
+#endif /* CONFIG_KRG_EPM */
 	psig = tsk->parent->sighand;
 	spin_lock_irqsave(&psig->siglock, flags);
 	if (!tsk->ptrace && sig == SIGCHLD &&
@@ -1456,6 +1815,10 @@
 		tsk = tsk->group_leader;
 		parent = tsk->real_parent;
 	}
+#ifdef CONFIG_KRG_EPM
+	if (parent == baby_sitter)
+		return;
+#endif
 
 	info.si_signo = SIGCHLD;
 	info.si_errno = 0;
@@ -1463,7 +1826,7 @@
 	 * see comment in do_notify_parent() abot the following 3 lines
 	 */
 	rcu_read_lock();
-	info.si_pid = task_pid_nr_ns(tsk, tsk->parent->nsproxy->pid_ns);
+	info.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(tsk->parent));
 	info.si_uid = __task_cred(tsk)->uid;
 	rcu_read_unlock();
 
@@ -1813,6 +2176,14 @@
 
 			if (!signr)
 				break; /* will return 0 */
+#ifdef CONFIG_KRG_EPM
+			if (info->si_code == SI_KERRIGHED) {
+				if (handle_kerrighed_signal(signr, info, regs))
+					/* It released the siglock.  */
+					goto relock;
+				continue;
+			}
+#endif
 
 			if (signr != SIGKILL) {
 				signr = ptrace_signal(signr, info,
@@ -2333,12 +2704,21 @@
 	struct task_struct *t = current;
 	struct k_sigaction *k;
 	sigset_t mask;
+#ifdef CONFIG_KRG_EPM
+	unsigned long sighand_id;
+#endif
 
 	if (!valid_signal(sig) || sig < 1 || (act && sig_kernel_only(sig)))
 		return -EINVAL;
 
 	k = &t->sighand->action[sig-1];
 
+#ifdef CONFIG_KRG_EPM
+	down_read(&kerrighed_init_sem);
+	sighand_id = current->sighand->krg_objid;
+	if (sighand_id)
+		krg_sighand_writelock(sighand_id);
+#endif
 	spin_lock_irq(&current->sighand->siglock);
 	if (oact)
 		*oact = *k;
@@ -2370,6 +2750,11 @@
 	}
 
 	spin_unlock_irq(&current->sighand->siglock);
+#ifdef CONFIG_KRG_EPM
+	if (sighand_id)
+		krg_sighand_unlock(sighand_id);
+	up_read(&kerrighed_init_sem);
+#endif
 	return 0;
 }
 
@@ -2617,6 +3002,14 @@
 	return NULL;
 }
 
+#ifdef CONFIG_KRG_PROC
+void remote_signals_init(void)
+{
+	rpc_register_int(PROC_KILL_PROC_INFO, handle_kill_proc_info, 0);
+	rpc_register_int(PROC_KILL_PG_INFO, handle_kill_pg_info, 0);
+}
+#endif /* CONFIG_KRG_PROC */
+
 void __init signals_init(void)
 {
 	sigqueue_cachep = KMEM_CACHE(sigqueue, SLAB_PANIC);
diff -ruN linux-2.6.29/kernel/sys.c android_cluster/linux-2.6.29/kernel/sys.c
--- linux-2.6.29/kernel/sys.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/sys.c	2014-05-27 23:04:10.454027342 -0700
@@ -39,6 +39,17 @@
 #include <linux/syscalls.h>
 #include <linux/kprobes.h>
 #include <linux/user_namespace.h>
+#ifdef CONFIG_KRG_PROC
+#include <net/krgrpc/rpc.h>
+#include <net/krgrpc/rpcid.h>
+#include <kerrighed/remote_syscall.h>
+#endif
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krginit.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/task.h>
+#include <kerrighed/children.h>
+#endif
 
 #include <asm/uaccess.h>
 #include <asm/io.h>
@@ -954,19 +965,29 @@
  * Auch. Had to add the 'did_exec' flag to conform completely to POSIX.
  * LBT 04.03.94
  */
+#ifdef CONFIG_KRG_EPM
+static int do_setpgid(pid_t pid, pid_t pgid, pid_t parent_session,
+		      struct pid_namespace *ns)
+#else
 SYSCALL_DEFINE2(setpgid, pid_t, pid, pid_t, pgid)
+#endif
 {
 	struct task_struct *p;
 	struct task_struct *group_leader = current->group_leader;
 	struct pid *pgrp;
+#ifdef CONFIG_KRG_EPM
+	bool from_remote_parent = parent_session >= 0;
+#endif
 	int err;
 
+#ifndef CONFIG_KRG_EPM
 	if (!pid)
 		pid = task_pid_vnr(group_leader);
 	if (!pgid)
 		pgid = pid;
 	if (pgid < 0)
 		return -EINVAL;
+#endif
 
 	/* From this point forward we keep holding onto the tasklist lock
 	 * so that our parent does not change from under us. -DaveM
@@ -974,7 +995,11 @@
 	write_lock_irq(&tasklist_lock);
 
 	err = -ESRCH;
+#ifdef CONFIG_KRG_EPM
+	p = find_task_by_pid_ns(pid, ns);
+#else
 	p = find_task_by_vpid(pid);
+#endif
 	if (!p)
 		goto out;
 
@@ -982,8 +1007,19 @@
 	if (!thread_group_leader(p))
 		goto out;
 
+#ifdef CONFIG_KRG_EPM
+	if (from_remote_parent
+	    || same_thread_group(p->real_parent, group_leader)) {
+#else
 	if (same_thread_group(p->real_parent, group_leader)) {
+#endif
 		err = -EPERM;
+#ifdef CONFIG_KRG_EPM
+		if (from_remote_parent) {
+			if (task_session_nr_ns(p, ns) != parent_session)
+				goto out;
+		} else
+#endif
 		if (task_session(p) != task_session(group_leader))
 			goto out;
 		err = -EACCES;
@@ -1003,9 +1039,17 @@
 	if (pgid != pid) {
 		struct task_struct *g;
 
+#ifdef CONFIG_KRG_EPM
+		pgrp = find_pid_ns(pgid, ns);
+#else
 		pgrp = find_vpid(pgid);
+#endif
 		g = pid_task(pgrp, PIDTYPE_PGID);
+#ifdef CONFIG_KRG_EPM
+		if (!g || task_session(g) != task_session(p))
+#else
 		if (!g || task_session(g) != task_session(group_leader))
+#endif
 			goto out;
 	}
 
@@ -1025,7 +1069,149 @@
 	return err;
 }
 
+#ifdef CONFIG_KRG_EPM
+struct setpgid_message {
+	pid_t pid;
+	pid_t pgid;
+	pid_t parent_session;
+};
+
+static
+int handle_forward_setpgid(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	const struct setpgid_message *msg = _msg;
+	struct pid_namespace *ns;
+	int retval;
+
+	ns = find_get_krg_pid_ns();
+	retval = do_setpgid(msg->pid, msg->pgid, msg->parent_session, ns);
+	put_pid_ns(ns);
+
+	return retval;
+}
+
+static int krg_forward_setpgid(kerrighed_node_t node, pid_t pid, pid_t pgid)
+{
+	struct children_kddm_object *children_obj = current->children_obj;
+	pid_t parent, real_parent;
+	struct setpgid_message msg;
+	int retval = -ESRCH;
+
+	if (__krg_get_parent(children_obj, pid, &parent, &real_parent))
+		goto out;
+
+	msg.pid = pid;
+	msg.pgid = pgid;
+	msg.parent_session = task_session_knr(current);
+
+	retval = rpc_sync(PROC_FORWARD_SETPGID, node, &msg, sizeof(msg));
+
+out:
+	return retval;
+}
+
+static
+struct children_kddm_object *
+krg_prepare_setpgid(pid_t pid, pid_t pgid, kerrighed_node_t *nodep)
+{
+	struct children_kddm_object *parent_children_obj = NULL;
+	pid_t real_parent_tgid;
+	kerrighed_node_t node = KERRIGHED_NODE_ID_NONE;
+	struct task_kddm_object *task_obj;
+	struct timespec backoff_time = {
+		.tv_sec = 1,
+		.tv_nsec = 0
+	};	/* 1 second */
+
+	down_read(&kerrighed_init_sem);
+
+	if (!current->nsproxy->krg_ns
+	    || !is_krg_pid_ns_root(task_active_pid_ns(current))
+	    || !(pid & GLOBAL_PID_MASK))
+		goto out;
+
+	if (pid == current->tgid) {
+		if (rcu_dereference(current->parent_children_obj))
+			parent_children_obj =
+				krg_parent_children_writelock(current,
+							      &real_parent_tgid);
+		goto out;
+	}
+
+	if (!rcu_dereference(current->children_obj))
+		goto out;
+
+
+	for (;;) {
+		parent_children_obj = __krg_children_writelock(current);
+		BUG_ON(!parent_children_obj);
+
+		task_obj = krg_task_readlock(pid);
+		if (!task_obj) {
+			krg_task_unlock(pid);
+			break;
+		}
+		node = task_obj->node;
+		if (node != KERRIGHED_NODE_ID_NONE)
+			break;
+
+		/* We might deadlock with migration. Back off. */
+		krg_task_unlock(pid);
+		krg_children_unlock(parent_children_obj);
+
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		schedule_timeout(timespec_to_jiffies(&backoff_time) + 1);
+	}
+
+out:
+	*nodep = node;
+	return parent_children_obj;
+}
+
+static
+void krg_cleanup_setpgid(pid_t pid, pid_t pgid,
+			 struct children_kddm_object *parent_children_obj,
+			 kerrighed_node_t node,
+			 bool success)
+{
+	if (parent_children_obj) {
+		if (node != KERRIGHED_NODE_ID_NONE)
+			krg_task_unlock(pid);
+		if (success)
+			__krg_set_child_pgid(parent_children_obj, pid, pgid);
+		krg_children_unlock(parent_children_obj);
+	}
+	up_read(&kerrighed_init_sem);
+}
+
+SYSCALL_DEFINE2(setpgid, pid_t, pid, pid_t, pgid)
+{
+	struct children_kddm_object *parent_children_obj;
+	kerrighed_node_t node;
+	int err;
+
+	if (!pid)
+		pid = task_pid_vnr(current->group_leader);
+	if (!pgid)
+		pgid = pid;
+	if (pgid < 0)
+		return -EINVAL;
+
+	parent_children_obj = krg_prepare_setpgid(pid, pgid, &node);
+	if (node != kerrighed_node_id && node != KERRIGHED_NODE_ID_NONE)
+		err = krg_forward_setpgid(node, pid, pgid);
+	else
+		err = do_setpgid(pid, pgid, -1, task_active_pid_ns(current));
+	krg_cleanup_setpgid(pid, pgid, parent_children_obj, node, !err);
+	return err;
+}
+#endif /* CONFIG_KRG_EPM */
+
+#ifdef CONFIG_KRG_PROC
+static int do_getpgid(pid_t pid, struct pid_namespace *ns)
+#else
 SYSCALL_DEFINE1(getpgid, pid_t, pid)
+#endif
 {
 	struct task_struct *p;
 	struct pid *grp;
@@ -1036,7 +1222,11 @@
 		grp = task_pgrp(current);
 	else {
 		retval = -ESRCH;
+#ifdef CONFIG_KRG_PROC
+		p = find_task_by_pid_ns(pid, ns);
+#else
 		p = find_task_by_vpid(pid);
+#endif
 		if (!p)
 			goto out;
 		grp = task_pgrp(p);
@@ -1047,12 +1237,55 @@
 		if (retval)
 			goto out;
 	}
+#ifdef CONFIG_KRG_PROC
+	retval = pid_nr_ns(grp, ns);
+#else
 	retval = pid_vnr(grp);
+#endif
 out:
 	rcu_read_unlock();
 	return retval;
 }
 
+#ifdef CONFIG_KRG_PROC
+static int handle_getpgid(struct rpc_desc *desc, void *msg, size_t size)
+{
+	struct pid *pid;
+	const struct cred *old_cred;
+	int retval;
+
+	pid = krg_handle_remote_syscall_begin(desc, msg, size,
+					      NULL, &old_cred);
+	if (IS_ERR(pid)) {
+		retval = PTR_ERR(pid);
+		goto out;
+	}
+
+	retval = do_getpgid(pid_knr(pid), ns_of_pid(pid)->krg_ns_root);
+
+	krg_handle_remote_syscall_end(pid, old_cred);
+
+out:
+	return retval;
+}
+
+static int krg_getpgid(pid_t pid)
+{
+	return krg_remote_syscall_simple(PROC_GETPGID, pid, NULL, 0);
+}
+
+SYSCALL_DEFINE1(getpgid, pid_t, pid)
+{
+	int retval;
+
+	retval = do_getpgid(pid, task_active_pid_ns(current));
+	if (retval == -ESRCH)
+		retval = krg_getpgid(pid);
+
+	return retval;
+}
+#endif /* CONFIG_KRG_PROC */
+
 #ifdef __ARCH_WANT_SYS_GETPGRP
 
 SYSCALL_DEFINE0(getpgrp)
@@ -1062,7 +1295,11 @@
 
 #endif
 
+#ifdef CONFIG_KRG_PROC
+static int do_getsid(pid_t pid, struct pid_namespace *ns)
+#else
 SYSCALL_DEFINE1(getsid, pid_t, pid)
+#endif
 {
 	struct task_struct *p;
 	struct pid *sid;
@@ -1073,7 +1310,11 @@
 		sid = task_session(current);
 	else {
 		retval = -ESRCH;
+#ifdef CONFIG_KRG_PROC
+		p = find_task_by_pid_ns(pid, ns);
+#else
 		p = find_task_by_vpid(pid);
+#endif
 		if (!p)
 			goto out;
 		sid = task_session(p);
@@ -1084,19 +1325,82 @@
 		if (retval)
 			goto out;
 	}
+#ifdef CONFIG_KRG_PROC
+	retval = pid_nr_ns(sid, ns);
+#else
 	retval = pid_vnr(sid);
+#endif
 out:
 	rcu_read_unlock();
 	return retval;
 }
 
+#ifdef CONFIG_KRG_PROC
+static int handle_getsid(struct rpc_desc *desc, void *msg, size_t size)
+{
+	struct pid *pid;
+	const struct cred *old_cred;
+	int retval;
+
+	pid = krg_handle_remote_syscall_begin(desc, msg, size,
+					      NULL, &old_cred);
+	if (IS_ERR(pid)) {
+		retval = PTR_ERR(pid);
+		goto out;
+	}
+
+	retval = do_getsid(pid_knr(pid), ns_of_pid(pid)->krg_ns_root);
+
+	krg_handle_remote_syscall_end(pid, old_cred);
+
+out:
+	return retval;
+}
+
+static int krg_getsid(pid_t pid)
+{
+	return krg_remote_syscall_simple(PROC_GETSID, pid, NULL, 0);;
+}
+
+SYSCALL_DEFINE1(getsid, pid_t, pid)
+{
+	int retval;
+
+	retval = do_getsid(pid, task_active_pid_ns(current));
+	if (retval == -ESRCH)
+		retval = krg_getsid(pid);
+
+	return retval;
+}
+
+void remote_sys_init(void)
+{
+	rpc_register_int(PROC_GETPGID, handle_getpgid, 0);
+	rpc_register_int(PROC_GETSID, handle_getsid, 0);
+#ifdef CONFIG_KRG_EPM
+	rpc_register_int(PROC_FORWARD_SETPGID, handle_forward_setpgid, 0);
+#endif
+}
+#endif /* CONFIG_KRG_PROC */
+
 SYSCALL_DEFINE0(setsid)
 {
 	struct task_struct *group_leader = current->group_leader;
 	struct pid *sid = task_pid(group_leader);
 	pid_t session = pid_vnr(sid);
+#ifdef CONFIG_KRG_EPM
+	struct children_kddm_object *parent_children_obj = NULL;
+	pid_t real_parent_tgid;
+#endif /* CONFIG_KRG_EPM */
 	int err = -EPERM;
 
+#ifdef CONFIG_KRG_EPM
+	down_read(&kerrighed_init_sem);
+	if (rcu_dereference(current->parent_children_obj))
+		parent_children_obj =
+			krg_parent_children_writelock(current,
+						      &real_parent_tgid);
+#endif /* CONFIG_KRG_EPM */
 	write_lock_irq(&tasklist_lock);
 	/* Fail if I am already a session leader */
 	if (group_leader->signal->leader)
@@ -1116,6 +1420,14 @@
 	err = session;
 out:
 	write_unlock_irq(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+	if (parent_children_obj) {
+		if (err >= 0)
+			krg_set_child_pgid(parent_children_obj, current);
+		krg_children_unlock(parent_children_obj);
+	}
+	up_read(&kerrighed_init_sem);
+#endif /* CONFIG_KRG_EPM */
 	return err;
 }
 
diff -ruN linux-2.6.29/kernel/timer.c android_cluster/linux-2.6.29/kernel/timer.c
--- linux-2.6.29/kernel/timer.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/kernel/timer.c	2014-05-27 23:04:10.458027260 -0700
@@ -37,6 +37,12 @@
 #include <linux/delay.h>
 #include <linux/tick.h>
 #include <linux/kallsyms.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/children.h>
+#endif
+#ifdef CONFIG_KRG_SCHED
+#include <kerrighed/scheduler/hooks.h>
+#endif
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -1057,6 +1063,11 @@
 
 EXPORT_SYMBOL(avenrun);
 
+#if defined(CONFIG_KRG_SCHED) && defined(CONFIG_MODULE_HOOK)
+struct module_hook_desc kmh_calc_load;
+EXPORT_SYMBOL(kmh_calc_load);
+#endif
+
 /*
  * calc_load - given tick count, update the avenrun load estimates.
  * This is called while holding a write_lock on xtime_lock.
@@ -1076,6 +1087,9 @@
 			count += LOAD_FREQ;
 		} while (count < 0);
 	}
+#if defined(CONFIG_KRG_SCHED) && defined(CONFIG_MODULE_HOOK)
+	module_hook_call(&kmh_calc_load, ticks);
+#endif
 }
 
 /*
@@ -1168,7 +1182,11 @@
 	int pid;
 
 	rcu_read_lock();
+#ifdef CONFIG_KRG_EPM
+	pid = krg_get_real_parent_tgid(current, task_active_pid_ns(current));
+#else
 	pid = task_tgid_vnr(current->real_parent);
+#endif
 	rcu_read_unlock();
 
 	return pid;
diff -ruN linux-2.6.29/kerrighed/capability/capability.c android_cluster/linux-2.6.29/kerrighed/capability/capability.c
--- linux-2.6.29/kerrighed/capability/capability.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/capability/capability.c	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,604 @@
+/*
+ *  kerrighed/capability/capability.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2006-2007 Louis Rilling - Kerlabs
+ */
+
+/** writen by David Margery (c) Inria 2004 */
+
+#include <linux/sched.h>
+#include <linux/nsproxy.h>
+#include <linux/cred.h>
+#include <linux/pid_namespace.h>
+#include <linux/rcupdate.h>
+#include <kerrighed/capabilities.h>
+#ifdef CONFIG_KRG_EPM
+#include <linux/pid_namespace.h>
+#include <kerrighed/children.h>
+#endif
+#include <linux/uaccess.h>
+
+#include <kerrighed/krg_syscalls.h>
+#include <kerrighed/krg_services.h>
+#include <kerrighed/remote_cred.h>
+#ifdef CONFIG_KRG_PROC
+#include <kerrighed/remote_syscall.h>
+#include <net/krgrpc/rpc.h>
+#include <net/krgrpc/rpcid.h>
+#endif
+
+int can_use_krg_cap(struct task_struct *task, int cap)
+{
+	return (cap_raised(task->krg_caps.effective, cap)
+		&& !atomic_read(&task->krg_cap_unavailable[cap])
+		&& !atomic_read(&task->krg_cap_unavailable_private[cap]));
+}
+
+void krg_cap_fork(struct task_struct *task, unsigned long clone_flags)
+{
+	kernel_krg_cap_t *caps = &current->krg_caps;
+	kernel_krg_cap_t *new_caps = &task->krg_caps;
+	kernel_cap_t new_krg_effective;
+	int i;
+
+#ifdef CONFIG_KRG_EPM
+	if (krg_current && !in_krg_do_fork())
+		/* Migration/restart: do not recompute krg caps */
+		return;
+#endif
+
+	/*
+	 * Compute the new capabilities and reset the private
+	 * krg_cap_unavailable array
+	 */
+	new_krg_effective = cap_intersect(caps->inheritable_effective,
+					  caps->inheritable_permitted);
+
+	new_caps->permitted = caps->inheritable_permitted;
+	new_caps->effective = new_krg_effective;
+
+	for (i = 0; i < CAP_SIZE; i++)
+		atomic_set(&task->krg_cap_unavailable_private[i], 0);
+	/* The other fields have been inherited by copy. */
+}
+
+int krg_cap_prepare_binprm(struct linux_binprm *bprm)
+{
+	/* The model needs changes with filesystem support ... */
+#if 0
+	cap_clear(bprm->krg_cap_forced);
+	cap_set_full(bprm->krg_cap_permitted);
+	cap_set_full(bprm->krg_cap_effective);
+#endif /* 0 */
+	return 0;
+}
+
+void krg_cap_finish_exec(struct linux_binprm *bprm)
+{
+	/* The model needs changes with filesystem support ... */
+#if 0
+	kernel_krg_cap_t *caps = &current->krg_caps;
+	kernel_cap_t new_krg_permitted, new_krg_effective;
+
+	/* added by David Margery (c) Inria 2004 */
+	/* Updated by Pascal Gallard (c) Inria 2005 */
+	task_lock(current);
+	new_krg_permitted = cap_intersect(caps->inheritable_permitted,
+					  bprm->krg_cap_permitted);
+	new_krg_permitted = cap_combine(new_krg_permitted,
+					bprm->krg_cap_forced);
+
+	new_krg_effective = cap_intersect(bprm->krg_cap_effective,
+					  new_krg_permitted);
+	new_krg_effective = cap_intersect(caps->inheritable_effective,
+					  new_krg_effective);
+
+	caps->permitted = new_krg_permitted;
+	caps->effective = new_krg_effective;
+	task_unlock(current);
+#endif /* 0 */
+}
+
+static int krg_set_cap(struct task_struct *tsk,
+		       const kernel_krg_cap_t *requested_cap)
+{
+	kernel_krg_cap_t *caps = &tsk->krg_caps;
+	kernel_cap_t tmp_cap;
+	struct nsproxy *nsp;
+	int res;
+	int i;
+
+	res = 0;
+	rcu_read_lock();
+	nsp = rcu_dereference(tsk->nsproxy);
+	if (!nsp || !nsp->krg_ns)
+		res = -EPERM;
+	rcu_read_unlock();
+	if (res)
+		goto out;
+
+	res = -EINVAL;
+	if (!cap_issubset(requested_cap->effective, requested_cap->permitted)
+	    || !cap_issubset(requested_cap->inheritable_permitted,
+			     requested_cap->permitted)
+	    || !cap_issubset(requested_cap->inheritable_effective,
+			     requested_cap->inheritable_permitted))
+		goto out;
+
+	res = -ENOSYS;
+	tmp_cap = KRG_CAP_SUPPORTED;
+	if (!cap_issubset(requested_cap->permitted, tmp_cap))
+		goto out;
+
+	res = -EPERM;
+	if (!permissions_ok(tsk))
+		goto out;
+
+	task_lock(tsk);
+
+	if (!cap_raised(caps->effective, CAP_CHANGE_KERRIGHED_CAP))
+		goto out_unlock;
+
+	res = -EBUSY;
+	for (i = 0; i < CAP_SIZE; i++)
+		if (atomic_read(&tsk->krg_cap_used[i])
+		    && !cap_raised(requested_cap->effective, i))
+			goto out_unlock;
+
+	tmp_cap = cap_intersect(caps->permitted, requested_cap->permitted);
+	caps->permitted = tmp_cap;
+	tmp_cap = cap_intersect(caps->permitted, requested_cap->effective);
+	caps->effective = tmp_cap;
+	tmp_cap = cap_intersect(caps->permitted,
+				requested_cap->inheritable_effective);
+	caps->inheritable_effective = tmp_cap;
+	tmp_cap = cap_intersect(caps->permitted,
+				requested_cap->inheritable_permitted);
+	caps->inheritable_permitted = tmp_cap;
+
+	res = 0;
+
+out_unlock:
+	task_unlock(tsk);
+
+out:
+	return res;
+}
+
+#ifdef CONFIG_KRG_PROC
+static int remote_set_pid_cap(pid_t pid, const kernel_krg_cap_t *cap);
+#endif
+
+static int krg_set_father_cap(struct task_struct *tsk,
+			      const kernel_krg_cap_t *requested_cap)
+{
+	int retval = 0;
+
+	read_lock(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+	if (tsk->real_parent != baby_sitter) {
+#endif
+		retval = krg_set_cap(tsk->real_parent, requested_cap);
+		read_unlock(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+	} else {
+		struct children_kddm_object *parent_children_obj;
+		pid_t real_parent_tgid;
+		pid_t parent_pid, real_parent_pid;
+		int retval;
+
+		read_unlock(&tasklist_lock);
+
+		parent_children_obj =
+			krg_parent_children_readlock(tsk, &real_parent_tgid);
+		if (!parent_children_obj)
+			/* Parent is init. Do not change init's capabilities! */
+			return -EPERM;
+		krg_get_parent(parent_children_obj, tsk,
+			       &parent_pid, &real_parent_pid);
+		retval = remote_set_pid_cap(real_parent_pid, requested_cap);
+		krg_children_unlock(parent_children_obj);
+	}
+#endif
+
+	return retval;
+}
+
+static int krg_set_pid_cap(pid_t pid, const kernel_krg_cap_t *requested_cap)
+{
+	struct task_struct *tsk;
+	int retval = -ESRCH;
+
+	rcu_read_lock();
+	tsk = find_task_by_vpid(pid);
+	if (tsk)
+		retval = krg_set_cap(tsk, requested_cap);
+	rcu_read_unlock();
+#ifdef CONFIG_KRG_PROC
+	if (!tsk)
+		retval = remote_set_pid_cap(pid, requested_cap);
+#endif
+
+	return retval;
+}
+
+#ifdef CONFIG_KRG_PROC
+static int handle_set_pid_cap(struct rpc_desc* desc, void *_msg, size_t size)
+{
+	struct pid *pid;
+	kernel_krg_cap_t cap;
+	const struct cred *old_cred;
+	int ret;
+
+	pid = krg_handle_remote_syscall_begin(desc, _msg, size,
+					      &cap, &old_cred);
+	if (IS_ERR(pid)) {
+		ret = PTR_ERR(pid);
+		goto out;
+	}
+
+	ret = krg_set_cap(pid_task(pid, PIDTYPE_PID), &cap);
+
+	krg_handle_remote_syscall_end(pid, old_cred);
+
+out:
+	return ret;
+}
+
+static int remote_set_pid_cap(pid_t pid, const kernel_krg_cap_t *cap)
+{
+	return krg_remote_syscall_simple(PROC_SET_PID_CAP, pid,
+					 cap, sizeof(*cap));
+}
+#endif /* CONFIG_KRG_PROC */
+
+static int krg_get_cap(struct task_struct *tsk, kernel_krg_cap_t *resulting_cap)
+{
+	kernel_krg_cap_t *caps = &tsk->krg_caps;
+	int res;
+
+	task_lock(tsk);
+
+	if (resulting_cap && permissions_ok(tsk)) {
+		*resulting_cap = *caps;
+		res = 0;
+	} else {
+		res = -EPERM;
+	}
+
+	task_unlock(tsk);
+
+	return res;
+}
+
+#ifdef CONFIG_KRG_PROC
+static int remote_get_pid_cap(pid_t pid, kernel_krg_cap_t *cap);
+#endif
+
+static int krg_get_father_cap(struct task_struct *son,
+			      kernel_krg_cap_t *resulting_cap)
+{
+	int retval = 0;
+
+	read_lock(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+	if (son->real_parent != baby_sitter) {
+#endif
+		retval = krg_get_cap(son->real_parent, resulting_cap);
+		read_unlock(&tasklist_lock);
+#ifdef CONFIG_KRG_EPM
+	} else {
+		struct children_kddm_object *parent_children_obj;
+		pid_t real_parent_tgid;
+		pid_t parent_pid, real_parent_pid;
+		int retval;
+
+		read_unlock(&tasklist_lock);
+
+		parent_children_obj =
+			krg_parent_children_readlock(son, &real_parent_tgid);
+		if (!parent_children_obj)
+			/* Parent is init. */
+			return krg_get_cap(task_active_pid_ns(son)->child_reaper,
+					   resulting_cap);
+		krg_get_parent(parent_children_obj, son,
+			       &parent_pid, &real_parent_pid);
+		retval = remote_get_pid_cap(real_parent_pid, resulting_cap);
+		krg_children_unlock(parent_children_obj);
+	}
+#endif
+
+	return retval;
+}
+
+static int krg_get_pid_cap(pid_t pid, kernel_krg_cap_t *resulting_cap)
+{
+	struct task_struct *tsk;
+	int retval = -ESRCH;
+
+	rcu_read_lock();
+	tsk = find_task_by_vpid(pid);
+	if (tsk)
+		retval = krg_get_cap(tsk, resulting_cap);
+	rcu_read_unlock();
+#ifdef CONFIG_KRG_PROC
+	if (!tsk)
+		retval = remote_get_pid_cap(pid, resulting_cap);
+#endif
+
+	return retval;
+}
+
+#ifdef CONFIG_KRG_PROC
+static int handle_get_pid_cap(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	struct pid *pid;
+	kernel_krg_cap_t cap;
+	const struct cred *old_cred;
+	int ret;
+
+	pid = krg_handle_remote_syscall_begin(desc, _msg, size,
+					      NULL, &old_cred);
+	if (IS_ERR(pid)) {
+		ret = PTR_ERR(pid);
+		goto out;
+	}
+
+	ret = krg_get_cap(pid_task(pid, PIDTYPE_PID), &cap);
+	if (ret)
+		goto out_end;
+
+	ret = rpc_pack_type(desc, cap);
+	if (ret)
+		goto err_cancel;
+
+out_end:
+	krg_handle_remote_syscall_end(pid, old_cred);
+
+out:
+	return ret;
+
+err_cancel:
+	rpc_cancel(desc);
+	goto out_end;
+}
+
+static int remote_get_pid_cap(pid_t pid, kernel_krg_cap_t *cap)
+{
+	struct rpc_desc *desc;
+	int err = -ESRCH;
+	int res;
+
+	desc = krg_remote_syscall_begin(PROC_GET_PID_CAP, pid, NULL, 0);
+	if (IS_ERR(desc)) {
+		err = PTR_ERR(desc);
+		goto out;
+	}
+
+	err = rpc_unpack_type(desc, res);
+	if (err)
+		goto err_cancel;
+	if (res) {
+		err = res;
+		goto out_end;
+	}
+	err = rpc_unpack_type(desc, *cap);
+	if (err)
+		goto err_cancel;
+
+out_end:
+	krg_remote_syscall_end(desc, pid);
+
+out:
+	return err;
+
+err_cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -EPIPE;
+	goto out_end;
+}
+#endif /* CONFIG_KRG_PROC */
+
+/* Kerrighed syscalls interface */
+
+static int user_to_kernel_krg_cap(const krg_cap_t __user *user_caps,
+				  kernel_krg_cap_t *caps)
+{
+	krg_cap_t ucaps;
+
+	if (copy_from_user(&ucaps, user_caps, sizeof(ucaps)))
+		return -EFAULT;
+
+	BUILD_BUG_ON(sizeof(kernel_cap_t) != 2 * sizeof(__u32));
+
+	caps->permitted = (kernel_cap_t){{ ucaps.krg_cap_permitted, 0 }};
+	caps->effective = (kernel_cap_t){{ ucaps.krg_cap_effective, 0 }};
+	caps->inheritable_permitted =
+		(kernel_cap_t){{ ucaps.krg_cap_inheritable_permitted, 0 }};
+	caps->inheritable_effective =
+		(kernel_cap_t){{ ucaps.krg_cap_inheritable_effective, 0 }};
+
+	return 0;
+}
+
+static int proc_set_pid_cap(void __user *arg)
+{
+	struct krg_cap_pid_desc desc;
+	kernel_krg_cap_t caps;
+	int r = -EFAULT;
+
+	if (copy_from_user(&desc, arg, sizeof(desc)))
+		goto out;
+
+	if (user_to_kernel_krg_cap(desc.caps, &caps))
+		goto out;
+
+	r = krg_set_pid_cap(desc.pid, &caps);
+
+out:
+	return r;
+}
+
+static int proc_set_father_cap(void __user *arg)
+{
+	kernel_krg_cap_t caps;
+	int r;
+
+	r = user_to_kernel_krg_cap(arg, &caps);
+	if (!r)
+		r = krg_set_father_cap(current, &caps);
+
+	return r;
+}
+
+static int proc_set_cap(void __user *arg)
+{
+	kernel_krg_cap_t caps;
+	int r;
+
+	r = user_to_kernel_krg_cap(arg, &caps);
+	if (!r)
+		r = krg_set_cap(current, &caps);
+
+	return r;
+}
+
+static int kernel_to_user_krg_cap(const kernel_krg_cap_t *caps,
+				  krg_cap_t __user *user_caps)
+{
+	krg_cap_t ucaps;
+	int r = 0;
+
+	ucaps.krg_cap_permitted = caps->permitted.cap[0];
+	ucaps.krg_cap_effective = caps->effective.cap[0];
+	ucaps.krg_cap_inheritable_permitted =
+		caps->inheritable_permitted.cap[0];
+	ucaps.krg_cap_inheritable_effective =
+		caps->inheritable_effective.cap[0];
+
+	if (copy_to_user(user_caps, &ucaps, sizeof(ucaps)))
+		r = -EFAULT;
+
+	return r;
+}
+
+static int proc_get_cap(void __user *arg)
+{
+	kernel_krg_cap_t caps;
+	int r;
+
+	r = krg_get_cap(current, &caps);
+	if (!r)
+		r = kernel_to_user_krg_cap(&caps, arg);
+
+	return r;
+}
+
+static int proc_get_father_cap(void __user *arg)
+{
+	kernel_krg_cap_t caps;
+	int r;
+
+	r = krg_get_father_cap(current, &caps);
+	if (!r)
+		r = kernel_to_user_krg_cap(&caps, arg);
+
+	return r;
+}
+
+static int proc_get_pid_cap(void __user *arg)
+{
+	struct krg_cap_pid_desc desc;
+	kernel_krg_cap_t caps;
+	int r = -EFAULT;
+
+	BUG_ON(sizeof(int) != sizeof(pid_t));
+
+	if (copy_from_user(&desc, arg, sizeof(desc)))
+		goto out;
+
+	r = krg_get_pid_cap(desc.pid, &caps);
+
+	if (!r)
+		r = kernel_to_user_krg_cap(&caps, desc.caps);
+
+out:
+	return r;
+}
+
+static int proc_get_supported_cap(void __user *arg)
+{
+	int __user *set = arg;
+	return put_user(KRG_CAP_SUPPORTED.cap[0], set);
+}
+
+int init_krg_cap(void)
+{
+	int r;
+
+	r = register_proc_service(KSYS_SET_CAP, proc_set_cap);
+	if (r != 0)
+		goto out;
+
+	r = register_proc_service(KSYS_GET_CAP, proc_get_cap);
+	if (r != 0)
+		goto unreg_set_cap;
+
+	r = register_proc_service(KSYS_SET_FATHER_CAP, proc_set_father_cap);
+	if (r != 0)
+		goto unreg_get_cap;
+
+	r = register_proc_service(KSYS_GET_FATHER_CAP, proc_get_father_cap);
+	if (r != 0)
+		goto unreg_set_father_cap;
+
+	r = register_proc_service(KSYS_SET_PID_CAP, proc_set_pid_cap);
+	if (r != 0)
+		goto unreg_get_father_cap;
+
+	r = register_proc_service(KSYS_GET_PID_CAP, proc_get_pid_cap);
+	if (r != 0)
+		goto unreg_set_pid_cap;
+
+	r = register_proc_service(KSYS_GET_SUPPORTED_CAP,
+				  proc_get_supported_cap);
+	if (r != 0)
+		goto unreg_get_pid_cap;
+
+#ifdef CONFIG_KRG_PROC
+	rpc_register_int(PROC_GET_PID_CAP, handle_get_pid_cap, 0);
+	rpc_register_int(PROC_SET_PID_CAP, handle_set_pid_cap, 0);
+#endif
+
+ out:
+	return r;
+
+ unreg_get_pid_cap:
+	unregister_proc_service(KSYS_GET_PID_CAP);
+ unreg_set_pid_cap:
+	unregister_proc_service(KSYS_SET_PID_CAP);
+ unreg_get_father_cap:
+	unregister_proc_service(KSYS_GET_FATHER_CAP);
+ unreg_set_father_cap:
+	unregister_proc_service(KSYS_SET_FATHER_CAP);
+ unreg_get_cap:
+	unregister_proc_service(KSYS_GET_CAP);
+ unreg_set_cap:
+	unregister_proc_service(KSYS_SET_CAP);
+	goto out;
+}
+
+void cleanup_krg_cap(void)
+{
+	unregister_proc_service(KSYS_GET_SUPPORTED_CAP);
+	unregister_proc_service(KSYS_GET_PID_CAP);
+	unregister_proc_service(KSYS_SET_PID_CAP);
+	unregister_proc_service(KSYS_GET_FATHER_CAP);
+	unregister_proc_service(KSYS_SET_FATHER_CAP);
+	unregister_proc_service(KSYS_GET_CAP);
+	unregister_proc_service(KSYS_SET_CAP);
+
+	return;
+}
diff -ruN linux-2.6.29/kerrighed/capability/Makefile android_cluster/linux-2.6.29/kerrighed/capability/Makefile
--- linux-2.6.29/kerrighed/capability/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/capability/Makefile	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,3 @@
+obj-$(CONFIG_KRG_CAP) += capability.o
+
+EXTRA_CFLAGS += -Wall -Werror
diff -ruN linux-2.6.29/kerrighed/epm/action.c android_cluster/linux-2.6.29/kerrighed/epm/action.c
--- linux-2.6.29/kerrighed/epm/action.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/action.c	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,153 @@
+/*
+ *  kerrighed/epm/action.c
+ *
+ *  Copyright (C) 2006-2007 Louis Rilling - Kerlabs
+ */
+/*
+ * Management of incompatibilities between Kerrighed actions and
+ * some Linux facilities
+ */
+
+#include <linux/sched.h>
+#include <linux/spinlock.h>
+#include <asm/atomic.h>
+#include <kerrighed/capabilities.h>
+#include <kerrighed/action.h>
+
+static int action_to_cap_mapping[] = {
+	[EPM_MIGRATE] = CAP_CAN_MIGRATE,
+	[EPM_REMOTE_CLONE] = CAP_DISTANT_FORK,
+	[EPM_CHECKPOINT] = CAP_CHECKPOINTABLE,
+};
+
+DEFINE_RWLOCK(krg_action_lock);
+
+static inline void action_lock_lock(void)
+{
+	lockdep_off();
+	write_lock(&krg_action_lock);
+}
+
+static inline void action_lock_unlock(void)
+{
+	write_unlock(&krg_action_lock);
+	lockdep_on();
+}
+
+static inline int action_to_flag(krg_epm_action_t action)
+{
+	if (unlikely(action <= EPM_NO_ACTION || action >= EPM_ACTION_MAX))
+		return 0;
+	else
+		return 1 << action;
+}
+
+static inline int action_to_cap(krg_epm_action_t action)
+{
+	if (unlikely(action <= EPM_NO_ACTION || action >= EPM_ACTION_MAX))
+		return -1;
+	else
+		return action_to_cap_mapping[action];
+}
+
+int krg_action_disable(struct task_struct *task, krg_epm_action_t action,
+		       int inheritable)
+{
+	unsigned long flag;
+	int retval = 0;
+
+	flag = action_to_flag(action);
+	if (unlikely(!flag))
+		return -EINVAL;
+
+	action_lock_lock();
+	if (unlikely(task->krg_action_flags & flag))
+		retval = -EAGAIN;
+	else {
+		atomic_t *array;
+
+		if (inheritable)
+			array = task->krg_cap_unavailable;
+		else
+			array = task->krg_cap_unavailable_private;
+		atomic_inc(&array[action_to_cap(action)]);
+	}
+	action_lock_unlock();
+
+	return retval;
+}
+
+int krg_action_enable(struct task_struct *task, krg_epm_action_t action,
+		      int inheritable)
+{
+	atomic_t *array;
+	int cap;
+
+	cap = action_to_cap(action);
+	if (unlikely(cap < 0))
+		return -EINVAL;
+
+	if (inheritable)
+		array = task->krg_cap_unavailable;
+	else
+		array = task->krg_cap_unavailable_private;
+	if (unlikely(atomic_add_negative(-1, &array[cap])))
+		BUG();
+
+	return 0;
+}
+
+int krg_action_start(struct task_struct *task, krg_epm_action_t action)
+{
+	unsigned long flag;
+	int retval = 0;
+
+	flag = action_to_flag(action);
+	if (unlikely(!flag))
+		return -EINVAL;
+
+	action_lock_lock();
+	if (!can_use_krg_cap(task, action_to_cap(action)))
+		retval = -EPERM;
+	else if (unlikely(task->krg_action_flags & flag))
+		retval = -EALREADY;
+	else if (unlikely(task->krg_action_flags))
+		retval = -EAGAIN;
+	else
+		task->krg_action_flags |= flag;
+	action_lock_unlock();
+
+	return retval;
+}
+
+int krg_action_stop(struct task_struct *task, krg_epm_action_t action)
+{
+	unsigned long flag;
+	int retval = 0;
+
+	flag = action_to_flag(action);
+	if (unlikely(!flag))
+		return -EINVAL;
+
+	action_lock_lock();
+	task->krg_action_flags &= ~flag;
+	action_lock_unlock();
+
+	return retval;
+}
+
+int krg_action_pending(struct task_struct *task, krg_epm_action_t action)
+{
+	unsigned long flag;
+	int retval;
+
+	flag = action_to_flag(action);
+	if (unlikely(!flag))
+		return 0;
+
+	action_lock_lock();
+	retval = task->krg_action_flags & flag;
+	action_lock_unlock();
+
+	return retval;
+}
diff -ruN linux-2.6.29/kerrighed/epm/application/app_checkpoint.c android_cluster/linux-2.6.29/kerrighed/epm/application/app_checkpoint.c
--- linux-2.6.29/kerrighed/epm/application/app_checkpoint.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/application/app_checkpoint.c	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,777 @@
+/*
+ *  kerrighed/epm/app_checkpoint.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2007-2008 Matthieu Fertré - INRIA
+ */
+
+#include <linux/compile.h>
+#include <linux/pid_namespace.h>
+#include <linux/cred.h>
+#include <linux/syscalls.h>
+#include <linux/version.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/task.h>
+#include <kerrighed/children.h>
+#include <kerrighed/kerrighed_signal.h>
+#include <kerrighed/action.h>
+#include <kerrighed/application.h>
+#include <kerrighed/app_shared.h>
+#include <kerrighed/sys/checkpoint.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/ghost_helpers.h>
+#include <kerrighed/remote_cred.h>
+#include <kerrighed/physical_fs.h>
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+#include "app_frontier.h"
+#include "app_utils.h"
+#include "../checkpoint.h"
+#include "../epm_internal.h"
+
+/*--------------------------------------------------------------------------*/
+
+static int save_app_kddm_object(struct app_kddm_object *obj)
+{
+	ghost_fs_t oldfs;
+	ghost_t *ghost;
+	int magic = 4342338;
+	int r = 0, err;
+	u32 linux_version;
+
+	__set_ghost_fs(&oldfs);
+
+	ghost = create_file_ghost(GHOST_WRITE, obj->app_id, obj->chkpt_sn,
+				  "global.bin");
+
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		ckpt_err(NULL, r,
+			 "Fail to create file /var/chkpt/%ld/v%d/global.bin",
+			 obj->app_id, obj->chkpt_sn);
+		goto exit;
+	}
+
+	/* write information about the Linux kernel version */
+	linux_version = LINUX_VERSION_CODE;
+	r = ghost_write(ghost, &linux_version, sizeof(linux_version));
+	if (r)
+		goto err_write;
+	r = ghost_write_string(ghost, UTS_MACHINE);
+	if (r)
+		goto err_write;
+	r = ghost_write_string(ghost, UTS_VERSION);
+	if (r)
+		goto err_write;
+	r = ghost_write_string(ghost, LINUX_COMPILE_TIME);
+	if (r)
+		goto err_write;
+	r = ghost_write_string(ghost, LINUX_COMPILE_BY);
+	if (r)
+		goto err_write;
+	r = ghost_write_string(ghost, LINUX_COMPILE_HOST);
+	if (r)
+		goto err_write;
+	r = ghost_write_string(ghost, LINUX_COMPILER);
+	if (r)
+		goto err_write;
+
+	/* write information about the checkpoint itself */
+	r = ghost_write(ghost, &obj->app_id, sizeof(obj->app_id));
+	if (r)
+		goto err_write;
+	r = ghost_write(ghost, &obj->chkpt_sn, sizeof(obj->chkpt_sn));
+	if (r)
+		goto err_write;
+	r = ghost_write(ghost, &obj->nodes, sizeof(obj->nodes));
+	if (r)
+		goto err_write;
+	r = ghost_write(ghost, &obj->user_data, sizeof(obj->user_data));
+	if (r)
+		goto err_write;
+	r = ghost_write(ghost, &magic, sizeof(magic));
+	if (r)
+		goto err_write;
+
+err_write:
+	/* End of the really interesting part */
+	err = ghost_close(ghost);
+	if (!r)
+		r = err;
+exit:
+	unset_ghost_fs(&oldfs);
+
+	return r;
+}
+
+static inline int write_task_parent_links(task_state_t *t,
+					  ghost_t *ghost)
+{
+	int r = 0;
+	pid_t parent, real_parent, real_parent_tgid;
+	pid_t pid, tgid, pgrp, session;
+	struct children_kddm_object *obj;
+
+	if (!can_be_checkpointed(t->task)) {
+		r = -EPERM;
+		goto error;
+	}
+
+	pid = task_pid_knr(t->task);
+	r = ghost_write(ghost, &pid, sizeof(pid_t));
+	if (r)
+		goto error;
+
+	tgid = task_tgid_knr(t->task);
+	r = ghost_write(ghost, &tgid, sizeof(pid_t));
+	if (r)
+		goto error;
+
+	obj = krg_parent_children_readlock(t->task, &real_parent_tgid);
+	if (obj) {
+		r = krg_get_parent(obj, t->task, &parent, &real_parent);
+		BUG_ON(r);
+		krg_children_unlock(obj);
+	} else {
+		struct task_struct *reaper =
+			task_active_pid_ns(t->task)->child_reaper;
+		parent = real_parent = task_pid_knr(reaper);
+		real_parent_tgid = parent;
+	}
+
+	r = ghost_write(ghost, &parent, sizeof(pid_t));
+	if (r)
+		goto error;
+	r = ghost_write(ghost, &real_parent, sizeof(pid_t));
+	if (r)
+		goto error;
+	r = ghost_write(ghost, &real_parent_tgid, sizeof(pid_t));
+	if (r)
+		goto error;
+
+	if (has_group_leader_pid(t->task)) {
+		pgrp = task_pgrp_knr(t->task);
+		r = ghost_write(ghost, &pgrp, sizeof(pid_t));
+		if (r)
+			goto error;
+
+		session = task_session_knr(t->task);
+		r = ghost_write(ghost, &session, sizeof(pid_t));
+		if (r)
+			goto error;
+	}
+
+error:
+	return r;
+}
+
+/*
+ * Store the _LOCAL_ checkpoint description in a file
+ */
+static inline int save_local_app(struct app_struct *app, int chkpt_sn)
+{
+	ghost_fs_t oldfs;
+	ghost_t *ghost;
+	int r = 0, err;
+	int null = -1;
+	task_state_t *t;
+
+	__set_ghost_fs(&oldfs);
+
+	ghost = create_file_ghost(GHOST_WRITE, app->app_id, chkpt_sn,
+				  "node_%d.bin", kerrighed_node_id);
+
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		ckpt_err(NULL, r,
+			 "Fail to create file /var/chkpt/%ld/v%d/node_%u.bin",
+			 app->app_id, chkpt_sn, kerrighed_node_id);
+		goto exit;
+	}
+
+	/* Here is the really interesting part */
+	r = ghost_write(ghost, &kerrighed_node_id, sizeof(kerrighed_node_t));
+	if (r)
+		goto err_write;
+
+	/*
+	 * write all the description of the local tasks involved in the
+	 * checkpoint
+	 * there is no need to lock the application list of processes because
+	 * all application processes are already stopped
+	 */
+	list_for_each_entry(t, &app->tasks, next_task) {
+		r = write_task_parent_links(t, ghost);
+		if (r)
+			goto err_write;
+	}
+
+	/* end of file marker */
+	r = ghost_write(ghost, &null, sizeof(int));
+	if (r)
+		goto err_write;
+	r = ghost_write(ghost, &null, sizeof(int));
+
+err_write:
+	/* End of the really interesting part */
+	err = ghost_close(ghost);
+	if (!r)
+		r = err;
+
+exit:
+	unset_ghost_fs(&oldfs);
+
+	return r;
+}
+
+/*
+ * "send a request" to checkpoint a local process
+ * an ack is send at the end of the checkpoint
+ */
+static void __chkpt_task_req(struct app_struct *app, task_state_t *tsk)
+{
+	struct task_struct *task = tsk->task;
+	ghost_t *ghost;
+	int r;
+
+	BUG_ON(!task);
+
+	tsk->checkpoint.ghost = NULL;
+	if (!can_be_checkpointed(task)) {
+		__set_task_result(task, -EPERM);
+		return;
+	}
+
+	ghost = create_file_ghost(GHOST_WRITE,
+				  app->app_id,
+				  app->chkpt_sn,
+				  "task_%d.bin",
+				  task_pid_knr(task));
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		ckpt_err(NULL, r,
+			 "Fail to create file /var/chkpt/%ld/v%d/task_%d.bin "
+			 "to checkpoint process %d (%s)",
+			 app->app_id, app->chkpt_sn,
+			 task_pid_knr(task),
+			 task_pid_knr(task), task->comm);
+		__set_task_result(task, r);
+		return;
+	}
+	tsk->checkpoint.ghost = ghost;
+
+	complete(&tsk->checkpoint.completion);
+}
+
+ghost_t *get_task_chkpt_ghost(struct app_struct *app, struct task_struct *task)
+{
+	ghost_t *ghost = NULL;
+	task_state_t *t;
+
+	mutex_lock(&app->mutex);
+
+	list_for_each_entry(t, &app->tasks, next_task)
+		if (task == t->task) {
+			ghost = t->checkpoint.ghost;
+			break;
+		}
+
+	mutex_unlock(&app->mutex);
+
+	return ghost;
+}
+
+/*--------------------------------------------------------------------------*/
+
+static inline int __get_next_chkptsn(long app_id, int original_sn)
+{
+	char *dirname;
+	int error;
+	struct nameidata nd;
+	int version = original_sn;
+
+	do {
+		version++;
+		dirname = get_chkpt_dir(app_id, version);
+		if (IS_ERR(dirname)) {
+			version = PTR_ERR(dirname);
+			goto error;
+		}
+
+		error = path_lookup(dirname, 0, &nd);
+		if (!error)
+			path_put(&nd.path);
+		kfree(dirname);
+	} while (error != -ENOENT);
+
+error:
+	return version;
+}
+
+/*--------------------------------------------------------------------------*/
+
+/*
+ * CHECKPOINT all the processes running _LOCALLY_ which are involved in the
+ * checkpoint of an application
+ *
+ */
+static inline int __local_do_chkpt(struct app_struct *app, int chkpt_sn)
+{
+	task_state_t *tsk;
+	struct task_struct *tmp = NULL;
+	int r;
+
+	BUG_ON(list_empty(&app->tasks));
+
+	app->chkpt_sn = chkpt_sn;
+
+	/* application is frozen, locking here is paranoiac */
+	mutex_lock(&app->mutex);
+
+	r = save_local_app(app, chkpt_sn);
+	if (r)
+		goto err;
+
+	/* Checkpoint all local processes involved in the checkpoint */
+	init_completion(&app->tasks_chkpted);
+
+	list_for_each_entry(tsk, &app->tasks, next_task) {
+		tmp = tsk->task;
+
+		tsk->checkpoint.result = PCUS_CHKPT_IN_PROGRESS;
+		BUG_ON(tmp == current);
+		__chkpt_task_req(app, tsk);
+	}
+
+	mutex_unlock(&app->mutex);
+
+	wait_for_completion(&app->tasks_chkpted);
+	r = get_local_tasks_chkpt_result(app);
+out:
+	return r;
+err:
+	mutex_unlock(&app->mutex);
+	goto out;
+}
+
+struct checkpoint_request_msg {
+	kerrighed_node_t requester;
+	long app_id;
+	int chkpt_sn;
+	int flags;
+};
+
+static void handle_do_chkpt(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	struct checkpoint_request_msg *msg = _msg;
+	struct app_struct *app = find_local_app(msg->app_id);
+	const struct cred *old_cred = NULL;
+	int r;
+
+	BUG_ON(!app);
+
+	BUG_ON(app->cred);
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred)) {
+		r = PTR_ERR(old_cred);
+		goto send_res;
+	}
+	app->cred = current_cred();
+
+	app->checkpoint.flags = msg->flags;
+
+	r = __local_do_chkpt(app, msg->chkpt_sn);
+
+send_res:
+	r = send_result(desc, r);
+	if (r) /* an error as occured on other node */
+		goto error;
+
+	r = local_chkpt_shared(desc, app, msg->chkpt_sn);
+
+	r = send_result(desc, r);
+	if (r)
+		goto error;
+
+error:
+	cr_free_mm_exclusions(app);
+
+	clear_shared_objects(app);
+	if (app->cred) {
+		app->cred = NULL;
+		revert_creds(old_cred);
+	}
+}
+
+static int global_do_chkpt(struct app_kddm_object *obj, int flags)
+{
+	struct rpc_desc *desc;
+	struct checkpoint_request_msg msg;
+	int r, err_rpc;
+
+	r = __get_next_chkptsn(obj->app_id, obj->chkpt_sn);
+	if (r < 0)
+		goto exit;
+
+	obj->chkpt_sn = r;
+
+	/* prepare message */
+	msg.requester = kerrighed_node_id;
+	msg.app_id = obj->app_id;
+	msg.chkpt_sn = obj->chkpt_sn;
+	msg.flags = flags;
+
+	desc = rpc_begin_m(APP_DO_CHKPT, &obj->nodes);
+	err_rpc = rpc_pack_type(desc, msg);
+	if (err_rpc)
+		goto err_rpc;
+	err_rpc = pack_creds(desc, current_cred());
+	if (err_rpc)
+		goto err_rpc;
+
+	/* waiting results from the nodes hosting the application */
+	r = app_wait_returns_from_nodes(desc, obj->nodes);
+	if (r)
+		goto err_chkpt;
+
+	err_rpc = rpc_pack_type(desc, r);
+	if (err_rpc)
+		goto err_rpc;
+
+	r = global_chkpt_shared(desc, obj);
+	if (r)
+		goto err_chkpt;
+
+	err_rpc = rpc_pack_type(desc, r);
+	if (err_rpc)
+		goto err_rpc;
+
+	r = save_app_kddm_object(obj);
+	if (r)
+		goto exit;
+
+err_chkpt:
+	err_rpc = rpc_pack_type(desc, r);
+	if (err_rpc)
+		goto err_rpc;
+exit_rpc:
+	rpc_end(desc, 0);
+
+exit:
+	return r;
+
+err_rpc:
+	r = err_rpc;
+	rpc_cancel(desc);
+	goto exit_rpc;
+}
+
+/*--------------------------------------------------------------------------*/
+
+static int _freeze_app(long appid)
+{
+	int r;
+	struct app_kddm_object *obj;
+
+	obj = kddm_grab_object_no_ft(kddm_def_ns, APP_KDDM_ID, appid);
+	if (!obj) {
+		r = -ESRCH;
+		ckpt_err(NULL, r,
+			 "Application %ld does not exist. Can not freeze it",
+			 appid);
+		goto exit_kddmput;
+	}
+
+	if (obj->state == APP_RUNNING_CS) {
+		r = -EAGAIN;
+		ckpt_err(NULL, r,
+			 "Application %ld is in critical section. Can not freeze it",
+			 appid);
+		goto exit_kddmput;
+	}
+
+	if (obj->state != APP_RUNNING) {
+		r = -EPERM;
+		ckpt_err(NULL, r,
+			 "Application %ld is not running. Can not freeze it",
+			 appid);
+		goto exit_kddmput;
+	}
+
+	r = global_stop(obj);
+	if (!r)
+		obj->state = APP_FROZEN;
+
+exit_kddmput:
+	kddm_put_object(kddm_def_ns, APP_KDDM_ID, appid);
+	return r;
+}
+
+static int _unfreeze_app(long appid, int signal)
+{
+	int r;
+	struct app_kddm_object *obj;
+
+	obj = kddm_grab_object_no_ft(kddm_def_ns, APP_KDDM_ID, appid);
+	if (!obj) {
+		r = -ESRCH;
+		ckpt_err(NULL, r,
+			 "Application %ld does not exist. Can not unfreeze it",
+			 appid);
+		goto exit_kddmput;
+	}
+
+	if (obj->state == APP_RUNNING) {
+		r = -EPERM;
+		ckpt_err(NULL, r,
+			 "Application %ld is running. Can not unfreeze it",
+			 appid);
+		goto exit_kddmput;
+	}
+
+	r = global_unfreeze(obj, signal);
+
+exit_kddmput:
+	kddm_put_object(kddm_def_ns, APP_KDDM_ID, appid);
+	return r;
+}
+
+static int _checkpoint_frozen_app(struct checkpoint_info *info)
+{
+	int r;
+	int prev_chkpt_sn;
+	struct app_kddm_object *obj;
+
+	obj = kddm_grab_object_no_ft(kddm_def_ns, APP_KDDM_ID, info->app_id);
+	if (!obj) {
+		r = -ESRCH;
+		ckpt_err(NULL, r,
+			 "Application %ld does not exist. Can not checkpoint it",
+			 info->app_id);
+		goto exit_kddmput;
+	}
+
+	if (obj->state != APP_FROZEN) {
+		r = -EPERM;
+		ckpt_err(NULL, r,
+			 "Application %ld is not frozen. Can not checkpoint it",
+			 info->app_id);
+		goto exit_kddmput;
+	}
+
+	prev_chkpt_sn = obj->chkpt_sn;
+
+	r = global_do_chkpt(obj, info->flags);
+
+	info->chkpt_sn = obj->chkpt_sn;
+	if (r)
+		obj->chkpt_sn = prev_chkpt_sn;
+
+exit_kddmput:
+	kddm_put_object(kddm_def_ns, APP_KDDM_ID, info->app_id);
+	return r;
+}
+
+static void handle_cr_exclude(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	struct app_struct *app;
+	struct cr_mm_region mm_region;
+	int r;
+	long *app_id = _msg;
+
+	app = find_local_app(*app_id);
+
+	do {
+		r = rpc_unpack(desc, 0, &mm_region, sizeof(struct cr_mm_region));
+		if (r)
+			goto error;
+
+		r = cr_exclude_mm_region(app, mm_region.pid, mm_region.addr,
+					 mm_region.size);
+		if (r)
+			goto error;
+
+	} while (mm_region.next);
+
+out:
+	return;
+error:
+	rpc_cancel(desc);
+	goto out;
+}
+
+int app_cr_exclude(struct cr_mm_region *mm_regions)
+{
+	long app_id;
+	struct app_kddm_object *obj;
+	struct rpc_desc *desc;
+	struct cr_mm_region *element;
+	int r;
+
+	if (!mm_regions)
+		return -EINVAL;
+
+	app_id = get_appid_from_pid(task_pid_knr(current));
+	if (app_id < 0)
+		return app_id;
+
+	obj = kddm_grab_object_no_ft(kddm_def_ns, APP_KDDM_ID, app_id);
+	if (!obj) {
+		r = -ESRCH;
+		ckpt_err(NULL, r,
+			 "Application %ld does not exist. Can not checkpoint it",
+			 app_id);
+		goto exit_kddmput;
+	}
+
+	if (obj->state == APP_RUNNING_CS) {
+		r = -EAGAIN;
+		ckpt_err(NULL, r,
+			 "Application %ld is in critical section. Can not"
+			 " exclude memory regions",
+			 app_id);
+		goto exit_kddmput;
+	}
+
+	if (obj->state != APP_RUNNING) {
+		r = -EPERM;
+		ckpt_err(NULL, r,
+			 "Application %ld is not running. Can not checkpoint it",
+			 app_id);
+		goto exit_kddmput;
+	}
+
+	desc = rpc_begin_m(APP_EXCL_MM_REGION, &obj->nodes);
+	if (!desc) {
+		r = -ENOMEM;
+		goto exit_kddmput;
+	}
+
+	r = rpc_pack_type(desc, app_id);
+	if (r)
+		goto exit_rpc;
+
+	element = mm_regions;
+	while (element) {
+		r = rpc_pack(desc, 0, element, sizeof(struct cr_mm_region));
+		if (r)
+			goto exit_rpc;
+
+		element = element->next;
+	}
+
+	rpc_end(desc, 0);
+exit_kddmput:
+	kddm_put_object(kddm_def_ns, APP_KDDM_ID, app_id);
+	return r;
+exit_rpc:
+	rpc_cancel(desc);
+	goto exit_kddmput;
+}
+
+/*--------------------------------------------------------------------------*/
+/*--------------------------------------------------------------------------*/
+
+static long get_appid(const struct checkpoint_info *info)
+{
+	long r;
+
+	/* check if user is stupid ;-) */
+	if ((info->app_id < 0 || !(info->app_id & GLOBAL_PID_MASK))
+	    || (info->signal < 0 || info->signal >= SIGRTMIN)) {
+		r = -EINVAL;
+		ckpt_err(NULL, r,
+			 "User request contains invalid value(s).");
+		goto exit;
+	}
+
+	if (info->flags & APP_FROM_PID) {
+		r = get_appid_from_pid(info->app_id);
+		if (r < 0)
+			ckpt_err(NULL, r,
+				 "Fail to find an application hosting process %ld.",
+				 info->app_id);
+	} else
+		r = info->app_id;
+
+exit:
+	return r;
+}
+
+int app_freeze(struct checkpoint_info *info)
+{
+	int r = -EPERM;
+	long app_id = get_appid(info);
+
+	if (app_id < 0) {
+		r = app_id;
+		goto exit;
+	}
+
+	/* check that an application does not try to freeze itself */
+	if (current->application && current->application->app_id == app_id) {
+		r = -EPERM;
+		ckpt_err(NULL, r,
+			 "Application %ld is trying to freeze itself.",
+			 app_id);
+		goto exit;
+	}
+
+	info->app_id = app_id;
+
+	r = _freeze_app(app_id);
+
+exit:
+	return r;
+}
+
+int app_unfreeze(struct checkpoint_info *info)
+{
+	int r = -EPERM;
+	long app_id = get_appid(info);
+
+	if (app_id < 0) {
+		r = app_id;
+		goto exit;
+	}
+
+	BUG_ON(current->application && current->application->app_id == app_id);
+	info->app_id = app_id;
+
+	r = _unfreeze_app(app_id, info->signal);
+exit:
+	return r;
+}
+
+int app_chkpt(struct checkpoint_info *info)
+{
+	int r = -EPERM;
+	long app_id = get_appid(info);
+
+	if (app_id < 0) {
+		r = app_id;
+		goto exit;
+	}
+
+	/* check that an application does not try to checkpoint itself */
+	if (current->application && current->application->app_id == app_id) {
+		r = -EPERM;
+		ckpt_err(NULL, r,
+			"Application %ld is trying to checkpoint itself.",
+			app_id);
+		goto exit;
+	}
+
+	info->app_id = app_id;
+
+	r = _checkpoint_frozen_app(info);
+exit:
+	return r;
+}
+
+void application_checkpoint_rpc_init(void)
+{
+	rpc_register_void(APP_DO_CHKPT, handle_do_chkpt, 0);
+	rpc_register_void(APP_EXCL_MM_REGION, handle_cr_exclude, 0);
+}
diff -ruN linux-2.6.29/kerrighed/epm/application/app_checkpoint.h android_cluster/linux-2.6.29/kerrighed/epm/application/app_checkpoint.h
--- linux-2.6.29/kerrighed/epm/application/app_checkpoint.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/application/app_checkpoint.h	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,21 @@
+/**
+ *  Application checkpoint
+ *  @author Matthieu Fertré
+ */
+
+#ifndef __APPLICATION_CHECKPOINT_H__
+#define __APPLICATION_CHECKPOINT_H__
+
+#include <kerrighed/sys/checkpoint.h>
+
+int app_freeze(struct checkpoint_info *info);
+
+int app_unfreeze(struct checkpoint_info *info);
+
+int app_chkpt(struct checkpoint_info *info);
+
+int app_cr_exclude(struct cr_mm_region *mm_regions);
+
+void application_checkpoint_rpc_init(void);
+
+#endif /* __APPLICATION_CHECKPOINT_H__ */
diff -ruN linux-2.6.29/kerrighed/epm/application/app_frontier.c android_cluster/linux-2.6.29/kerrighed/epm/application/app_frontier.c
--- linux-2.6.29/kerrighed/epm/application/app_frontier.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/application/app_frontier.c	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,191 @@
+/*
+ *  kerrighed/epm/app_frontier.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2007-2008 Matthieu Fertré - INRIA
+ */
+
+#include <linux/sched.h>
+#include <linux/list.h>
+#include <linux/cred.h>
+#include <kerrighed/remote_cred.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/application.h>
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include "../checkpoint.h"
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *       USEFULL TO TRAVERSE FILIATION TREE                                 *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+static inline struct task_struct *p_cptr(struct task_struct *task)
+{
+	if (list_empty(&task->children))
+		return NULL;
+
+	return list_entry((&(task->children))->next, struct task_struct,
+			  sibling);
+}
+
+static inline struct task_struct *p_osptr(struct task_struct *task)
+{
+	return list_entry(task->sibling.next, struct task_struct, sibling);
+}
+
+static inline int no_more_brother(struct task_struct *task)
+{
+	return ((task->sibling.next) == &((task->parent)->children));
+}
+
+#define begin_for_each_son_recursive(task,son) \
+{					       \
+	int gone_up = 0;                       \
+	son = task;                            \
+	while ( !(son==task && gone_up) ) {    \
+		if (!gone_up) {
+
+#define end_for_each_son_recursive(task,son);	       \
+		}				       \
+		if (p_cptr(son) != NULL && !gone_up) { \
+			son = p_cptr(son);	       \
+			gone_up = 0;		       \
+		} else if (son != task) {	       \
+			if ( no_more_brother(son) ) {  \
+				son = son->parent;     \
+				gone_up = 1;	       \
+			} else {		       \
+				son = p_osptr(son);    \
+				gone_up = 0;	       \
+			}			       \
+		} else break;			       \
+	} son = NULL;				       \
+}
+
+/*--------------------------------------------------------------------------*
+ *--------------------------------------------------------------------------*/
+
+static inline long __get_appid_from_task(struct task_struct *task)
+{
+	long r = 0;
+
+	if (!can_be_checkpointed(task)) {
+		r = -EPERM;
+		goto exit;
+	}
+
+	if (!task->application)
+		r = create_application(task);
+
+	if (r)
+		goto exit;
+
+	BUG_ON(!task->application);
+	r = task->application->app_id;
+exit:
+	return r;
+}
+
+struct getappid_request_msg {
+	kerrighed_node_t requester;
+	pid_t pid;
+};
+
+static inline long __get_appid_from_local_pid(pid_t pid)
+{
+	struct task_struct * task;
+
+	rcu_read_lock();
+	task = find_task_by_kpid(pid);
+	rcu_read_unlock();
+	if (task)
+		return __get_appid_from_task(task);
+
+	return -ESRCH;
+}
+
+long get_appid_from_pid(pid_t pid)
+{
+	struct rpc_desc *desc;
+	kerrighed_node_t n = KERRIGHED_NODE_ID_NONE;
+	struct getappid_request_msg msg;
+	long app_id;
+	int err = 0;
+
+	/* lock the task to be sure it does not exit */
+	n = krg_lock_pid_location(pid);
+	if (n == KERRIGHED_NODE_ID_NONE)
+		return -ESRCH;
+
+	/* the task is local */
+	if (n == kerrighed_node_id) {
+		app_id =  __get_appid_from_local_pid(pid);
+		if (app_id < 0)
+			err = app_id;
+		goto out_unlock;
+	}
+
+	err = -ENOMEM;
+	msg.requester = kerrighed_node_id;
+	msg.pid = pid;
+
+	desc = rpc_begin(APP_REMOTE_CHKPT, n);
+	if (!desc)
+		goto out_unlock;
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto err;
+	err = pack_creds(desc, current_cred());
+	if (err)
+		goto err;
+
+	err = rpc_unpack_type(desc, app_id);
+	if (err)
+		goto err;
+out_end:
+	rpc_end(desc, 0);
+
+out_unlock:
+	krg_unlock_pid_location(pid);
+	if (err)
+		return err;
+	return app_id;
+
+err:
+	rpc_cancel(desc);
+	goto out_end;
+}
+
+static
+void handle_get_appid_from_pid(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	struct getappid_request_msg *msg = _msg;
+	long app_id;
+	const struct cred *old_cred;
+	int err;
+
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred)) {
+		err = PTR_ERR(old_cred);
+		goto out;
+	}
+
+	app_id = __get_appid_from_local_pid(msg->pid);
+
+	revert_creds(old_cred);
+
+	err = rpc_pack_type(desc, app_id);
+
+out:
+	if (err)
+		rpc_cancel(desc);
+}
+
+void application_frontier_rpc_init(void)
+{
+	rpc_register_void(APP_REMOTE_CHKPT, handle_get_appid_from_pid, 0);
+}
diff -ruN linux-2.6.29/kerrighed/epm/application/app_frontier.h android_cluster/linux-2.6.29/kerrighed/epm/application/app_frontier.h
--- linux-2.6.29/kerrighed/epm/application/app_frontier.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/application/app_frontier.h	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,13 @@
+/**
+ *  Application frontier
+ *  @author Matthieu Fertré
+ */
+
+#ifndef __APPLICATION_FRONTIER_H__
+#define __APPLICATION_FRONTIER_H__
+
+long get_appid_from_pid(pid_t pid);
+
+void application_frontier_rpc_init(void);
+
+#endif /* __APPLICATION_FRONTIER_H__ */
diff -ruN linux-2.6.29/kerrighed/epm/application/application.c android_cluster/linux-2.6.29/kerrighed/epm/application/application.c
--- linux-2.6.29/kerrighed/epm/application/application.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/application/application.c	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,1260 @@
+/*
+ *  kerrighed/epm/application.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2007-2008 Matthieu Fertré - INRIA
+ */
+
+#include <linux/sched.h>
+#include <linux/nsproxy.h>
+#include <linux/cred.h>
+#include <linux/hashtable.h>
+#include <kerrighed/remote_cred.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/libproc.h>
+#include <kerrighed/application.h>
+#include <kerrighed/app_shared.h>
+#include <kerrighed/ghost_helpers.h>
+#include <kerrighed/kerrighed_signal.h>
+#include <kerrighed/action.h>
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+#include "../epm_internal.h"
+#include "../checkpoint.h"
+#include "app_checkpoint.h"
+#include "app_frontier.h"
+#include "app_restart.h"
+#include "app_utils.h"
+
+/*--------------------------------------------------------------------------*
+ *--------------------------------------------------------------------------*/
+
+static hashtable_t *app_struct_table;
+
+struct app_struct *find_local_app(long app_id)
+{
+	return (struct app_struct *)hashtable_find(app_struct_table, app_id);
+}
+
+/*--------------------------------------------------------------------------*
+ *--------------------------------------------------------------------------*/
+
+static struct kddm_set *app_kddm_set;
+static struct kmem_cache *app_kddm_obj_cachep;
+static struct kmem_cache *app_struct_cachep;
+static struct kmem_cache *task_state_cachep;
+
+static int app_alloc_object(struct kddm_obj *obj_entry,
+			    struct kddm_set *kddm, objid_t objid)
+{
+	struct app_kddm_object *a;
+
+	a = kmem_cache_alloc(app_kddm_obj_cachep, GFP_KERNEL);
+	if (a == NULL)
+		return -ENOMEM;
+
+	a->app_id = 0;
+	a->chkpt_sn = 0;
+	a->state = APP_RUNNING;
+	a->user_data = 0;
+	obj_entry->object = a;
+
+	return 0;
+}
+
+static int app_remove_object(void *obj, struct kddm_set *kddm,
+			     objid_t objid)
+{
+	struct app_kddm_object *a = obj;
+	kmem_cache_free(app_kddm_obj_cachep, a);
+
+	return 0;
+}
+
+static struct iolinker_struct app_io_linker = {
+	.linker_name = "app ",
+	.linker_id = APP_LINKER,
+	.alloc_object = app_alloc_object,
+	.remove_object = app_remove_object,
+	.default_owner = global_pid_default_owner,
+};
+
+/*--------------------------------------------------------------------------*/
+
+struct app_struct *new_local_app(long app_id)
+{
+	struct app_struct *app;
+	int r;
+
+	app = kmem_cache_zalloc(app_struct_cachep, GFP_KERNEL);
+	if (!app) {
+		app = ERR_PTR(-ENOMEM);
+		goto exit;
+	}
+
+	app->app_id = app_id;
+	app->chkpt_sn = 0;
+
+	mutex_init(&app->mutex);
+	init_completion(&app->tasks_chkpted);
+	INIT_LIST_HEAD(&app->tasks);
+	app->shared_objects.root = RB_ROOT;
+	spin_lock_init(&app->shared_objects.lock);
+
+	/*
+	 * it may fail if:
+	 * - a previous restart has failed and cleaning is not
+	 *   yet completely finished.
+	 * - there is a lack of memory
+	 */
+	r = hashtable_add_unique(app_struct_table, app_id, app);
+	if (r)
+		goto error_hash;
+
+exit:
+	return app;
+
+error_hash:
+	mutex_destroy(&app->mutex);
+	kmem_cache_free(app_struct_cachep, app);
+	app = ERR_PTR(r);
+	goto exit;
+}
+
+int __delete_local_app(struct app_struct *app)
+{
+	mutex_lock(&app->mutex);
+	/* should be really rare ...*/
+	if (!local_tasks_list_empty(app))
+		goto exit_wo_deleting;
+
+	hashtable_remove(app_struct_table, app->app_id);
+	mutex_unlock(&app->mutex);
+
+	clear_shared_objects(app);
+	mutex_destroy(&app->mutex);
+	kmem_cache_free(app_struct_cachep, app);
+	return 0;
+
+exit_wo_deleting:
+	mutex_unlock(&app->mutex);
+	return -EAGAIN;
+}
+
+void delete_app(struct app_struct *app)
+{
+	int r = 0;
+	struct app_kddm_object *obj = NULL;
+
+	mutex_lock(&app->mutex);
+	if (!local_tasks_list_empty(app)) {
+		mutex_unlock(&app->mutex);
+		return;
+	}
+	mutex_unlock(&app->mutex);
+
+	obj = _kddm_grab_object_no_ft(app_kddm_set, app->app_id);
+	if (!obj) /* another process was running delete_app concurrently */
+		goto exit;
+
+	r = __delete_local_app(app);
+	if (r)
+		goto exit_put;
+
+	krgnode_clear(kerrighed_node_id, obj->nodes);
+
+	if (krgnodes_empty(obj->nodes)) {
+		_kddm_remove_frozen_object(app_kddm_set, obj->app_id);
+		goto exit;
+	}
+
+exit_put:
+	_kddm_put_object(app_kddm_set, obj->app_id);
+exit:
+	return;
+}
+
+/*--------------------------------------------------------------------------*/
+
+int create_application(struct task_struct *task)
+{
+	struct app_struct *app;
+	struct app_kddm_object *obj;
+	long app_id = task_pid_knr(task);
+	int r = 0;
+
+	obj = _kddm_grab_object(app_kddm_set, app_id);
+
+	if (obj->app_id == app_id) {
+		_kddm_put_object(app_kddm_set, app_id);
+		r = -EBUSY;
+		goto exit;
+	}
+
+	obj->app_id = app_id;
+	obj->chkpt_sn = 0;
+
+	krgnodes_clear(obj->nodes);
+	krgnode_set(kerrighed_node_id, obj->nodes);
+	app = new_local_app(app_id);
+	if (IS_ERR(app)) {
+		r = PTR_ERR(app);
+		task->application = NULL;
+		_kddm_remove_frozen_object(app_kddm_set, app_id);
+		goto exit;
+	}
+
+	register_task_to_app(app, task);
+	_kddm_put_object(app_kddm_set, app_id);
+exit:
+	return r;
+}
+
+static inline task_state_t *__alloc_task_state(void)
+{
+	task_state_t *t;
+	t = kmem_cache_zalloc(task_state_cachep, GFP_KERNEL);
+	if (!t) {
+		t = ERR_PTR(-ENOMEM);
+		goto err_mem;
+	}
+err_mem:
+	return t;
+}
+
+static inline task_state_t *alloc_task_state_from_task(
+	struct task_struct *task)
+{
+	task_state_t *t = __alloc_task_state();
+
+	BUG_ON(!task);
+
+	if (!IS_ERR(t))
+		t->task = task;
+
+	return t;
+}
+
+task_state_t *alloc_task_state_from_pids(pid_t pid,
+					 pid_t tgid,
+					 pid_t parent,
+					 pid_t real_parent,
+					 pid_t real_parent_tgid,
+					 pid_t pgrp,
+					 pid_t session)
+{
+	task_state_t *t = __alloc_task_state();
+
+	if (IS_ERR(t))
+		goto err;
+
+	t->task = NULL;
+	t->restart.pid = pid;
+	t->restart.tgid = tgid;
+	t->restart.parent = parent;
+	t->restart.real_parent = real_parent;
+	t->restart.real_parent_tgid = real_parent_tgid;
+	t->restart.pgrp = pgrp;
+	t->restart.session = session;
+
+err:
+	return t;
+}
+
+void free_task_state(task_state_t *t)
+{
+	kmem_cache_free(task_state_cachep, t);
+}
+
+int register_task_to_app(struct app_struct *app,
+			 struct task_struct *task)
+{
+	int r = 0;
+	task_state_t *t;
+
+	BUG_ON(!app);
+	BUG_ON(!task);
+
+	t = alloc_task_state_from_task(task);
+	if (IS_ERR(t)) {
+		r = PTR_ERR(t);
+		goto err;
+	}
+	t->checkpoint.result = PCUS_RUNNING;
+
+	mutex_lock(&app->mutex);
+	task->application = app;
+	list_add_tail(&t->next_task, &app->tasks);
+	mutex_unlock(&app->mutex);
+
+err:
+	return r;
+}
+
+static int register_task_to_appid(long app_id,
+				  struct task_struct *task)
+{
+	int r;
+	struct app_struct *app;
+	struct app_kddm_object *obj;
+
+	obj = _kddm_grab_object_no_ft(app_kddm_set, app_id);
+	BUG_ON(!obj);
+
+	app = find_local_app(app_id);
+	if (!app) {
+		app = new_local_app(app_id);
+		if (IS_ERR(app)) {
+			r = PTR_ERR(app);
+			goto error;
+		}
+		krgnode_set(kerrighed_node_id, obj->nodes);
+	}
+	r = register_task_to_app(app, task);
+
+error:
+	_kddm_put_object(app_kddm_set, app_id);
+	return r;
+}
+
+void unregister_task_to_app(struct app_struct *app, struct task_struct *task)
+{
+	struct list_head *tmp, *element;
+	task_state_t *t;
+
+	BUG_ON(!app);
+
+	/* remove the task */
+	mutex_lock(&app->mutex);
+	task->application = NULL;
+
+	list_for_each_safe(element, tmp, &app->tasks) {
+		t = list_entry(element, task_state_t, next_task);
+		if (task == t->task) {
+			list_del(element);
+			free_task_state(t);
+			goto exit;
+		}
+	}
+	BUG();
+
+exit:
+	mutex_unlock(&app->mutex);
+	delete_app(app);
+}
+
+/*--------------------------------------------------------------------------*/
+
+/* app->mutex must be taken */
+task_state_t *__set_task_result(struct task_struct *task, int result)
+{
+	struct app_struct *app;
+	task_state_t *t, *ret = NULL;
+	int done_for_all_tasks = 1;
+
+	app = task->application;
+
+	list_for_each_entry(t, &app->tasks, next_task) {
+		if (task == t->task) {
+			ret = t;
+
+			if (t->checkpoint.result == PCUS_RUNNING) {
+				/* result has been forced to cancel operation */
+				ret = ERR_PTR(-ECANCELED);
+				goto out;
+			}
+
+			t->checkpoint.result = result;
+		}
+
+		if (t->checkpoint.result == PCUS_CHKPT_IN_PROGRESS ||
+		    t->checkpoint.result == PCUS_STOP_STEP1 ||
+		    t->checkpoint.result == PCUS_STOP_STEP2)
+			done_for_all_tasks = 0;
+	}
+
+	if (done_for_all_tasks)
+		complete(&app->tasks_chkpted);
+
+out:
+	BUG_ON(!ret);
+
+	return ret;
+}
+
+task_state_t *set_result_wait(int result)
+{
+	struct app_struct *app;
+	task_state_t *current_state;
+
+	app = current->application;
+	BUG_ON(!app);
+
+	mutex_lock(&app->mutex);
+	current_state = __set_task_result(current, result);
+	if (IS_ERR(current_state)) {
+		mutex_unlock(&app->mutex);
+		goto out;
+	}
+
+	init_completion(&current_state->checkpoint.completion);
+	mutex_unlock(&app->mutex);
+
+	/*
+	 * the task_state_t can disappear only:
+	 * 1) when aborting a restart
+	 * 2) when the process itself exits
+	 *
+	 * both are impossible here, we can safely release app->mutex before
+	 * waiting for the completion.
+	 */
+	wait_for_completion(&current_state->checkpoint.completion);
+
+out:
+	return current_state;
+}
+
+/* before running this method, be sure stops are completed */
+static int get_local_tasks_stop_result(struct app_struct* app)
+{
+	int r = 0, pcus_result = 0;
+	task_state_t *t;
+
+	mutex_lock(&app->mutex);
+
+	list_for_each_entry(t, &app->tasks, next_task) {
+		pcus_result = t->checkpoint.result;
+		BUG_ON(pcus_result == PCUS_STOP_STEP1);
+
+		if (pcus_result == PCUS_RUNNING) {
+			/* one process has been forgotten! try again!! */
+			r = pcus_result;
+			goto exit;
+		} else if (t->task->state == TASK_DEAD) {
+			/* Process is zombie !! */
+			r = -E_CR_TASKDEAD;
+			ckpt_err(NULL, r,
+				 "Process %d (%s) of application %ld is dead"
+				 " or zombie",
+				 t->task->pid, t->task->comm, app->app_id);
+			goto exit;
+		}
+		if (!r)
+			r = pcus_result;
+	}
+
+exit:
+	mutex_unlock(&app->mutex);
+
+	return r;
+}
+
+/* before running this method, be sure checkpoints are completed */
+int get_local_tasks_chkpt_result(struct app_struct* app)
+{
+	int r = 0, pcus_result = 0;
+	task_state_t *t;
+
+	mutex_lock(&app->mutex);
+
+	list_for_each_entry(t, &app->tasks, next_task) {
+		pcus_result = t->checkpoint.result;
+		if (pcus_result == PCUS_RUNNING) {
+			/* one process has been forgotten! try again!! */
+			r = pcus_result;
+			goto exit;
+		}
+
+		if (t->checkpoint.ghost) {
+			if (pcus_result < 0)
+				unlink_file_ghost(t->checkpoint.ghost);
+			r = ghost_close(t->checkpoint.ghost);
+			t->checkpoint.ghost = NULL;
+		}
+
+		if (!r)
+			r = pcus_result;
+	}
+
+exit:
+	mutex_unlock(&app->mutex);
+
+	return r;
+}
+
+/*--------------------------------------------------------------------------*/
+
+int krg_copy_application(struct task_struct *task)
+{
+	int r = 0;
+	task->application = NULL;
+
+	if (!task->nsproxy->krg_ns)
+		return 0;
+
+	/* father is no more checkpointable? */
+	if (!cap_raised(current->krg_caps.effective, CAP_CHECKPOINTABLE) &&
+	    current->application)
+		unregister_task_to_app(current->application, current);
+
+
+	/* did we get the CHECKPOINTABLE capability? */
+	if (!cap_raised(task->krg_caps.effective, CAP_CHECKPOINTABLE))
+		return 0;
+
+	/*
+	 * father is CHECKPOINTABLE but is not associatied to an application,
+	 * fix it!
+	 */
+	if (cap_raised(current->krg_caps.effective, CAP_CHECKPOINTABLE) &&
+	    !current->application)
+		r = create_application(current);
+
+	if (r)
+		goto err;
+
+	if (current->application)
+		r = register_task_to_app(current->application, task);
+
+	/*
+	 * The following can be done only when needed. Doing this will optimize
+	 * the forking time.
+	 */
+	/* else
+	   r = create_application(task);*/
+
+err:
+	return r;
+}
+
+void krg_exit_application(struct task_struct *task)
+{
+	if (task->application)
+		unregister_task_to_app(task->application, task);
+}
+
+/*--------------------------------------------------------------------------*/
+
+int export_application(struct epm_action *action,
+		       ghost_t *ghost, struct task_struct *task)
+{
+	int r = 0;
+	long app_id = -1;
+
+	BUG_ON(!task);
+
+	/* leave an application if no more checkpointable */
+	if (!cap_raised(task->krg_caps.effective, CAP_CHECKPOINTABLE) &&
+	    task->application)
+		unregister_task_to_app(task->application, task);
+
+	/* Lazy creation of application (step 2/2) */
+	/* If process is checkpointable but not in an application
+	   and action = REMOTE_CLONE, create the application */
+	if (cap_raised(task->krg_caps.effective, CAP_CHECKPOINTABLE) &&
+	    !task->application && action->type == EPM_REMOTE_CLONE)
+		create_application(task);
+
+	if (!task->application)
+		app_id = -1;
+	else
+		app_id = task->application->app_id;
+
+	r = ghost_write(ghost, &app_id, sizeof(long));
+
+	return r;
+}
+
+int import_application(struct epm_action *action,
+		       ghost_t *ghost, struct task_struct *task)
+{
+	int r;
+	long app_id;
+
+	task->application = NULL;
+
+	r = ghost_read(ghost, &app_id, sizeof(long));
+	if (r)
+		goto out;
+
+	if (action->type == EPM_CHECKPOINT)
+		return 0;
+
+	if (!cap_raised(task->krg_caps.effective, CAP_CHECKPOINTABLE))
+		return 0;
+
+	if (app_id == -1) {
+		/* this can be done later ... (lazy creation of application) */
+		/* create_application(task); */
+	} else
+		r = register_task_to_appid(app_id, task);
+out:
+	return r;
+}
+
+void unimport_application(struct epm_action *action,
+			  ghost_t *ghost, struct task_struct *task)
+{
+	if (!task->application)
+		return;
+
+	unregister_task_to_app(task->application, task);
+}
+
+/*--------------------------------------------------------------------------*/
+
+/* app->mutex must be held */
+static void local_cancel_stop(struct app_struct *app)
+{
+	task_state_t *tsk;
+	int r;
+
+	list_for_each_entry(tsk, &app->tasks, next_task) {
+		if (tsk->checkpoint.result == PCUS_RUNNING)
+			goto out;
+		r = krg_action_stop(tsk->task, EPM_CHECKPOINT);
+		BUG_ON(r);
+		if (tsk->checkpoint.result == PCUS_OPERATION_OK)
+			complete(&tsk->checkpoint.completion);
+		tsk->checkpoint.result = PCUS_RUNNING;
+	}
+
+out:
+	return;
+}
+
+/* app->mutex must be held */
+static int local_prepare_stop(struct app_struct *app)
+{
+	task_state_t *tsk;
+	int r = 0;
+
+	list_for_each_entry(tsk, &app->tasks, next_task) {
+		if (tsk->checkpoint.result == PCUS_RUNNING) {
+			if (!can_be_checkpointed(tsk->task)) {
+				r = -EPERM;
+				goto error;
+			}
+
+			/* Process is zombie !! */
+			if (tsk->task->state == TASK_DEAD) {
+				r = -E_CR_TASKDEAD;
+				goto error;
+			}
+
+			r = krg_action_start(tsk->task, EPM_CHECKPOINT);
+			if (r) {
+				ckpt_err(NULL, r,
+					 "krg_action_start fails for "
+					 "process %d %s",
+					 tsk->task->pid, tsk->task->comm);
+				goto error;
+			}
+
+			tsk->checkpoint.result = PCUS_STOP_STEP1;
+		}
+	}
+
+error:
+	return r;
+}
+
+/* app->mutex must be held */
+static void local_complete_stop(struct app_struct *app)
+{
+	task_state_t *tsk;
+	struct siginfo info;
+	int r, signo;
+
+	signo = KRG_SIG_CHECKPOINT;
+	info.si_errno = 0;
+	info.si_pid = 0;
+	info.si_uid = 0;
+
+	list_for_each_entry(tsk, &app->tasks, next_task) {
+		if (tsk->checkpoint.result == PCUS_STOP_STEP1) {
+			tsk->checkpoint.result = PCUS_STOP_STEP2;
+			r = send_kerrighed_signal(signo, &info, tsk->task);
+			BUG_ON(r);
+		}
+	}
+}
+
+/* app->mutex must be NOT held */
+static int local_wait_stop(struct app_struct *app)
+{
+	int r = PCUS_STOP_STEP2;
+
+	while (r == PCUS_STOP_STEP2) {
+		/* waiting for timeout is needed for process becoming zombie */
+		wait_for_completion_timeout(&app->tasks_chkpted, 100);
+
+		r = get_local_tasks_stop_result(app);
+
+		/*
+		 * A process may have been forgotten because it is a child of
+		 * a process which has forked before handling the signal but
+		 * after looping on each processes of the application
+		 */
+		if (r == PCUS_RUNNING) {
+			r = -EAGAIN;
+			goto error;
+		}
+	}
+
+error:
+	return r;
+}
+
+struct app_stop_msg {
+	kerrighed_node_t requester;
+	long app_id;
+};
+
+static void handle_app_stop(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	int r;
+	struct app_stop_msg *msg = _msg;
+	struct app_struct *app;
+	const struct cred *old_cred;
+
+	app = find_local_app(msg->app_id);
+	BUG_ON(!app);
+
+	mutex_lock(&app->mutex);
+
+	r = 0;
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred))
+		r = PTR_ERR(old_cred);
+
+	/*
+	 * Check there is still some processes
+	 * A freeze may happen just before deletion of the local app_struct
+	 */
+	if (!r && list_empty(&app->tasks))
+		r = -EAGAIN;
+
+	r = send_result(desc, r);
+	if (r)
+		goto out_unlock;
+
+	init_completion(&app->tasks_chkpted);
+
+	r = local_prepare_stop(app);
+
+	r = send_result(desc, r);
+	if (r)
+		goto out_cancel_stop;
+
+	local_complete_stop(app);
+
+	mutex_unlock(&app->mutex);
+
+	r = local_wait_stop(app);
+
+	r = send_result(desc, r);
+	if (r)
+		goto out_wait_failed;
+
+out:
+	if (!IS_ERR(old_cred))
+		revert_creds(old_cred);
+
+	if (r)
+		rpc_cancel(desc);
+
+	return;
+
+out_wait_failed:
+	mutex_lock(&app->mutex);
+out_cancel_stop:
+	local_cancel_stop(app);
+out_unlock:
+	mutex_unlock(&app->mutex);
+	goto out;
+}
+
+int global_stop(struct app_kddm_object *obj)
+{
+	struct rpc_desc *desc;
+	struct app_stop_msg msg;
+	int err_rpc, r;
+
+	/* prepare message */
+	msg.requester = kerrighed_node_id;
+	msg.app_id = obj->app_id;
+
+	desc = rpc_begin_m(APP_STOP, &obj->nodes);
+	err_rpc = rpc_pack_type(desc, msg);
+	if (err_rpc)
+		goto err_rpc;
+
+	err_rpc = pack_creds(desc, current_cred());
+	if (err_rpc)
+		goto err_rpc;
+
+	/* waiting results from the node hosting the application */
+	r = app_wait_returns_from_nodes(desc, obj->nodes);
+	if (r)
+		goto error;
+
+	/* asking to prepare stop */
+	r = ask_nodes_to_continue(desc, obj->nodes, r);
+	if (r)
+		goto error;
+
+	/* asking to complete and wait stop */
+	r = ask_nodes_to_continue(desc, obj->nodes, r);
+	if (r)
+		goto error;
+
+	/* informing nodes that everyting is fine */
+	err_rpc = rpc_pack_type(desc, r);
+	if (err_rpc)
+		goto err_rpc;
+
+exit:
+	rpc_end(desc, 0);
+	return r;
+
+err_rpc:
+	r = err_rpc;
+error:
+	rpc_cancel(desc);
+	goto exit;
+}
+
+/*--------------------------------------------------------------------------*/
+
+/* wake up a local process (blocking request) */
+static void __continue_task(task_state_t *tsk, int first_run)
+{
+	BUG_ON(!tsk);
+
+	krg_action_stop(tsk->task, EPM_CHECKPOINT);
+	tsk->checkpoint.result = PCUS_RUNNING;
+	tsk->checkpoint.ghost = NULL;
+
+	if (!first_run)
+		complete(&tsk->checkpoint.completion);
+	else
+		wake_up_new_task(tsk->task, CLONE_VM);
+}
+
+static void __local_continue(struct app_struct *app, int first_run)
+{
+	task_state_t *tsk;
+
+	BUG_ON(!app);
+
+	mutex_lock(&app->mutex);
+
+	BUG_ON(list_empty(&app->tasks));
+
+	/* make all the local processes of the application going back to
+	 * computation */
+	list_for_each_entry(tsk, &app->tasks, next_task) {
+		__continue_task(tsk, first_run);
+	}
+
+	mutex_unlock(&app->mutex);
+}
+
+struct app_continue_msg {
+	kerrighed_node_t requester;
+	long app_id;
+	int first_run;
+};
+
+static void handle_app_continue(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	int r = 0;
+	struct app_continue_msg *msg = _msg;
+	struct app_struct *app = find_local_app(msg->app_id);
+
+	BUG_ON(!app);
+
+	__local_continue(app, msg->first_run);
+
+	r = rpc_pack_type(desc, r);
+	if (r)
+		goto err;
+
+	return;
+
+err:
+	rpc_cancel(desc);
+}
+
+static int global_continue(struct app_kddm_object *obj)
+{
+	struct rpc_desc *desc;
+	struct app_continue_msg msg;
+	int r = 0;
+
+	/* prepare message */
+	msg.requester = kerrighed_node_id;
+	msg.app_id = obj->app_id;
+
+	BUG_ON(obj->state != APP_RESTARTED && obj->state != APP_FROZEN);
+
+	if (obj->state == APP_RESTARTED)
+		msg.first_run = 1;
+	else
+		msg.first_run = 0;
+
+	desc = rpc_begin_m(APP_CONTINUE, &obj->nodes);
+
+	r = rpc_pack_type(desc, msg);
+	if (r)
+		goto err_rpc;
+
+	/* waiting results from the node hosting the application */
+	r = app_wait_returns_from_nodes(desc, obj->nodes);
+
+exit:
+	rpc_end(desc, 0);
+
+	return r;
+
+err_rpc:
+	rpc_cancel(desc);
+	goto exit;
+}
+
+/*--------------------------------------------------------------------------*/
+
+static int _kill_process(task_state_t *tsk, int signal)
+{
+	int r;
+	if (!can_be_checkpointed(tsk->task)) {
+		r = -EPERM;
+		goto exit;
+	}
+
+	r = kill_pid(task_pid(tsk->task), signal, 1);
+	if (r)
+		goto exit;
+
+exit:
+	return r;
+}
+
+static inline int __local_kill(struct app_struct *app, int signal)
+{
+	int retval = 0;
+	int r = 0;
+	task_state_t *tsk;
+
+	BUG_ON(!app);
+
+	mutex_lock(&app->mutex);
+
+	BUG_ON(list_empty(&app->tasks));
+
+	/* signal all the local processes of the application */
+	list_for_each_entry(tsk, &app->tasks, next_task) {
+		retval = _kill_process(tsk, signal);
+		r = retval | r;
+	}
+
+	mutex_unlock(&app->mutex);
+
+	return r;
+}
+
+struct app_kill_msg {
+	kerrighed_node_t requester;
+	long app_id;
+	int signal;
+};
+
+static void handle_app_kill(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	int r;
+	struct app_kill_msg *msg = _msg;
+	struct app_struct *app = find_local_app(msg->app_id);
+	const struct cred *old_cred;
+
+	BUG_ON(!app);
+
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred)) {
+		r = PTR_ERR(old_cred);
+		goto send_res;
+	}
+
+	r = __local_kill(app, msg->signal);
+
+	revert_creds(old_cred);
+
+send_res:
+	r = rpc_pack_type(desc, r);
+	if (r)
+		goto err;
+
+	return;
+err:
+	rpc_cancel(desc);
+}
+
+static int global_kill(struct app_kddm_object *obj, int signal)
+{
+	struct rpc_desc *desc;
+	struct app_kill_msg msg;
+	int r = 0;
+
+	/* prepare message */
+	msg.requester = kerrighed_node_id;
+	msg.app_id = obj->app_id;
+	msg.signal = signal;
+
+	desc = rpc_begin_m(APP_KILL, &obj->nodes);
+
+	r = rpc_pack_type(desc, msg);
+	if (r)
+		goto err_rpc;
+	r = pack_creds(desc, current_cred());
+	if (r)
+		goto err_rpc;
+
+	/* waiting results from the node hosting the application */
+	r = app_wait_returns_from_nodes(desc, obj->nodes);
+
+exit:
+	rpc_end(desc, 0);
+
+	return r;
+
+err_rpc:
+	rpc_cancel(desc);
+	goto exit;
+}
+
+int global_unfreeze(struct app_kddm_object *obj, int signal)
+{
+	int r;
+
+	if (obj->state != APP_FROZEN
+	    && obj->state != APP_RESTARTED) {
+		r = -EPERM;
+		goto err;
+	}
+
+	if (signal) {
+		r = global_kill(obj, signal);
+		if (r)
+			goto err;
+	}
+
+	r = global_continue(obj);
+	if (r)
+		goto err;
+
+	obj->state = APP_RUNNING;
+err:
+	return r;
+}
+
+int app_set_userdata(__u64 user_data)
+{
+	int r = 0;
+	struct app_kddm_object *obj;
+
+	if (!can_be_checkpointed(current)) {
+		r = -EPERM;
+		goto exit;
+	}
+
+	if (!current->application) {
+		r = create_application(current);
+		if (r)
+			goto exit;
+	}
+
+	obj = kddm_grab_object_no_ft(kddm_def_ns, APP_KDDM_ID,
+				     current->application->app_id);
+	if (!obj) {
+		r = -ESRCH;
+		goto exit_kddmput;
+	}
+
+	obj->user_data = user_data;
+
+exit_kddmput:
+	kddm_put_object(kddm_def_ns, APP_KDDM_ID, current->application->app_id);
+exit:
+	return r;
+}
+
+int app_get_userdata(long _appid, int flags, __u64 *user_data)
+{
+	int r = 0;
+	long app_id = _appid;
+	struct app_kddm_object *obj;
+
+	if (app_id < 0) {
+		r = -EINVAL;
+		goto exit;
+	}
+
+	if (flags & APP_FROM_PID) {
+		app_id = get_appid_from_pid(_appid);
+		if (app_id < 0) {
+			r = app_id;
+			goto exit;
+		}
+	}
+
+	obj = kddm_get_object_no_ft(kddm_def_ns, APP_KDDM_ID, app_id);
+	if (!obj) {
+		r = -ESRCH;
+		goto exit_kddmput;
+	}
+
+	*user_data = obj->user_data;
+
+exit_kddmput:
+	kddm_put_object(kddm_def_ns, APP_KDDM_ID, app_id);
+exit:
+	return r;
+}
+
+int app_cr_disable(void)
+{
+	int r = 0;
+	struct app_kddm_object *obj;
+
+	if (!can_be_checkpointed(current)) {
+		r = -EPERM;
+		goto exit;
+	}
+
+	if (!current->application) {
+		r = create_application(current);
+		if (r)
+			goto exit;
+	}
+
+	obj = kddm_grab_object_no_ft(kddm_def_ns, APP_KDDM_ID,
+				     current->application->app_id);
+	if (!obj) {
+		r = -ESRCH;
+		goto exit_kddmput;
+	}
+
+	if (obj->state == APP_RUNNING_CS)
+		r = -EALREADY;
+	else if (obj->state == APP_RUNNING)
+		obj->state = APP_RUNNING_CS;
+	else
+		BUG();
+
+exit_kddmput:
+	kddm_put_object(kddm_def_ns, APP_KDDM_ID, current->application->app_id);
+exit:
+	return r;
+}
+
+int app_cr_enable(void)
+{
+	int r = 0;
+	struct app_kddm_object *obj;
+
+	if (!can_be_checkpointed(current)) {
+		r = -EPERM;
+		goto exit;
+	}
+
+	if (!current->application) {
+		r = create_application(current);
+		if (r)
+			goto exit;
+	}
+
+	obj = kddm_grab_object_no_ft(kddm_def_ns, APP_KDDM_ID,
+				     current->application->app_id);
+	if (!obj) {
+		r = -ESRCH;
+		goto exit_kddmput;
+	}
+
+	if (obj->state == APP_RUNNING)
+		r = -EALREADY;
+	else if (obj->state == APP_RUNNING_CS)
+		obj->state = APP_RUNNING;
+	else
+		BUG();
+
+exit_kddmput:
+	kddm_put_object(kddm_def_ns, APP_KDDM_ID, current->application->app_id);
+exit:
+	return r;
+}
+
+void do_ckpt_msg(int err, char *fmt, ...)
+{
+	va_list args;
+	char *buffer;
+
+	va_start(args, fmt);
+	buffer = kvasprintf(GFP_KERNEL, fmt, args);
+	va_end(args);
+
+	if (buffer) {
+		printk("%s\n", buffer);
+		kfree(buffer);
+	} else
+		printk("WARNING: Memory is low\n"
+		       "Chekpoint/Restart operation failed with error %d\n",
+		       err);
+}
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *          APPLICATION CHECKPOINT SERVER MANAGEMENT                        *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+void application_cr_server_init(void)
+{
+	unsigned long cache_flags = SLAB_PANIC;
+#ifdef CONFIG_DEBUG_SLAB
+	cache_flags |= SLAB_POISON;
+#endif
+
+	app_struct_table = hashtable_new(5);
+
+	/*------------------------------------------------------------------*/
+
+	register_io_linker(APP_LINKER, &app_io_linker);
+
+	app_kddm_set = create_new_kddm_set(kddm_def_ns, APP_KDDM_ID,
+					   APP_LINKER, KDDM_CUSTOM_DEF_OWNER,
+					   sizeof(struct app_kddm_object),
+					   KDDM_LOCAL_EXCLUSIVE);
+	if (IS_ERR(app_kddm_set))
+		OOM;
+
+	app_kddm_obj_cachep = KMEM_CACHE(app_kddm_object, cache_flags);
+	app_struct_cachep = KMEM_CACHE(app_struct, cache_flags);
+	task_state_cachep = KMEM_CACHE(task_and_state, cache_flags);
+
+	rpc_register_void(APP_STOP, handle_app_stop, 0);
+	rpc_register_void(APP_CONTINUE, handle_app_continue, 0);
+	rpc_register_void(APP_KILL, handle_app_kill, 0);
+
+	application_frontier_rpc_init();
+	application_checkpoint_rpc_init();
+	application_restart_rpc_init();
+}
+
+void application_cr_server_finalize(void)
+{
+	if (kerrighed_node_id == 0) {
+		_destroy_kddm_set(app_kddm_set);
+	}
+	hashtable_free(app_struct_table);
+}
diff -ruN linux-2.6.29/kerrighed/epm/application/application_cr_api.c android_cluster/linux-2.6.29/kerrighed/epm/application/application_cr_api.c
--- linux-2.6.29/kerrighed/epm/application/application_cr_api.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/application/application_cr_api.c	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,103 @@
+/*
+ *  kerrighed/epm/application_cr_api.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2007-2008 Matthieu Fertré - INRIA
+ */
+
+#include <kerrighed/pid.h>
+#include <kerrighed/sys/checkpoint.h>
+#include <kerrighed/application.h>
+#include "application_cr_api.h"
+#include "app_checkpoint.h"
+#include "app_restart.h"
+
+/*****************************************************************************/
+/*                                                                           */
+/*                                SYS CALL FUNCTIONS                         */
+/*                                                                           */
+/*****************************************************************************/
+
+/**
+ *  System call function to freeze an application.
+ *  @author Matthieu Fertré
+ */
+int sys_app_freeze(struct checkpoint_info *infos)
+{
+	return app_freeze(infos);
+}
+
+/**
+ *  System call function to unfreeze an application.
+ *  @author Matthieu Fertré
+ */
+int sys_app_unfreeze(struct checkpoint_info *infos)
+{
+	return app_unfreeze(infos);
+}
+
+/**
+ *  System call function to checkpoint an application.
+ *  @author Matthieu Fertré
+ */
+int sys_app_chkpt(struct checkpoint_info *infos)
+{
+	return app_chkpt(infos);
+}
+
+/**
+ *  System call function to restart an application
+ *  @author Matthieu Fertré
+ */
+int sys_app_restart(struct restart_request *req)
+{
+	task_identity_t requester;
+
+	requester.pid = task_pid_knr(current);
+	requester.tgid = task_tgid_knr(current);
+
+	return app_restart(req, &requester);
+}
+
+/**
+ *  System call function to set a user data per application
+ *  @author Matthieu Fertré
+ */
+int sys_app_set_userdata(__u64 data)
+{
+	return app_set_userdata(data);
+}
+
+/**
+ *  System call function to get a user data per application
+ *  @author Matthieu Fertré
+ */
+int sys_app_get_userdata(struct app_userdata_request *data_req)
+{
+	return app_get_userdata(data_req->app_id, data_req->flags,
+				&data_req->user_data);
+}
+
+/**
+ *  System call function to disable use of checkpoint for current application
+ *  @author Matthieu Fertré
+ */
+int sys_app_cr_disable(void)
+{
+	return app_cr_disable();
+}
+
+/**
+ *  System call function to enable again use of checkpoint for
+ *  current application
+ *  @author Matthieu Fertré
+ */
+int sys_app_cr_enable(void)
+{
+	return app_cr_enable();
+}
+
+int sys_app_cr_exclude(struct cr_mm_region *mm_regions)
+{
+	return app_cr_exclude(mm_regions);
+}
diff -ruN linux-2.6.29/kerrighed/epm/application/application_cr_api.h android_cluster/linux-2.6.29/kerrighed/epm/application/application_cr_api.h
--- linux-2.6.29/kerrighed/epm/application/application_cr_api.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/application/application_cr_api.h	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,74 @@
+/**
+ *  Application checkpoint and restart API interface.
+ *  @file application_cr_api.h
+ *
+ *  Definition of global coordinated process checkpointing and restarting
+ *  interface.
+ *
+ *  @author Matthieu Fertré
+ */
+
+#ifndef __APPLICATION_CR_API_H__
+#define __APPLICATION_CR_API_H__
+
+#include <kerrighed/sys/checkpoint.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+/**
+ *  System call function to checkpoint an application
+ *  @author Matthieu Fertré
+ */
+int sys_app_freeze(struct checkpoint_info *info);
+
+/**
+ *  System call function to checkpoint an application
+ *  @author Matthieu Fertré
+ */
+int sys_app_unfreeze(struct checkpoint_info *info);
+
+/**
+ *  System call function to checkpoint an application
+ *  @author Matthieu Fertré
+ */
+int sys_app_chkpt(struct checkpoint_info *info);
+
+/**
+ *  System call function to restart an application
+ *  @author Matthieu Fertré
+ */
+int sys_app_restart(struct restart_request *req);
+
+/**
+ *  System call function to set a user data per application
+ *  @author Matthieu Fertré
+ */
+int sys_app_set_userdata(__u64 data);
+
+/**
+ *  System call function to get a user data per application
+ *  @author Matthieu Fertré
+ */
+int sys_app_get_userdata(struct app_userdata_request *data_req);
+
+/**
+ *  System call function to disable use of checkpoint for current application
+ *  @author Matthieu Fertré
+ */
+int sys_app_cr_disable(void);
+
+/**
+ *  System call function to enable again use of checkpoint for
+ *  current application
+ *  @author Matthieu Fertré
+ */
+int sys_app_cr_enable(void);
+
+
+int sys_app_cr_exclude(struct cr_mm_region *mm_regions);
+
+#endif /* __APPLICATION_CR_API_H__ */
diff -ruN linux-2.6.29/kerrighed/epm/application/app_restart.c android_cluster/linux-2.6.29/kerrighed/epm/application/app_restart.c
--- linux-2.6.29/kerrighed/epm/application/app_restart.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/application/app_restart.c	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,1387 @@
+/*
+ *  kerrighed/epm/app_restart.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2007-2008 Matthieu Fertré - INRIA
+ */
+
+#include <linux/sched.h>
+#include <linux/compile.h>
+#include <linux/version.h>
+#include <linux/cred.h>
+#include <kerrighed/task.h>
+#include <kerrighed/children.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/application.h>
+#include <kerrighed/app_shared.h>
+#include <kerrighed/remote_cred.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/ghost_helpers.h>
+#include <kerrighed/physical_fs.h>
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+#include "../pid.h"
+#include "../restart.h"
+#include "../epm_internal.h"
+#include "app_utils.h"
+
+static int restore_app_kddm_object(struct app_kddm_object *obj,
+				   long app_id, int chkpt_sn)
+{
+	ghost_fs_t oldfs;
+	ghost_t *ghost;
+	long r_appid;
+	int r_chkpt_sn;
+	int r = 0;
+	int r_magic, magic = 4342338;
+	u32 linux_version;
+	char compile_info[MAX_GHOST_STRING];
+
+	__set_ghost_fs(&oldfs);
+
+	ghost = create_file_ghost(GHOST_READ, app_id, chkpt_sn,
+				  "global.bin");
+
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		ckpt_err(NULL, r,
+			 "Fail to open file /var/chkpt/%ld/v%d/global.bin",
+			 app_id, chkpt_sn);
+		goto err_open;
+	}
+
+	/* check some information about the Linux kernel version */
+	r = ghost_read(ghost, &linux_version, sizeof(linux_version));
+	if (r)
+		goto err_read;
+	if (linux_version != LINUX_VERSION_CODE)
+		goto err_kernel_version;
+
+	r = ghost_read_string(ghost, compile_info);
+	if (r)
+		goto err_read;
+	if (strncmp(UTS_MACHINE, compile_info, MAX_GHOST_STRING))
+		goto err_kernel_version;
+
+	r = ghost_read_string(ghost, compile_info);
+	if (r)
+		goto err_read;
+#ifndef CONFIG_KRG_DEBUG
+	if (strncmp(UTS_VERSION, compile_info, MAX_GHOST_STRING))
+		goto err_kernel_version;
+#endif
+
+	r = ghost_read_string(ghost, compile_info);
+	if (r)
+		goto err_read;
+#ifndef CONFIG_KRG_DEBUG
+	if (strncmp(LINUX_COMPILE_TIME, compile_info, MAX_GHOST_STRING))
+		goto err_kernel_version;
+#endif
+
+	r = ghost_read_string(ghost, compile_info);
+	if (r)
+		goto err_read;
+	if (strncmp(LINUX_COMPILE_BY, compile_info, MAX_GHOST_STRING))
+		goto err_kernel_version;
+
+	r = ghost_read_string(ghost, compile_info);
+	if (r)
+		goto err_read;
+	if (strncmp(LINUX_COMPILE_HOST, compile_info, MAX_GHOST_STRING))
+		goto err_kernel_version;
+
+	r = ghost_read_string(ghost, compile_info);
+	if (r)
+		goto err_read;
+	if (strncmp(LINUX_COMPILER, compile_info, MAX_GHOST_STRING))
+		goto err_kernel_version;
+
+	/* check some information about the checkpoint itself */
+	r = ghost_read_type(ghost, r_appid);
+	if (r)
+		goto err_read;
+
+	if (r_appid != app_id) {
+		r = -E_CR_BADDATA;
+		goto err_read;
+	}
+
+	r = ghost_read_type(ghost, r_chkpt_sn);
+	if (r)
+		goto err_read;
+
+	if (r_chkpt_sn != chkpt_sn) {
+		r = -E_CR_BADDATA;
+		goto err_read;
+	}
+
+	/* initialize app_kddm_object */
+	obj->app_id = app_id;
+	obj->chkpt_sn = chkpt_sn;
+
+	r = ghost_read(ghost, &obj->nodes, sizeof(obj->nodes));
+	if (r)
+		goto err_read;
+
+	r = ghost_read(ghost, &obj->user_data, sizeof(obj->user_data));
+	if (r)
+		goto err_read;
+
+	r = ghost_read_type(ghost, r_magic);
+	if (r)
+		goto err_read;
+
+	if (r_magic != magic) {
+		r = -E_CR_BADDATA;
+		goto err_read;
+	}
+
+err_read:
+	/* End of the really interesting part */
+	ghost_close(ghost);
+
+err_open:
+	unset_ghost_fs(&oldfs);
+
+	return r;
+
+err_kernel_version:
+	r = -E_CR_BADDATA;
+	ckpt_err(NULL, r,
+		 "Restart %ld aborted: checkpoint was done on another"
+		 " kernel",
+		 app_id);
+
+	goto err_read;
+}
+
+static int was_checkpointed(struct app_struct *app, pid_t pid)
+{
+	/* What is the right way to check that ? */
+
+	int error;
+	struct nameidata nd;
+	struct prev_root prev_root;
+
+	char *filename = get_chkpt_filebase(app->app_id, app->chkpt_sn,
+					    "task_%d.bin", pid);
+	if (IS_ERR(filename))
+		return PTR_ERR(filename);
+
+	chroot_to_physical_root(&prev_root);
+	error = path_lookup(filename, 0, &nd);
+	chroot_to_prev_root(&prev_root);
+	if (!error)
+		path_put(&nd.path);
+	kfree(filename);
+
+	if (!error)
+		return 1;
+
+	return 0;
+}
+
+static int read_task_parent_links(struct app_struct *app, ghost_t *ghost,
+				  pid_t pid)
+{
+	int r = 0;
+	task_state_t *task_desc = NULL;
+	pid_t tgid, parent, real_parent, real_parent_tgid;
+	pid_t pgrp, session;
+
+	r = ghost_read(ghost, &tgid, sizeof(pid_t));
+	if (r)
+		goto err_read;
+	r = ghost_read(ghost, &parent, sizeof(pid_t));
+	if (r)
+		goto err_read;
+	r = ghost_read(ghost, &real_parent, sizeof(pid_t));
+	if (r)
+		goto err_read;
+	r = ghost_read(ghost, &real_parent_tgid, sizeof(pid_t));
+	if (r)
+		goto err_read;
+
+	if (pid == tgid) {
+		r = ghost_read(ghost, &pgrp, sizeof(pid_t));
+		if (r)
+			goto err_read;
+		r = ghost_read(ghost, &session, sizeof(pid_t));
+		if (r)
+			goto err_read;
+	} else {
+		pgrp = 0;
+		session = 0;
+	}
+
+	if (app->restart.substitution_pgrp && !was_checkpointed(app, pgrp))
+		pgrp = app->restart.substitution_pgrp;
+
+	if (app->restart.substitution_sid && !was_checkpointed(app, session))
+		session = app->restart.substitution_sid;
+
+	task_desc = alloc_task_state_from_pids(pid, tgid,
+					       parent,
+					       real_parent,
+					       real_parent_tgid,
+					       pgrp, session);
+
+	if (IS_ERR(task_desc)) {
+		r = PTR_ERR(task_desc);
+		goto err_alloc;
+	}
+
+	mutex_lock(&app->mutex);
+	list_add_tail(&task_desc->next_task, &app->tasks);
+	mutex_unlock(&app->mutex);
+
+err_read:
+err_alloc:
+	return r;
+}
+
+static int restore_local_app(long app_id, int chkpt_sn,
+			     kerrighed_node_t node_id, int duplicate,
+			     pid_t substitution_pgrp, pid_t substitution_sid)
+{
+	int r = 0;
+	ghost_fs_t oldfs;
+	ghost_t *ghost;
+	pid_t pid;
+	pid_t null = -1;
+	pid_t prev = -1;
+
+	struct app_struct *app = NULL;
+	kerrighed_node_t r_node_id;
+
+	__set_ghost_fs(&oldfs);
+
+	ghost = create_file_ghost(GHOST_READ, app_id, chkpt_sn,
+				  "node_%d.bin", node_id);
+
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		ckpt_err(NULL, r,
+			 "Fail to open file /var/chkpt/%ld/v%d/node_%u.bin",
+			 app_id, chkpt_sn, node_id);
+		goto err_open;
+	}
+
+	if (node_id == kerrighed_node_id || !duplicate) {
+		app = new_local_app(app_id);
+		if (IS_ERR(app)) {
+			r = PTR_ERR(app);
+			if (r == -EEXIST)
+				/*
+				 * cleaning of a previous failed restart is
+				 * in progress
+				 */
+				r = -EAGAIN;
+			goto err_read;
+		}
+
+		krgnodes_clear(app->restart.replacing_nodes);
+	} else {
+		do {
+			__set_current_state(TASK_UNINTERRUPTIBLE);
+			schedule_timeout(HZ);
+			app = find_local_app(app_id);
+		} while (app == NULL);
+	}
+
+	krgnode_set(node_id, app->restart.replacing_nodes);
+
+	app->chkpt_sn = chkpt_sn;
+
+	/* read the node_id */
+	r = ghost_read(ghost, &r_node_id, sizeof(kerrighed_node_t));
+	if (r)
+		goto err_read;
+
+	if (r_node_id != node_id) {
+		r = -E_CR_BADDATA;
+		goto err_read;
+	}
+
+	/* get description of each process */
+	r = ghost_read(ghost, &pid, sizeof(pid_t));
+	if (r)
+		goto err_read;
+
+	if (pid == null) {
+		/* there must be at least one process */
+		r = -E_CR_BADDATA;
+		goto err_read;
+	}
+
+	app->restart.substitution_pgrp = substitution_pgrp;
+	app->restart.substitution_sid = substitution_sid;
+
+	while (pid != null) {
+
+		r = read_task_parent_links(app, ghost, pid);
+		if (r)
+			goto err_read;
+
+		/* next! */
+		prev = pid;
+		r = ghost_read(ghost, &pid, sizeof(pid_t));
+		if (r)
+			goto err_read;
+
+		/* a process must to not be twice in the checkpoint! */
+		BUG_ON(pid == prev);
+	}
+
+err_read:
+	/* End of the really interesting part */
+	ghost_close(ghost);
+
+err_open:
+	unset_ghost_fs(&oldfs);
+
+	/* the local app_struct will be deleted later in case of error */
+	return r;
+}
+
+/*--------------------------------------------------------------------------*/
+
+struct init_restart_msg {
+	kerrighed_node_t requester;
+	long app_id;
+	int chkpt_sn;
+	int recovery;
+	pid_t substitution_pgrp;
+	pid_t substitution_sid;
+};
+
+static void handle_init_restart(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	struct init_restart_msg *msg = _msg;
+	kerrighed_node_t n = kerrighed_node_id;
+	int duplicate = 0;
+	const struct cred *old_cred;
+	int r;
+
+	if (msg->recovery) {
+		r = rpc_unpack_type(desc, n);
+		if (r)
+			goto err_rpc;
+		r = rpc_unpack_type(desc, duplicate);
+		if (r)
+			goto err_rpc;
+	}
+
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred)) {
+		r = PTR_ERR(old_cred);
+		goto send_res;
+	}
+
+	r = restore_local_app(msg->app_id, msg->chkpt_sn, n, duplicate,
+			      msg->substitution_pgrp, msg->substitution_sid);
+
+	revert_creds(old_cred);
+send_res:
+	r = rpc_pack_type(desc, r);
+	if (r)
+		goto err_rpc;
+
+	return;
+
+err_rpc:
+	rpc_cancel(desc);
+	return;
+}
+
+static inline kerrighed_node_t
+__find_node_for_restart(kerrighed_node_t *first_avail_node,
+			int *duplicate,
+			struct app_kddm_object *obj,
+			kerrighed_node_t node_id)
+{
+	int n;
+
+	/* looking for a node not involved in the application */
+	for (n = krgnode_next_online(*first_avail_node);
+	     n < KERRIGHED_MAX_NODES;
+	     n = krgnode_next_online(n)) {
+
+		if (!krgnode_isset(n, obj->nodes)) {
+			krgnode_set(n, obj->nodes);
+			goto out;
+		}
+	}
+
+	/* all nodes are implied in the application,
+	   selecting the first existing node... */
+	for (n = krgnode_next_online(0); n < KERRIGHED_MAX_NODES;
+	     n = krgnode_next_online(n)) {
+		*first_avail_node = n+1;
+		*duplicate = 1;
+		goto out;
+	}
+
+	BUG();
+
+out:
+	*first_avail_node = n+1;
+	return n;
+}
+
+static int global_init_restart(struct app_kddm_object *obj, int chkpt_sn, int flags)
+{
+	struct rpc_desc *desc;
+	struct init_restart_msg msg;
+	krgnodemask_t nodes, nodes_to_replace;
+	kerrighed_node_t prev_available_node = 0;
+	kerrighed_node_t node, recovery_node;
+	int duplicate = 0;
+	int r;
+
+	r = restore_app_kddm_object(obj, obj->app_id, chkpt_sn);
+	if (r)
+		goto exit;
+
+	/* prepare message */
+	msg.requester = kerrighed_node_id;
+	msg.app_id = obj->app_id;
+	msg.chkpt_sn = chkpt_sn;
+	msg.recovery = 0;
+
+	if (flags & APP_REPLACE_PGRP_SID) {
+		struct pid *pid;
+		msg.substitution_pgrp = task_pgrp_knr(current);
+		msg.substitution_sid = task_session_knr(current);
+
+		pid = task_pgrp(current);
+		r = cr_create_pid_kddm_object(pid);
+		if (r)
+			goto exit;
+
+		pid = task_session(current);
+		r = cr_create_pid_kddm_object(pid);
+		if (r)
+			goto exit;
+
+	} else {
+		msg.substitution_pgrp = 0;
+		msg.substitution_sid = 0;
+	}
+
+	/* prepare nodes vector */
+	krgnodes_clear(nodes);
+	krgnodes_clear(nodes_to_replace);
+	for_each_krgnode_mask(node, obj->nodes){
+		if (likely(krgnode_online(node)))
+			krgnode_set(node, nodes);
+		else
+			krgnode_set(node, nodes_to_replace);
+	}
+
+	if (!krgnodes_empty(nodes)) {
+		desc = rpc_begin_m(APP_INIT_RESTART, &nodes);
+
+		r = rpc_pack_type(desc, msg);
+		if (r)
+			goto err_rpc;
+		r = pack_creds(desc, current_cred());
+		if (r)
+			goto err_rpc;
+
+		/* waiting results */
+		r = app_wait_returns_from_nodes(desc, nodes);
+		rpc_end(desc, 0);
+	}
+
+	/* some nodes may be unavailable */
+	msg.recovery = 1;
+	for_each_krgnode_mask(node, nodes_to_replace) {
+		duplicate = 0;
+
+		recovery_node = __find_node_for_restart(
+			&prev_available_node, &duplicate, obj, node);
+
+		krgnode_set(recovery_node, nodes);
+
+		desc = rpc_begin(APP_INIT_RESTART, recovery_node);
+		r = rpc_pack_type(desc, msg);
+		if (r)
+			goto err_rpc;
+
+		r = rpc_pack_type(desc, node);
+		if (r)
+			goto err_rpc;
+
+		r = rpc_pack_type(desc, duplicate);
+		if (r)
+			goto err_rpc;
+
+		r = pack_creds(desc, current_cred());
+		if (r)
+			goto err_rpc;
+
+		r = rpc_unpack_type_from(desc, recovery_node, r);
+		if (r)
+			goto err_rpc;
+
+		rpc_end(desc, 0);
+	}
+
+	krgnodes_copy(obj->nodes, nodes);
+exit:
+	return r;
+
+err_rpc:
+	rpc_cancel(desc);
+	rpc_end(desc, 0);
+	goto exit;
+}
+
+/*--------------------------------------------------------------------------*/
+
+enum process_role {
+	THREAD_LEADER,
+	PGRP_LEADER,
+	SESSION_LEADER,
+	NOT_A_LEADER,
+};
+
+static inline int is_thread_leader(task_state_t *t)
+{
+	return (t->restart.tgid == t->restart.pid);
+}
+
+static inline int is_pgrp_leader(task_state_t *t)
+{
+	return (t->restart.pgrp == t->restart.pid);
+}
+
+static inline int is_session_leader(task_state_t *t)
+{
+	return (t->restart.session == t->restart.pid);
+}
+
+static inline int __restart_process(struct app_struct *app,
+				    task_state_t *t)
+{
+	int r = 0;
+	int flags = 0;
+	struct task_struct *task;
+
+	if (t->restart.pgrp == app->restart.substitution_pgrp)
+		flags |= APP_REPLACE_PGRP;
+	if (t->restart.session == app->restart.substitution_sid)
+		flags |= APP_REPLACE_SID;
+
+	task = restart_process(app, t->restart.pid, flags);
+
+	if (IS_ERR(task)) {
+		r = PTR_ERR(task);
+		goto error;
+	}
+
+	/* Attach to application */
+	BUG_ON(!task);
+	task->application = app;
+	t->task = task;
+error:
+	return r;
+}
+
+static int local_reserve_pid_processes(struct app_struct *app)
+{
+	task_state_t *t, *tfail;
+	int err = 0;
+
+	list_for_each_entry(t, &app->tasks, next_task) {
+		err = reserve_pid(t->restart.pid);
+		if (err) {
+			tfail = t;
+			goto error;
+		}
+	}
+
+	return 0;
+
+error:
+	list_for_each_entry(t, &app->tasks, next_task) {
+		if (t == tfail)
+			goto err_exit;
+
+		end_pid_reservation(t->restart.pid);
+	}
+err_exit:
+	return err;
+}
+
+static int local_end_reserve_pid_processes(struct app_struct *app)
+{
+	task_state_t *t;
+	int retval = 0, err = 0;
+
+	list_for_each_entry(t, &app->tasks, next_task) {
+		err = end_pid_reservation(t->restart.pid);
+		if (err) {
+			printk("kerrighed: %s:%d - End reservation of pid %d"
+			       " fails with %d. This pid cannot be reserved"
+			       " anymore until next reboot.\n",
+			       __PRETTY_FUNCTION__, __LINE__,
+			       t->restart.pid, err);
+			retval = err;
+		}
+	}
+
+	return retval;
+}
+
+typedef struct {
+	int nb;
+	struct list_head pids;
+} pids_list_t;
+
+typedef struct {
+	pid_t pid;
+	int reserved;
+	struct list_head next;
+} unique_pid_t;
+
+static inline int add_unique_pid(pids_list_t *orphan_pids, pid_t pid)
+{
+	int r = 0;
+	unique_pid_t *upid;
+
+	/* check the pid is not already in the list */
+	list_for_each_entry(upid, &(orphan_pids->pids), next) {
+		if (upid->pid == pid)
+			goto end;
+	}
+
+	/* add the pid in the list */
+	upid = kmalloc(sizeof(unique_pid_t), GFP_KERNEL);
+	if (!upid) {
+		r = -ENOMEM;
+		goto end;
+	}
+	upid->pid = pid;
+	upid->reserved = 0;
+	list_add_tail(&upid->next, &(orphan_pids->pids));
+	orphan_pids->nb++;
+end:
+	return r;
+}
+
+static inline int send_pids_list(pids_list_t *orphan_pids,
+				 struct rpc_desc *desc)
+{
+	int r = 0;
+	unique_pid_t *upid;
+
+	r = rpc_pack_type(desc, orphan_pids->nb);
+	if (r)
+		goto err;
+	list_for_each_entry(upid, &(orphan_pids->pids), next) {
+		r = rpc_pack_type(desc, upid->pid);
+		if (r)
+			goto err;
+	}
+
+err:
+	return r;
+}
+
+static inline void free_pids_list(pids_list_t *orphan_pids)
+{
+	unique_pid_t *upid;
+	struct list_head *element, *tmp;
+
+	list_for_each_safe(element, tmp, &(orphan_pids->pids)) {
+		upid = list_entry(element, unique_pid_t, next);
+
+		list_del(element);
+		kfree(upid);
+	}
+	orphan_pids->nb = 0;
+}
+
+static inline int return_orphan_sessions_and_prgps(struct app_struct *app,
+						   struct rpc_desc *desc)
+{
+	int r = 0, checkpointed;
+	task_state_t *t;
+	pids_list_t orphan_pids;
+	INIT_LIST_HEAD(&orphan_pids.pids);
+	orphan_pids.nb = 0;
+
+	/* first, build a list of orphan pids of session(s) and pgrp(s) */
+	list_for_each_entry(t, &app->tasks, next_task) {
+
+		if (t->restart.session != app->restart.substitution_sid) {
+			checkpointed = was_checkpointed(app, t->restart.session);
+			if (!checkpointed) {
+				r = add_unique_pid(&orphan_pids, t->restart.session);
+				if (r)
+					goto err;
+			} else if (checkpointed < 0) {
+				r = checkpointed;
+				goto err;
+			}
+		}
+
+		if (t->restart.pgrp != app->restart.substitution_pgrp) {
+			checkpointed = was_checkpointed(app, t->restart.pgrp);
+			if (!checkpointed) {
+				r = add_unique_pid(&orphan_pids, t->restart.pgrp);
+				if (r)
+					goto err;
+			} else if (checkpointed < 0) {
+				r = checkpointed;
+				goto err;
+			}
+		}
+	}
+
+	/* secondly, send it to the global coordinator */
+	r = send_pids_list(&orphan_pids, desc);
+
+err:
+	/* thirdly, free the list */
+	free_pids_list(&orphan_pids);
+	return r;
+}
+
+static inline int get_orphan_sessions_and_pgrps(struct rpc_desc *desc,
+						krgnodemask_t nodes,
+						pids_list_t *orphan_pids)
+{
+	kerrighed_node_t node;
+	int i, r = 0;
+
+	INIT_LIST_HEAD(&orphan_pids->pids);
+	orphan_pids->nb = 0;
+
+	for_each_krgnode_mask(node, nodes) {
+		int local_orphans;
+		pid_t pid;
+
+		r = rpc_unpack_type_from(desc, node, local_orphans);
+		if (r)
+			goto err;
+
+		for (i=0; i< local_orphans; i++) {
+			r = rpc_unpack_type_from(desc, node, pid);
+			if (r)
+				goto err;
+
+			r = add_unique_pid(orphan_pids, pid);
+			if (r)
+				goto err;
+		}
+
+		r = rpc_unpack_type_from(desc, node, r);
+		if (r)
+			goto err;
+        }
+
+out:
+	return r;
+err:
+	free_pids_list(orphan_pids);
+	goto out;
+}
+
+/* must be call for each reserved pid in error case or in normal case */
+static int end_rebuild_orphan_pids(pids_list_t *orphan_pids)
+{
+	int r = 0, ret;
+	unique_pid_t *upid;
+
+	list_for_each_entry(upid, &(orphan_pids->pids), next) {
+		if (upid->reserved) {
+			ret = end_pid_reservation(upid->pid);
+			if (ret)
+				r |= ret;
+			else
+				upid->reserved = 0;
+		}
+	}
+
+	return r;
+}
+
+static inline int rebuild_orphan_pids(pids_list_t *orphan_pids)
+{
+	int r = 0;
+	unique_pid_t *upid;
+
+	list_for_each_entry(upid, &(orphan_pids->pids), next) {
+		BUG_ON(upid->reserved);
+		r = reserve_pid(upid->pid);
+		if (r)
+			goto err;
+		upid->reserved = 1;
+	}
+
+err:
+	return r;
+}
+
+static inline int local_do_restart(struct app_struct *app,
+				   enum process_role role)
+{
+	int r = 0;
+	task_state_t *t;
+
+	BUG_ON(app == NULL);
+	BUG_ON(list_empty(&app->tasks));
+
+	list_for_each_entry(t, &app->tasks, next_task) {
+
+		if ((role == SESSION_LEADER && is_session_leader(t))
+		    || (role == PGRP_LEADER && is_pgrp_leader(t)
+			&& !t->task)
+		    || (role == THREAD_LEADER && is_thread_leader(t)
+			&& !t->task)
+		    || (role == NOT_A_LEADER && !t->task)) {
+
+			BUG_ON(t->task);
+
+			r = __restart_process(app, t);
+			if (r)
+				goto exit;
+		}
+	}
+exit:
+	return r;
+}
+
+static int local_replace_parent(struct app_struct *app,
+				const task_identity_t *requester,
+				pid_t *root_pid)
+{
+	task_state_t *t;
+	int r = 0, checkpointed;
+
+	list_for_each_entry(t, &app->tasks, next_task) {
+
+		checkpointed = was_checkpointed(app, t->restart.parent);
+
+		if (!checkpointed) {
+			if (t->restart.parent != 1) {
+				/* parent was not checkpointed and was not
+				   "init" process, we will reparent to the
+				   restart cmd */
+				t->restart.parent = requester->pid;
+				t->restart.real_parent = requester->pid;
+				t->restart.real_parent_tgid = requester->tgid;
+
+				*root_pid = t->restart.tgid;
+			}
+		} else if (checkpointed < 0) {
+			r = checkpointed;
+			goto err;
+		}
+	}
+
+err:
+	return r;
+}
+
+static int local_restore_task_object(struct app_struct *app)
+{
+	int r = 0;
+	task_state_t *t;
+	struct task_struct *task;
+
+	list_for_each_entry(t, &app->tasks, next_task) {
+		task = t->task;
+
+		if (t->restart.parent != 1) {
+
+			task->task_obj = __krg_task_writelock(task);
+
+			write_lock_irq(&tasklist_lock);
+
+			task->task_obj->parent = t->restart.parent;
+			task->task_obj->real_parent = t->restart.real_parent;
+			task->task_obj->real_parent_tgid =
+				t->restart.real_parent_tgid;
+
+			task->parent = baby_sitter;
+			task->real_parent = baby_sitter;
+			list_move(&task->sibling, &baby_sitter->children);
+
+			write_unlock_irq(&tasklist_lock);
+
+			__krg_task_unlock(task);
+		}
+
+		BUG_ON(task->task_obj->group_leader != t->restart.tgid);
+	}
+
+	return r;
+}
+
+static inline int task_restore_children_object(task_state_t *t)
+{
+	int r = 0;
+	struct children_kddm_object *obj;
+
+	if (t->restart.real_parent_tgid == 1)
+		goto exit;
+
+	BUG_ON(!(t->restart.real_parent_tgid & GLOBAL_PID_MASK));
+	obj = krg_children_writelock(t->restart.real_parent_tgid);
+
+	r = krg_new_child(obj, t->restart.real_parent, t->task);
+
+	t->task->parent_children_obj = obj;
+	krg_children_get(t->task->parent_children_obj);
+
+	krg_children_unlock(obj);
+
+exit:
+	return r;
+}
+
+static inline int local_restore_children_object(struct app_struct *app)
+{
+	int r = 0;
+	task_state_t *t;
+
+	list_for_each_entry(t, &app->tasks, next_task) {
+		task_restore_children_object(t);
+	}
+
+	return r;
+}
+
+static inline
+struct task_struct *find_thread_leader(struct app_struct *app, task_state_t *th)
+{
+	task_state_t *t;
+	struct task_struct *leader = NULL;
+
+	list_for_each_entry(t, &app->tasks, next_task) {
+		if (t->restart.pid == th->restart.tgid) {
+			BUG_ON(t->restart.pid != t->restart.tgid);
+			BUG_ON(!t->task);
+			leader = t->task;
+			goto found;
+		}
+	}
+found:
+	return leader;
+}
+
+static inline void local_join_relatives(struct app_struct *app)
+{
+	task_state_t *t;
+	struct task_struct *tsk;
+
+	list_for_each_entry(t, &app->tasks, next_task) {
+		tsk = t->task;
+
+		join_local_relatives(tsk);
+		krg_pid_link_task(task_pid_knr(tsk));
+	}
+}
+
+/*
+ * After local_abort_restart, the local app_struct does not exist
+ * anymore and the global application may not exist (it depends
+ * on other nodes).
+ */
+static void local_abort_restart(struct app_struct *app,
+				struct task_struct *fake)
+{
+	struct list_head *element;
+	task_state_t *t;
+	struct task_struct *task;
+	int r;
+
+	/* killall restarted processes */
+	mutex_lock(&app->mutex);
+	while (!list_empty(&app->tasks)) {
+		element = (&app->tasks)->next;
+		t = list_entry(element, task_state_t, next_task);
+		task = t->task;
+		list_del(element);
+		free_task_state(t);
+
+		if (task) {
+			mutex_unlock(&app->mutex);
+
+			/* kill the process which was already restarted */
+			/*
+			 * We first need to unregister the task from the
+			 * application else the task will try to do it by
+			 * itself.
+			 */
+			task->application = NULL;
+			release_task(task);
+
+			mutex_lock(&app->mutex);
+		}
+	}
+	mutex_unlock(&app->mutex);
+
+	/* destroying the shared objects that were restored for nothing */
+	if (fake)
+		destroy_shared_objects(app, fake);
+
+	r = __delete_local_app(app);
+	BUG_ON(r);
+	/* The local application does not exist anymore */
+}
+
+struct restart_request_msg {
+	kerrighed_node_t requester;
+	long app_id;
+	task_identity_t requester_task;
+};
+
+static void handle_do_restart(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	int pid_err, r;
+	pid_t root_pid = 0;
+	struct restart_request_msg *msg = _msg;
+	struct app_struct *app = find_local_app(msg->app_id);
+	struct task_struct *fake = NULL;
+	const struct cred *old_cred = NULL;
+
+	BUG_ON(app == NULL);
+
+	BUG_ON(app->cred);
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred)) {
+		r = PTR_ERR(old_cred);
+		goto err_end_pid;
+	}
+	app->cred = current_cred();
+
+	/* reserve pid of processes running locally */
+	pid_err = local_reserve_pid_processes(app);
+
+	r = send_result(desc, pid_err);
+	if (r)
+		goto error;
+
+	/* return the list of orphan sessions and pgrp */
+	r = return_orphan_sessions_and_prgps(app, desc);
+
+	r = send_result(desc, r);
+	if (r)
+		goto error;
+
+	/* restore the shared objects */
+	fake = alloc_shared_fake_task_struct(app);
+	if (IS_ERR(fake)) {
+		r = PTR_ERR(fake);
+		fake = NULL;
+		goto error;
+	}
+
+#ifdef CONFIG_KRG_DEBUG
+	{
+		int magic;
+		r = rpc_unpack_type(desc, magic);
+		BUG_ON(r);
+		BUG_ON(magic != 40);
+	}
+#endif
+
+	r = local_restart_shared(desc, app, fake, app->chkpt_sn);
+	if (r)
+		goto error;
+
+	/* restore the session leader(s) */
+	r = local_do_restart(app, SESSION_LEADER);
+
+	r = send_result(desc, r);
+	if (r)
+		goto error;
+
+	/* restore the group leader(s) */
+	r = local_do_restart(app, PGRP_LEADER);
+
+	r = send_result(desc, r);
+	if (r)
+		goto error;
+
+	/* restore the thread leader(s) */
+	r = local_do_restart(app, THREAD_LEADER);
+
+	r = send_result(desc, r);
+	if (r)
+		goto error;
+
+	/* restore the other process(es) */
+	r = local_do_restart(app, NOT_A_LEADER);
+
+	r = send_result(desc, r);
+	if (r)
+		goto error;
+
+	r = local_replace_parent(app, &msg->requester_task, &root_pid);
+	if (r) {
+		r = send_result(desc, r);
+		goto error;
+	}
+
+	r = send_result(desc, root_pid);
+	if (r)
+		goto error;
+
+	/* restore task object */
+	r = local_restore_task_object(app);
+
+	r = send_result(desc, r);
+	if (r)
+		goto error;
+
+	/* restore children object */
+	r = local_restore_children_object(app);
+
+	r = send_result(desc, r);
+	if (r) /* an error as occured on other node */
+		goto error;
+
+	/* join all together */
+	local_join_relatives(app);
+
+	/* complete the import of shared objects */
+	local_restart_shared_complete(app, fake);
+
+	r = local_end_reserve_pid_processes(app);
+
+	r = send_result(desc, r);
+	if (r)
+		goto err_end_pid;
+
+#ifdef CONFIG_KRG_DEBUG
+	{
+		int magic;
+		r = rpc_unpack_type(desc, magic);
+		BUG_ON(r);
+		BUG_ON(magic != 48);
+	}
+#endif
+
+err_end_pid:
+	if (app->cred) {
+		app->cred = NULL;
+		revert_creds(old_cred);
+	}
+
+	if (r) {
+		local_abort_restart(app, fake);
+		app = NULL;
+		r = rpc_pack_type(desc, r);
+		rpc_cancel(desc);
+	}
+
+	if (fake)
+		free_shared_fake_task_struct(fake);
+
+	return;
+
+error:
+	if (!pid_err)
+		local_end_reserve_pid_processes(app);
+	goto err_end_pid;
+}
+
+static int global_do_restart(struct app_kddm_object *obj,
+			     const task_identity_t *requester,
+			     struct restart_request *req)
+{
+	struct rpc_desc *desc;
+	struct restart_request_msg msg;
+	pids_list_t orphan_pids;
+	pid_t *root_pid = &req->root_pid;
+	int r = 0, err;
+
+	/* prepare message */
+	msg.requester = kerrighed_node_id;
+	msg.app_id = obj->app_id;
+	msg.requester_task = *requester;
+
+	desc = rpc_begin_m(APP_DO_RESTART, &obj->nodes);
+	if (!desc)
+		return -ENOMEM;
+
+	r = rpc_pack_type(desc, msg);
+	if (r)
+		goto err_no_pids;
+	r = pack_creds(desc, current_cred());
+	if (r)
+		goto err_no_pids;
+
+	/* waiting for clients to have reserved not orphan pids */
+	r = app_wait_returns_from_nodes(desc, obj->nodes);
+	if (r)
+		goto err_no_pids;
+
+	r = rpc_pack_type(desc, r);
+	if (r)
+		goto err_no_pids;
+
+	/* get the list of orphan sessions/groups */
+	r = get_orphan_sessions_and_pgrps(desc, obj->nodes, &orphan_pids);
+	if (r)
+		goto err_no_pids;
+
+	/* reserve orphan session/pgrp pids */
+	r = rebuild_orphan_pids(&orphan_pids);
+	if (r)
+		goto error;
+
+	/* loading of shared objects */
+	r = rpc_pack_type(desc, r);
+	if (r)
+		goto error;
+
+#ifdef CONFIG_KRG_DEBUG
+	{
+		int magic = 40;
+		r = rpc_pack_type(desc, magic);
+		BUG_ON(r);
+	}
+#endif
+
+	r = global_restart_shared(desc, obj, req);
+	if (r)
+		goto error;
+
+	/* waiting for clients to have rebuilt session leader(s) */
+	r = app_wait_returns_from_nodes(desc, obj->nodes);
+	if (r)
+		goto error;
+
+	/* asking to rebuild group leader(s) */
+	r = ask_nodes_to_continue(desc, obj->nodes, r);
+	if (r)
+		goto error;
+
+	/* asking to rebuild thread leader(s) */
+	r = ask_nodes_to_continue(desc, obj->nodes, r);
+	if (r)
+		goto error;
+
+	/* asking to rebuild other processes */
+	r = ask_nodes_to_continue(desc, obj->nodes, r);
+	if (r)
+		goto error;
+
+	/* requesting root_pid */
+	r = rpc_pack_type(desc, r);
+	if (r)
+		goto error;
+
+	*root_pid = app_wait_returns_from_nodes(desc, obj->nodes);
+	if (*root_pid < 0) {
+		r = *root_pid;
+		goto error;
+	}
+
+	/* asking to rebuild task_kddm_obj if r == 0 */
+	r = ask_nodes_to_continue(desc, obj->nodes, r);
+	if (r)
+		goto error;
+
+	/* asking to rebuild children_object if r == 0 */
+	r = ask_nodes_to_continue(desc, obj->nodes, r);
+	if (r)
+		goto error;
+
+	/*
+	 * asking to finish the restart:
+	 * - complete the restart of shared objects
+	 * - complete the reservation of pids
+	 */
+	r = ask_nodes_to_continue(desc, obj->nodes, r);
+
+	/* inform other nodes about current restart status */
+	r = rpc_pack_type(desc, r);
+	if (r)
+		goto error;
+
+#ifdef CONFIG_KRG_DEBUG
+	{
+		int magic = 48;
+		r = rpc_pack_type(desc, magic);
+		BUG_ON(r);
+	}
+#endif
+
+exit_free_pid:
+	err = end_rebuild_orphan_pids(&orphan_pids);
+	if (err && !r)
+		r = err;
+
+	free_pids_list(&orphan_pids);
+
+exit:
+	rpc_end(desc, 0);
+
+	return r;
+
+error:
+	rpc_cancel(desc);
+	goto exit_free_pid;
+
+err_no_pids:
+	rpc_cancel(desc);
+	goto exit;
+}
+
+/*--------------------------------------------------------------------------*/
+
+/*--------------------------------------------------------------------------*/
+
+/**
+ *  Main application restarting interface.
+ *  @author Matthieu Fertré
+ */
+int app_restart(struct restart_request *req,
+		const task_identity_t *requester)
+{
+	struct app_kddm_object *obj;
+	int r = 0;
+
+	obj = kddm_grab_object(kddm_def_ns, APP_KDDM_ID, req->app_id);
+
+	if (obj->app_id == req->app_id) {
+		r = -E_CR_APPBUSY;
+		goto exit_app_busy;
+	}
+	obj->app_id = req->app_id;
+
+	/* open the files and recreate the struct app_struct */
+	r = global_init_restart(obj, req->chkpt_sn, req->flags);
+	if (r)
+		goto exit;
+
+	/* recreate all the tasks */
+	r = global_do_restart(obj, requester, req);
+	if (!r)
+		obj->state = APP_RESTARTED;
+
+exit:
+	if (r)
+		kddm_remove_frozen_object(kddm_def_ns, APP_KDDM_ID, req->app_id);
+	else
+exit_app_busy:
+		kddm_put_object(kddm_def_ns, APP_KDDM_ID, req->app_id);
+
+	return r;
+}
+
+void application_restart_rpc_init(void)
+{
+	rpc_register_void(APP_INIT_RESTART, handle_init_restart, 0);
+	rpc_register_void(APP_DO_RESTART, handle_do_restart, 0);
+}
diff -ruN linux-2.6.29/kerrighed/epm/application/app_restart.h android_cluster/linux-2.6.29/kerrighed/epm/application/app_restart.h
--- linux-2.6.29/kerrighed/epm/application/app_restart.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/application/app_restart.h	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,13 @@
+/** Application restart
+ *  @author Matthieu Fertré
+ */
+
+#ifndef __APPLICATION_RESTART_H__
+#define __APPLICATION_RESTART_H__
+
+int app_restart(struct restart_request *req,
+		const task_identity_t *requester);
+
+void application_restart_rpc_init(void);
+
+#endif /* __APPLICATION_RESTART_H__ */
diff -ruN linux-2.6.29/kerrighed/epm/application/app_shared.c android_cluster/linux-2.6.29/kerrighed/epm/application/app_shared.c
--- linux-2.6.29/kerrighed/epm/application/app_shared.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/application/app_shared.c	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,1879 @@
+/** Application management of struct(s) shared by several processes
+ *
+ * The following definitions help to understand the code
+ * - shared: an object that *can* be linked to several processes
+ * - dist(ributed): an object that *is* linked to several processes on
+ *                  different nodes
+ * - local: an object that is *not distributed*. the object *may* be linked
+ *          to severall processes, but in that case, they are all on
+ *          the *same* node
+ *
+ *
+ *  Copyright (C) 2007-2008 Matthieu Fertré - INRIA
+ */
+
+#include <linux/file.h>
+#include <linux/list.h>
+#include <linux/rbtree.h>
+#include <linux/sched.h>
+#include <linux/stat.h>
+#include <linux/unique_id.h>
+#include <linux/nsproxy.h>
+#include <linux/utsname.h>
+#include <linux/ipc_namespace.h>
+#include <linux/mnt_namespace.h>
+#include <linux/pid_namespace.h>
+#include <net/net_namespace.h>
+#include <kerrighed/faf.h>
+#include <kerrighed/faf_file_mgr.h>
+#include <kerrighed/file.h>
+#include <kerrighed/namespace.h>
+#include <kerrighed/regular_file_mgr.h>
+#include <kerrighed/task.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+#include <kerrighed/application.h>
+#include <kerrighed/ghost_helpers.h>
+#include <kerrighed/app_shared.h>
+#include <kerrighed/ghost_helpers.h>
+#include "../epm_internal.h"
+#include "app_utils.h"
+
+/*--------------------------------------------------------------------------*/
+
+extern struct shared_object_operations cr_shared_pipe_inode_ops;
+extern struct shared_object_operations cr_shared_file_ops;
+extern struct shared_object_operations cr_shared_unsupported_file_ops;
+extern struct shared_object_operations cr_shared_files_struct_ops;
+extern struct shared_object_operations cr_shared_fs_struct_ops;
+#ifdef CONFIG_KRG_MM
+extern struct shared_object_operations cr_shared_mm_struct_ops;
+#endif
+extern struct shared_object_operations cr_shared_semundo_ops;
+extern struct shared_object_operations cr_shared_sighand_struct_ops;
+extern struct shared_object_operations cr_shared_signal_struct_ops;
+
+static struct shared_object_operations * get_shared_ops(
+	enum shared_obj_type type)
+{
+	struct shared_object_operations * s_ops = NULL;
+
+	switch (type) {
+	case PIPE_INODE:
+		s_ops = &cr_shared_pipe_inode_ops;
+		break;
+	case LOCAL_FILE:
+	case DVFS_FILE:
+		s_ops = &cr_shared_file_ops;
+		break;
+	case FILES_STRUCT:
+		s_ops = &cr_shared_files_struct_ops;
+		break;
+	case FS_STRUCT:
+		s_ops = &cr_shared_fs_struct_ops;
+		break;
+#ifdef CONFIG_KRG_MM
+	case MM_STRUCT:
+		s_ops = &cr_shared_mm_struct_ops;
+		break;
+#endif
+	case SEMUNDO_LIST:
+		s_ops = &cr_shared_semundo_ops;
+		break;
+	case SIGHAND_STRUCT:
+		s_ops = &cr_shared_sighand_struct_ops;
+		break;
+	case SIGNAL_STRUCT:
+		s_ops = &cr_shared_signal_struct_ops;
+		break;
+	default:
+		BUG();
+		break;
+	}
+	return s_ops;
+}
+
+/*--------------------------------------------------------------------------*/
+
+struct shared_index {
+	struct rb_node node;
+
+	enum shared_obj_type type;
+	unsigned long key;
+};
+
+static struct rb_node * search_node(struct rb_root *root,
+				    enum shared_obj_type type,
+				    unsigned long key)
+{
+	struct rb_node *node = root->rb_node;
+
+	while (node) {
+		struct shared_index *idx =
+			container_of(node, struct shared_index, node);
+		int result;
+
+		result = type - idx->type;
+
+		if (result < 0)
+			node = node->rb_left;
+		else if (result > 0)
+			node = node->rb_right;
+		else {
+			result = key - idx->key;
+			if (result < 0)
+				node = node->rb_left;
+			else if (result > 0)
+				node = node->rb_right;
+			else
+				return node;
+		}
+	}
+	return NULL;
+}
+
+static struct shared_index *search_shared_index(struct rb_root *root,
+						enum shared_obj_type type,
+						unsigned long key)
+{
+	struct rb_node *node = root->rb_node;
+
+	while (node) {
+		struct shared_index *idx;
+		int result;
+
+		idx = container_of(node, struct shared_index, node);
+		result = type - idx->type;
+
+		if (result < 0)
+			node = node->rb_left;
+		else if (result > 0)
+			node = node->rb_right;
+		else {
+			result = key - idx->key;
+			if (result < 0)
+				node = node->rb_left;
+			else if (result > 0)
+				node = node->rb_right;
+			else
+				return idx;
+		}
+	}
+	return NULL;
+}
+
+static struct shared_index *__insert_shared_index(struct rb_root *root,
+						  struct shared_index *idx)
+{
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+
+	/* Figure out where to put new node */
+	while (*new) {
+		struct shared_index *this =
+			container_of(*new, struct shared_index, node);
+
+		int result = idx->type - this->type;
+
+		parent = *new;
+		if (result < 0)
+			new = &((*new)->rb_left);
+		else if (result > 0)
+			new = &((*new)->rb_right);
+		else {
+			result = idx->key - this->key;
+			if (result < 0)
+				new = &((*new)->rb_left);
+			else if (result > 0)
+				new = &((*new)->rb_right);
+			else
+				return this;
+		}
+	}
+
+	/* Add new node and rebalance tree. */
+	rb_link_node(&idx->node, parent, new);
+	rb_insert_color(&idx->node, root);
+
+	return idx;
+}
+
+static int insert_shared_index(struct rb_root *root, struct shared_index *idx)
+{
+	struct shared_index *idx2;
+	idx2 = __insert_shared_index(root, idx);
+	if (idx2 == idx)
+		return 0;
+
+	return -ENOKEY;
+}
+
+/*--------------------------------------------------------------------------*/
+
+struct shared_object {
+	struct shared_index index;
+	struct shared_object_operations *ops;
+
+	union {
+		struct {
+			struct export_obj_info export;
+			enum object_locality locality;
+		} checkpoint;
+		struct {
+			/* SHARED_ANY must not be used at restart */
+			enum object_locality locality;
+
+			void *data;
+			size_t data_size;
+		} restart;
+	};
+};
+
+static struct shared_object *search_shared_object(struct rb_root *root,
+						  enum shared_obj_type type,
+						  unsigned long key)
+{
+	struct shared_object *data = NULL;
+	struct shared_index *idx = search_shared_index(root, type, key);
+
+	if (!idx)
+		goto out;
+
+	data = container_of(idx, struct shared_object, index);
+
+out:
+	return data;
+}
+
+void * get_imported_shared_object(struct app_struct *app,
+				  enum shared_obj_type type,
+				  unsigned long key)
+{
+	void *data = NULL;
+	struct shared_object *s;
+
+	spin_lock(&app->shared_objects.lock);
+	s = search_shared_object(&app->shared_objects.root, type, key);
+	if (s)
+		data = s->restart.data;
+	spin_unlock(&app->shared_objects.lock);
+
+	return data;
+}
+
+/* to use only at checkpoint time! */
+int add_to_shared_objects_list(struct app_struct *app,
+			       enum shared_obj_type type,
+			       unsigned long key,
+			       enum object_locality locality,
+			       struct task_struct *exporting_task,
+			       union export_args *args,
+			       int force)
+{
+	int r;
+	struct shared_object_operations *s_ops;
+	struct shared_object *s;
+
+	s_ops = get_shared_ops(type);
+	s = kmalloc(sizeof(struct shared_object), GFP_KERNEL);
+
+	if (!s)
+		return -ENOMEM;
+
+	s->index.type = type;
+	s->index.key = key;
+
+	spin_lock(&app->shared_objects.lock);
+
+	r = insert_shared_index(&app->shared_objects.root, &s->index);
+	if (r) {
+		/* shared object is already in the list */
+		kfree(s);
+
+		/* we should record additionnal info about exporting task */
+		if (force) {
+			struct export_obj_info *export;
+
+			s = search_shared_object(&app->shared_objects.root,
+						 type, key);
+			BUG_ON(!s);
+
+			export = kmalloc(sizeof(struct export_obj_info),
+					 GFP_KERNEL);
+			if (!export) {
+				kfree(s);
+				goto err_unlock;
+			}
+
+			export->task = exporting_task;
+			if (args)
+				export->args = *args;
+			list_add_tail(&export->next,
+				      &s->checkpoint.export.next);
+			r = 0;
+		}
+#ifdef CONFIG_KRG_DEBUG
+		else {
+			s = search_shared_object(&app->shared_objects.root,
+						 type, key);
+			BUG_ON(!s);
+		}
+#endif
+	} else {
+		/* the object was not in the list, finishing initialization */
+		s->ops = s_ops;
+
+		s->checkpoint.export.task = exporting_task;
+		if (args)
+			s->checkpoint.export.args = *args;
+		INIT_LIST_HEAD(&s->checkpoint.export.next);
+
+		s->checkpoint.locality = locality;
+	}
+err_unlock:
+	spin_unlock(&app->shared_objects.lock);
+
+	return r;
+}
+
+static void clear_one_shared_object(struct rb_node *node,
+				    struct app_struct *app)
+{
+	struct shared_index *idx =
+		container_of(node, struct shared_index, node);
+
+	struct shared_object *this =
+		container_of(idx, struct shared_object, index);
+
+	struct list_head *tmp, *element;
+	struct export_obj_info *export;
+
+	/* this is called only after checkpoint, we can safely do that */
+	list_for_each_safe(element, tmp, &this->checkpoint.export.next) {
+		export = list_entry(element, struct export_obj_info, next);
+		list_del(element);
+		kfree(export);
+	}
+
+	rb_erase(node, &app->shared_objects.root);
+	kfree(this);
+}
+
+void clear_shared_objects(struct app_struct *app)
+{
+	struct rb_node *node;
+
+	while ((node = rb_first(&app->shared_objects.root)))
+		clear_one_shared_object(node, app);
+}
+
+struct task_struct *alloc_shared_fake_task_struct(struct app_struct *app)
+{
+	struct task_struct *fake;
+	struct krg_namespace *krg_ns;
+
+	fake = alloc_task_struct();
+	if (!fake) {
+		fake = ERR_PTR(-ENOMEM);
+		goto exit;
+	}
+
+	fake->nsproxy = kmem_cache_alloc(nsproxy_cachep, GFP_KERNEL);
+	if (!fake->nsproxy) {
+		fake = ERR_PTR(-ENOMEM);
+		goto exit;
+	}
+
+	krg_ns = find_get_krg_ns();
+	if (!krg_ns) {
+		fake = ERR_PTR(-EPERM);
+		goto err_ns;
+	}
+
+	get_uts_ns(krg_ns->root_nsproxy.uts_ns);
+	fake->nsproxy->uts_ns = krg_ns->root_nsproxy.uts_ns;
+	get_ipc_ns(krg_ns->root_nsproxy.ipc_ns);
+	fake->nsproxy->ipc_ns = krg_ns->root_nsproxy.ipc_ns;
+	get_mnt_ns(krg_ns->root_nsproxy.mnt_ns);
+	fake->nsproxy->mnt_ns = krg_ns->root_nsproxy.mnt_ns;
+	get_pid_ns(krg_ns->root_nsproxy.pid_ns);
+	fake->nsproxy->pid_ns = krg_ns->root_nsproxy.pid_ns;
+	get_net(krg_ns->root_nsproxy.net_ns);
+	fake->nsproxy->net_ns = krg_ns->root_nsproxy.net_ns;
+
+	fake->nsproxy->krg_ns = krg_ns;
+
+	fake->application = app;
+
+exit:
+	return fake;
+err_ns:
+	kmem_cache_free(nsproxy_cachep, fake->nsproxy);
+	goto exit;
+}
+
+void free_shared_fake_task_struct(struct task_struct *fake)
+{
+	free_nsproxy(fake->nsproxy);
+
+	free_task_struct(fake);
+}
+
+static inline void reset_fake_task_struct(struct task_struct *fake)
+{
+	struct nsproxy *ns;
+	struct app_struct *app;
+
+	ns = fake->nsproxy;
+	app = fake->application;
+
+	memset(fake, 0, sizeof(struct task_struct));
+
+	fake->nsproxy = ns;
+	fake->application = app;
+	spin_lock_init(&fake->alloc_lock);
+}
+
+static void destroy_one_shared_object(struct rb_node *node,
+				      struct app_struct *app,
+				      struct task_struct *fake)
+{
+	struct shared_index *idx =
+		container_of(node, struct shared_index, node);
+
+	struct shared_object *this =
+		container_of(idx, struct shared_object, index);
+
+	rb_erase(node, &app->shared_objects.root);
+	this->ops->delete(fake, this->restart.data);
+
+	kfree(this);
+}
+
+void destroy_shared_objects(struct app_struct *app,
+			    struct task_struct *fake)
+{
+	struct rb_node *node;
+
+	reset_fake_task_struct(fake);
+
+	while ((node = rb_first(&app->shared_objects.root)))
+		destroy_one_shared_object(node, app, fake);
+}
+
+/*--------------------------------------------------------------------------*/
+
+static int __export_one_shared_object_kernel(ghost_t *ghost,
+					     struct epm_action *action,
+					     struct shared_object *this)
+{
+	int r = 0;
+
+	if (this->checkpoint.locality != SHARED_MASTER
+	    && this->checkpoint.locality != LOCAL_ONLY)
+		/* nothing to do, we are not responsible of this object */
+		goto error;
+
+	r = ghost_write(ghost, &this->index.type,
+			sizeof(enum shared_obj_type));
+	if (r)
+		goto error;
+	r = ghost_write(ghost, &this->index.key, sizeof(long));
+	if (r)
+		goto error;
+
+	r = ghost_write(ghost, &this->checkpoint.locality,
+			sizeof(enum object_locality));
+	if (r)
+		goto error;
+
+	r = this->ops->export_now(action, ghost,
+				  this->checkpoint.export.task,
+				  &this->checkpoint.export.args);
+
+error:
+	return r;
+}
+
+static int export_one_shared_object(ghost_t *ghost,
+				    ghost_t *user_ghost,
+				    struct epm_action *action,
+				    struct shared_object *this)
+{
+	int r;
+
+	BUG_ON(!this->ops->export_user_info &&
+	       this->checkpoint.locality == SHARED_SLAVE);
+	BUG_ON(this->checkpoint.locality != LOCAL_ONLY
+	       && this->checkpoint.locality != SHARED_MASTER
+	       && this->checkpoint.locality != SHARED_SLAVE);
+
+	r = __export_one_shared_object_kernel(ghost, action, this);
+	if (r)
+		goto error;
+
+	if (this->ops->export_user_info)
+		r = this->ops->export_user_info(action, user_ghost,
+						this->index.key,
+						&this->checkpoint.export);
+
+error:
+	if (r)
+		ckpt_err(NULL, r,
+			 "Fail to checkpoint object of type: %u and key: %lu",
+			 this->index.type, this->index.key);
+
+	return r;
+}
+
+static int export_shared_objects(ghost_t *ghost,
+				 ghost_t *user_ghost,
+				 struct app_struct *app,
+				 enum shared_obj_type from,
+				 enum shared_obj_type to)
+{
+	int r = 0;
+	enum shared_obj_type end = NO_OBJ;
+	struct epm_action action;
+	struct rb_node *node, *next_node;
+
+	action.type = EPM_CHECKPOINT;
+	action.checkpoint.shared = CR_SAVE_NOW;
+
+	node = rb_first(&app->shared_objects.root);
+	while (node) {
+		struct shared_index *idx;
+		struct shared_object *this;
+
+		next_node = rb_next(node);
+
+		idx = container_of(node, struct shared_index, node);
+		this = container_of(idx, struct shared_object, index);
+
+		if (idx->type < from)
+			goto next_node;
+
+		if (idx->type > to)
+			goto exit_write_end;
+
+		r = export_one_shared_object(ghost, user_ghost, &action, this);
+		clear_one_shared_object(node, app);
+
+		if (r)
+			goto error;
+
+	next_node:
+		node = next_node;
+	}
+
+exit_write_end:
+	r = ghost_write(ghost, &end,
+			sizeof(enum shared_obj_type));
+error:
+	return r;
+}
+
+static int chkpt_shared_objects(struct app_struct *app, int chkpt_sn)
+{
+	int r, err;
+
+	ghost_fs_t oldfs;
+	ghost_t *ghost, *user_ghost;
+
+	__set_ghost_fs(&oldfs);
+
+	ghost = create_file_ghost(GHOST_WRITE, app->app_id, chkpt_sn,
+				  "shared_obj_%d.bin", kerrighed_node_id);
+
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		ckpt_err(NULL, r,
+			 "Fail to create file "
+			 "/var/chkpt/%ld/v%d/shared_obj_%u.bin",
+			 app->app_id, chkpt_sn, kerrighed_node_id);
+		goto exit_unset_fs;
+	}
+
+	user_ghost = create_file_ghost(GHOST_WRITE, app->app_id, chkpt_sn,
+				       "user_info_%u.txt", kerrighed_node_id);
+
+	if (IS_ERR(user_ghost)) {
+		r = PTR_ERR(user_ghost);
+		ckpt_err(NULL, r,
+			 "Fail to create file "
+			 "/var/chkpt/%ld/v%d/%s/user_info_%u.txt",
+			 app->app_id, chkpt_sn, kerrighed_node_id);
+		goto exit_close_ghost;
+	}
+
+	r = export_shared_objects(ghost, user_ghost, app,
+				  PIPE_INODE, DVFS_FILE);
+	if (r)
+		goto exit_close_user_ghost;
+
+	r = export_shared_objects(ghost, user_ghost, app,
+				  FILES_STRUCT, SIGNAL_STRUCT);
+
+exit_close_user_ghost:
+	err = ghost_close(user_ghost);
+	if (!r)
+		r = err;
+
+exit_close_ghost:
+	/* End of the really interesting part */
+	err = ghost_close(ghost);
+	if (!r)
+		r = err;
+
+exit_unset_fs:
+	unset_ghost_fs(&oldfs);
+
+	return r;
+}
+
+/*--------------------------------------------------------------------------*/
+
+static int send_dist_objects_list(struct rpc_desc *desc,
+				  struct app_struct *app)
+{
+	enum shared_obj_type end = NO_OBJ;
+	struct rb_node *node;
+	int r;
+
+	for (node = rb_first(&app->shared_objects.root);
+	     node ; node = rb_next(node) ) {
+		struct shared_index *idx;
+		struct shared_object *this;
+		idx = container_of(node, struct shared_index, node);
+		this = container_of(idx, struct shared_object, index);
+
+		if (this->checkpoint.locality != LOCAL_ONLY) {
+			r = rpc_pack_type(desc, idx->type);
+			if (r)
+				goto err_pack;
+
+			r = rpc_pack_type(desc, idx->key);
+			if (r)
+				goto err_pack;
+
+			r = rpc_pack_type(desc, this->checkpoint.locality);
+			if (r)
+				goto err_pack;
+		}
+	}
+	r = rpc_pack_type(desc, end);
+
+err_pack:
+	return r;
+}
+
+struct dist_shared_index {
+	struct shared_index index;
+	kerrighed_node_t master_node;
+	krgnodemask_t nodes;
+};
+
+static void clear_one_dist_shared_index(struct rb_node *node,
+					struct rb_root *dist_shared_indexes)
+{
+	struct shared_index *idx =
+		container_of(node, struct shared_index, node);
+
+	struct dist_shared_index *this =
+		container_of(idx, struct dist_shared_index, index);
+
+	rb_erase(node, dist_shared_indexes);
+	kfree(this);
+}
+
+static void clear_dist_shared_indexes(struct rb_root *dist_shared_indexes)
+{
+	struct rb_node *node;
+
+	while ((node = rb_first(dist_shared_indexes)))
+		clear_one_dist_shared_index(node, dist_shared_indexes);
+}
+
+static int rcv_dist_objects_list_from(struct rpc_desc *desc,
+				      struct rb_root *dist_shared_indexes,
+				      kerrighed_node_t node)
+{
+	int r;
+	enum shared_obj_type type;
+
+	r = rpc_unpack_type_from(desc, node, type);
+	if (r)
+		goto error;
+
+	while (type != NO_OBJ) {
+		struct dist_shared_index *s;
+		struct shared_index *idx;
+		unsigned long key;
+		enum object_locality locality = LOCAL_ONLY;
+
+		r = rpc_unpack_type_from(desc, node, key);
+		if (r)
+			goto error;
+
+		r = rpc_unpack_type_from(desc, node, locality);
+		if (r)
+			goto error;
+
+		s = kmalloc(sizeof(struct dist_shared_index), GFP_KERNEL);
+		if (!s) {
+			r = -ENOMEM;
+			goto error;
+		}
+
+		s->index.type = type;
+		s->index.key = key;
+		s->master_node = KERRIGHED_NODE_ID_NONE;
+		krgnodes_clear(s->nodes);
+
+		idx = __insert_shared_index(dist_shared_indexes, &s->index);
+		if (idx != &s->index) {
+			kfree(s);
+			s = container_of(idx, struct dist_shared_index, index);
+		}
+
+		BUG_ON(locality == LOCAL_ONLY);
+
+		if (s->master_node == KERRIGHED_NODE_ID_NONE) {
+			if (locality == SHARED_MASTER
+			    || locality == SHARED_ANY)
+				s->master_node = node;
+		} else
+			/* only one master per object */
+			BUG_ON(locality == SHARED_MASTER);
+
+		krgnode_set(node, s->nodes);
+
+		/* next ! */
+		r = rpc_unpack_type_from(desc, node, type);
+		if (r)
+			goto error;
+	}
+
+error:
+	return r;
+}
+
+static int send_full_dist_objects_list(struct rpc_desc *desc,
+				       struct rb_root *dist_shared_indexes)
+{
+	enum shared_obj_type end = NO_OBJ;
+	struct rb_node *node;
+	int r;
+
+	for (node = rb_first(dist_shared_indexes);
+	     node ; node = rb_next(node) ) {
+		struct dist_shared_index *this;
+		struct shared_index *idx;
+
+		idx = container_of(node, struct shared_index, node);
+		this = container_of(idx, struct dist_shared_index, index);
+
+		if (this->master_node == KERRIGHED_NODE_ID_NONE) {
+			/* the master node for this object is
+			 * not implied in the checkpoint
+			 */
+			r = -ENOSYS;
+			goto err;
+		}
+
+		r = rpc_pack_type(desc, idx->type);
+		if (r)
+			goto err;
+
+		r = rpc_pack_type(desc, idx->key);
+		if (r)
+			goto err;
+
+		r = rpc_pack_type(desc, this->master_node);
+		if (r)
+			goto err;
+
+		r = rpc_pack_type(desc, this->nodes);
+		if (r)
+			goto err;
+	}
+	r = rpc_pack_type(desc, end);
+
+err:
+	return r;
+}
+
+
+static int rcv_full_dist_objects_list(struct rpc_desc *desc,
+				      struct app_struct *app)
+
+{
+	int r;
+	struct rb_node *node;
+	struct dist_shared_index s;
+
+	r = rpc_unpack_type(desc, s.index.type);
+	if (r)
+		goto error;
+
+	while (s.index.type != NO_OBJ) {
+
+		r = rpc_unpack_type(desc, s.index.key);
+		if (r)
+			goto error;
+
+		r = rpc_unpack_type(desc, s.master_node);
+		if (r)
+			goto error;
+
+		r = rpc_unpack_type(desc, s.nodes);
+		if (r)
+			goto error;
+
+		node = search_node(&app->shared_objects.root,
+				   s.index.type, s.index.key);
+
+		if (node) {
+			struct shared_index *idx;
+			struct shared_object *obj;
+
+			idx = container_of(node, struct shared_index, node);
+			obj = container_of(idx, struct shared_object, index);
+
+			if (krgnode_is_unique(kerrighed_node_id, s.nodes))
+				obj->checkpoint.locality = LOCAL_ONLY;
+			else if (s.master_node == kerrighed_node_id)
+				obj->checkpoint.locality = SHARED_MASTER;
+			else if (obj->ops->export_user_info)
+				obj->checkpoint.locality = SHARED_SLAVE;
+			else
+				clear_one_shared_object(node, app);
+		}
+
+		/* next ! */
+		r = rpc_unpack_type(desc, s.index.type);
+		if (r)
+			goto error;
+	}
+
+error:
+	return r;
+}
+
+int local_chkpt_shared(struct rpc_desc *desc,
+		       struct app_struct *app,
+		       int chkpt_sn)
+{
+	int r = 0;
+
+	/* 1) send list of distributed objects */
+	r = send_dist_objects_list(desc, app);
+
+	r = send_result(desc, r);
+	if (r)
+		goto error;
+
+	/* 2) receive the list of which node should dump
+	 * which distributed object */
+	r = rcv_full_dist_objects_list(desc, app);
+
+	r = send_result(desc, r);
+	if (r)
+		goto error;
+
+	/* 4) dump the shared objects for which we are responsible */
+	r = chkpt_shared_objects(app, chkpt_sn);
+error:
+	return r;
+}
+
+int global_chkpt_shared(struct rpc_desc *desc,
+			struct app_kddm_object *obj)
+{
+	int r = 0;
+	kerrighed_node_t node;
+	struct rb_root dist_shared_indexes = RB_ROOT;
+
+	/* 1) waiting the list of shared objects */
+
+	for_each_krgnode_mask(node, obj->nodes) {
+		r = rcv_dist_objects_list_from(desc,
+					       &dist_shared_indexes,
+					       node);
+		if (r)
+			goto err_clear_shared;
+	}
+
+	/* is it really ok */
+	r = app_wait_returns_from_nodes(desc, obj->nodes);
+	if (r)
+		goto err_clear_shared;
+
+	/* 2) send the list to every node
+	 * this is not optimized but otherwise, we need to open
+	 * a new RPC desc to each node */
+
+	/* go ahead, nodes should prepare to receive the list */
+	r = rpc_pack_type(desc, r);
+	if (r)
+		goto err_clear_shared;
+
+	r = send_full_dist_objects_list(desc, &dist_shared_indexes);
+	if (r)
+		goto err_clear_shared;
+
+	/* waiting results from the nodes hosting the application */
+	r = app_wait_returns_from_nodes(desc, obj->nodes);
+	if (r)
+		goto err_clear_shared;
+
+	/* 4) request them to dump the shared obj */
+	r = ask_nodes_to_continue(desc, obj->nodes, r);
+
+err_clear_shared:
+	clear_dist_shared_indexes(&dist_shared_indexes);
+	return r;
+}
+
+/*--------------------------------------------------------------------------*/
+
+
+static int import_one_shared_object(ghost_t *ghost, struct epm_action *action,
+				    struct task_struct *fake,
+				    enum shared_obj_type type)
+{
+	int r;
+	struct shared_object_operations *s_ops;
+	struct shared_object stmp, *s;
+	int is_local;
+
+	s_ops = get_shared_ops(type);
+
+	BUG_ON(type == NO_OBJ);
+
+	stmp.index.type = type;
+	stmp.index.key = 0;
+	stmp.ops = s_ops;
+	stmp.restart.data = NULL;
+	stmp.restart.data_size = 0;
+
+	r = ghost_read(ghost, &stmp.index.key, sizeof(long));
+	if (r)
+		goto err;
+
+	r = ghost_read(ghost, &stmp.restart.locality,
+		       sizeof(enum object_locality));
+	if (r)
+		goto err;
+
+	if (stmp.restart.locality == LOCAL_ONLY)
+		is_local = 1;
+	else {
+		BUG_ON(stmp.restart.locality != SHARED_MASTER);
+		is_local = 0;
+	}
+
+	/* Look for already imported object.
+	 *
+	 * It can occur with files replaced at restart time
+	 */
+	s = search_shared_object(&fake->application->shared_objects.root,
+				 stmp.index.type, stmp.index.key);
+	if (s) {
+		stmp.restart.data = s->restart.data;
+		stmp.restart.data_size = s->restart.data_size;
+	}
+
+	/* if stmp.restart.data != NULL, data must be read from the ghost
+	 * but object must not be imported
+	 */
+	r = s_ops->import_now(action, ghost, fake, is_local,
+			      &stmp.restart.data,
+			      &stmp.restart.data_size);
+	if (r || s)
+		goto err;
+
+	s = kmalloc(sizeof(struct shared_object) + stmp.restart.data_size,
+		    GFP_KERNEL);
+	if (!s) {
+		r = -ENOMEM;
+		goto err;
+	}
+
+	*s = stmp;
+	if (stmp.restart.data_size) {
+		s->restart.data = &s[1];
+		memcpy(s->restart.data, stmp.restart.data,s->restart.data_size);
+	}
+
+	r = insert_shared_index(&fake->application->shared_objects.root,
+				&s->index);
+	if (r)
+		kfree(s);
+err:
+	if (r)
+		ckpt_err(NULL, r,
+			 "Fail to restore object of type: %u and key: %lu",
+			 type, stmp.index.key);
+	return r;
+}
+
+static int import_shared_objects(ghost_t *ghost, struct app_struct *app,
+				 struct task_struct *fake)
+{
+	int r;
+	struct epm_action action;
+	enum shared_obj_type type = NO_OBJ;
+
+	action.type = EPM_CHECKPOINT;
+	action.restart.shared = CR_LOAD_NOW;
+	action.restart.app = app;
+
+	r = ghost_read(ghost, &type, sizeof(enum shared_obj_type));
+	if (r)
+		goto error;
+
+	reset_fake_task_struct(fake);
+
+	while (type != NO_OBJ) {
+
+		r = import_one_shared_object(ghost, &action, fake, type);
+		if (r)
+			goto error;
+
+		r = ghost_read(ghost, &type, sizeof(enum shared_obj_type));
+		if (r)
+			goto error;
+	}
+
+error:
+	return r;
+}
+
+static int send_restored_objects(struct rpc_desc *desc, struct app_struct *app,
+				 enum shared_obj_type from,
+				 enum shared_obj_type to)
+{
+	enum shared_obj_type end = NO_OBJ;
+	struct rb_node *node;
+	int r;
+
+	for (node = rb_first(&app->shared_objects.root);
+	     node ; node = rb_next(node) ) {
+
+		struct shared_object *this;
+		struct shared_index *idx;
+
+		idx = container_of(node, struct shared_index, node);
+		this = container_of(idx, struct shared_object, index);
+
+		BUG_ON(this->restart.locality == SHARED_ANY);
+
+		if (this->restart.locality == SHARED_MASTER &&
+		    (idx->type >= from && idx->type <= to)) {
+			r = rpc_pack_type(desc, idx->type);
+			if (r)
+				goto err_pack;
+
+			r = rpc_pack_type(desc, idx->key);
+			if (r)
+				goto err_pack;
+
+			r = rpc_pack_type(desc, this->restart.data_size);
+			if (r)
+				goto err_pack;
+
+			if (this->restart.data_size)
+				r = rpc_pack(desc, 0, this->restart.data,
+					     this->restart.data_size);
+			else
+				r = rpc_pack_type(desc, this->restart.data);
+			if (r)
+				goto err_pack;
+		}
+	}
+	r = rpc_pack_type(desc, end);
+
+err_pack:
+	return r;
+}
+
+struct restored_dist_shared_index {
+	struct shared_index index;
+	size_t data_size;
+	void *data;
+};
+
+static void clear_one_restored_dist_shared_index(
+	struct rb_node *node,
+	struct rb_root *dist_shared_indexes)
+{
+	struct shared_index *idx =
+		container_of(node, struct shared_index, node);
+
+	struct restored_dist_shared_index *this =
+		container_of(idx, struct restored_dist_shared_index, index);
+
+	rb_erase(node, dist_shared_indexes);
+	kfree(this);
+}
+
+static void clear_restored_dist_shared_indexes(
+	struct rb_root *dist_shared_indexes)
+{
+	struct rb_node *node;
+
+	while ((node = rb_first(dist_shared_indexes)))
+		clear_one_restored_dist_shared_index(node, dist_shared_indexes);
+}
+
+static int rcv_restored_dist_objects_list_from(
+	struct rpc_desc *desc,
+	struct rb_root *dist_shared_indexes,
+	kerrighed_node_t node)
+{
+	int r;
+	enum shared_obj_type type;
+
+	r = rpc_unpack_type_from(desc, node, type);
+	if (r)
+		goto error;
+
+	while (type != NO_OBJ) {
+		struct restored_dist_shared_index *s;
+		unsigned long key;
+		void *data;
+		size_t data_size;
+
+		r = rpc_unpack_type_from(desc, node, key);
+		if (r)
+			goto error;
+
+		r = rpc_unpack_type_from(desc, node, data_size);
+		if (r)
+			goto error;
+
+		s = kmalloc(sizeof(struct restored_dist_shared_index)
+			    + data_size, GFP_KERNEL);
+		if (!s) {
+			r = -ENOMEM;
+			goto error;
+		}
+
+		s->index.type = type;
+		s->index.key = key;
+		s->data_size = data_size;
+
+		if (data_size) {
+			data = &s[1];
+			r = rpc_unpack_from(desc, node, 0, data, data_size);
+		} else
+			r = rpc_unpack_type_from(desc, node, data);
+
+		if (r) {
+			kfree(s);
+			goto error;
+		}
+
+		s->data = data;
+
+		r = insert_shared_index(dist_shared_indexes, &s->index);
+		if (r) {
+			kfree(s);
+			s = NULL;
+		}
+
+		/* next ! */
+		r = rpc_unpack_type_from(desc, node, type);
+		if (r)
+			goto error;
+	}
+
+error:
+	return r;
+}
+
+static int send_full_restored_dist_objects_list(
+	struct rpc_desc *desc,
+	struct rb_root *dist_shared_indexes)
+{
+	enum shared_obj_type end = NO_OBJ;
+	struct rb_node *node;
+	int r;
+
+	for (node = rb_first(dist_shared_indexes);
+	     node ; node = rb_next(node) ) {
+		struct restored_dist_shared_index *this;
+		struct shared_index *idx;
+
+		idx = container_of(node, struct shared_index, node);
+		this = container_of(idx, struct restored_dist_shared_index,
+				    index);
+
+		r = rpc_pack_type(desc, idx->type);
+		if (r)
+			goto err_pack;
+
+		r = rpc_pack_type(desc, idx->key);
+		if (r)
+			goto err_pack;
+
+		r = rpc_pack_type(desc, this->data_size);
+		if (r)
+			goto err_pack;
+
+		if (this->data_size)
+			r = rpc_pack(desc, 0, this->data,
+				     this->data_size);
+		else
+			r = rpc_pack_type(desc, this->data);
+
+		if (r)
+			goto err_pack;
+	}
+	r = rpc_pack_type(desc, end);
+
+err_pack:
+	return r;
+}
+
+static int rcv_full_restored_objects(
+	struct rpc_desc *desc,
+	struct app_struct *app)
+{
+	int r;
+	struct restored_dist_shared_index s;
+
+	r = rpc_unpack_type(desc, s.index.type);
+	if (r)
+		goto error;
+
+	while (s.index.type != NO_OBJ) {
+		struct shared_object *obj;
+
+		r = rpc_unpack_type(desc, s.index.key);
+		if (r)
+			goto error;
+
+		r = rpc_unpack_type(desc, s.data_size);
+		if (r)
+			goto error;
+
+		obj = kmalloc(sizeof(struct shared_object) + s.data_size,
+			      GFP_KERNEL);
+
+		if (!obj) {
+			r = -ENOMEM;
+			goto error;
+		}
+
+		obj->index.type = s.index.type;
+		obj->index.key = s.index.key;
+
+		if (s.data_size) {
+			s.data = &obj[1];
+			r = rpc_unpack(desc, 0, s.data, s.data_size);
+		}
+		else
+			r = rpc_unpack_type(desc, s.data);
+
+		if (r) {
+			kfree(obj);
+			goto error;
+		}
+
+		obj->restart.locality = SHARED_SLAVE;
+		obj->restart.data = s.data;
+		obj->ops = get_shared_ops(obj->index.type);
+
+		/* try to add it */
+		r = insert_shared_index(&app->shared_objects.root, &obj->index);
+		if (r)
+			kfree(obj);
+
+		/* next ! */
+		r = rpc_unpack_type(desc, s.index.type);
+		if (r)
+			goto error;
+	}
+
+error:
+	return r;
+}
+
+/*--------------------------------------------------------------------------*/
+
+struct substitution_file {
+	struct shared_index index;
+	kerrighed_node_t node;
+	size_t data_size;
+	void *data;
+	struct file *file;
+};
+
+static void clear_one_substitution_file(struct rb_node *node,
+					struct rb_root *files)
+{
+	struct shared_index *idx =
+		container_of(node, struct shared_index, node);
+
+	struct substitution_file *this =
+		container_of(idx, struct substitution_file, index);
+
+	rb_erase(node, files);
+
+	if (this->data_size)
+		kfree(this->data);
+
+	fput(this->file);
+
+	kfree(this);
+}
+
+static void clear_substitution_files(struct rb_root *files)
+{
+	struct rb_node *node;
+
+	while ((node = rb_first(files)))
+		clear_one_substitution_file(node, files);
+}
+
+static int send_substitution_files(struct rpc_desc *desc,
+				   struct rb_root *substitution_files)
+{
+	enum shared_obj_type end = NO_OBJ;
+	struct rb_node *node;
+	int r;
+
+	for (node = rb_first(substitution_files);
+	     node ; node = rb_next(node)) {
+		struct substitution_file *this;
+		struct shared_index *idx;
+
+		idx = container_of(node, struct shared_index, node);
+		this = container_of(idx, struct substitution_file, index);
+
+		r = rpc_pack_type(desc, idx->type);
+		if (r)
+			goto err_pack;
+
+		r = rpc_pack_type(desc, idx->key);
+		if (r)
+			goto err_pack;
+
+		r = rpc_pack_type(desc, this->node);
+		if (r)
+			goto err_pack;
+
+		r = rpc_pack_type(desc, this->data_size);
+		if (r)
+			goto err_pack;
+
+		if (this->data_size)
+			r = rpc_pack(desc, 0, this->data,
+				     this->data_size);
+		else
+			r = rpc_pack_type(desc, this->data);
+
+		if (r)
+			goto err_pack;
+	}
+	r = rpc_pack_type(desc, end);
+
+err_pack:
+	return r;
+}
+
+static int rcv_substitution_files(struct rpc_desc *desc,
+				  struct app_struct *app)
+{
+	int r;
+	struct shared_index index;
+	kerrighed_node_t node;
+	size_t data_size;
+	void *data;
+
+	r = rpc_unpack_type(desc, index.type);
+	if (r)
+		goto error;
+
+	while (index.type != NO_OBJ) {
+		struct shared_object *obj;
+
+		if (index.type != DVFS_FILE
+		    && index.type != LOCAL_FILE) {
+			r = -EINVAL;
+			goto error;
+		}
+
+		r = rpc_unpack_type(desc, index.key);
+		if (r)
+			goto error;
+
+		r = rpc_unpack_type(desc, node);
+		if (r)
+			goto error;
+
+		r = rpc_unpack_type(desc, data_size);
+		if (r)
+			goto error;
+
+		obj = kmalloc(sizeof(struct shared_object) + data_size,
+			      GFP_KERNEL);
+
+		if (!obj) {
+			r = -ENOMEM;
+			goto error;
+		}
+
+		if (data_size) {
+			data = &obj[1];
+			r = rpc_unpack(desc, 0, data, data_size);
+		}
+		else
+			r = rpc_unpack_type(desc, data);
+
+		if (r) {
+			kfree(obj);
+			goto error;
+		}
+
+		if (node == KERRIGHED_NODE_ID_NONE
+		    || krgnode_isset(node, app->restart.replacing_nodes)) {
+
+			/* the object is useful on this node, add it */
+			obj->index.type = index.type;
+			obj->index.key = index.key;
+
+			obj->restart.locality = SHARED_SLAVE;
+			obj->restart.data = data;
+			obj->ops = get_shared_ops(obj->index.type);
+
+			/* try to add it */
+			r = insert_shared_index(&app->shared_objects.root,
+						&obj->index);
+			if (r) {
+				kfree(obj);
+				goto error;
+			}
+		} else
+			/* object is useless here
+			 * one day, with a good implementation of rpc_pack_to,
+			 * this code path may disappear
+			 */
+			kfree(obj);
+
+		/* next ! */
+		r = rpc_unpack_type(desc, index.type);
+		if (r)
+			goto error;
+	}
+
+error:
+	return r;
+}
+
+static int __insert_one_substitution_file(struct rb_root *files,
+					  enum shared_obj_type type,
+					  unsigned long key,
+					  kerrighed_node_t node,
+					  size_t data_size,
+					  void *data,
+					  struct file *file)
+{
+	int r;
+	struct shared_index *idx;
+	struct substitution_file *obj;
+
+	obj = kmalloc(sizeof(struct substitution_file), GFP_KERNEL);
+	if (!obj) {
+		r = -ENOMEM;
+		goto error;
+	}
+
+	obj->index.type = type;
+	obj->index.key = key;
+	obj->node = node;
+	obj->data_size = data_size;
+	obj->data = data;
+	obj->file = file;
+
+	idx = __insert_shared_index(files, &obj->index);
+	if (idx != &obj->index) {
+		/* a substitution is already registered for this key */
+		kfree(obj);
+
+		obj = container_of(idx, struct substitution_file, index);
+		if (obj->file == file)
+			r = -EALREADY;
+		else
+			r = -ENOKEY;
+	} else
+		r = 0;
+error:
+	return r;
+}
+
+static int insert_one_substitution_file(struct rb_root *files,
+					enum shared_obj_type type,
+					unsigned long key,
+					kerrighed_node_t node,
+					int fd)
+{
+	int r, fdesc_size;
+	size_t file_link_size;
+	struct file *file;
+	void *fdesc, *cr_file_link;
+
+	if (type != LOCAL_FILE
+	    && type != DVFS_FILE) {
+		r = -EINVAL;
+		goto error;
+	}
+
+	file = fget(fd);
+	if (!file) {
+		r = -EINVAL;
+		goto error;
+	}
+
+	if (!file->f_objid) {
+		r = create_kddm_file_object(file);
+		if (r)
+			goto err_put_file;
+	}
+
+	r = setup_faf_file_if_needed(file);
+	if (r)
+		goto err_put_file;
+
+	if (file->f_flags & (O_FAF_SRV|O_FAF_CLT))
+		r = get_faf_file_krg_desc(file, &fdesc, &fdesc_size);
+	else
+		r = get_regular_file_krg_desc(file, &fdesc, &fdesc_size);
+
+	if (r)
+		goto err_put_file;
+
+	r = prepare_restart_data_shared_file(file, fdesc, fdesc_size,
+					     &cr_file_link, &file_link_size,
+					     true);
+	if (r)
+		goto err_free_desc;
+
+	r = __insert_one_substitution_file(files, type, key, node,
+					   file_link_size, cr_file_link, file);
+	if (r) {
+		if (r == -EALREADY)
+			/* the same file substitution has already
+			 * been registered */
+			r = 0;
+
+		goto err_free_file_link;
+	}
+
+error:
+	return r;
+
+err_free_file_link:
+	kfree(cr_file_link);
+err_free_desc:
+	kfree(fdesc);
+err_put_file:
+	fput(file);
+	goto error;
+}
+
+static int parse_file_identifier(char *str, enum shared_obj_type *type,
+				 unsigned long *key, kerrighed_node_t *node)
+{
+	int r, nodelen, keylen, len;
+	char saved_c;
+
+	nodelen = sizeof(*node)*2;
+	keylen = sizeof(*key)*2;
+	len = strlen(str);
+
+	if (len != nodelen + keylen)
+		goto err_invalid;
+
+	/* read the key */
+	r = sscanf(str + nodelen, "%lX", key);
+	if (r != 1)
+		goto err_invalid;
+
+	/* read the node */
+	saved_c = str[nodelen];
+	str[nodelen] = '\0';
+
+	r = sscanf(str, "%hX", node);
+
+	str[nodelen]= saved_c;
+
+	if (r != 1)
+		goto err_invalid;
+
+	r = 0;
+
+	if (*node == KERRIGHED_NODE_ID_NONE)
+		*type = DVFS_FILE;
+	else
+		*type = LOCAL_FILE;
+
+out:
+	return r;
+
+err_invalid:
+	r = -EINVAL;
+	goto out;
+}
+
+static int insert_substitution_files(struct rb_root *files,
+				     struct restart_request *req)
+{
+	int i, r = 0;
+	enum shared_obj_type type;
+	unsigned long key;
+	kerrighed_node_t node;
+
+	for (i = 0; i < req->substitution.nr; i++) {
+
+		r = parse_file_identifier(req->substitution.files[i].file_id,
+					  &type, &key, &node);
+		if (r)
+			goto error;
+
+		r = insert_one_substitution_file(
+			files, type, key, node,
+			req->substitution.files[i].fd);
+		if (r)
+			goto error;
+	}
+
+error:
+	return r;
+}
+
+/*--------------------------------------------------------------------------*/
+
+int local_restart_shared_complete(struct app_struct *app,
+				  struct task_struct *fake)
+{
+	struct rb_node *node;
+
+	reset_fake_task_struct(fake);
+
+	while ((node = rb_first(&app->shared_objects.root))) {
+		struct shared_index *idx =
+			container_of(node, struct shared_index, node);
+
+		struct shared_object *this =
+			container_of(idx, struct shared_object, index);
+
+		BUG_ON(this->restart.locality == SHARED_ANY);
+
+		if (this->restart.locality == LOCAL_ONLY
+		    || this->restart.locality == SHARED_MASTER)
+			this->ops->import_complete(fake, this->restart.data);
+
+		rb_erase(node, &app->shared_objects.root);
+		kfree(this);
+	}
+
+	return 0;
+}
+
+static int local_restart_shared_objects(struct rpc_desc *desc,
+					struct app_struct *app,
+					struct task_struct *fake,
+					int chkpt_sn,
+					enum shared_obj_type from,
+					enum shared_obj_type to,
+					loff_t ghost_offsets[])
+{
+	int r = -EINVAL;
+	int idx = 0;
+	kerrighed_node_t node;
+	ghost_t *ghost;
+
+	/* 1) restore objects for which we are master */
+	for_each_krgnode_mask(node, app->restart.replacing_nodes) {
+
+		ghost = create_file_ghost(GHOST_READ, app->app_id, chkpt_sn,
+					  "shared_obj_%d.bin", node);
+
+		if (IS_ERR(ghost)) {
+			r = PTR_ERR(ghost);
+			ckpt_err(NULL, r,
+				 "Fail to open file "
+				 "/var/chkpt/%ld/v%d/shared_obj_%u.bin",
+				 app->app_id, chkpt_sn, kerrighed_node_id);
+			goto err_import;
+		}
+
+		set_file_ghost_pos(ghost, ghost_offsets[idx]);
+
+		r = import_shared_objects(ghost, app, fake);
+		if (r)
+			goto err_close_ghost;
+
+		ghost_offsets[idx] = get_file_ghost_pos(ghost);
+
+		ghost_close(ghost);
+
+		idx++;
+	}
+
+err_import:
+	r = send_result(desc, r);
+	if (r)
+		goto err;
+
+	/* 2) send list of restored objects that are shared with other nodes */
+	r = send_restored_objects(desc, app, from, to);
+
+	r = send_result(desc, r);
+	if (r)
+		goto err;
+
+	/* 3) receive objects information from other nodes */
+	r = rcv_full_restored_objects(desc, app);
+	if (r)
+		goto err;
+
+err:
+	return r;
+
+err_close_ghost:
+	ghost_close(ghost);
+	goto err;
+}
+
+int local_restart_shared(struct rpc_desc *desc,
+			 struct app_struct *app,
+			 struct task_struct *fake,
+			 int chkpt_sn)
+{
+
+	loff_t *ghost_offsets;
+	ghost_fs_t oldfs;
+	int r, nb_nodes;
+
+	__set_ghost_fs(&oldfs);
+
+	nb_nodes = krgnodes_weight(app->restart.replacing_nodes);
+
+	ghost_offsets = kzalloc(nb_nodes * sizeof(int), GFP_KERNEL);
+	if (!ghost_offsets) {
+		r = -ENOMEM;
+		goto err_ghost_fs;
+	}
+
+	/* 1) get files replaced by coordinator */
+	r = rcv_substitution_files(desc, app);
+	if (r)
+		goto err_ghost_offset;
+
+	/* 2) restore pipes and files */
+	r = local_restart_shared_objects(desc, app, fake, chkpt_sn,
+					 PIPE_INODE, DVFS_FILE,
+					 ghost_offsets);
+	if (r)
+		goto err_ghost_offset;
+
+	/* 3) restore other objects */
+	r = local_restart_shared_objects(desc, app, fake, chkpt_sn,
+					 FILES_STRUCT, SIGNAL_STRUCT,
+					 ghost_offsets);
+	if (r)
+		goto err_ghost_offset;
+
+err_ghost_offset:
+	kfree(ghost_offsets);
+
+err_ghost_fs:
+	unset_ghost_fs(&oldfs);
+	return r;
+}
+
+static int global_restart_shared_objects(struct rpc_desc *desc,
+					 struct app_kddm_object *obj)
+{
+	int r = 0;
+	int err_rpc = 0;
+	kerrighed_node_t node;
+	struct rb_root dist_shared_indexes = RB_ROOT;
+
+	/* 1) waiting nodes to have restored objects they are master for */
+	r = app_wait_returns_from_nodes(desc, obj->nodes);
+	if (r)
+		goto error;
+
+	/* 2) request the list of restored distributed objects */
+	err_rpc = rpc_pack_type(desc, r);
+	if (err_rpc)
+		goto err_rpc;
+
+	for_each_krgnode_mask(node, obj->nodes) {
+		r = rcv_restored_dist_objects_list_from(desc,
+							&dist_shared_indexes,
+							node);
+		if (r)
+			goto error;
+	}
+	r = app_wait_returns_from_nodes(desc, obj->nodes);
+	if (r)
+		goto error;
+
+	/* 3) Send the list to every node
+	 * this is not optimized but otherwise, we need to open
+	 * a new RPC desc to each node */
+	err_rpc = rpc_pack_type(desc, r);
+	if (err_rpc)
+		goto err_rpc;
+
+	r = send_full_restored_dist_objects_list(desc, &dist_shared_indexes);
+
+error:
+	clear_restored_dist_shared_indexes(&dist_shared_indexes);
+	return r;
+err_rpc:
+	r = err_rpc;
+	goto error;
+}
+
+int global_restart_shared(struct rpc_desc *desc,
+			  struct app_kddm_object *obj,
+			  struct restart_request *req)
+{
+	int r = 0;
+	struct rb_root substitute_files = RB_ROOT;
+
+	r = insert_substitution_files(&substitute_files, req);
+	if (r)
+		goto error;
+
+	/* send list of files replaced at restart time by coordinator */
+	r = send_substitution_files(desc, &substitute_files);
+	if (r)
+		goto error;
+
+	/* manage shared pipes and files */
+	r = global_restart_shared_objects(desc, obj);
+	if (r)
+		goto error;
+
+	/* manage shared objects */
+	r = global_restart_shared_objects(desc, obj);
+	if (r)
+		goto error;
+
+error:
+	clear_substitution_files(&substitute_files);
+	if (r)
+		ckpt_err(NULL, r,
+			 "Fail to restore shared objects of application %ld",
+			 obj->app_id);
+
+	return r;
+}
+
diff -ruN linux-2.6.29/kerrighed/epm/application/app_utils.h android_cluster/linux-2.6.29/kerrighed/epm/application/app_utils.h
--- linux-2.6.29/kerrighed/epm/application/app_utils.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/application/app_utils.h	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,73 @@
+/*
+ *  Kerrighed/modules/epm/app_utils.h
+ *
+ *  Copyright (C) 2008 INRIA
+ *
+ *  @author Matthieu Fertré
+ */
+#ifndef __APP_UTILS_H__
+#define __APP_UTILS_H__
+
+#include <net/krgrpc/rpc.h>
+
+static inline int app_wait_returns_from_nodes(struct rpc_desc *desc,
+					      krgnodemask_t nodes)
+{
+	kerrighed_node_t node;
+	int ret, r=0;
+	enum rpc_error error;
+
+	for_each_krgnode_mask(node, nodes) {
+		error = rpc_unpack_type_from(desc, node, ret);
+		if (error) /* unpack has failed */
+			r = error;
+		else if (ret)
+			r = ret;
+        }
+
+	return r;
+}
+
+static inline int send_result(struct rpc_desc *desc, int result)
+{
+	int r;
+	enum rpc_error error;
+
+	error = rpc_pack_type(desc, result);
+	if (error)
+		goto err_rpc;
+	error = rpc_unpack_type(desc, r);
+	if (error)
+		goto err_rpc;
+
+exit:
+	return r;
+err_rpc:
+	r = error;
+	goto exit;
+}
+
+static inline int ask_nodes_to_continue(struct rpc_desc *desc,
+					krgnodemask_t nodes,
+					int result)
+{
+	int r;
+	enum rpc_error error;
+
+	error = rpc_pack_type(desc, result);
+	if (error)
+		goto err_rpc;
+
+	r = app_wait_returns_from_nodes(desc, nodes);
+exit:
+	return r;
+err_rpc:
+	r = error;
+	goto exit;
+}
+
+struct task_struct *alloc_shared_fake_task_struct(struct app_struct *app);
+
+void free_shared_fake_task_struct(struct task_struct *fake);
+
+#endif /* __APP_UTILS_H__ */
diff -ruN linux-2.6.29/kerrighed/epm/application/Makefile android_cluster/linux-2.6.29/kerrighed/epm/application/Makefile
--- linux-2.6.29/kerrighed/epm/application/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/application/Makefile	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,12 @@
+#
+# Kerrighed's Enhanced Process Management (EPM) - Application
+#
+
+obj-$(CONFIG_KRG_EPM) := krg_epm_application.o
+
+krg_epm_application-y := app_shared.o \
+	app_frontier.o application.o \
+	app_checkpoint.o app_restart.o \
+	application_cr_api.o
+
+EXTRA_CFLAGS += -Wall -Werror
diff -ruN linux-2.6.29/kerrighed/epm/checkpoint.c android_cluster/linux-2.6.29/kerrighed/epm/checkpoint.c
--- linux-2.6.29/kerrighed/epm/checkpoint.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/checkpoint.c	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,231 @@
+/*
+ *  kerrighed/epm/checkpoint.c
+ *
+ *  Copyright (C) 1999-2008 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2008-2009 Matthieu Fertré - Kerlabs
+ */
+
+/**
+ *  Process checkpointing.
+ *  @file checkpoint.c
+ *
+ *  @author Geoffroy Vallée, Matthieu Fertré
+ */
+
+#include <linux/sched.h>
+#include <linux/nsproxy.h>
+#include <linux/file.h>
+#include <linux/cred.h>
+#include <linux/kernel.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/application.h>
+#include <kerrighed/kerrighed_signal.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/action.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/ghost_helpers.h>
+#include <kerrighed/remote_cred.h>
+#include <kerrighed/debug.h>
+#include "ghost.h"
+#include "epm_internal.h"
+#include "checkpoint.h"
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              TOOLS FUNCTIONS                              */
+/*                                                                           */
+/*****************************************************************************/
+
+int can_be_checkpointed(struct task_struct *task_to_checkpoint)
+{
+	struct nsproxy *nsp;
+
+	/* Task must live in the Kerrighed container. */
+	rcu_read_lock();
+	nsp = rcu_dereference(task_to_checkpoint->nsproxy);
+	if (!nsp || !nsp->krg_ns) {
+		rcu_read_unlock();
+		goto exit;
+	}
+	rcu_read_unlock();
+
+	/* Check permissions */
+	if (!permissions_ok(task_to_checkpoint))
+		goto exit;
+
+	/* Check capabilities */
+	if (!can_use_krg_cap(task_to_checkpoint, CAP_CHECKPOINTABLE))
+		goto exit;
+
+	return 1; /* means true */
+
+exit:
+	return 0; /* means false */
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                            CHECKPOINT FUNCTIONS                           */
+/*                                                                           */
+/*****************************************************************************/
+
+/**
+ *  This function save the process information in a ghost
+ *  @author Geoffroy Vallée, Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param task_to_checkpoint	Pointer on the task to checkpoint
+ *
+ *  @return			0 if everythink ok, negative value otherwise.
+ */
+static int checkpoint_task_to_ghost(struct epm_action *action,
+				    ghost_t *ghost,
+				    struct task_struct *task_to_checkpoint,
+				    struct pt_regs *regs)
+{
+	int r = -EINVAL;
+
+	if (task_to_checkpoint == NULL) {
+		PANIC("Task to checkpoint is NULL!!\n");
+		goto exit;
+	}
+
+	if (regs == NULL) {
+		PANIC("Regs are NULL!!\n");
+		goto exit;
+	}
+
+	r = export_process(action, ghost, task_to_checkpoint, regs);
+	if (!r)
+		post_export_process(action, ghost, task_to_checkpoint);
+
+exit:
+	return r;
+}
+
+/**
+ *  This function saves the process information in a file
+ *  @author Geoffroy Vallée, Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param task_to_checkpoint	Pointer to the task to checkpoint
+ *
+ *  @return 0			if everythink ok, negative value otherwise.
+ */
+static
+int checkpoint_task_on_disk(struct epm_action *action,
+			    struct task_struct *task_to_checkpoint,
+			    struct pt_regs *regs)
+{
+	ghost_t *ghost;
+	int r = -EINVAL;
+
+	struct app_struct *app = task_to_checkpoint->application;
+	BUG_ON(!app);
+
+	ghost = get_task_chkpt_ghost(app, task_to_checkpoint);
+	if (!ghost) {
+		__WARN();
+		return r;
+	}
+
+	/* Do the process ghosting */
+	return checkpoint_task_to_ghost(action, ghost,
+				        task_to_checkpoint, regs);
+}
+
+/**
+ *  This function saves the process information
+ *  @author Geoffroy Vallée, Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param task_to_checkpoint	Pointer to the task to checkpoint
+ *
+ *  @return 0			if everythink ok, negative value otherwise.
+ */
+static int checkpoint_task(struct epm_action *action,
+			   struct task_struct *task_to_checkpoint,
+			   struct pt_regs *regs)
+{
+	int r;
+	struct app_struct *app = task_to_checkpoint->application;
+	ghost_fs_t oldfs;
+
+	BUG_ON(!action);
+	BUG_ON(!task_to_checkpoint);
+	BUG_ON(!regs);
+	BUG_ON(!app);
+
+	r = set_ghost_fs(&oldfs, app->cred->fsuid, app->cred->fsgid);
+	if (r)
+		goto out;
+
+	/* Do the process ghosting */
+	r = checkpoint_task_on_disk(action, task_to_checkpoint, regs);
+
+	unset_ghost_fs(&oldfs);
+
+	if (r)
+		ckpt_err(action, r,
+			 "Fail to checkpoint process %d (%s)",
+			 task_pid_knr(task_to_checkpoint),
+			 task_to_checkpoint->comm);
+out:
+	return r;
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                             REQUEST HELPER FUNCTIONS                      */
+/*                                                                           */
+/*****************************************************************************/
+
+/* Checkpoint signal handler */
+static void krg_task_checkpoint(int sig, struct siginfo *info,
+				struct pt_regs *regs)
+{
+	struct epm_action action;
+	task_state_t *current_state;
+	int r = 0;
+
+	/*
+	 * process must not be frozen while its father
+	 * waiting in vfork
+	 */
+	if (current->vfork_done) {
+		mutex_lock(&current->application->mutex);
+		r = -EAGAIN;
+		ckpt_err(NULL, r,
+			 "Application %ld can not be frozen because process "
+			 "%d (%s) has been created by vfork() and has not yet "
+			 "called exec(). Thus, its parent process is blocked.",
+			 current->application->app_id,
+			 task_pid_knr(current), current->comm);
+		__set_task_result(current, r);
+		mutex_unlock(&current->application->mutex);
+		goto out;
+	}
+
+	/* freeze */
+	current_state = set_result_wait(PCUS_OPERATION_OK);
+	if (IS_ERR(current_state))
+		goto out;
+
+	/*
+	 * checkpoint may be requested several times once
+	 * application is frozen.
+	 */
+	while (current_state->checkpoint.ghost) {
+		action.type = EPM_CHECKPOINT;
+		action.checkpoint.shared = CR_SAVE_LATER;
+		r = checkpoint_task(&action, current, regs);
+
+		/* PCUS_OPERATION_OK == 0 */
+		current_state = set_result_wait(r);
+	}
+
+out:
+	return;
+}
+
+void register_checkpoint_hooks(void)
+{
+	hook_register(&krg_handler[KRG_SIG_CHECKPOINT], krg_task_checkpoint);
+}
diff -ruN linux-2.6.29/kerrighed/epm/checkpoint.h android_cluster/linux-2.6.29/kerrighed/epm/checkpoint.h
--- linux-2.6.29/kerrighed/epm/checkpoint.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/checkpoint.h	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,24 @@
+/**
+ *  Process checkpoint interface.
+ *  @file checkpoint.h
+ *
+ *  Definition of process checkpointing interface.
+ *  @author Geoffroy Vallée, Renaud Lottiaux
+ */
+
+#ifndef __CHECKPOINT_H__
+#define __CHECKPOINT_H__
+
+#include <linux/types.h>
+
+struct task_struct;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+int can_be_checkpointed(struct task_struct *task_to_checkpoint);
+
+#endif /* __CHECKPOINT_H__ */
diff -ruN linux-2.6.29/kerrighed/epm/children.c android_cluster/linux-2.6.29/kerrighed/epm/children.c
--- linux-2.6.29/kerrighed/epm/children.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/children.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,1687 @@
+/*
+ *  kerrighed/epm/children.c
+ *
+ *  Copyright (C) 2006-2007 Louis Rilling - Kerlabs
+ */
+
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/pid_namespace.h>
+#include <linux/list.h>
+#include <linux/kernel.h>
+#include <linux/rwsem.h>
+#include <linux/kref.h>
+#include <linux/rcupdate.h>
+#include <linux/slab.h>
+#include <linux/hashtable.h>
+#include <kerrighed/children.h>
+#include <kerrighed/task.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/krginit.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+#include <kerrighed/krg_exit.h>	/* For remote zombies handling */
+#include <kerrighed/libproc.h>
+
+struct remote_child {
+	struct list_head sibling;
+	struct list_head thread_group;
+	pid_t pid;
+	pid_t tgid;
+	pid_t pgid;
+	pid_t sid;
+	pid_t parent;
+	pid_t real_parent;
+	int ptraced;
+	int exit_signal;
+	long exit_state;
+	kerrighed_node_t node;
+};
+
+struct children_kddm_object {
+	pid_t tgid;
+	struct list_head children;
+	unsigned long nr_children;
+	unsigned long nr_threads;
+	u32 self_exec_id;
+
+	/* Remaining fields are not shared */
+	struct rw_semaphore sem;
+	int write_locked;
+
+	int alive;
+	struct kref kref;
+
+	struct rcu_head rcu;
+};
+
+static struct kmem_cache *children_obj_cachep;
+static struct kmem_cache *remote_child_cachep;
+struct kddm_set;
+static struct kddm_set *children_kddm_set;
+
+static struct kmem_cache *krg_parent_head_cachep;
+/* Size of krg_parent task struct list hash table */
+#define PROCESS_HASH_TABLE_SIZE 1024
+static hashtable_t *krg_parent_table; /* list_head of local children */
+
+/************************************************************************
+ * Global children list of a thread group				*
+ ************************************************************************/
+
+void krg_children_get(struct children_kddm_object *obj)
+{
+	if (obj)
+		kref_get(&obj->kref);
+}
+
+static void children_free(struct children_kddm_object *obj)
+{
+	struct remote_child *child, *next;
+
+	list_for_each_entry_safe(child, next, &obj->children, sibling) {
+		list_del(&child->sibling);
+		kmem_cache_free(remote_child_cachep, child);
+	}
+	kmem_cache_free(children_obj_cachep, obj);
+}
+
+static void delayed_children_free(struct rcu_head *rhp)
+{
+	struct children_kddm_object *obj =
+		container_of(rhp, struct children_kddm_object, rcu);
+	children_free(obj);
+}
+
+static void children_free_rcu(struct kref *kref)
+{
+	struct children_kddm_object *obj =
+		container_of(kref, struct children_kddm_object, kref);
+	call_rcu(&obj->rcu, delayed_children_free);
+}
+
+void krg_children_put(struct children_kddm_object *obj)
+{
+	if (obj)
+		kref_put(&obj->kref, children_free_rcu);
+}
+
+static inline void remove_child_links(struct children_kddm_object *obj,
+				      struct remote_child *child)
+{
+	list_del(&child->sibling);
+	list_del(&child->thread_group);
+}
+
+static void set_child_links(struct children_kddm_object *obj,
+			    struct remote_child *child)
+{
+	struct remote_child *item;
+
+	INIT_LIST_HEAD(&child->thread_group);
+	if (child->pid != child->tgid) {
+		list_for_each_entry(item, &obj->children, sibling)
+			if (item->tgid == child->tgid) {
+				list_add_tail(&child->thread_group,
+					      &item->thread_group);
+				break;
+			}
+		BUG_ON(list_empty(&child->thread_group));
+	}
+	list_add_tail(&child->sibling, &obj->children);
+}
+
+static int children_alloc_object(struct kddm_obj *obj_entry,
+				 struct kddm_set *set, objid_t objid)
+{
+	struct children_kddm_object *obj;
+	pid_t tgid = objid;
+
+	obj = kmem_cache_alloc(children_obj_cachep, GFP_KERNEL);
+	if (!obj)
+		return -ENOMEM;
+
+	obj->tgid = tgid;
+	INIT_LIST_HEAD(&obj->children);
+	obj->nr_children = 0;
+	obj->nr_threads = 0;
+	obj->self_exec_id = 0;
+	init_rwsem(&obj->sem);
+	obj->alive = 1;
+	kref_init(&obj->kref);
+	obj_entry->object = obj;
+
+	return 0;
+}
+
+static int children_first_touch(struct kddm_obj *obj_entry,
+				struct kddm_set *set, objid_t objid,int flags)
+{
+	return children_alloc_object(obj_entry, set, objid);
+}
+
+static int children_export_object(struct rpc_desc *desc,
+				  struct kddm_set *set,
+				  struct kddm_obj *obj_entry,
+				  objid_t objid,
+				  int flags)
+{
+	struct children_kddm_object *obj = obj_entry->object;
+	struct remote_child *child;
+	int retval = 0;
+
+	BUG_ON(!obj);
+	BUG_ON(!(obj->tgid & GLOBAL_PID_MASK));
+
+	retval = rpc_pack_type(desc, obj->nr_children);
+	if (unlikely(retval))
+		goto out;
+	retval = rpc_pack_type(desc, obj->nr_threads);
+	if (unlikely(retval))
+		goto out;
+	retval = rpc_pack_type(desc, obj->self_exec_id);
+	if (unlikely(retval))
+		goto out;
+	list_for_each_entry(child, &obj->children, sibling) {
+		retval = rpc_pack_type(desc, *child);
+		if (unlikely(retval))
+			goto out;
+	}
+
+out:
+	return retval;
+}
+
+static int children_import_object(struct rpc_desc *desc,
+				  struct kddm_set *set,
+				  struct kddm_obj *obj_entry,
+				  objid_t objid,
+				  int flags)
+{
+	struct children_kddm_object *obj = obj_entry->object;
+	struct remote_child *child, *next;
+	typeof(obj->nr_children) nr_children;
+	typeof(obj->nr_children) min_children;
+	typeof(min_children) i;
+	LIST_HEAD(children_head);
+	int retval = 0;
+
+	BUG_ON(!obj);
+	BUG_ON(!(obj->tgid & GLOBAL_PID_MASK));
+
+	retval = rpc_unpack_type(desc, nr_children);
+	if (unlikely(retval))
+		goto out;
+	retval = rpc_unpack_type(desc, obj->nr_threads);
+	if (unlikely(retval))
+		goto out;
+	retval = rpc_unpack_type(desc, obj->self_exec_id);
+	if (unlikely(retval))
+		goto out;
+
+	min_children = min(nr_children, obj->nr_children);
+
+	/* Reuse allocated elements as much as possible */
+
+	/* First, delete elements that won't be used anymore */
+	i = 0;
+	list_for_each_entry_safe(child, next, &obj->children, sibling) {
+		if (i + min_children == obj->nr_children)
+			break;
+		remove_child_links(obj, child);
+		kmem_cache_free(remote_child_cachep, child);
+		i++;
+	}
+	BUG_ON(i + min_children != obj->nr_children);
+
+	/* Second, fill in already allocated elements */
+	i = 0;
+	list_splice_init(&obj->children, &children_head);
+	list_for_each_entry_safe(child, next, &children_head, sibling) {
+		/* Does not need that child be linked to the obj->children
+		 * list, but only to a list */
+		remove_child_links(obj, child);
+		retval = rpc_unpack_type(desc, *child);
+		if (unlikely(retval))
+			goto err_free_child;
+		/* Put the child to the obj->children list */
+		set_child_links(obj, child);
+		i++;
+	}
+	BUG_ON(i != min_children);
+
+	/* Third, allocate, fill in, and add remaininig elements to import */
+	for (; i < nr_children; i++) {
+		child = kmem_cache_alloc(remote_child_cachep, GFP_KERNEL);
+		if (unlikely(!child)) {
+			retval = -ENOMEM;
+			goto out;
+		}
+		retval = rpc_unpack_type(desc, *child);
+		if (unlikely(retval))
+			goto err_free_child;
+		set_child_links(obj, child);
+	}
+	BUG_ON(i != nr_children);
+
+	obj->nr_children = nr_children;
+
+out:
+	return retval;
+
+err_free_child:
+	kmem_cache_free(remote_child_cachep, child);
+	goto out;
+}
+
+static int children_remove_object(void *object, struct kddm_set *set,
+				  objid_t objid)
+{
+	struct children_kddm_object *obj;
+
+	obj = object;
+	BUG_ON(!obj);
+
+	obj->alive = 0;
+	krg_children_put(obj);
+
+	return 0;
+}
+
+static struct iolinker_struct children_io_linker = {
+	.linker_name   = "children ",
+	.linker_id     = CHILDREN_LINKER,
+	.alloc_object  = children_alloc_object,
+	.first_touch   = children_first_touch,
+	.export_object = children_export_object,
+	.import_object = children_import_object,
+	.remove_object = children_remove_object,
+	.default_owner = global_pid_default_owner,
+};
+
+struct children_kddm_object *krg_children_readlock(pid_t tgid)
+{
+	struct children_kddm_object *obj;
+
+	/* Filter well known cases of no children kddm object. */
+	if (!(tgid & GLOBAL_PID_MASK))
+		return NULL;
+
+	obj = _kddm_get_object_no_ft(children_kddm_set, tgid);
+	if (obj) {
+		down_read(&obj->sem);
+		/* Marker for unlock. Dirty but temporary. */
+		obj->write_locked = 0;
+	} else {
+		_kddm_put_object(children_kddm_set, tgid);
+	}
+
+	return obj;
+}
+
+struct children_kddm_object *__krg_children_readlock(struct task_struct *task)
+{
+	return krg_children_readlock(task_tgid_knr(task));
+}
+
+static struct children_kddm_object *children_writelock(pid_t tgid, int nested)
+{
+	struct children_kddm_object *obj;
+
+	/* Filter well known cases of no children kddm object. */
+	if (!(tgid & GLOBAL_PID_MASK))
+		return NULL;
+
+	obj = _kddm_grab_object_no_ft(children_kddm_set, tgid);
+	if (obj) {
+		if (!nested)
+			down_write(&obj->sem);
+		else
+			down_write_nested(&obj->sem, SINGLE_DEPTH_NESTING);
+		/* Marker for unlock. Dirty but temporary. */
+		obj->write_locked = 1;
+	} else {
+		_kddm_put_object(children_kddm_set, tgid);
+	}
+
+	return obj;
+}
+
+struct children_kddm_object *krg_children_writelock(pid_t tgid)
+{
+	return children_writelock(tgid, 0);
+}
+
+struct children_kddm_object *__krg_children_writelock(struct task_struct *task)
+{
+	return children_writelock(task_tgid_knr(task), 0);
+}
+
+struct children_kddm_object *krg_children_writelock_nested(pid_t tgid)
+{
+	return children_writelock(tgid, 1);
+}
+
+static struct children_kddm_object *children_create_writelock(pid_t tgid)
+{
+	struct children_kddm_object *obj;
+
+	BUG_ON(!(tgid & GLOBAL_PID_MASK));
+
+	obj = _kddm_grab_object(children_kddm_set, tgid);
+	if (obj) {
+		down_write(&obj->sem);
+		/* Marker for unlock. Dirty but temporary. */
+		obj->write_locked = 1;
+	} else {
+		_kddm_put_object(children_kddm_set, tgid);
+	}
+
+	return obj;
+}
+
+void krg_children_unlock(struct children_kddm_object *obj)
+{
+	pid_t tgid = obj->tgid;
+
+	if (obj->write_locked)
+		up_write(&obj->sem);
+	else
+		up_read(&obj->sem);
+
+	_kddm_put_object(children_kddm_set, tgid);
+}
+
+static
+struct children_kddm_object *
+children_alloc(struct task_struct *task, pid_t tgid)
+{
+	struct children_kddm_object *obj;
+
+	/* Filter well known cases of no children kddm object. */
+	BUG_ON(!(tgid & GLOBAL_PID_MASK));
+
+	obj = children_create_writelock(tgid);
+	if (obj) {
+		obj->nr_threads = 1;
+		obj->self_exec_id = task->self_exec_id;
+		rcu_assign_pointer(task->children_obj, obj);
+		krg_children_unlock(obj);
+	}
+
+	return obj;
+}
+
+struct children_kddm_object *krg_children_alloc(struct task_struct *task)
+{
+	return children_alloc(task, task_tgid_knr(task));
+}
+
+static void free_children(struct task_struct *task)
+{
+	struct children_kddm_object *obj = task->children_obj;
+
+	BUG_ON(!obj);
+	BUG_ON(!list_empty(&obj->children));
+	BUG_ON(obj->nr_threads);
+
+	rcu_assign_pointer(task->children_obj, NULL);
+
+	up_write(&obj->sem);
+	_kddm_remove_frozen_object(children_kddm_set, obj->tgid);
+}
+
+void __krg_children_share(struct task_struct *task)
+{
+	struct children_kddm_object *obj = task->children_obj;
+	obj->nr_threads++;
+}
+
+void krg_children_share(struct task_struct *task)
+{
+	struct children_kddm_object *obj = task->children_obj;
+
+	obj = krg_children_writelock(obj->tgid);
+	BUG_ON(!obj);
+	BUG_ON(obj != task->children_obj);
+	__krg_children_share(task);
+	krg_children_unlock(obj);
+
+}
+
+/* Must be called under krg_children_writelock */
+void krg_children_exit(struct task_struct *task)
+{
+	struct children_kddm_object *obj = task->children_obj;
+	int free;
+
+	free = !(--obj->nr_threads);
+	if (free) {
+		free_children(task);
+	} else {
+		rcu_assign_pointer(task->children_obj, NULL);
+		krg_children_unlock(obj);
+	}
+}
+
+static int krg_children_alive(struct children_kddm_object *obj)
+{
+	return obj && obj->alive;
+}
+
+static int new_child(struct children_kddm_object *obj,
+		     pid_t parent_pid,
+		     struct pid *pid, struct pid *tgid,
+		     struct pid *pgrp, struct pid *session,
+		     int exit_signal)
+{
+	struct remote_child *item;
+
+	if (!obj)
+		return 0;
+	BUG_ON(parent_pid == 1);
+
+	item = kmem_cache_alloc(remote_child_cachep, GFP_ATOMIC);
+	if (!item)
+		return -ENOMEM;
+
+	item->pid = pid_knr(pid);
+	item->tgid = pid_knr(tgid);
+	item->pgid = pid_knr(pgrp);
+	item->sid = pid_knr(session);
+	item->parent = item->real_parent = parent_pid;
+	item->ptraced = 0;
+	item->exit_signal = exit_signal;
+	item->exit_state = 0;
+	item->node = kerrighed_node_id;
+	set_child_links(obj, item);
+	obj->nr_children++;
+
+	return 0;
+}
+
+int krg_new_child(struct children_kddm_object *obj,
+		  pid_t parent_pid,
+		  struct task_struct *child)
+{
+	return new_child(obj, parent_pid, task_pid(child), task_tgid(child),
+			 task_pgrp(child), task_session(child),
+			 child->exit_signal);
+}
+
+/* Expects obj write locked */
+void __krg_set_child_pgid(struct children_kddm_object *obj,
+			  pid_t pid, pid_t pgid)
+{
+	struct remote_child *item;
+
+	if (!obj)
+		return;
+
+	list_for_each_entry(item, &obj->children, sibling)
+		if (item->pid == pid) {
+			item->pgid = pgid;
+			break;
+		}
+}
+
+void krg_set_child_pgid(struct children_kddm_object *obj,
+			struct task_struct *child)
+{
+	__krg_set_child_pgid(obj, task_pid_knr(child), task_pgrp_knr(child));
+}
+
+int krg_set_child_ptraced(struct children_kddm_object *obj,
+			  struct task_struct *child, int ptraced)
+{
+	pid_t pid = task_pid_knr(child);
+	struct remote_child *item;
+
+	if (unlikely(!obj))
+		return 0;
+
+	list_for_each_entry(item, &obj->children, sibling)
+		if (item->pid == pid) {
+			if (ptraced && item->ptraced)
+				return -EBUSY;
+			item->ptraced = ptraced;
+			return 0;
+		}
+	BUG();
+	return -ECHILD;
+}
+
+/* Expects obj write locked */
+void krg_set_child_exit_signal(struct children_kddm_object *obj,
+			       struct task_struct *child)
+{
+	pid_t pid = task_pid_knr(child);
+	struct remote_child *item;
+
+	if (!obj)
+		return;
+
+	list_for_each_entry(item, &obj->children, sibling)
+		if (item->pid == pid) {
+			item->exit_signal = child->exit_signal;
+			break;
+		}
+}
+
+/* Expects obj write locked */
+void krg_set_child_exit_state(struct children_kddm_object *obj,
+			      struct task_struct *child)
+{
+	pid_t pid = task_pid_knr(child);
+	struct remote_child *item;
+
+	if (!obj)
+		return;
+
+	list_for_each_entry(item, &obj->children, sibling)
+		if (item->pid == pid) {
+			item->exit_state = child->exit_state;
+			break;
+		}
+}
+
+void krg_set_child_location(struct children_kddm_object *obj,
+			    struct task_struct *child)
+{
+	pid_t pid = task_pid_knr(child);
+	struct remote_child *item;
+
+	if (!obj)
+		return;
+
+	list_for_each_entry(item, &obj->children, sibling)
+		if (item->pid == pid) {
+			item->node = kerrighed_node_id;
+			break;
+		}
+}
+
+/* Expects obj write locked */
+static void remove_child(struct children_kddm_object *obj,
+			 struct remote_child *child)
+{
+	remove_child_links(obj, child);
+	kmem_cache_free(remote_child_cachep, child);
+	obj->nr_children--;
+}
+
+static void reparent_child(struct children_kddm_object *obj,
+			   struct remote_child *child,
+			   pid_t reaper_pid, int same_group)
+{
+	/*
+	 * A child can be reparented:
+	 * either to another thread of the same thread group,
+	 * or to its child reaper -> local child reaper
+	 */
+
+	BUG_ON(child->real_parent == reaper_pid);
+	if (!same_group)
+		/*
+		 * Local child reaper doesn't need a children
+		 * kddm object
+		 */
+		/* TODO: Is it true with PID namespaces? */
+		remove_child(obj, child);
+	else {
+		BUG_ON(!(reaper_pid & GLOBAL_PID_MASK));
+		/*
+		 * For ptraced children, child->parent was already wrong since
+		 * it is not assigned when ptrace-attaching. So keep it wrong
+		 * the same way.
+		 */
+		child->parent = child->real_parent = reaper_pid;
+	}
+}
+
+/*
+ * Expects parent->children_obj write locked
+ * and tasklist_lock write locked
+ */
+void krg_forget_original_remote_parent(struct task_struct *parent,
+				       struct task_struct *reaper)
+{
+	int threaded_reparent = same_thread_group(reaper, parent);
+	struct children_kddm_object *obj = parent->children_obj;
+	struct remote_child *child, *tmp_child;
+	pid_t ppid = task_pid_knr(parent);
+
+	list_for_each_entry_safe(child, tmp_child, &obj->children, sibling)
+		if (child->real_parent == ppid) {
+			if (!threaded_reparent
+			    && child->exit_state == EXIT_ZOMBIE
+			    && child->node != kerrighed_node_id)
+				/* Have it reaped by its local child reaper */
+				/* Asynchronous */
+				notify_remote_child_reaper(child->pid,
+							   child->node);
+			reparent_child(obj, child,
+				       task_pid_knr(reaper), threaded_reparent);
+		}
+}
+
+/* Expects obj write locked */
+void
+krg_remove_child(struct children_kddm_object *obj, struct task_struct *child)
+{
+	pid_t child_pid = task_pid_knr(child);
+	struct remote_child *item;
+
+	if (!obj)
+		return;
+
+	list_for_each_entry(item, &obj->children, sibling)
+		if (item->pid == child_pid) {
+			remove_child(obj, item);
+			break;
+		}
+}
+
+/* Expects obj at least read locked */
+static int is_child(struct children_kddm_object *obj, pid_t pid)
+{
+	struct remote_child *item;
+	int retval = 0;
+
+	if (!obj)
+		return 0;
+
+	list_for_each_entry(item, &obj->children, sibling)
+		if (item->pid == pid) {
+			retval = 1;
+			break;
+		}
+
+	return retval;
+}
+
+static int krg_eligible_child(struct children_kddm_object *obj,
+			      enum pid_type type, pid_t pid, int options,
+			      struct remote_child *child)
+{
+	int retval = 0;
+
+	switch (type) {
+	case PIDTYPE_PID:
+		if (child->pid != pid)
+			goto out;
+		break;
+	case PIDTYPE_PGID:
+		if (child->pgid != pid)
+			goto out;
+		break;
+	case PIDTYPE_MAX:
+		break;
+	default:
+		BUG();
+	}
+
+	/* Wait for all children (clone and not) if __WALL is set;
+	 * otherwise, wait for clone children *only* if __WCLONE is
+	 * set; otherwise, wait for non-clone children *only*.  (Note:
+	 * A "clone" child here is one that reports to its parent
+	 * using a signal other than SIGCHLD.) */
+	if (((child->exit_signal != SIGCHLD) ^ ((options & __WCLONE) != 0))
+	    && !(options & __WALL))
+		goto out;
+
+	/* No support for remote LSM check */
+
+	retval = 1;
+
+out:
+	return retval;
+}
+
+static int krg_delay_group_leader(struct remote_child *child)
+{
+	return child->pid == child->tgid && !list_empty(&child->thread_group);
+}
+
+static
+bool krg_wait_consider_task(struct children_kddm_object *obj,
+			    struct remote_child *child,
+			    int *notask_error,
+			    enum pid_type type, pid_t pid, int options,
+			    struct siginfo __user *infop, int __user *stat_addr,
+			    struct rusage __user *ru,
+			    int *tsk_result)
+{
+	int ret;
+
+	ret = krg_eligible_child(obj, type, pid, options, child);
+	if (!ret)
+		return false;
+
+/*         if (unlikely(ret < 0)) { */
+/*                 |+ */
+/*                  * If we have not yet seen any eligible child, */
+/*                  * then let this error code replace -ECHILD. */
+/*                  * A permission error will give the user a clue */
+/*                  * to look for security policy problems, rather */
+/*                  * than for mysterious wait bugs. */
+/*                  +| */
+/*                 if (*notask_error) */
+/*                         *notask_error = ret; */
+/*         } */
+
+	if (unlikely(child->ptraced)) {
+		/*
+		 * This child is hidden by ptrace.
+		 * We aren't allowed to see it now, but eventually we will.
+		 */
+		*notask_error = 0;
+		return false;
+	}
+
+	/*
+	 * item->exit_state should not reach EXIT_DEAD since this can
+	 * only happen when a thread self-reaps, and the thread is
+	 * removed from its parent's children object before releasing
+	 * the lock on it.
+	 */
+	BUG_ON(child->exit_state == EXIT_DEAD);
+
+	/*
+	 * We don't reap group leaders with subthreads.
+	 */
+	if (child->exit_state == EXIT_ZOMBIE
+	    && !krg_delay_group_leader(child)) {
+		/* Avoid doing an RPC when we already know the result */
+		if (!likely(options & WEXITED))
+			return false;
+
+		krg_children_unlock(current->children_obj);
+		*tsk_result = krg_wait_task_zombie(child->pid, child->node,
+						   options,
+						   infop, stat_addr, ru);
+		return true;
+	}
+
+	/*
+	 * It's stopped or running now, so it might
+	 * later continue, exit, or stop again.
+	 */
+	*notask_error = 0;
+
+	/* Check for stopped and continued task is not implemented right now. */
+	return false;
+}
+
+/*
+ * Expects obj locked. Releases obj lock.
+ *
+ * @return	pid (> 1) of task reaped, if any (init cannot be reaped), or
+ *		0 and
+ *		 0 in *notask_error if some tasks could be reaped later, or
+ *		 *notask_error untouched if no task will be ever reapable, or
+ *		 negative error code in *notask_error if an error occurs when
+ *			reaping a task (do_wait() should abort)
+ */
+int krg_do_wait(struct children_kddm_object *obj, int *notask_error,
+		enum pid_type type, pid_t pid, int options,
+		struct siginfo __user *infop, int __user *stat_addr,
+		struct rusage __user *ru)
+{
+	struct remote_child *item;
+	pid_t current_pid = task_pid_knr(current);
+	int ret = 0;
+
+	if (!is_krg_pid_ns_root(task_active_pid_ns(current)))
+		goto out_unlock;
+
+	/*
+	 * Children attached by ptrace cannot be remote, so we only examine
+	 * regular children.
+	 */
+
+retry:
+	/* Remote version of do_wait_thread() */
+	list_for_each_entry(item, &obj->children, sibling) {
+		if ((options & __WNOTHREAD)
+		    && item->real_parent != current_pid)
+			continue;
+
+		/*
+		 * Do not consider detached threads.
+		 */
+		if (item->exit_signal != -1) {
+			if (krg_wait_consider_task(obj, item, notask_error,
+						     type, pid, options,
+						     infop, stat_addr, ru,
+						     &ret)) {
+				if (ret)
+					goto out;
+				/* Raced with another thread. Retry. */
+				__krg_children_readlock(current);
+				goto retry;
+			}
+		}
+	}
+
+out_unlock:
+	krg_children_unlock(current->children_obj);
+out:
+	return ret;
+}
+
+void krg_update_self_exec_id(struct task_struct *task)
+{
+	struct children_kddm_object *obj;
+
+	if (rcu_dereference(task->children_obj)) {
+		obj = __krg_children_writelock(task);
+		BUG_ON(!obj);
+		obj->self_exec_id = task->self_exec_id;
+		krg_children_unlock(obj);
+	}
+}
+
+u32 krg_get_real_parent_self_exec_id(struct task_struct *task,
+				     struct children_kddm_object *obj)
+{
+	u32 id;
+
+	if (task->real_parent == baby_sitter)
+		id = obj->self_exec_id;
+	else
+		id = task->real_parent->self_exec_id;
+
+	return id;
+}
+
+/* Must be called under rcu_read_lock() */
+pid_t krg_get_real_parent_tgid(struct task_struct *task,
+			       struct pid_namespace *ns)
+{
+	pid_t real_parent_tgid;
+	struct task_struct *real_parent = rcu_dereference(task->real_parent);
+
+	if (!pid_alive(task))
+		return 0;
+
+	if (real_parent != baby_sitter) {
+		real_parent_tgid = task_tgid_nr_ns(real_parent, ns);
+	} else if (!ns->krg_ns_root) {
+		/*
+		 * ns is an ancestor of task's root Kerrighed namespace, and
+		 * thus has no names for remote parents.
+		 */
+		real_parent_tgid = 0;
+	} else {
+		struct task_kddm_object *task_obj =
+			rcu_dereference(task->task_obj);
+		struct children_kddm_object *parent_children_obj =
+			rcu_dereference(task->parent_children_obj);
+
+		BUG_ON(!is_krg_pid_ns_root(ns));
+
+		if (task_obj && krg_children_alive(parent_children_obj)) {
+			real_parent_tgid = task_obj->real_parent_tgid;
+		} else {
+			struct pid_namespace *_ns = task_active_pid_ns(task);
+			if (_ns) {
+				BUG_ON(_ns != ns);
+				real_parent_tgid = 1;
+			} else {
+				real_parent_tgid = 0;
+			}
+		}
+	}
+
+	return real_parent_tgid;
+}
+
+/* Expects obj locked */
+int __krg_get_parent(struct children_kddm_object *obj, pid_t pid,
+		     pid_t *parent_pid, pid_t *real_parent_pid)
+{
+	struct remote_child *item;
+	int retval = -ESRCH;
+
+	if (!obj)
+		goto out;
+
+	list_for_each_entry(item, &obj->children, sibling)
+		if (item->pid == pid) {
+			*parent_pid = item->parent;
+			*real_parent_pid = item->real_parent;
+			retval = 0;
+			goto out;
+		}
+
+out:
+	return retval;
+}
+
+int krg_get_parent(struct children_kddm_object *obj, struct task_struct *child,
+		   pid_t *parent_pid, pid_t *real_parent_pid)
+{
+	return __krg_get_parent(obj, task_pid_knr(child),
+				parent_pid, real_parent_pid);
+}
+
+struct children_kddm_object *
+krg_parent_children_writelock(struct task_struct *task, pid_t *parent_tgid)
+{
+	struct children_kddm_object *obj;
+	struct pid_namespace *ns = task_active_pid_ns(task)->krg_ns_root;
+	pid_t tgid, reaper_tgid;
+
+	BUG_ON(!ns);
+	rcu_read_lock();
+	tgid = krg_get_real_parent_tgid(task, ns);
+	obj = rcu_dereference(task->parent_children_obj);
+	rcu_read_unlock();
+	BUG_ON(!tgid);
+	reaper_tgid = task_tgid_knr(task_active_pid_ns(task)->child_reaper);
+	if (!obj || tgid == reaper_tgid) {
+		obj = NULL;
+		goto out;
+	}
+
+	obj = krg_children_writelock(tgid);
+	/*
+	 * Check that thread group tgid is really the parent of task.
+	 * If not, unlock obj immediately, and return NULL.
+	 *
+	 * is_child may also return 0 if task's parent is init. In that case, it
+	 * is still correct to return NULL as long as parent_tgid is set.
+	 */
+	if (!is_child(obj, task_pid_knr(task))) {
+		if (obj)
+			krg_children_unlock(obj);
+		obj = NULL;
+		tgid = reaper_tgid;
+		goto out;
+	}
+
+out:
+	*parent_tgid = tgid;
+	return obj;
+}
+
+struct children_kddm_object *
+krg_parent_children_readlock(struct task_struct *task, pid_t *parent_tgid)
+{
+	struct children_kddm_object *obj;
+	struct pid_namespace *ns = task_active_pid_ns(task)->krg_ns_root;
+	pid_t tgid, reaper_tgid;
+
+	BUG_ON(!ns);
+	rcu_read_lock();
+	tgid = krg_get_real_parent_tgid(task, ns);
+	obj = rcu_dereference(task->parent_children_obj);
+	rcu_read_unlock();
+	BUG_ON(!tgid);
+	reaper_tgid = task_tgid_knr(task_active_pid_ns(task)->child_reaper);
+	if (!obj || tgid == reaper_tgid) {
+		obj = NULL;
+		goto out;
+	}
+
+	obj = krg_children_readlock(tgid);
+	/*
+	 * Check that thread group tgid is really the parent of task.
+	 * If not, unlock obj immediately, and return NULL.
+	 *
+	 * is_child may also return 0 if task's parent is init. In that case, it
+	 * is still correct to return NULL as long as parent_tgid is set.
+	 */
+	if (!is_child(obj, task_pid_knr(task))) {
+		if (obj)
+			krg_children_unlock(obj);
+		obj = NULL;
+		tgid = reaper_tgid;
+		goto out;
+	}
+
+out:
+	*parent_tgid = tgid;
+	return obj;
+}
+
+pid_t krg_get_real_parent_pid(struct task_struct *task)
+{
+	struct children_kddm_object *parent_obj;
+	pid_t real_parent_pid, parent_pid, real_parent_tgid;
+
+	if (task->real_parent != baby_sitter)
+		return task_pid_vnr(task->real_parent);
+
+	BUG_ON(!is_krg_pid_ns_root(task_active_pid_ns(current)));
+	parent_obj = krg_parent_children_readlock(task, &real_parent_tgid);
+	if (!parent_obj) {
+		real_parent_pid = 1;
+	} else {
+		/* gcc ... */
+		real_parent_pid = 0;
+		krg_get_parent(parent_obj, task,
+			       &parent_pid, &real_parent_pid);
+		krg_children_unlock(parent_obj);
+	}
+
+	return real_parent_pid;
+}
+EXPORT_SYMBOL(krg_get_real_parent_pid);
+
+/************************************************************************
+ * Local children list of a task					*
+ ************************************************************************/
+
+static inline struct list_head *new_krg_parent_entry(pid_t key)
+{
+	struct list_head *entry;
+
+	entry = kmem_cache_alloc(krg_parent_head_cachep, GFP_ATOMIC);
+	if (!entry)
+		return NULL;
+
+	INIT_LIST_HEAD(entry);
+	__hashtable_add(krg_parent_table, key, entry);
+
+	return entry;
+}
+
+static inline struct list_head *get_krg_parent_entry(pid_t key)
+{
+	return __hashtable_find(krg_parent_table, key);
+}
+
+static inline void delete_krg_parent_entry(pid_t key)
+{
+	struct list_head *entry;
+
+	entry = __hashtable_find(krg_parent_table, key);
+	BUG_ON(!entry);
+	BUG_ON(!list_empty(entry));
+
+	__hashtable_remove(krg_parent_table, key);
+	kmem_cache_free(krg_parent_head_cachep, entry);
+}
+
+static inline void add_to_krg_parent(struct task_struct *tsk, pid_t parent_pid)
+{
+	struct list_head *children;
+
+	children = get_krg_parent_entry(parent_pid);
+	if (children == NULL) {
+		children = new_krg_parent_entry(parent_pid);
+		if (!children)
+			OOM;
+	}
+	list_add_tail(&tsk->sibling, children);
+}
+
+static inline void remove_from_krg_parent(struct task_struct *tsk,
+					  pid_t parent_pid)
+{
+	struct list_head *children;
+
+	children = get_krg_parent_entry(parent_pid);
+	BUG_ON(!children);
+
+	list_del(&tsk->sibling);
+	if (list_empty(children))
+		delete_krg_parent_entry(parent_pid);
+}
+
+/*
+ * Used in two cases:
+ * 1/ When child considers its parent as remote, and this parent is now local
+ *    -> link directly in parent's children list
+ * 2/ When child considers its parent as local, and parent is leaving the node
+ *    -> unlink child from any process children list,
+ *       and add it to its parent's entry in krg_parent table
+ * In both cases, child->real_parent is assumed to be correctly set according to
+ * the desired result.
+ */
+static inline void fix_chain_to_parent(struct task_struct *child,
+				       pid_t parent_pid)
+{
+	if (child->real_parent != baby_sitter) {
+		/* Child may be still linked in baby_sitter's children */
+		list_move_tail(&child->sibling, &child->real_parent->children);
+		return;
+	}
+
+	/*
+	 * At this point, child is chained in baby_sitter's or local parent's
+	 * children.
+	 * Fix this right now.
+	 */
+	list_del(&child->sibling);
+	add_to_krg_parent(child, parent_pid);
+}
+
+/*
+ * Parent was remote, and can now be considered as local for its local children
+ * Relink all its local children in its children list
+ *
+ * Assumes at least a read lock on parent's children kddm object
+ */
+static inline void rechain_local_children(struct task_struct *parent)
+{
+	pid_t ppid = task_pid_knr(parent);
+	struct list_head *children;
+	struct task_struct *child, *tmp;
+
+	children = get_krg_parent_entry(ppid);
+	if (!children)
+		return;
+
+	list_for_each_entry_safe(child, tmp, children, sibling) {
+		/* TODO: This will need more serious work to support ptrace */
+		/*
+		 * If parent has reused a pid, this krg_parent list may still
+		 * contain children of a former user of this pid.
+		 */
+		if (!likely(is_child(parent->children_obj, task_pid_knr(child))))
+			continue;
+		child->real_parent = parent;
+		list_move_tail(&child->sibling, &parent->children);
+		if (child->parent == baby_sitter &&
+		    child->task_obj->parent == ppid)
+			child->parent = parent;
+	}
+
+	if (likely(list_empty(children)))
+		delete_krg_parent_entry(ppid);
+}
+
+/* Expects write lock on tasklist held */
+static inline void update_links(struct task_struct *orphan)
+{
+	fix_chain_to_parent(orphan, orphan->task_obj->real_parent);
+	rechain_local_children(orphan);
+}
+
+static inline struct task_struct *find_relative(pid_t pid)
+{
+	struct task_struct *p;
+
+	p = find_task_by_kpid(pid);
+	if (p && !unlikely(p->flags & PF_AWAY))
+		return p;
+	else
+		return baby_sitter;
+}
+
+static inline struct task_struct *find_live_relative(pid_t pid)
+{
+	struct task_struct *p;
+
+	p = find_relative(pid);
+	if (p != baby_sitter && (p->flags & PF_EXITING))
+		return baby_sitter;
+	else
+		return p;
+}
+
+static void update_relatives(struct task_struct *task)
+{
+	struct task_kddm_object *task_obj = task->task_obj;
+
+	/*
+	 * In case of local (real_)parent's death, delay reparenting as if
+	 * (real_)parent was still remote
+	 */
+	if (task->parent == baby_sitter)
+		task->parent = find_live_relative(task_obj->parent);
+	if (task->real_parent == baby_sitter)
+		task->real_parent = find_live_relative(task_obj->real_parent);
+	if (task->group_leader == baby_sitter)
+		task->group_leader = find_relative(task_obj->group_leader);
+}
+
+/* Used by import_process() */
+void join_local_relatives(struct task_struct *orphan)
+{
+	__krg_children_readlock(orphan);
+	write_lock_irq(&tasklist_lock);
+
+	/*
+	 * Need to do it early to avoid a group leader task to consider itself
+	 * as remote when updating the group leader pointer
+	 */
+	orphan->flags &= ~PF_AWAY;
+
+	update_relatives(orphan);
+	update_links(orphan);
+
+	write_unlock_irq(&tasklist_lock);
+	krg_children_unlock(orphan->children_obj);
+}
+
+/* Expects write lock on tasklist held */
+static void __reparent_to_baby_sitter(struct task_struct *orphan,
+				      pid_t parend_pid)
+{
+	orphan->parent = baby_sitter;
+
+	if (orphan->real_parent == baby_sitter)
+		return;
+	orphan->real_parent = baby_sitter;
+	list_move_tail(&orphan->sibling, &baby_sitter->children);
+}
+
+/* Expects write lock on tasklist held */
+static void reparent_to_baby_sitter(struct task_struct *orphan,
+				    pid_t parent_pid)
+{
+	__reparent_to_baby_sitter(orphan, parent_pid);
+	fix_chain_to_parent(orphan, parent_pid);
+}
+
+/* Expects write lock on tasklist held */
+static void leave_baby_sitter(struct task_struct *tsk, pid_t old_parent)
+{
+	BUG_ON(tsk->real_parent != baby_sitter);
+	update_relatives(tsk);
+	BUG_ON(tsk->parent == baby_sitter);
+	BUG_ON(tsk->real_parent == baby_sitter);
+
+	remove_from_krg_parent(tsk, old_parent);
+	list_add_tail(&tsk->sibling, &tsk->real_parent->children);
+}
+
+/*
+ * Used by migration
+ * Expects write lock on tsk->task_obj object held
+ */
+void leave_all_relatives(struct task_struct *tsk)
+{
+	struct task_struct *child, *tmp;
+
+	write_lock_irq(&tasklist_lock);
+
+	tsk->flags |= PF_AWAY;
+
+	/*
+	 * Update task_obj in case parent exited while
+	 * we were chained in its regular children list
+	 */
+	if (tsk->parent != baby_sitter)
+		tsk->task_obj->parent = task_pid_knr(tsk->parent);
+	if (tsk->real_parent != baby_sitter) {
+		tsk->task_obj->real_parent = task_pid_knr(tsk->real_parent);
+		tsk->task_obj->real_parent_tgid = task_tgid_knr(tsk->real_parent);
+	}
+
+	/* Make local children act as if tsk were already remote */
+	list_for_each_entry_safe(child, tmp, &tsk->children, sibling)
+		reparent_to_baby_sitter(child, task_pid_knr(tsk));
+
+	/* Make parent act as if tsk were already remote */
+	if (tsk->real_parent == baby_sitter) {
+		/*
+		 * parent is remote, but tsk is still linked to the local
+		 * children list of its parent
+		 */
+		remove_from_krg_parent(tsk, tsk->task_obj->real_parent);
+		list_add_tail(&tsk->sibling, &baby_sitter->children);
+	} else {
+		__reparent_to_baby_sitter(tsk, task_pid_knr(tsk->real_parent));
+	}
+
+	write_unlock_irq(&tasklist_lock);
+}
+
+/* Per syscall hooks */
+
+/* fork() */
+
+int krg_children_prepare_fork(struct task_struct *task,
+			      struct pid *pid,
+			      unsigned long clone_flags)
+{
+	struct children_kddm_object *obj, *parent_obj;
+	pid_t tgid;
+	int err = 0;
+
+	rcu_assign_pointer(task->children_obj, NULL);
+	rcu_assign_pointer(task->parent_children_obj, NULL);
+
+	if (!is_krg_pid_ns_root(task_active_pid_ns(task)))
+		goto out;
+
+	if (krg_current) {
+		rcu_assign_pointer(task->children_obj,
+				   krg_current->children_obj);
+		BUG_ON(!task->children_obj);
+		rcu_assign_pointer(task->parent_children_obj,
+				   krg_current->parent_children_obj);
+		goto out;
+	}
+
+	if (clone_flags & CLONE_THREAD)
+		tgid = task_tgid_knr(current);
+	else
+		tgid = pid_knr(pid);
+
+	/* Kernel threads and local pids must not use the children kddm set. */
+	if (!(tgid & GLOBAL_PID_MASK) || (current->flags & PF_KTHREAD))
+		goto out;
+
+	/* Attach task to the children kddm object of its thread group */
+	if (!(clone_flags & CLONE_THREAD)) {
+		obj = children_alloc(task, tgid);
+		if (!obj) {
+			err = -ENOMEM;
+			goto out;
+		}
+		rcu_assign_pointer(task->children_obj, obj);
+	} else {
+		obj = current->children_obj;
+		if (obj) {
+			rcu_assign_pointer(task->children_obj, obj);
+			krg_children_share(task);
+		}
+	}
+
+	/* Prepare to put task in the children kddm object of its parent */
+	if ((clone_flags & (CLONE_PARENT | CLONE_THREAD))) {
+		pid_t parent_tgid;
+		parent_obj = krg_parent_children_writelock(current,
+							   &parent_tgid);
+	} else {
+		parent_obj = __krg_children_writelock(current);
+	}
+	krg_children_get(parent_obj);
+	rcu_assign_pointer(task->parent_children_obj, parent_obj);
+
+out:
+	return err;
+}
+
+int krg_children_fork(struct task_struct *task,
+		      struct pid *pid,
+		      unsigned long clone_flags)
+{
+	struct children_kddm_object *obj = task->parent_children_obj;
+	pid_t parent_pid;
+	struct pid *tgid;
+	int err = 0;
+
+	if (krg_current)
+		goto out;
+
+	if (!obj)
+		goto out;
+
+	if (task->real_parent == baby_sitter)
+		/*
+		 * task's task_obj is not setup yet, but
+		 * one of CLONE_THREAD and CLONE_PARENT must be set, so we can
+		 * use current's task_obj.
+		 */
+		parent_pid = current->task_obj->real_parent;
+	else
+		parent_pid = task_pid_knr(task->real_parent);
+	tgid = pid;
+	if (clone_flags & CLONE_THREAD)
+		tgid = task_tgid(current);
+	err = new_child(obj, parent_pid,
+			pid, tgid,
+			task_pgrp(current), task_session(current),
+			task->exit_signal);
+
+	if (!err && task->real_parent == baby_sitter)
+		add_to_krg_parent(task, parent_pid);
+
+out:
+	return err;
+}
+
+void krg_children_commit_fork(struct task_struct *task)
+{
+	struct children_kddm_object *parent_obj = task->parent_children_obj;
+
+	if (!krg_current && parent_obj)
+		krg_children_unlock(parent_obj);
+}
+
+void krg_children_abort_fork(struct task_struct *task)
+{
+	struct children_kddm_object *parent_obj = task->parent_children_obj;
+	struct children_kddm_object *obj = task->children_obj;
+
+	if (krg_current)
+		return;
+
+	if (parent_obj) {
+		krg_children_unlock(parent_obj);
+		krg_children_put(parent_obj);
+	}
+
+	if (obj) {
+		krg_children_writelock(obj->tgid);
+		krg_children_exit(task);
+	}
+}
+
+/* de_thread() (execve()) */
+
+/* Expects tasklist writelocked */
+static void krg_children_unlink_krg_parent(struct task_struct *task)
+{
+	if (task->real_parent == baby_sitter) {
+		/*
+		 * leader will lose its task KDDM object, make it a
+		 * simple child of baby_sitter.
+		 */
+		remove_from_krg_parent(task, task->task_obj->real_parent);
+		list_add_tail(&task->sibling, &baby_sitter->children);
+	}
+}
+
+/* Expects tasklist writelocked */
+static void krg_children_relink_krg_parent(struct task_struct *task)
+{
+	if (task->real_parent == baby_sitter) {
+		list_del(&task->sibling);
+		add_to_krg_parent(task, task->task_obj->real_parent);
+	}
+}
+
+struct children_kddm_object *
+krg_children_prepare_de_thread(struct task_struct *task)
+{
+	struct children_kddm_object *obj = NULL;
+	struct task_struct *leader = task->group_leader;
+	pid_t real_parent_tgid;
+
+	if (rcu_dereference(task->parent_children_obj)) {
+		obj = krg_parent_children_writelock(task, &real_parent_tgid);
+		write_lock_irq(&tasklist_lock);
+		/*
+		 * leader should not be considered as a child after the PID
+		 * switch.
+		 */
+		krg_children_unlink_krg_parent(leader);
+		/*
+		 * krg_parent may change because leader's
+		 * task_obj will be more up to date.
+		 */
+		krg_children_unlink_krg_parent(task);
+		write_unlock_irq(&tasklist_lock);
+		krg_remove_child(obj, task);
+	}
+	if (rcu_dereference(task->children_obj)) {
+		struct children_kddm_object *children_obj;
+
+		children_obj = __krg_children_writelock(task);
+		BUG_ON(!children_obj);
+		/*
+		 * All children were reparented to task, but the children
+		 * object knows them as children of task's pid, while task
+		 * is taking leader's pid.
+		 * krg_forget_original_remote_parent() will only record
+		 * the pid of leader, so this is safe.
+		 */
+		krg_forget_original_remote_parent(task, leader);
+	}
+
+	return obj;
+}
+
+void krg_children_finish_de_thread(struct children_kddm_object *obj,
+				   struct task_struct *task)
+{
+	if (rcu_dereference(task->children_obj))
+		krg_children_unlock(task->children_obj);
+	if (obj) {
+		write_lock_irq(&tasklist_lock);
+		krg_children_relink_krg_parent(task);
+		write_unlock_irq(&tasklist_lock);
+		krg_set_child_exit_signal(obj, task);
+		krg_set_child_exit_state(obj, task);
+		krg_children_unlock(obj);
+	}
+}
+
+/* exit()/release_task() */
+
+/*
+ * Expects tasklist and task KDDM object writelocked,
+ * and real parent's children KDDM object locked
+ */
+void
+krg_update_parents(struct task_struct *task, pid_t parent, pid_t real_parent)
+{
+	if (task->real_parent == baby_sitter) {
+		remove_from_krg_parent(task, task->task_obj->real_parent);
+		list_add_tail(&task->sibling, &baby_sitter->children);
+	}
+
+	task->task_obj->parent = parent;
+	task->task_obj->real_parent = real_parent;
+
+	/* Real parent is alive */
+	if (task->parent == baby_sitter)
+		task->parent = find_relative(parent);
+	if (task->real_parent == baby_sitter)
+		task->real_parent = find_relative(real_parent);
+
+	fix_chain_to_parent(task, real_parent);
+}
+
+/* Expects task kddm object write locked and tasklist lock write locked */
+void krg_reparent_to_local_child_reaper(struct task_struct *task)
+{
+	struct task_kddm_object *task_obj = task->task_obj;
+	struct task_struct *reaper = task_active_pid_ns(task)->child_reaper;
+	pid_t parent_pid, reaper_pid, reaper_tgid;
+
+	/*
+	 * If task is ptraced, the ptracer is local and we can safely set
+	 * task_obj->parent to parent's pid.
+	 */
+	parent_pid = task_obj->real_parent;
+	reaper_pid = task_pid_knr(reaper);
+	reaper_tgid = task_tgid_knr(reaper);
+	task_obj->real_parent = reaper_pid;
+	task_obj->real_parent_tgid = reaper_tgid;
+	if (task->parent == baby_sitter)
+		task_obj->parent = reaper_pid;
+	else
+		BUG_ON(task_obj->parent != task_pid_knr(task->parent));
+	leave_baby_sitter(task, parent_pid);
+	BUG_ON(task->real_parent != reaper);
+	BUG_ON(task->parent == baby_sitter);
+
+	if (!task_detached(task))
+		task->exit_signal = SIGCHLD;
+}
+
+void krg_unhash_process(struct task_struct *tsk)
+{
+	pid_t real_parent_tgid;
+	struct children_kddm_object *obj;
+
+	if (!task_active_pid_ns(tsk)->krg_ns_root)
+		return;
+
+	if (tsk->exit_state == EXIT_MIGRATION)
+		return;
+
+	/*
+	 * If we are inside de_thread() and tsk is an old thread group leader
+	 * being reaped by the new thread group leader, we do not want to remove
+	 * tsk's pid from the global children list of tsk's parent.
+	 * Moreover tsk is already reparented to baby_sitter, so we have nothing
+	 * to do here.
+	 */
+	if (has_group_leader_pid(tsk) && !thread_group_leader(tsk))
+		return;
+
+	obj = krg_parent_children_writelock(tsk, &real_parent_tgid);
+	/*
+	 * After that, obj may still be NULL if real_parent does not
+	 * have a children kddm object.
+	 */
+	/* Won't do anything if obj is NULL. */
+	krg_remove_child(obj, tsk);
+	write_lock_irq(&tasklist_lock);
+	if (tsk->real_parent == baby_sitter) {
+		remove_from_krg_parent(tsk, tsk->task_obj->real_parent);
+		list_add_tail(&tsk->sibling, &baby_sitter->children);
+	}
+	write_unlock_irq(&tasklist_lock);
+	if (obj)
+		krg_children_unlock(obj);
+}
+
+void krg_children_cleanup(struct task_struct *task)
+{
+	struct children_kddm_object *obj = task->parent_children_obj;
+
+	if (obj) {
+		rcu_assign_pointer(task->parent_children_obj, NULL);
+		krg_children_put(obj);
+	}
+}
+
+/**
+ * @author Louis Rilling
+ */
+void epm_children_start(void)
+{
+	unsigned long cache_flags = SLAB_PANIC;
+
+#ifdef CONFIG_DEBUG_SLAB
+	cache_flags |= SLAB_POISON;
+#endif
+	children_obj_cachep = KMEM_CACHE(children_kddm_object, cache_flags);
+	remote_child_cachep = KMEM_CACHE(remote_child,cache_flags);
+	krg_parent_head_cachep = kmem_cache_create("krg_parent_head",
+						   sizeof(struct list_head),
+						   sizeof(void *), cache_flags,
+						   NULL);
+
+	register_io_linker(CHILDREN_LINKER, &children_io_linker);
+
+	children_kddm_set = create_new_kddm_set(kddm_def_ns,CHILDREN_KDDM_ID,
+						CHILDREN_LINKER,
+						KDDM_CUSTOM_DEF_OWNER,
+						0, 0);
+	if (IS_ERR(children_kddm_set))
+		OOM;
+	krg_parent_table = hashtable_new(PROCESS_HASH_TABLE_SIZE);
+	if (!krg_parent_table)
+		OOM;
+}
+
+/**
+ * @author Louis Rilling
+ */
+void epm_children_exit(void)
+{
+	hashtable_free(krg_parent_table);
+}
diff -ruN linux-2.6.29/kerrighed/epm/epm.c android_cluster/linux-2.6.29/kerrighed/epm/epm.c
--- linux-2.6.29/kerrighed/epm/epm.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/epm.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,134 @@
+/*
+ *  kerrighed/epm/epm.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ */
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/hashtable.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/krgsyms.h>
+#include <kerrighed/debug.h>
+#include "epm_internal.h"
+
+struct task_struct *baby_sitter;
+
+static void init_baby_sitter(void)
+{
+	baby_sitter = alloc_task_struct();
+	if (!baby_sitter)
+		OOM;
+
+	memset(baby_sitter, 0, sizeof(*baby_sitter));
+	baby_sitter->pid = -1;
+	baby_sitter->tgid = baby_sitter->pid;
+	baby_sitter->state = TASK_UNINTERRUPTIBLE;
+	INIT_LIST_HEAD(&baby_sitter->children);
+	baby_sitter->real_parent = baby_sitter;
+	baby_sitter->parent = baby_sitter;
+	strncpy(baby_sitter->comm, "baby sitter", 15);
+}
+
+/* Krgsyms to register for restart_blocks in ghost processes */
+extern int compat_krgsyms_register(void);
+extern int hrtimer_krgsyms_register(void);
+extern int posix_cpu_timers_krgsyms_register(void);
+extern int select_krgsyms_register(void);
+extern int futex_krgsyms_register(void);
+extern int compat_krgsyms_unregister(void);
+extern int hrtimer_krgsyms_unregister(void);
+extern int posix_cpu_timers_krgsyms_unregister(void);
+extern int select_krgsyms_unregister(void);
+extern int futex_krgsyms_unregister(void);
+
+static int restart_block_krgsyms_register(void)
+{
+	int retval;
+
+	retval = krgsyms_register(KRGSYMS_DO_NO_RESTART_SYSCALL,
+			do_no_restart_syscall);
+#ifdef CONFIG_COMPAT
+	if (!retval)
+		retval = compat_krgsyms_register();
+#endif
+	if (!retval)
+		retval = hrtimer_krgsyms_register();
+	if (!retval)
+		retval = posix_cpu_timers_krgsyms_register();
+	if (!retval)
+		retval = select_krgsyms_register();
+	if (!retval)
+		retval = futex_krgsyms_register();
+
+	return retval;
+}
+
+static int restart_block_krgsyms_unregister(void)
+{
+	int retval;
+
+	retval = krgsyms_unregister(KRGSYMS_DO_NO_RESTART_SYSCALL);
+#ifdef CONFIG_COMPAT
+	if (!retval)
+		retval = compat_krgsyms_unregister();
+#endif
+	if (!retval)
+		retval = hrtimer_krgsyms_unregister();
+	if (!retval)
+		retval = posix_cpu_timers_krgsyms_unregister();
+	if (!retval)
+		retval = select_krgsyms_unregister();
+	if (!retval)
+		retval = futex_krgsyms_unregister();
+
+	return retval;
+}
+
+int init_epm(void)
+{
+	printk("EPM initialisation: start\n");
+
+	restart_block_krgsyms_register();
+
+	init_baby_sitter();
+
+	epm_signal_start();
+	epm_sighand_start();
+	epm_children_start();
+
+	epm_pidmap_start();
+	epm_pid_start();
+
+	epm_remote_clone_start();
+	register_remote_clone_hooks();
+
+	epm_migration_start();
+
+	register_checkpoint_hooks();
+
+	epm_procfs_start();
+
+	application_cr_server_init();
+
+	epm_hotplug_init();
+
+	printk("EPM initialisation: done\n");
+	return 0;
+}
+
+void cleanup_epm(void)
+{
+	epm_hotplug_cleanup();
+	application_cr_server_finalize();
+	epm_procfs_exit();
+	epm_migration_exit();
+	epm_remote_clone_exit();
+	epm_pid_exit();
+	epm_pidmap_exit();
+	epm_children_exit();
+	epm_sighand_exit();
+	epm_signal_exit();
+	restart_block_krgsyms_unregister();
+}
diff -ruN linux-2.6.29/kerrighed/epm/epm_internal.h android_cluster/linux-2.6.29/kerrighed/epm/epm_internal.h
--- linux-2.6.29/kerrighed/epm/epm_internal.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/epm_internal.h	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,87 @@
+#ifndef __EPM_INTERNAL_H__
+#define __EPM_INTERNAL_H__
+
+#ifdef CONFIG_KRG_EPM
+
+#include <linux/thread_info.h>
+#include <linux/slab.h>
+#include <kerrighed/sys/types.h>
+#include <asm/signal.h>
+
+#define KRG_SIG_MIGRATE		SIGRTMIN
+#define KRG_SIG_CHECKPOINT	(SIGRTMIN + 1)
+#ifdef CONFIG_KRG_FD
+#define KRG_SIG_FORK_DELAY_STOP	(SIGRTMIN + 2)
+#endif
+
+struct task_struct;
+
+/* Used by migration and restart */
+void __krg_children_share(struct task_struct *task);
+void leave_all_relatives(struct task_struct *tsk);
+void join_local_relatives(struct task_struct *tsk);
+
+/* Copy-paste from kernel/fork.c + unstatify task_struct_cachep */
+
+#ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR
+# define alloc_task_struct()	kmem_cache_alloc(task_struct_cachep, GFP_KERNEL)
+# define free_task_struct(tsk)	kmem_cache_free(task_struct_cachep, (tsk))
+extern struct kmem_cache *task_struct_cachep;
+#endif
+
+#ifndef __HAVE_ARCH_THREAD_INFO_ALLOCATOR
+static inline struct thread_info *alloc_thread_info(struct task_struct *tsk)
+{
+#ifdef CONFIG_DEBUG_STACK_USAGE
+	gfp_t mask = GFP_KERNEL | __GFP_ZERO;
+#else
+	gfp_t mask = GFP_KERNEL;
+#endif
+	return (struct thread_info *)__get_free_pages(mask, THREAD_SIZE_ORDER);
+}
+
+static inline void free_thread_info(struct thread_info *ti)
+{
+	free_pages((unsigned long)ti, THREAD_SIZE_ORDER);
+}
+#endif
+
+struct hotplug_context;
+
+int epm_hotplug_init(void);
+void epm_hotplug_cleanup(void);
+
+int epm_signal_start(void);
+void epm_signal_exit(void);
+
+int epm_sighand_start(void);
+void epm_sighand_exit(void);
+
+void epm_children_start(void);
+void epm_children_exit(void);
+
+void epm_pidmap_start(void);
+void epm_pidmap_exit(void);
+int pidmap_map_add(struct hotplug_context *ctx);
+
+void epm_pid_start(void);
+void epm_pid_exit(void);
+
+int epm_procfs_start(void);
+void epm_procfs_exit(void);
+
+void register_remote_clone_hooks(void);
+int epm_remote_clone_start(void);
+void epm_remote_clone_exit(void);
+
+int epm_migration_start(void);
+void epm_migration_exit(void);
+
+void register_checkpoint_hooks(void);
+
+void application_cr_server_init(void);
+void application_cr_server_finalize(void);
+
+#endif /* CONFIG_KRG_EPM */
+
+#endif /* __EPM_INTERNAL_H__ */
diff -ruN linux-2.6.29/kerrighed/epm/ghost.c android_cluster/linux-2.6.29/kerrighed/epm/ghost.c
--- linux-2.6.29/kerrighed/epm/ghost.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/ghost.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,2012 @@
+/*
+ *  kerrighed/epm/ghost.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2006-2007 Pascal Gallard - Kerlabs, Louis Rilling - Kerlabs
+ *
+ *  @author Geoffroy Vallée
+ */
+
+#include <linux/types.h>
+#include <linux/sched.h>
+#include <linux/signal.h>
+#include <linux/pid.h>
+#include <linux/pid_namespace.h>
+#include <linux/fdtable.h>
+#include <linux/thread_info.h>
+#include <linux/delayacct.h>
+#include <linux/nsproxy.h>
+#include <linux/utsname.h>
+#include <linux/iocontext.h>
+#include <linux/ioprio.h>
+#include <linux/list.h>
+#include <linux/rcupdate.h>
+#include <net/net_namespace.h>
+#include <kddm/kddm_info.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/namespace.h>
+#include <kerrighed/krgsyms.h>
+#include <kerrighed/children.h>
+#include <kerrighed/task.h>
+#include <kerrighed/signal.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/application.h>
+#include <kerrighed/app_shared.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/ghost_helpers.h>
+#include <kerrighed/action.h>
+#include <kerrighed/debug.h>
+#include <asm/ptrace.h>
+#include "epm_internal.h"
+
+/* Export */
+
+/* Arch helpers */
+int export_exec_domain(struct epm_action *action,
+		       ghost_t *ghost, struct task_struct *task)
+{
+	if (task_thread_info(task)->exec_domain != &default_exec_domain)
+		return -EPERM;
+
+	return 0;
+}
+
+int export_restart_block(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task)
+{
+	struct thread_info *ti = task_thread_info(task);
+	enum krgsyms_val fn_id;
+	int r;
+
+	fn_id = krgsyms_export(ti->restart_block.fn);
+	if (fn_id == KRGSYMS_UNDEF) {
+		r = -EBUSY;
+		goto out;
+	}
+	r = ghost_write(ghost, &fn_id, sizeof(fn_id));
+	if (r)
+		goto out;
+	r = ghost_write(ghost, &ti->restart_block, sizeof(ti->restart_block));
+
+out:
+	return r;
+}
+
+/* Regular helpers */
+
+/* export_thread_info() is located in <arch>/kerrighed/ghost.c */
+
+/* export_sched() is located in kernel/sched.c */
+
+static int export_preempt_notifiers(struct epm_action *action,
+				   ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+
+#ifdef CONFIG_PREEMPT_NOTIFIERS
+	if (action->type != EPM_REMOTE_CLONE) {
+		if (!hlist_empty(&task->preempt_notifiers))
+			err = -EBUSY;
+	}
+#endif
+
+	return err;
+}
+
+static int export_sched_info(struct epm_action *action,
+			     ghost_t *ghost, struct task_struct *tsk)
+{
+	/* Nothing to do... */
+	return 0;
+}
+
+/* export_mm() is located in kerrighed/mm/mobility.c */
+
+static int export_binfmt(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task)
+{
+	int binfmt_id;
+
+	binfmt_id = krgsyms_export(task->binfmt);
+	if (binfmt_id == KRGSYMS_UNDEF)
+		return -EPERM;
+
+	return ghost_write(ghost, &binfmt_id, sizeof(int));
+}
+
+static int export_children(struct epm_action *action,
+			   ghost_t *ghost, struct task_struct *task)
+{
+	/* Managed by children kddm object */
+	return 0;
+}
+
+static int export_group_leader(struct epm_action *action,
+			       ghost_t *ghost, struct task_struct *task)
+{
+	pid_t tgid = task_tgid_knr(task);
+	int err = 0;
+
+	if (action->type == EPM_CHECKPOINT && !thread_group_leader(task))
+		err = ghost_write(ghost, &tgid, sizeof(tgid));
+
+	return err;
+}
+
+static int export_ptraced(struct epm_action *action,
+			  ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+
+	if (action->type != EPM_REMOTE_CLONE) {
+		/* TODO */
+		if (!list_empty(&task->ptraced))
+			err = -EBUSY;
+	}
+
+	return err;
+}
+
+static int export_bts(struct epm_action *action,
+		      ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+
+#ifdef CONFIG_X86_PTRACE_BTS
+	/* TODO */
+	if (task->bts)
+		err = -EBUSY;
+#endif
+
+	return err;
+}
+
+static int export_pids(struct epm_action *action,
+		       ghost_t *ghost, struct task_struct *task)
+{
+	enum pid_type type, max_type;
+	struct pid_link *link;
+	int retval = 0; /* Prevent gcc from warning */
+
+#ifdef CONFIG_KRG_SCHED
+	retval = export_process_set_links_start(action, ghost, task);
+	if (retval)
+		goto out;
+#endif /* CONFIG_KRG_SCHED */
+
+	if ((action->type == EPM_REMOTE_CLONE
+	     && (action->remote_clone.clone_flags & CLONE_THREAD))
+	    || (action->type != EPM_REMOTE_CLONE && !thread_group_leader(task)))
+		max_type = PIDTYPE_PID + 1;
+	else
+		max_type = PIDTYPE_MAX;
+
+	type = PIDTYPE_PID;
+	if (action->type == EPM_REMOTE_CLONE)
+		type++;
+
+	for (; type < max_type; type++) {
+		if (type == PIDTYPE_PID)
+			link = &task->pids[type];
+		else
+			link = &task->group_leader->pids[type];
+
+		retval = export_pid(action, ghost, link);
+		if (retval)
+			goto err;
+#ifdef CONFIG_KRG_SCHED
+		retval = export_process_set_links(action, ghost,
+						  link->pid, type);
+		if (retval)
+			goto err;
+#endif /* CONFIG_KRG_SCHED */
+	}
+
+out:
+	return retval;
+
+err:
+#ifdef CONFIG_KRG_SCHED
+	export_process_set_links_end(action, ghost, task);
+#endif
+	goto out;
+}
+
+static void post_export_pids(struct epm_action *action,
+			     ghost_t *ghost, struct task_struct *task)
+{
+#ifdef CONFIG_KRG_SCHED
+	export_process_set_links_end(action, ghost, task);
+#endif
+}
+
+/* export_vfork_done() is located in kerrighed/epm/remote_clone.c */
+
+static int export_cpu_timers(struct epm_action *action,
+			     ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+
+	/* TODO */
+	if (action->type != EPM_REMOTE_CLONE) {
+		if (!list_empty(&task->cpu_timers[0])
+		    || !list_empty(&task->cpu_timers[1])
+		    || !list_empty(&task->cpu_timers[2]))
+			err = -EBUSY;
+	}
+
+	return err;
+}
+
+/* export_cred() is located in kerrighed/proc/remote_cred.c */
+
+#ifdef CONFIG_KRG_IPC
+/* export_sysv_sem() is located in kerrighed/ipc/mobility.c */
+#endif /* !CONFIG_KRG_IPC */
+
+/* export_thread_struct() is located in <arch>/kerrighed/ghost.c */
+
+/* export_fs_struct() is located in kerrighed/fs/mobility.c */
+
+/* export_files_struct() is located in kerrighed/fs/mobility.c */
+
+static int export_uts_namespace(struct epm_action *action,
+				ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+
+	if (task->nsproxy->uts_ns != task->nsproxy->krg_ns->root_nsproxy.uts_ns)
+		/* UTS namespace sharing is not implemented yet */
+		err = -EPERM;
+
+	return err;
+}
+
+static int export_net_namespace(struct epm_action *action,
+				ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+
+	if (task->nsproxy->net_ns != task->nsproxy->krg_ns->root_nsproxy.net_ns)
+		/* Net namespace sharing is not implemented yet */
+		err = -EPERM;
+
+	return err;
+}
+
+static int export_nsproxy(struct epm_action *action,
+			  ghost_t *ghost, struct task_struct *task)
+{
+	int retval;
+
+	BUG_ON(!task->nsproxy->krg_ns);
+
+	retval = export_uts_namespace(action, ghost, task);
+	if (retval)
+		goto out;
+	retval = export_ipc_namespace(action, ghost, task);
+	if (retval)
+		goto out;
+	retval = export_mnt_namespace(action, ghost, task);
+	if (retval)
+		goto out;
+	retval = export_pid_namespace(action, ghost, task);
+	if (retval)
+		goto out;
+	retval = export_net_namespace(action, ghost, task);
+
+out:
+	return retval;
+}
+
+/* export_signal_struct() is located in kerrighed/epm/signal.c */
+
+/* export_sighand_struct() is located in kerrighed/epm/sighand.c */
+
+/* export_private_signals() is located in kerrighed/epm/signal.c */
+
+static int export_notifier(struct epm_action *action,
+			   ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+
+	if (action->type != EPM_REMOTE_CLONE) {
+		/* TODO */
+		if (task->notifier)
+			err = -EBUSY;
+	}
+
+	return err;
+}
+
+/* export_audit_context() is located in kernel/auditsc.c */
+
+static int export_exec_ids(struct epm_action *action,
+			   ghost_t *ghost, struct task_struct *task)
+{
+	return 0;
+}
+
+static int export_rt_mutexes(struct epm_action *action,
+			     ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+
+#ifdef CONFIG_RT_MUTEXES
+	if (action->type != EPM_REMOTE_CLONE) {
+		/* TODO */
+		if (!plist_head_empty(&task->pi_waiters) || task->pi_blocked_on)
+			err = -EBUSY;
+	}
+#endif
+
+	return err;
+}
+
+static int export_io_context(struct epm_action *action,
+			     ghost_t *ghost, struct task_struct *task)
+{
+#ifdef CONFIG_BLOCK
+	struct io_context *ioc = task->io_context;
+
+	if (!ioc)
+		return 0;
+
+	return ghost_write(ghost, &ioc->ioprio, sizeof(ioc->ioprio));
+#else
+	return 0;
+#endif
+}
+
+static int export_last_siginfo(struct epm_action *action,
+			       ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+
+	if (action->type != EPM_REMOTE_CLONE) {
+		/* TODO (ptrace) */
+		if (task->last_siginfo)
+			err = -EBUSY;
+	}
+
+	return err;
+}
+
+/* export_cgroups() is located in kernel/cgroup.c */
+
+static int export_pi_state(struct epm_action *action,
+			   ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+
+#ifdef CONFIG_FUTEX
+	if (!list_empty(&task->pi_state_list))
+		err = -EBUSY;
+#endif
+
+	return err;
+}
+
+static int export_mempolicy(struct epm_action *action,
+			    ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+
+#ifdef CONFIG_NUMA
+	if (task->mempolicy)
+		err = -EBUSY;
+#endif
+
+	return err;
+}
+
+static int export_delays(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+
+#ifdef CONFIG_TASK_DELAY_ACCT
+	if (action->type != EPM_REMOTE_CLONE && task->delays)
+		err = ghost_write(ghost, task->delays, sizeof(*task->delays));
+#endif
+
+	return err;
+}
+
+static int export_krg_structs(struct epm_action *action,
+			      ghost_t *ghost, struct task_struct *task)
+{
+	int retval = 0;
+
+	if (action->type == EPM_MIGRATE) {
+		/*
+		 * The task kddm object must be linked to at most one task in
+		 * the cluster, and after import_krg_structs() it will be
+		 * linked to the migrated task.
+		 * Unlink and let import_krg_structs() proceed.
+		 *
+		 * Now that pids are globalized, remote procfs can see that task
+		 * even without a link to its task kddm object.
+		 */
+		krg_task_unlink(task->task_obj, 1);
+		retval = ghost_write(ghost, &retval, sizeof(retval));
+	}
+
+	return retval;
+}
+
+/**
+ *  Export a process task struct.
+ *  @author  Renaud Lottiaux, Geoffroy Vallée
+ *
+ *  @param action	Descriptor of the type of export.
+ *  @param ghost	Ghost where file data should be stored.
+ *  @param task		Task to export file data from.
+ *  @param l_regs	Registers of the task to export.
+ *
+ *  @return		0 if everything ok.
+ *			Negative value otherwise.
+ */
+static int export_task(struct epm_action *action,
+		       ghost_t *ghost,
+		       struct task_struct *task,
+		       struct pt_regs *l_regs)
+{
+	int r;
+
+#define GOTO_ERROR goto ERROR_LABEL
+#define ERROR_LABEL error
+
+	BUG_ON(task->journal_info);
+
+	/* Check against what we cannot manage right now */
+	if ((r = export_preempt_notifiers(action, ghost, task))
+	    || (r = export_ptraced(action, ghost, task))
+	    || (r = export_bts(action, ghost, task))
+	    || (r = export_cpu_timers(action, ghost, task))
+	    || (r = export_notifier(action, ghost, task))
+	    || (r = export_rt_mutexes(action, ghost, task))
+	    || (r = export_last_siginfo(action, ghost, task))
+	    || (r = export_pi_state(action, ghost, task))
+	    || (r = export_mempolicy(action, ghost, task)))
+		GOTO_ERROR;
+
+#ifndef CONFIG_KRG_IPC
+	if (task->sysvsem.undo_list) {
+		r = -EBUSY;
+		GOTO_ERROR;
+	}
+#endif
+
+	/* Export the task struct, and registers */
+	prepare_to_export(task);
+	r = ghost_write(ghost, task, sizeof(*task));
+	if (r)
+		GOTO_ERROR;
+	r = ghost_write(ghost, l_regs, sizeof(*l_regs));
+	if (r)
+		GOTO_ERROR;
+
+	r = export_thread_info(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+	r = export_nsproxy(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+	r = export_pids(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+#undef ERROR_LABEL
+#define ERROR_LABEL error_pids
+
+	r = export_group_leader(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+#ifdef CONFIG_KRG_SCHED
+	r = export_krg_sched_info(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+#endif
+
+	r = export_sched_info(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+#ifdef CONFIG_KRG_MM
+	r = export_mm_struct(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+#endif
+
+	r = export_binfmt(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+	r = export_vfork_done(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+	r = export_cred(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+	r = export_audit_context(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+	r = export_thread_struct(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+#ifdef CONFIG_KRG_DVFS
+	r = export_fs_struct(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+	r = export_files_struct(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+#endif
+
+	r = export_cgroups(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+	r = export_sched(action,ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+	r = export_children(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+	r = export_krg_structs(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+	r = export_kddm_info_struct(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+	r = export_private_signals(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+	r = export_signal_struct(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+	r = export_sighand_struct(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+#ifdef CONFIG_KRG_IPC
+	r = export_sysv_sem(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+#endif
+
+	r = export_delays(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+	r = export_exec_ids(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+	r = export_io_context(action, ghost, task);
+	if (r)
+		GOTO_ERROR;
+
+#undef ERROR_LABEL
+#undef GOTO_ERROR
+
+error:
+	return r;
+
+error_pids:
+	post_export_pids(action, ghost, task);
+	goto error;
+}
+
+static void post_export_task(struct epm_action *action,
+			     ghost_t *ghost,
+			     struct task_struct *task)
+{
+	post_export_pids(action, ghost, task);
+}
+
+int export_process(struct epm_action *action,
+		   ghost_t *ghost,
+		   struct task_struct *task,
+		   struct pt_regs *regs)
+{
+	int r;
+
+	r = export_task(action, ghost, task, regs);
+	if (r)
+		goto error;
+
+	r = export_application(action, ghost, task);
+	if (r)
+		goto error_app;
+
+error:
+	return r;
+
+error_app:
+	post_export_task(action, ghost, task);
+	goto error;
+}
+
+void post_export_process(struct epm_action *action,
+			 ghost_t *ghost,
+			 struct task_struct *task)
+{
+	post_export_task(action, ghost, task);
+}
+
+/* Unimport */
+
+/* Regular helpers */
+
+static void unimport_krg_structs(struct epm_action  *action,
+				 struct task_struct *task)
+{
+	switch (action->type) {
+	case EPM_REMOTE_CLONE:
+	case EPM_CHECKPOINT:
+		__krg_task_free(task);
+		break;
+	default:
+		break;
+	}
+}
+
+static void unimport_delays(struct task_struct *task)
+{
+	delayacct_tsk_free(task);
+}
+
+static void unimport_mempolicy(struct task_struct *task)
+{
+	/* TODO */
+}
+
+static void unimport_pi_state(struct task_struct *task)
+{
+	/* TODO */
+}
+
+/* unimport_cgroups() is located in kernel/cgroup.c */
+
+static void unimport_last_siginfo(struct task_struct *task)
+{
+	/* TODO (ptrace) */
+}
+
+static void unimport_io_context(struct task_struct *task)
+{
+	put_io_context(task->io_context);
+}
+
+static void unimport_rt_mutexes(struct task_struct *task)
+{
+	/* TODO */
+}
+
+static void unimport_exec_ids(struct task_struct *task)
+{
+}
+
+/* unimport_audit_context() is located in kernel/auditsc.c */
+
+static void unimport_notifier(struct task_struct *task)
+{
+	/* TODO */
+}
+
+/* unimport_private_signals() is located in kerrighed/epm/signal.c */
+
+/* unimport_sighand_struct() is located in kerrighed/epm/sighand.c */
+
+/* unimport_signal_struct() is located in kerrighed/epm/signal.c */
+
+static void unimport_nsproxy(struct task_struct *task)
+{
+	put_nsproxy(task->nsproxy);
+}
+
+/* unimport_files_struct() is located in kerrighed/fs/mobility.c */
+
+/* unimport_fs_struct() is located in kerrighed/fs/mobility.c */
+
+/* unimport_thread_struct() is located in <arch>/kerrighed/ghost.c */
+
+/* unimport_sysv_sem() is located in kerrighed/ipc/mobility.c */
+
+/* unimport_cred() is located in kerrighed/proc/remote_cred.c */
+
+static void unimport_cpu_timers(struct task_struct *task)
+{
+	/* TODO */
+}
+
+/* unimport_vfork_done() is located in kerrighed/epm/remote_clone.c */
+
+static void __unimport_pids(struct task_struct *task, enum pid_type max_type)
+{
+	enum pid_type type;
+
+	for (type = 0; type < max_type; type++)
+		unimport_pid(&task->pids[type]);
+}
+
+static void unimport_pids(struct task_struct *task)
+{
+	__unimport_pids(task, task->pid == task->tgid ? PIDTYPE_MAX :
+							PIDTYPE_PID + 1);
+}
+
+static void unimport_bts(struct task_struct *task)
+{
+#ifdef CONFIG_X86_PTRACE_BTS
+	/* TODO */
+#endif
+}
+
+static
+void unimport_ptraced(struct task_struct *task)
+{
+	/* TODO */
+}
+
+static
+void unimport_group_leader(struct task_struct *task)
+{
+	/* Nothing to do... */
+}
+
+static
+void unimport_children(struct epm_action *action, struct task_struct *task)
+{
+	switch (action->type) {
+	case EPM_REMOTE_CLONE:
+	case EPM_CHECKPOINT:
+		__krg_children_writelock(task);
+		krg_children_exit(task);
+		break;
+	default:
+		break;
+	}
+}
+
+static void unimport_binfmt(struct task_struct *task)
+{
+	/* Nothing to do... */
+}
+
+/* unimport_mm() is located in kerrighed/mm/mobility.c */
+
+static void unimport_sched_info(struct task_struct *task)
+{
+	/* Nothing to do... */
+}
+
+static void unimport_preempt_notifiers(struct task_struct *task)
+{
+#ifdef CONFIG_PREEMPT_NOTIFIERS
+	/* TODO */
+#endif
+}
+
+/* unimport_sched() is located in kernel/sched.c */
+
+/* unimport_thread_info() is located in <arch>/kerrighed/ghost.c */
+
+/* No arch helpers */
+
+static void unimport_task(struct epm_action *action,
+			  struct task_struct *ghost_task)
+{
+	unimport_io_context(ghost_task);
+	unimport_exec_ids(ghost_task);
+	unimport_delays(ghost_task);
+#ifdef CONFIG_KRG_IPC
+	unimport_sysv_sem(ghost_task);
+#endif
+	unimport_sighand_struct(ghost_task);
+	unimport_signal_struct(ghost_task);
+	unimport_private_signals(ghost_task);
+	unimport_kddm_info_struct(ghost_task);
+	unimport_krg_structs(action, ghost_task);
+	unimport_children(action, ghost_task);
+	unimport_sched(ghost_task);
+	unimport_cgroups(ghost_task);
+#ifdef CONFIG_KRG_DVFS
+	unimport_files_struct(ghost_task);
+	unimport_fs_struct(ghost_task);
+#endif
+	unimport_thread_struct(ghost_task);
+	unimport_audit_context(ghost_task);
+	unimport_cred(ghost_task);
+	unimport_binfmt(ghost_task);
+#ifdef CONFIG_KRG_MM
+	unimport_mm_struct(ghost_task);
+#endif
+	unimport_sched_info(ghost_task);
+#ifdef CONFIG_KRG_SCHED
+	unimport_krg_sched_info(ghost_task);
+#endif
+	unimport_group_leader(ghost_task);
+	unimport_pids(ghost_task);
+	unimport_nsproxy(ghost_task);
+	unimport_thread_info(ghost_task);
+	free_task_struct(ghost_task);
+
+	/* No-op calls, here for symmetry */
+	unimport_mempolicy(NULL);
+	unimport_pi_state(NULL);
+	unimport_last_siginfo(NULL);
+	unimport_rt_mutexes(NULL);
+	unimport_notifier(NULL);
+	unimport_cpu_timers(NULL);
+	unimport_bts(NULL);
+	unimport_ptraced(NULL);
+	unimport_preempt_notifiers(NULL);
+}
+
+/* TODO unimport_process() */
+
+/* Import */
+
+/* Arch helpers */
+
+struct exec_domain *import_exec_domain(struct epm_action *action,
+				       ghost_t *ghost)
+{
+	return &default_exec_domain;
+}
+
+int import_restart_block(struct epm_action *action,
+			 ghost_t *ghost, struct restart_block *p)
+{
+	enum krgsyms_val fn_id;
+	int r;
+
+	r = ghost_read(ghost, &fn_id, sizeof(fn_id));
+	if (r)
+		goto err_read;
+	r = ghost_read(ghost, p, sizeof(*p));
+	if (r)
+		goto err_read;
+	p->fn = krgsyms_import(fn_id);
+
+err_read:
+	return r;
+}
+
+/* Regular helpers */
+
+/* import_thread_info() is located in <arch>/kerrighed/ghost.c */
+
+/* import_sched() is located in kernel/sched.c */
+
+static int import_preempt_notifiers(struct epm_action *action,
+				    ghost_t *ghost, struct task_struct *task)
+{
+#ifdef CONFIG_PREEMPT_NOTIFIERS
+	/* TODO */
+#endif
+	return 0;
+}
+
+static int import_sched_info(struct epm_action *action,
+			     ghost_t *ghost, struct task_struct *task)
+{
+#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
+	task->sched_info.pcount = 0;
+#endif
+	return 0;
+}
+
+/* import_mm() is located in kerrighed/mm/mobility.c */
+
+static int import_binfmt(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task)
+{
+	int binfmt_id;
+	int err;
+
+	err = ghost_read(ghost, &binfmt_id, sizeof(int));
+	if (err)
+		goto out;
+	task->binfmt = krgsyms_import(binfmt_id);
+out:
+	return err;
+}
+
+static int import_children(struct epm_action *action,
+			   ghost_t *ghost, struct task_struct *task)
+{
+	int r = 0;
+
+	switch (action->type) {
+	case EPM_MIGRATE:
+		task->children_obj = __krg_children_readlock(task);
+		BUG_ON(!task->children_obj);
+		krg_children_unlock(task->children_obj);
+		break;
+
+	case EPM_REMOTE_CLONE:
+	case EPM_CHECKPOINT:
+		/*
+		 * C/R: children are restored later in
+		 * app_restart.c:local_restore_children_objects()
+		 */
+		if (thread_group_leader(task)) {
+			task->children_obj = krg_children_alloc(task);
+		} else {
+			task->children_obj = __krg_children_writelock(task);
+			BUG_ON(!task->children_obj);
+			__krg_children_share(task);
+			krg_children_unlock(task->children_obj);
+		}
+		if (!task->children_obj)
+			r = -ENOMEM;
+		break;
+
+	default:
+		break;
+	}
+
+	return r;
+}
+
+static int import_group_leader(struct epm_action *action,
+			       ghost_t *ghost, struct task_struct *task)
+{
+	struct task_struct *leader = task;
+	pid_t tgid;
+	int err = 0;
+
+	/*
+	 * import_pids() set task->tgid to task->pid for a group leader, and 0
+	 * otherwise.
+	 */
+	if (task->pid != task->tgid) {
+		BUG_ON(action->type != EPM_CHECKPOINT);
+
+		err = ghost_read(ghost, &tgid, sizeof(tgid));
+		if (err)
+			goto out;
+
+		leader = find_task_by_kpid(tgid);
+		BUG_ON(!leader);
+		task->tgid = leader->pid;
+	}
+
+	task->group_leader = leader;
+
+out:
+	return err;
+}
+
+static int import_ptraced(struct epm_action *action,
+			  ghost_t *ghost, struct task_struct *task)
+{
+	/* TODO */
+	return 0;
+}
+
+static int import_bts(struct epm_action *action,
+		      ghost_t *ghost, struct task_struct *task)
+{
+#ifdef CONFIG_X86_PTRACE_BTS
+	/* TODO */
+#endif
+	return 0;
+}
+
+static int import_pids(struct epm_action *action,
+		       ghost_t *ghost, struct task_struct *task)
+{
+	enum pid_type type, max_type;
+	bool leader;
+	int retval = 0;
+
+	leader = !((action->type == EPM_REMOTE_CLONE
+		    && (action->remote_clone.clone_flags & CLONE_THREAD))
+		   || (action->type != EPM_REMOTE_CLONE
+		       && task->pid != task->tgid));
+	if (!leader)
+		max_type = PIDTYPE_PID + 1;
+	else
+		max_type = PIDTYPE_MAX;
+
+	type = PIDTYPE_PID;
+	if (action->type == EPM_REMOTE_CLONE) {
+		struct pid *pid = alloc_pid(task->nsproxy->pid_ns);
+		if (!pid) {
+			retval = -ENOMEM;
+			goto out;
+		} else {
+			task->pids[PIDTYPE_PID].pid = pid;
+		}
+
+		type++;
+	}
+
+	for (; type < max_type; type++) {
+		retval = import_pid(action, ghost, &task->pids[type], type);
+		if (retval) {
+			__unimport_pids(task, type);
+			break;
+		}
+
+#ifdef CONFIG_KRG_SCHED
+		retval = import_process_set_links(action, ghost,
+						  task->pids[type].pid, type);
+		if (retval) {
+			__unimport_pids(task, type + 1);
+			break;
+		}
+#endif /* CONFIG_KRG_SCHED */
+	}
+
+	task->pid = pid_nr(task_pid(task));
+	/*
+	 * Marker for import_group_leader(), and unimport_pids() whenever
+	 * import_group_leader() fails.
+	 */
+	task->tgid = leader ? task->pid : 0;
+
+out:
+	return retval;
+}
+
+/* import_vfork_done() is located in kerrighed/epm/remote_clone.c */
+
+static int import_cpu_timers(struct epm_action *action,
+			     ghost_t *ghost, struct task_struct *task)
+{
+	/* TODO */
+	return 0;
+}
+
+/* import_cred() is located in kerrighed/proc/remote_cred.c */
+
+/* import_sysv_sem() is located in kerrighed/ipc/mobility.c */
+
+/* import_thread_struct() is located in <arch>/kerrighed/ghost.c */
+
+/* import_fs_struct() is located in kerrighed/fs/mobility.c */
+
+/* import_files_struct() is located in kerrighed/fs/mobility.c */
+
+static int import_uts_namespace(struct epm_action *action,
+				ghost_t *ghost, struct task_struct *task)
+{
+	struct uts_namespace *ns = task->nsproxy->krg_ns->root_nsproxy.uts_ns;
+
+	get_uts_ns(ns);
+	task->nsproxy->uts_ns = ns;
+
+	return 0;
+}
+
+static int import_net_namespace(struct epm_action *action,
+				ghost_t *ghost, struct task_struct *task)
+{
+	struct net *ns = task->nsproxy->krg_ns->root_nsproxy.net_ns;
+
+	get_net(ns);
+	task->nsproxy->net_ns = ns;
+
+	return 0;
+}
+
+static int import_nsproxy(struct epm_action *action,
+			  ghost_t *ghost, struct task_struct *task)
+{
+	struct nsproxy *ns;
+	int retval = -ENOMEM;
+
+	ns = kmem_cache_zalloc(nsproxy_cachep, GFP_KERNEL);
+	task->nsproxy = ns;
+	if (!ns)
+		goto out;
+
+	atomic_set(&ns->count, 1);
+
+	ns->krg_ns = find_get_krg_ns();
+
+	retval = import_uts_namespace(action, ghost, task);
+	if (retval)
+		goto err;
+	retval = import_ipc_namespace(action, ghost, task);
+	if (retval)
+		goto err;
+	retval = import_mnt_namespace(action, ghost, task);
+	if (retval)
+		goto err;
+	retval = import_pid_namespace(action, ghost, task);
+	if (retval)
+		goto err;
+	retval = import_net_namespace(action, ghost, task);
+	if (retval)
+		goto err;
+
+out:
+	return retval;
+
+err:
+	if (!ns->net_ns)
+		ns->net_ns = get_net(&init_net);
+	free_nsproxy(ns);
+	goto out;
+}
+
+/* import_signal_struct() is located in kerrighed/epm/signal.c */
+
+/* import_sighand_struct() is located in kerrighed/epm/sighand.c */
+
+/* import_private_signals() is located in kerrighed/epm/signal.c */
+
+static int import_notifier(struct epm_action *action,
+			   ghost_t *ghost, struct task_struct *task)
+{
+	/* TODO */
+	return 0;
+}
+
+/* import_audit_context() is located in kernel/auditsc.c */
+
+static int import_exec_ids(struct epm_action *action,
+			   ghost_t *ghost, struct task_struct *task)
+{
+	if (action->type == EPM_REMOTE_CLONE
+	    && !(action->remote_clone.clone_flags & (CLONE_PARENT|CLONE_THREAD)))
+		task->parent_exec_id = task->self_exec_id;
+	return 0;
+}
+
+static int import_rt_mutexes(struct epm_action *action,
+			     ghost_t *ghost, struct task_struct *task)
+{
+	/* TODO */
+	return 0;
+}
+
+static int import_io_context(struct epm_action *action,
+			     ghost_t *ghost, struct task_struct *task)
+{
+#ifdef CONFIG_BLOCK
+	struct io_context *ioc;
+	unsigned short ioprio;
+	int err;
+
+	if (!task->io_context)
+		return 0;
+
+	err = ghost_read(ghost, &ioprio, sizeof(ioprio));
+	if (err)
+		return err;
+
+	if (!ioprio_valid(ioprio)) {
+		task->io_context = NULL;
+		return 0;
+	}
+
+	ioc = alloc_io_context(GFP_KERNEL, -1);
+	if (!ioc)
+		return -ENOMEM;
+	ioc->ioprio = ioprio;
+
+	task->io_context = ioc;
+#endif
+
+	return 0;
+}
+
+static int import_last_siginfo(struct epm_action *action,
+			       ghost_t *ghost, struct task_struct *task)
+{
+	/* TODO (ptrace) */
+	return 0;
+}
+
+/* import_cgroups() is located in kernel/cgroup.c */
+
+static int import_pi_state(struct epm_action *action,
+			   ghost_t *ghost, struct task_struct *task)
+{
+	/* TODO */
+	return 0;
+}
+
+static int import_mempolicy(struct epm_action *action,
+			    ghost_t *ghost, struct task_struct *task)
+{
+	/* TODO */
+	return 0;
+}
+
+static int import_delays(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+#ifdef CONFIG_TASK_DELAY_ACCT
+	struct task_delay_info *delays;
+
+	if (!task->delays || action->type == EPM_REMOTE_CLONE) {
+		delayacct_tsk_init(task);
+		goto out;
+	}
+
+	delays = kmem_cache_alloc(delayacct_cache, GFP_KERNEL);
+	if (!delays) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	err = ghost_read(ghost, delays, sizeof(*delays));
+	if (err) {
+		kmem_cache_free(delayacct_cache, delays);
+		goto out;
+	}
+	spin_lock_init(&delays->lock);
+
+	task->delays = delays;
+
+out:
+#endif
+
+	return err;
+}
+
+static int import_krg_structs(struct epm_action *action,
+			      ghost_t *ghost, struct task_struct *tsk)
+{
+	struct task_struct *reaper;
+	/* Inits are only needed to prevent compiler warnings. */
+	pid_t parent_pid = 0, real_parent_pid = 0, real_parent_tgid = 0;
+	pid_t group_leader_pid = 0;
+	struct task_kddm_object *obj;
+	int retval = 0, dummy;
+
+	if (action->type == EPM_MIGRATE) {
+		/* Synchronize with export_krg_structs() */
+		retval = ghost_read(ghost, &dummy, sizeof(dummy));
+		if (retval)
+			goto out;
+	}
+
+	/* Initialization of the shared part of the task_struct */
+
+	if (action->type == EPM_REMOTE_CLONE) {
+		if (action->remote_clone.clone_flags & CLONE_THREAD) {
+			struct task_kddm_object *item;
+
+			item = krg_task_readlock(action->remote_clone.from_pid);
+			BUG_ON(!item);
+			parent_pid = item->parent;
+			real_parent_pid = item->real_parent;
+			real_parent_tgid = item->real_parent_tgid;
+			BUG_ON(item->group_leader != action->remote_clone.from_tgid);
+			krg_task_unlock(action->remote_clone.from_pid);
+
+			group_leader_pid = action->remote_clone.from_tgid;
+		} else {
+			parent_pid = action->remote_clone.from_pid;
+			real_parent_pid = action->remote_clone.from_pid;
+			real_parent_tgid = action->remote_clone.from_tgid;
+			group_leader_pid = task_tgid_knr(tsk);
+		}
+	}
+
+	/*
+	 * Not a simple write lock because with REMOTE_CLONE and CHECKPOINT the
+	 * task container object does not exist yet.
+	 */
+	obj = krg_task_create_writelock(task_pid_knr(tsk));
+	BUG_ON(!obj);
+
+	switch (action->type) {
+	case EPM_REMOTE_CLONE:
+		obj->parent = parent_pid;
+		obj->real_parent = real_parent_pid;
+		obj->real_parent_tgid = real_parent_tgid;
+		obj->group_leader = group_leader_pid;
+		break;
+	case EPM_MIGRATE:
+		break;
+	case EPM_CHECKPOINT:
+		/*
+		 * Initialization of the (real) parent pid and real parent
+		 * tgid in case of restart
+		 */
+		/*
+		 * Bringing restarted processes to foreground will need more
+		 * work
+		 */
+		reaper = task_active_pid_ns(tsk)->child_reaper;
+		obj->parent = task_pid_knr(reaper);
+		obj->real_parent = obj->parent;
+		obj->real_parent_tgid = task_tgid_knr(reaper);
+		/*
+		 * obj->group_leader has already been set when creating the
+		 * object. Fix it for non leader threads.
+		 */
+		obj->group_leader = task_tgid_knr(tsk);
+		break;
+	default:
+		BUG();
+	}
+
+	krg_task_unlock(obj->pid);
+
+out:
+	return retval;
+}
+
+/**
+ *  Import a process task struct.
+ *  @author  Renaud Lottiaux, Geoffroy Vallée
+ *
+ *  @param action	Descriptor of the type of import.
+ *  @param ghost	Ghost where file data should be stored.
+ *  @param l_regs	Registers of the task to imported task.
+ *
+ *  @return		The task struct of the imported process.
+ *			Error code otherwise.
+ */
+static struct task_struct *import_task(struct epm_action *action,
+				       ghost_t *ghost,
+				       struct pt_regs *l_regs)
+{
+	struct task_struct *task;
+	int retval;
+
+	/* No-op calls, here for symmetry */
+	BUG_ON(import_preempt_notifiers(action, ghost, NULL)
+	       || import_ptraced(action, ghost, NULL)
+	       || import_bts(action, ghost, NULL)
+	       || import_cpu_timers(action, ghost, NULL)
+	       || import_notifier(action, ghost, NULL)
+	       || import_rt_mutexes(action, ghost, NULL)
+	       || import_last_siginfo(action, ghost, NULL)
+	       || import_pi_state(action, ghost, NULL)
+	       || import_mempolicy(action, ghost, NULL));
+
+	/* Import the task struct, and registers */
+	task = alloc_task_struct();
+	if (!task) {
+		retval = -ENOMEM;
+		goto err_alloc_task;
+	}
+
+	retval = ghost_read(ghost, task, sizeof(struct task_struct));
+	if (retval)
+		goto err_task;
+	retval = ghost_read(ghost, l_regs, sizeof(struct pt_regs));
+	if (retval)
+		goto err_regs;
+
+	/*
+	 * Init fields. Paranoia to avoid dereferencing a pointer which has no
+	 * meaning on this node.
+	 */
+	atomic_set(&task->usage, 2);
+#ifdef CONFIG_PREEMPT_NOTIFIERS
+	INIT_HLIST_HEAD(&task->preempt_notifiers);
+#endif
+	INIT_LIST_HEAD(&task->tasks);
+	INIT_LIST_HEAD(&task->ptraced);
+	INIT_LIST_HEAD(&task->ptrace_entry);
+	task->real_parent = NULL;
+	task->parent = NULL;
+	INIT_LIST_HEAD(&task->children);
+	INIT_LIST_HEAD(&task->sibling);
+	task->group_leader = NULL;
+	INIT_LIST_HEAD(&task->ptraced);
+	INIT_LIST_HEAD(&task->ptrace_entry);
+#ifdef CONFIG_X86_PTRACE_BTS
+	BUG_ON(task->bts);
+	BUG_ON(task->bts_buffer);
+#endif
+	INIT_LIST_HEAD(&task->thread_group);
+	INIT_LIST_HEAD(&task->cpu_timers[0]);
+	INIT_LIST_HEAD(&task->cpu_timers[1]);
+	INIT_LIST_HEAD(&task->cpu_timers[2]);
+	mutex_init(&task->cred_exec_mutex);
+#ifndef CONFIG_KRG_IPC
+	BUG_ON(task->sysvsem.undo_list);
+#endif
+	BUG_ON(task->notifier);
+	BUG_ON(task->notifier_data);
+	task->notifier_mask = NULL;
+	spin_lock_init(&task->alloc_lock);
+#ifdef CONFIG_GENERIC_HARDIRQS
+//	BUG_ON(task->irqaction);
+#endif
+	spin_lock_init(&task->pi_lock);
+#ifdef CONFIG_RT_MUTEXES
+	plist_head_init(&task->pi_waiters, &task->pi_lock);
+	BUG_ON(task->pi_blocked_on);
+#endif
+#ifdef CONFIG_DEBUG_MUTEXES
+	BUG_ON(task->blocked_on); /* not blocked yet */
+#endif
+	/* Almost copy paste from fork.c for lock debugging stuff, to avoid
+	 * fooling this node with traces from the exporting node */
+#ifdef CONFIG_TRACE_IRQFLAGS
+	task->irq_events = 0;
+	task->hardirqs_enabled = 1;
+	task->hardirq_enable_ip = _THIS_IP_;
+	task->hardirq_enable_event = 0;
+	task->hardirq_disable_ip = 0;
+	task->hardirq_disable_event = 0;
+	task->softirqs_enabled = 1;
+	task->softirq_enable_ip = _THIS_IP_;
+	task->softirq_enable_event = 0;
+	task->softirq_disable_ip = 0;
+	task->softirq_disable_event = 0;
+	task->hardirq_context = 0;
+	task->softirq_context = 0;
+#endif
+#ifdef CONFIG_LOCKDEP
+	task->lockdep_depth = 0; /* no locks held yet */
+	task->curr_chain_key = 0;
+	task->lockdep_recursion = 0;
+#endif
+	/* End of lock debugging stuff */
+	if (action->type == EPM_CHECKPOINT)
+		task->journal_info = NULL;
+	else
+		BUG_ON(task->journal_info);
+	BUG_ON(task->bio_list);
+	BUG_ON(task->bio_tail);
+	BUG_ON(task->reclaim_state);
+	if (action->type == EPM_CHECKPOINT)
+		task->backing_dev_info = NULL;
+	else
+		BUG_ON(task->backing_dev_info);
+	BUG_ON(task->last_siginfo);
+#ifdef CONFIG_FUTEX
+	INIT_LIST_HEAD(&task->pi_state_list);
+	task->pi_state_cache = NULL;
+#endif
+#ifdef CONFIG_NUMA
+	BUG_ON(task->mempolicy);
+#endif
+	task->splice_pipe = NULL;
+	BUG_ON(task->scm_work_list);
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+	task->ret_stack = NULL;
+#endif
+	task->task_obj = NULL;
+	rcu_assign_pointer(task->parent_children_obj, NULL);
+	task->children_obj = NULL;
+	task->application = NULL;
+
+	/* Now, let's resume importing the process. */
+
+	retval = import_thread_info(action, ghost, task);
+	if (retval)
+		goto err_thread_info;
+
+	retval = import_nsproxy(action, ghost, task);
+	if (retval)
+		goto err_nsproxy;
+
+	retval = import_pids(action, ghost, task);
+	if (retval)
+		goto err_pids;
+	retval = import_group_leader(action, ghost, task);
+	if (retval)
+		goto err_group_leader;
+
+#ifdef CONFIG_KRG_SCHED
+	retval = import_krg_sched_info(action, ghost, task);
+	if (retval)
+		goto err_krg_sched_info;
+#endif
+
+	retval = import_sched_info(action, ghost, task);
+	if (retval)
+		goto err_sched_info;
+
+#ifdef CONFIG_KRG_MM
+	retval = import_mm_struct(action, ghost, task);
+	if (retval)
+		goto err_mm_struct;
+#endif
+
+	retval = import_binfmt(action, ghost, task);
+	if (retval)
+		goto err_binfmt;
+
+	retval = import_vfork_done(action, ghost, task);
+	if (retval)
+		goto err_vfork_done;
+
+	retval = import_cred(action, ghost, task);
+	if (retval)
+		goto err_cred;
+
+	retval = import_audit_context(action, ghost, task);
+	if (retval)
+		goto err_audit_context;
+
+	retval = import_thread_struct(action, ghost, task);
+	if (retval)
+		goto err_thread_struct;
+
+#ifdef CONFIG_KRG_DVFS
+	retval = import_fs_struct(action, ghost, task);
+	if (retval)
+		goto err_fs_struct;
+	retval = import_files_struct(action, ghost, task);
+	if (retval)
+		goto err_files_struct;
+#endif
+
+	retval = import_cgroups(action, ghost, task);
+	if (retval)
+		goto err_cgroups;
+
+	retval = import_sched(action, ghost, task);
+	if (retval)
+		goto err_sched;
+
+	retval = import_children(action, ghost, task);
+	if (retval)
+		goto err_children;
+	retval = import_krg_structs(action, ghost, task);
+	if (retval)
+		goto err_krg_structs;
+	retval = import_kddm_info_struct(action, ghost, task);
+	if (retval)
+		goto err_kddm_info_struct;
+
+	retval = import_private_signals(action, ghost, task);
+	if (retval)
+		goto err_signals;
+	retval = import_signal_struct(action, ghost, task);
+	if (retval)
+		goto err_signal_struct;
+
+	retval = import_sighand_struct(action, ghost, task);
+	if (retval)
+		goto err_sighand_struct;
+
+#ifdef CONFIG_KRG_IPC
+	retval = import_sysv_sem(action, ghost, task);
+	if (retval)
+		goto err_sysv_sem;
+#endif
+
+	retval = import_delays(action, ghost, task);
+	if (retval)
+		goto err_delays;
+
+	retval = import_exec_ids(action, ghost, task);
+	if (retval)
+		goto err_exec_ids;
+
+	retval = import_io_context(action, ghost, task);
+	if (retval)
+		goto err_io_context;
+
+	return task;
+
+err_io_context:
+	unimport_exec_ids(task);
+err_exec_ids:
+	unimport_delays(task);
+err_delays:
+#ifdef CONFIG_KRG_IPC
+	unimport_sysv_sem(task);
+err_sysv_sem:
+#endif
+	unimport_sighand_struct(task);
+err_sighand_struct:
+	unimport_signal_struct(task);
+err_signal_struct:
+	unimport_private_signals(task);
+err_signals:
+	unimport_kddm_info_struct(task);
+err_kddm_info_struct:
+	unimport_krg_structs(action, task);
+err_krg_structs:
+	unimport_children(action, task);
+err_children:
+	unimport_sched(task);
+err_sched:
+	unimport_cgroups(task);
+err_cgroups:
+#ifdef CONFIG_KRG_DVFS
+	unimport_files_struct(task);
+err_files_struct:
+	unimport_fs_struct(task);
+err_fs_struct:
+#endif
+	unimport_thread_struct(task);
+err_thread_struct:
+	unimport_audit_context(task);
+err_audit_context:
+	unimport_cred(task);
+err_cred:
+	unimport_vfork_done(task);
+err_vfork_done:
+	unimport_binfmt(task);
+err_binfmt:
+#ifdef CONFIG_KRG_MM
+	unimport_mm_struct(task);
+err_mm_struct:
+#endif
+	unimport_sched_info(task);
+err_sched_info:
+#ifdef CONFIG_KRG_SCHED
+	unimport_krg_sched_info(task);
+err_krg_sched_info:
+#endif
+	unimport_group_leader(task);
+err_group_leader:
+	unimport_pids(task);
+err_pids:
+	unimport_nsproxy(task);
+err_nsproxy:
+	unimport_thread_info(task);
+err_thread_info:
+/*	unimport_regs(task); */
+err_regs:
+/*	unimport_task_struct(task); */
+err_task:
+	free_task_struct(task);
+err_alloc_task:
+	/* No-op calls, here for symmetry */
+	unimport_mempolicy(NULL);
+	unimport_pi_state(NULL);
+	unimport_last_siginfo(NULL);
+	unimport_rt_mutexes(NULL);
+	unimport_notifier(NULL);
+	unimport_cpu_timers(NULL);
+	unimport_bts(NULL);
+	unimport_ptraced(NULL);
+	unimport_preempt_notifiers(NULL);
+	return ERR_PTR(retval);
+}
+
+/* Ghost release */
+
+static void free_ghost_task(struct task_struct *task)
+{
+	BUG_ON(!task);
+	BUG_ON(!list_empty(&task->sibling));
+	BUG_ON(!list_empty(&task->ptrace_entry));
+	BUG_ON(!list_empty(&task->thread_group));
+	free_task_struct(task);
+}
+
+void free_ghost_process(struct task_struct *ghost)
+{
+#ifdef CONFIG_KRG_MM
+	free_ghost_mm(ghost);
+#endif
+
+#ifdef CONFIG_KRG_DVFS
+	free_ghost_files(ghost);
+#endif
+
+	free_ghost_audit_context(ghost);
+	free_ghost_cred(ghost);
+
+	free_ghost_cgroups(ghost);
+	put_nsproxy(ghost->nsproxy);
+	kmem_cache_free(kddm_info_cachep, ghost->kddm_info);
+
+	free_ghost_thread_info(ghost);
+
+	free_ghost_task(ghost);
+}
+
+static int register_pids(struct task_struct *task, struct epm_action *action)
+{
+	enum pid_type type;
+
+	for (type = 0; type < PIDTYPE_MAX; type++) {
+		if (!thread_group_leader(task) && type > PIDTYPE_PID)
+			break;
+		if ((type != PIDTYPE_PID || action->type != EPM_REMOTE_CLONE)
+		    && task->pids[type].pid->kddm_obj)
+			krg_end_get_pid(task->pids[type].pid);
+	}
+
+	return 0;
+}
+
+/**
+ * This function creates the new process from the ghost process
+ * @author Geoffroy Vallée, Louis Rilling
+ *
+ * @param tskRecv               Pointer on the ghost process
+ * @param regs			Pointer on the registers of the ghost process
+ * @param action		Migration, checkpoint, creation, etc.
+ *
+ * @return                      Pointer on the running task
+ *                              (created with the ghost process)
+ */
+static
+struct task_struct *create_new_process_from_ghost(struct task_struct *tskRecv,
+						  struct pt_regs *l_regs,
+						  struct epm_action *action)
+{
+	struct pid *pid;
+	struct task_struct *newTsk;
+	struct task_kddm_object *obj;
+	unsigned long flags;
+	unsigned long stack_start;
+	unsigned long stack_size;
+	int *parent_tidptr;
+	int *child_tidptr;
+	struct children_kddm_object *parent_children_obj;
+	pid_t real_parent_tgid;
+	int retval;
+
+	BUG_ON(!l_regs || !tskRecv);
+
+	/*
+	 * The active process must be considered as remote until all links
+	 * with parent and children are restored atomically.
+	 */
+	tskRecv->parent = tskRecv->real_parent = baby_sitter;
+
+	/* Re-attach to the children kddm object of the parent. */
+	if (action->type == EPM_REMOTE_CLONE) {
+		real_parent_tgid = action->remote_clone.from_tgid;
+		/* We need writelock to declare the new child later. */
+		parent_children_obj = krg_children_writelock(real_parent_tgid);
+		BUG_ON(!parent_children_obj);
+		krg_children_get(parent_children_obj);
+		rcu_assign_pointer(tskRecv->parent_children_obj,
+				   parent_children_obj);
+	} else {
+		pid_t parent, real_parent;
+
+		/*
+		 * We must not call krg_parent_children_readlock since we are
+		 * restoring here the data needed for this function to work.
+		 */
+		obj = __krg_task_readlock(tskRecv);
+		real_parent_tgid = obj->real_parent_tgid;
+		__krg_task_unlock(tskRecv);
+
+		parent_children_obj =
+			krg_children_readlock(real_parent_tgid);
+		if (!krg_get_parent(parent_children_obj, tskRecv,
+				    &parent, &real_parent)) {
+			krg_children_get(parent_children_obj);
+			rcu_assign_pointer(tskRecv->parent_children_obj,
+					   parent_children_obj);
+		}
+		if (parent_children_obj)
+			krg_children_unlock(parent_children_obj);
+	}
+
+	flags = (tskRecv->exit_signal & CSIGNAL) | CLONE_VM | CLONE_THREAD
+		| CLONE_SIGHAND;
+	stack_start = user_stack_pointer(l_regs);
+	/*
+	 * Will BUG as soon as used in copy_thread (e.g. ia64, but not i386 and
+	 * x86_64)
+	 */
+	stack_size = 0;
+	parent_tidptr = NULL;
+	child_tidptr = NULL;
+
+	if (action->type == EPM_REMOTE_CLONE) {
+		/* Adjust do_fork parameters */
+
+		/*
+		 * Do not pollute exit signal of the child with bits from
+		 * parent's exit_signal
+		 */
+		flags &= ~CSIGNAL;
+		flags = flags | action->remote_clone.clone_flags;
+		stack_start = action->remote_clone.stack_start;
+		stack_size = action->remote_clone.stack_size;
+		parent_tidptr = action->remote_clone.parent_tidptr;
+		child_tidptr = action->remote_clone.child_tidptr;
+	}
+
+	pid = task_pid(tskRecv);
+	BUG_ON(!pid);
+
+	obj = __krg_task_writelock(tskRecv);
+
+	krg_current = tskRecv;
+	newTsk = copy_process(flags, stack_start, l_regs, stack_size,
+			      child_tidptr, pid, 0);
+	krg_current = NULL;
+
+	if (IS_ERR(newTsk)) {
+		__krg_task_unlock(tskRecv);
+
+		if (action->type == EPM_REMOTE_CLONE)
+			krg_children_unlock(tskRecv->parent_children_obj);
+		krg_children_put(parent_children_obj);
+
+		return newTsk;
+	}
+
+	BUG_ON(newTsk->task_obj);
+	BUG_ON(obj->task);
+	write_lock_irq(&tasklist_lock);
+	newTsk->task_obj = obj;
+	obj->task = newTsk;
+	write_unlock_irq(&tasklist_lock);
+	BUG_ON(newTsk->parent_children_obj != tskRecv->parent_children_obj);
+	BUG_ON(!newTsk->children_obj);
+
+	BUG_ON(newTsk->exit_signal != (flags & CSIGNAL));
+	BUG_ON(action->type == EPM_MIGRATE &&
+	       newTsk->exit_signal != tskRecv->exit_signal);
+
+	if (action->type == EPM_CHECKPOINT)
+		newTsk->exit_signal = tskRecv->exit_signal;
+
+	/* TODO: distributed threads */
+	BUG_ON(newTsk->group_leader->pid != newTsk->tgid);
+	BUG_ON(newTsk->task_obj->group_leader != task_tgid_knr(newTsk));
+
+	if (action->type == EPM_REMOTE_CLONE) {
+		retval = krg_new_child(parent_children_obj,
+				       action->remote_clone.from_pid,
+				       newTsk);
+
+		krg_children_unlock(parent_children_obj);
+		if (retval)
+			PANIC("Remote child %d of %d created"
+			      " but could not be registered!",
+			      task_pid_knr(newTsk),
+			      action->remote_clone.from_pid);
+	}
+
+	if (action->type == EPM_MIGRATE || action->type == EPM_CHECKPOINT)
+		newTsk->did_exec = tskRecv->did_exec;
+
+	__krg_task_unlock(tskRecv);
+
+	retval = register_pids(newTsk, action);
+	BUG_ON(retval);
+
+	if (action->type == EPM_MIGRATE
+	    || action->type == EPM_CHECKPOINT) {
+		/*
+		 * signals should be copied from the ghost, as do_fork does not
+		 * clone the signal queue
+		 */
+		if (!sigisemptyset(&tskRecv->pending.signal)
+		    || !list_empty(&tskRecv->pending.list)) {
+			unsigned long flags;
+
+			if (!lock_task_sighand(newTsk, &flags))
+				BUG();
+			list_splice(&tskRecv->pending.list,
+				    &newTsk->pending.list);
+			sigorsets(&newTsk->pending.signal,
+				  &newTsk->pending.signal,
+				  &tskRecv->pending.signal);
+			unlock_task_sighand(newTsk, &flags);
+
+			init_sigpending(&tskRecv->pending);
+		}
+		/*
+		 * Always set TIF_SIGPENDING, since migration/checkpoint
+		 * interrupted the task as an (ignored) signal. This way
+		 * interrupted syscalls are transparently restarted.
+		 */
+		set_tsk_thread_flag(newTsk, TIF_SIGPENDING);
+	}
+
+	newTsk->files->next_fd = tskRecv->files->next_fd;
+
+	if (action->type == EPM_MIGRATE
+	    || action->type == EPM_CHECKPOINT) {
+		/* Remember process times until now (cleared by do_fork) */
+		newTsk->utime = tskRecv->utime;
+		/* stime will be updated later to account for migration time */
+		newTsk->stime = tskRecv->stime;
+		newTsk->gtime = tskRecv->gtime;
+		newTsk->utimescaled = tskRecv->utimescaled;
+		newTsk->stimescaled = tskRecv->stimescaled;
+		newTsk->prev_utime = tskRecv->prev_utime;
+		newTsk->prev_stime = tskRecv->prev_stime;
+
+		/* Restore flags changed by copy_process() */
+		newTsk->flags = tskRecv->flags;
+	}
+	newTsk->flags &= ~PF_STARTING;
+
+	/*
+	 * Atomically restore links with local relatives and allow relatives
+	 * to consider newTsk as local.
+	 * Until now, newTsk is linked to baby sitter and not linked to any
+	 * child.
+	 */
+	join_local_relatives(newTsk);
+
+#ifdef CONFIG_KRG_SCHED
+	post_import_krg_sched_info(newTsk);
+#endif
+
+	/* Now the process can be made world-wide visible. */
+	krg_set_pid_location(newTsk);
+
+	return newTsk;
+}
+
+struct task_struct *import_process(struct epm_action *action,
+				   ghost_t *ghost)
+{
+	struct task_struct *ghost_task;
+	struct task_struct *active_task;
+	struct pt_regs regs;
+	int err;
+
+	/* Process importation */
+
+	if (action->type == EPM_MIGRATE) {
+		/*
+		 * Ensure that no task struct survives from a previous stay of
+		 * the process on this node.
+		 * This can happen if a process comes back very quickly
+		 * and before the call to do_exit_wo_notify() ending
+		 * the previous migration.
+		 */
+		struct pid *pid;
+
+		rcu_read_lock();
+		pid = find_kpid(action->migrate.pid);
+		if (pid) {
+			get_pid(pid);
+			while (pid_task(pid, PIDTYPE_PID)) {
+				rcu_read_unlock();
+				schedule();
+				rcu_read_lock();
+			}
+			put_pid(pid);
+		}
+		rcu_read_unlock();
+	}
+
+	ghost_task = import_task(action, ghost, &regs);
+	if (IS_ERR(ghost_task)) {
+		err = PTR_ERR(ghost_task);
+		goto err_task;
+	}
+	BUG_ON(!ghost_task);
+
+	active_task = create_new_process_from_ghost(ghost_task, &regs, action);
+	if (IS_ERR(active_task)) {
+		err = PTR_ERR(active_task);
+		goto err_active_task;
+	}
+	BUG_ON(!active_task);
+
+	free_ghost_process(ghost_task);
+
+	err = import_application(action, ghost, active_task);
+	if (err)
+		goto err_application;
+
+	return active_task;
+
+err_application:
+	unimport_application(action, ghost, active_task);
+	goto err_task;
+err_active_task:
+	unimport_task(action, ghost_task);
+err_task:
+	return ERR_PTR(err);
+}
diff -ruN linux-2.6.29/kerrighed/epm/ghost.h android_cluster/linux-2.6.29/kerrighed/epm/ghost.h
--- linux-2.6.29/kerrighed/epm/ghost.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/ghost.h	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,47 @@
+/**
+ *  Definition of process mobility function interface.
+ *  @author Geoffroy Vallée
+ */
+
+#ifndef __EPM_GHOST_H__
+#define __EPM_GHOST_H__
+
+#include <kerrighed/ghost_types.h>
+
+struct task_struct;
+struct pt_regs;
+struct epm_action;
+
+/**
+ *  Export a process into a ghost.
+ *  @author  Geoffroy Vallée
+ *
+ *  @param action	Type of export.
+ *  @param ghost	Ghost to export the task to.
+ *  @param task		Task to export.
+ *  @param regs		Userspace registers of the task.
+ *
+ *  @return		0 if everything ok.
+ *			Negative value otherwise.
+ */
+int export_process(struct epm_action *action,
+		   ghost_t *ghost,
+		   struct task_struct *task,
+		   struct pt_regs *regs);
+void post_export_process(struct epm_action *action,
+			 ghost_t *ghost,
+			 struct task_struct *task);
+
+/**
+ *  Import a process from a ghost.
+ *  @author  Geoffroy Vallée
+ *
+ *  @param action	Type of import.
+ *  @param ghost	Ghost to import the task from.
+
+ *  @return		Pointer to the imported task struct.
+ */
+struct task_struct *import_process(struct epm_action *action,
+				   ghost_t *ghost);
+
+#endif /* __EPM_GHOST_H__ */
diff -ruN linux-2.6.29/kerrighed/epm/hotplug.c android_cluster/linux-2.6.29/kerrighed/epm/hotplug.c
--- linux-2.6.29/kerrighed/epm/hotplug.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/hotplug.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,102 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+
+#include <linux/sched.h>
+#include <linux/nsproxy.h>
+#include <kerrighed/capabilities.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/migration.h>
+
+#include "epm_internal.h"
+
+static int epm_add(struct hotplug_context *ctx)
+{
+	return pidmap_map_add(ctx);
+}
+
+/* migrate all processes that we can migrate */
+static int epm_remove(const krgnodemask_t *vector)
+{
+	struct task_struct *tsk;
+	kerrighed_node_t dest_node = kerrighed_node_id;
+
+	printk("epm_remove...\n");
+
+	/* Here we assume that all nodes of the cluster are not removed */
+	dest_node = krgnode_next_online_in_ring(dest_node);
+	BUG_ON(__krgnode_isset(dest_node, vector));
+
+	read_lock(&tasklist_lock);
+	for_each_process(tsk) {
+		if (!tsk->nsproxy->krg_ns)
+			continue;
+
+		if (cap_raised(tsk->krg_caps.effective, CAP_CAN_MIGRATE)) {
+			/* have to migrate this process */
+			printk("try to migrate %d %s to %d\n",
+			       task_pid_knr(tsk), tsk->comm, dest_node);
+
+			__migrate_linux_threads(tsk, MIGR_LOCAL_PROCESS,
+						dest_node);
+
+			/*
+			 * Here we assume that all nodes of the cluster are not
+			 * removed.
+			 */
+			dest_node = krgnode_next_online_in_ring(dest_node);
+			BUG_ON(__krgnode_isset(dest_node, vector));
+
+			continue;
+		}
+
+		if (cap_raised(tsk->krg_caps.effective, CAP_USE_REMOTE_MEMORY)) {
+			/* have to kill this process */
+			printk("epm_remove: have to kill %d (%s)\n",
+			       task_pid_knr(tsk), tsk->comm);
+			continue;
+		}
+	}
+	read_unlock(&tasklist_lock);
+
+	return 0;
+}
+
+static int epm_notification(struct notifier_block *nb, hotplug_event_t event,
+			    void *data)
+{
+	struct hotplug_context *ctx;
+	struct hotplug_node_set *node_set;
+	int err;
+
+	switch(event){
+	case HOTPLUG_NOTIFY_ADD:
+		ctx = data;
+		err = epm_add(ctx);
+		break;
+	case HOTPLUG_NOTIFY_REMOVE:
+		node_set = data;
+		err = epm_remove(&node_set->v);
+		break;
+	default:
+		err = 0;
+		break;
+	}
+
+	if (err)
+		return notifier_from_errno(err);
+	return NOTIFY_OK;
+}
+
+int epm_hotplug_init(void)
+{
+	register_hotplug_notifier(epm_notification, HOTPLUG_PRIO_EPM);
+	return 0;
+}
+
+void epm_hotplug_cleanup(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/epm/Makefile android_cluster/linux-2.6.29/kerrighed/epm/Makefile
--- linux-2.6.29/kerrighed/epm/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/Makefile	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,15 @@
+#
+# Kerrighed's Enhanced Process Management (EPM)
+#
+
+obj-$(CONFIG_KRG_EPM) := krg_epm.o
+obj-$(CONFIG_KRG_EPM) += application/
+
+krg_epm-y := epm.o \
+	signal.o sighand.o children.o pid.o pidmap.o \
+	action.o ghost.o network_ghost.o \
+	remote_clone.o migration.o checkpoint.o restart.o \
+	hotplug.o \
+	procfs.o
+
+EXTRA_CFLAGS += -Wall -Werror
diff -ruN linux-2.6.29/kerrighed/epm/migration.c android_cluster/linux-2.6.29/kerrighed/epm/migration.c
--- linux-2.6.29/kerrighed/epm/migration.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/migration.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,478 @@
+/*
+ *  kerrighed/epm/migration.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2006-2007 Pascal Gallard - Kerlabs, Louis Rilling - Kerlabs
+ */
+
+/**
+ *  Migration interface.
+ *  @file migration.c
+ *
+ *  Implementation of migration functions.
+ *
+ *  @author Geoffroy Vallée
+ */
+
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/fdtable.h>
+#include <linux/fs_struct.h>
+#include <linux/pid_namespace.h>
+#include <linux/rcupdate.h>
+#include <linux/uaccess.h>
+#include <kerrighed/kerrighed_signal.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/remote_cred.h>
+#include <kerrighed/remote_syscall.h>
+#ifdef CONFIG_KRG_CAP
+#include <kerrighed/capabilities.h>
+#endif
+#ifdef CONFIG_KRG_SYSCALL_EXIT_HOOK
+#include <kerrighed/syscalls.h>
+#endif
+#include <kerrighed/task.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/signal.h>
+#include <kerrighed/action.h>
+#include <kerrighed/migration.h>
+#include <kerrighed/hotplug.h>
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include "remote_clone.h"
+#include "network_ghost.h"
+#include "epm_internal.h"
+
+#if defined(CONFIG_KRG_SCHED) && defined(CONFIG_MODULE_HOOK)
+struct module_hook_desc kmh_migration_start;
+struct module_hook_desc kmh_migration_end;
+struct module_hook_desc kmh_migration_aborted;
+EXPORT_SYMBOL(kmh_migration_start);
+EXPORT_SYMBOL(kmh_migration_end);
+EXPORT_SYMBOL(kmh_migration_aborted);
+#endif
+
+#define si_node(info)	(*(kerrighed_node_t *)&(info)._sifields._pad)
+
+static int migration_implemented(struct task_struct *task)
+{
+	int ret = 0;
+
+	if (!task->sighand->krg_objid || !task->signal->krg_objid
+	    || !task->task_obj || !task->children_obj
+	    || (task->real_parent != baby_sitter
+		&& !is_container_init(task->real_parent)
+		&& !task->parent_children_obj))
+		goto out;
+
+	/*
+	 * Note: currently useless, since CLONE_THREAD implies CLONE_VM, but
+	 * will become useful when CLONE_VM will be supported.
+	 */
+	if (!thread_group_empty(task))
+		goto out;
+
+	task_lock(task);
+
+	/* No kernel thread, no task sharing its VM */
+	if ((task->flags & PF_KTHREAD)
+	    || !task->mm
+	    || atomic_read(&task->mm->mm_ltasks) > 1)
+		goto out_unlock;
+
+	/* No task sharing its signal handlers */
+	/*
+	 * Note: currently useless since CLONE_SIGHAND implies CLONE_VM, but
+	 * will become useful when CLONE_VM will be supported
+	 */
+	if (atomic_read(&task->sighand->count) > 1)
+		goto out_unlock;
+
+	/* No task sharing its file descriptors table */
+	if (!task->files || atomic_read(&task->files->count) > 1)
+		goto out_unlock;
+
+	/* No task sharing its fs_struct */
+//	if (!task->fs || task->fs->users > 1)
+//		goto out_unlock;
+
+	ret = 1;
+out_unlock:
+	task_unlock(task);
+out:
+	return ret;
+}
+
+int __may_migrate(struct task_struct *task)
+{
+	return (pid_alive(task)
+		/* check permissions */
+		&& permissions_ok(task)
+#ifdef CONFIG_KRG_CAP
+		/* check capabilities */
+		&& can_use_krg_cap(task, CAP_CAN_MIGRATE)
+#endif /* CONFIG_KRG_CAP */
+		&& !krg_action_pending(task, EPM_MIGRATE)
+		/* Implementation limitation */
+		&& migration_implemented(task));
+}
+
+int may_migrate(struct task_struct *task)
+{
+	int retval;
+
+	read_lock(&tasklist_lock);
+	retval = __may_migrate(task);
+	read_unlock(&tasklist_lock);
+
+	return retval;
+}
+EXPORT_SYMBOL(may_migrate);
+
+void migration_aborted(struct task_struct *tsk)
+{
+#ifdef CONFIG_KRG_SCHED
+	module_hook_call(&kmh_migration_aborted, (unsigned long) tsk);
+#endif
+	krg_action_stop(tsk, EPM_MIGRATE);
+}
+
+static int do_task_migrate(struct task_struct *tsk, struct pt_regs *regs,
+			   kerrighed_node_t target)
+{
+	struct epm_action migration;
+	struct rpc_desc *desc;
+	pid_t remote_pid;
+
+	BUG_ON(tsk == NULL);
+	BUG_ON(regs == NULL);
+
+	/*
+	 * Check again that we actually are able to migrate tsk
+	 * For instance fork() may have created a thread right after the
+	 * migration request.
+	 */
+#ifdef CONFIG_KRG_CAP
+	if (!can_use_krg_cap(tsk, CAP_CAN_MIGRATE))
+		return -ENOSYS;
+#endif
+	if (!migration_implemented(tsk))
+		return -ENOSYS;
+
+	desc = rpc_begin(RPC_EPM_MIGRATE, target);
+	if (!desc)
+		return -ENOMEM;
+
+	migration.type = EPM_MIGRATE;
+	migration.migrate.pid = task_pid_knr(tsk);
+	migration.migrate.target = target;
+
+	krg_unset_pid_location(tsk);
+
+	__krg_task_writelock(tsk);
+	leave_all_relatives(tsk);
+	__krg_task_unlock(tsk);
+
+	/*
+	 * Prevent the migrated task from removing the sighand_struct and
+	 * signal_struct copies before migration cleanup ends
+	 */
+	krg_sighand_pin(tsk->sighand);
+	krg_signal_pin(tsk->signal);
+	mm_struct_pin(tsk->mm);
+
+	remote_pid = send_task(desc, tsk, regs, &migration);
+
+	if (remote_pid < 0)
+		rpc_cancel(desc);
+	rpc_end(desc, 0);
+
+	if (remote_pid < 0) {
+		struct task_kddm_object *obj;
+
+		mm_struct_unpin(tsk->mm);
+
+		krg_signal_writelock(tsk->signal);
+		krg_signal_unlock(tsk->signal);
+		krg_signal_unpin(tsk->signal);
+
+		krg_sighand_writelock(tsk->sighand->krg_objid);
+		krg_sighand_unlock(tsk->sighand->krg_objid);
+		krg_sighand_unpin(tsk->sighand);
+
+		obj = __krg_task_writelock(tsk);
+		BUG_ON(!obj);
+		write_lock_irq(&tasklist_lock);
+		obj->task = tsk;
+		tsk->task_obj = obj;
+		write_unlock_irq(&tasklist_lock);
+		__krg_task_unlock(tsk);
+
+		join_local_relatives(tsk);
+
+		krg_set_pid_location(tsk);
+	} else {
+		BUG_ON(remote_pid != task_pid_knr(tsk));
+		/* Do not notify a task having done vfork() */
+		cleanup_vfork_done(tsk);
+	}
+
+	return remote_pid > 0 ? 0 : remote_pid;
+}
+
+static void krg_task_migrate(int sig, struct siginfo *info,
+			     struct pt_regs *regs)
+{
+	struct task_struct *tsk = current;
+	int r = 0;
+
+	r = do_task_migrate(tsk, regs, si_node(*info));
+
+	if (!r) {
+#ifdef CONFIG_KRG_SCHED
+		module_hook_call(&kmh_migration_end, 0);
+#endif
+		do_exit_wo_notify(0); /* Won't return */
+	}
+
+	/* Migration failed */
+	migration_aborted(tsk);
+}
+
+/**
+ *  Process migration handler.
+ *  @author Renaud Lottiaux, Geoffroy Vallée
+ */
+static void handle_migrate(struct rpc_desc *desc, void *msg, size_t size)
+{
+	struct epm_action *action = msg;
+	struct task_struct *task;
+
+	task = recv_task(desc, action);
+	if (!task) {
+		rpc_cancel(desc);
+		return;
+	}
+
+#ifdef CONFIG_KRG_SCHED
+	module_hook_call(&kmh_migration_end, (unsigned long)task);
+#endif
+	krg_action_stop(task, EPM_MIGRATE);
+
+	wake_up_new_task(task, CLONE_VM);
+}
+
+/* Expects tasklist_lock locked */
+static int do_migrate_process(struct task_struct *task,
+			      kerrighed_node_t destination_node_id)
+{
+	struct siginfo info;
+	int retval;
+
+	if (!krgnode_online(destination_node_id))
+		return -ENONET;
+
+	if (destination_node_id == kerrighed_node_id)
+		return 0;
+
+	if (!migration_implemented(task)) {
+		printk("do_migrate_process: trying to migrate a thread"
+		       " of a multi-threaded process!\n Aborting...\n");
+		return -ENOSYS;
+	}
+
+	retval = krg_action_start(task, EPM_MIGRATE);
+	if (retval)
+		return retval;
+
+#ifdef CONFIG_KRG_SCHED
+	module_hook_call(&kmh_migration_start, (unsigned long)task);
+#endif
+
+	info.si_errno = 0;
+	info.si_pid = 0;
+	info.si_uid = 0;
+	si_node(info) = destination_node_id;
+
+	retval = send_kerrighed_signal(KRG_SIG_MIGRATE, &info, task);
+	if (retval)
+		migration_aborted(task);
+
+	return retval;
+}
+
+/* Kernel-level API */
+
+int __migrate_linux_threads(struct task_struct *task,
+			    enum migration_scope scope,
+			    kerrighed_node_t dest_node)
+{
+	int r = -EPERM;
+
+	read_lock(&tasklist_lock);
+	if (!__may_migrate(task))
+		goto exit;
+
+	switch (scope) {
+	case MIGR_THREAD:
+		r = do_migrate_process(task, dest_node);
+		break;
+	case MIGR_GLOBAL_PROCESS:
+		/* Until distributed threads are re-enabled, we can do it! */
+#if 0
+		printk("MIGR_GLOBAL_PROCESS: Not implemented\n");
+		r = -ENOSYS;
+		break;
+#endif
+	case MIGR_LOCAL_PROCESS: {
+		struct task_struct *t;
+
+		/*
+		 * TODO: Wait until all threads are able to migrate before
+		 * migrating the first one.
+		 */
+		t = task;
+		do {
+			r = do_migrate_process(t, dest_node);
+			if (r)
+				break;
+		} while ((t = next_thread(t)) != task);
+
+		break;
+	} default:
+		printk("migr_scope: %d\n", scope);
+		BUG();
+	}
+
+exit:
+	read_unlock(&tasklist_lock);
+
+	return r;
+}
+EXPORT_SYMBOL(__migrate_linux_threads);
+
+struct migration_request_msg {
+	pid_t pid;
+	enum migration_scope scope;
+	kerrighed_node_t destination_node_id;
+};
+
+static int handle_migrate_remote_process(struct rpc_desc *desc,
+					 void *_msg, size_t size)
+{
+	struct migration_request_msg msg;
+	struct pid *pid;
+	const struct cred *old_cred;
+	int retval;
+
+	pid = krg_handle_remote_syscall_begin(desc, _msg, size,
+					      &msg, &old_cred);
+	if (IS_ERR(pid)) {
+		retval = PTR_ERR(pid);
+		goto out;
+	}
+	retval = __migrate_linux_threads(pid_task(pid, PIDTYPE_PID), msg.scope,
+					 msg.destination_node_id);
+	krg_handle_remote_syscall_end(pid, old_cred);
+out:
+	return retval;
+}
+
+static int migrate_remote_process(pid_t pid,
+				  enum migration_scope scope,
+				  kerrighed_node_t destination_node_id)
+{
+	struct migration_request_msg msg;
+
+	msg.pid = pid;
+	msg.scope = scope;
+	msg.destination_node_id = destination_node_id;
+
+	return krg_remote_syscall_simple(PROC_REQUEST_MIGRATION, pid,
+					 &msg, sizeof(msg));
+}
+
+int migrate_linux_threads(pid_t pid,
+			  enum migration_scope scope,
+			  kerrighed_node_t dest_node)
+{
+	struct task_struct *task;
+	int r;
+
+	/* Check the destination node */
+	/* Just an optimization to avoid doing a useless remote request */
+	if (!krgnode_online(dest_node))
+		return -ENONET;
+
+	rcu_read_lock();
+	task = find_task_by_vpid(pid);
+
+	if (!task || (task->flags & PF_AWAY)) {
+		rcu_read_unlock();
+		return migrate_remote_process(pid, scope, dest_node);
+	}
+
+	r = __migrate_linux_threads(task, scope, dest_node);
+	rcu_read_unlock();
+
+	return r;
+}
+EXPORT_SYMBOL(migrate_linux_threads);
+
+/* Syscall API */
+
+/**
+ *  System call to migrate a process
+ *  @author Geoffroy Vallée, Pascal Gallard
+ *
+ *  @param tgid		tgid of the process to migrate.
+ *  @param dest_node	Id of the node to migrate the process to.
+ */
+int sys_migrate_process(pid_t tgid, kerrighed_node_t dest_node)
+{
+	if (dest_node < 0 || dest_node >= KERRIGHED_MAX_NODES)
+		return -EINVAL;
+	return migrate_linux_threads(tgid, MIGR_GLOBAL_PROCESS, dest_node);
+}
+
+/**
+ *  System call to migrate a thread.
+ *  @author Geoffroy Vallée
+ *
+ *  @param pid		pid of the thread to migrate.
+ *  @param dest_node	Id of the node to migrate the process to.
+ */
+int sys_migrate_thread(pid_t pid, kerrighed_node_t dest_node)
+{
+	if (dest_node < 0 || dest_node >= KERRIGHED_MAX_NODES)
+		return -EINVAL;
+	return migrate_linux_threads(pid, MIGR_THREAD, dest_node);
+}
+
+#ifdef CONFIG_KRG_SYSCALL_EXIT_HOOK
+void krg_syscall_exit(long syscall_nr)
+{
+	__migrate_linux_threads(current, MIGR_LOCAL_PROCESS,
+				krgnode_next_online_in_ring(kerrighed_node_id));
+}
+#endif
+
+int epm_migration_start(void)
+{
+	krg_handler[KRG_SIG_MIGRATE] = krg_task_migrate;
+	if (rpc_register_void(RPC_EPM_MIGRATE, handle_migrate, 0))
+		BUG();
+	if (rpc_register_int(PROC_REQUEST_MIGRATION,
+			     handle_migrate_remote_process, 0))
+		BUG();
+
+	return 0;
+}
+
+void epm_migration_exit(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/epm/migration.h android_cluster/linux-2.6.29/kerrighed/epm/migration.h
--- linux-2.6.29/kerrighed/epm/migration.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/migration.h	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,10 @@
+#ifndef __MIGRATION_H__
+#define __MIGRATION_H__
+
+#include <linux/types.h>
+#include <kerrighed/sys/types.h>
+
+int sys_migrate_process(pid_t tgid, kerrighed_node_t dest_node);
+int sys_migrate_thread(pid_t pid, kerrighed_node_t dest_node);
+
+#endif /* __MIGRATION_H__ */
diff -ruN linux-2.6.29/kerrighed/epm/network_ghost.c android_cluster/linux-2.6.29/kerrighed/epm/network_ghost.c
--- linux-2.6.29/kerrighed/epm/network_ghost.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/network_ghost.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,82 @@
+/*
+ *  kerrighed/epm/network_ghost.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2006-2007 Pascal Gallard - Kerlabs, Louis Rilling - Kerlabs
+ */
+
+#include <linux/sched.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/action.h>
+#include <kerrighed/ghost.h>
+#include <net/krgrpc/rpc.h>
+#include "ghost.h"
+
+pid_t send_task(struct rpc_desc *desc,
+		struct task_struct *tsk,
+		struct pt_regs *task_regs,
+		struct epm_action *action)
+{
+	pid_t pid_remote_task = -1;
+	ghost_t *ghost;
+	int err;
+
+	ghost = create_network_ghost(GHOST_WRITE | GHOST_READ, desc);
+	if (IS_ERR(ghost)) {
+		err = PTR_ERR(ghost);
+		goto out;
+	}
+
+	err = rpc_pack_type(desc, *action);
+	if (err)
+		goto out_close;
+
+	err = export_process(action, ghost, tsk, task_regs);
+	if (err)
+		goto out_close;
+
+	err = rpc_unpack_type(desc, pid_remote_task);
+	post_export_process(action, ghost, tsk);
+	if (err) {
+		if (err == RPC_EPIPE)
+			err = -EPIPE;
+		BUG_ON(err > 0);
+	}
+
+out_close:
+	ghost_close(ghost);
+
+out:
+	return err ? err : pid_remote_task;
+}
+
+struct task_struct *recv_task(struct rpc_desc *desc, struct epm_action *action)
+{
+	struct task_struct *new_tsk;
+	ghost_t *ghost;
+	pid_t pid;
+	int err;
+
+	ghost = create_network_ghost(GHOST_READ | GHOST_WRITE, desc);
+	if (IS_ERR(ghost))
+		goto err_ghost;
+
+	new_tsk = import_process(action, ghost);
+	if (IS_ERR(new_tsk))
+		goto err_close;
+
+	pid = task_pid_knr(new_tsk);
+	err = rpc_pack_type(desc, pid);
+	if (err)
+		goto err_close;
+
+	ghost_close(ghost);
+
+	return new_tsk;
+
+err_close:
+	ghost_close(ghost);
+err_ghost:
+	/* TODO: send a custom error code */
+	return NULL;
+}
diff -ruN linux-2.6.29/kerrighed/epm/network_ghost.h android_cluster/linux-2.6.29/kerrighed/epm/network_ghost.h
--- linux-2.6.29/kerrighed/epm/network_ghost.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/network_ghost.h	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,15 @@
+#ifndef __EPM_NETWORK_GHOST_H__
+#define __EPM_NETWORK_GHOST_H__
+
+struct rpc_desc;
+struct task_struct;
+struct pt_regs;
+struct epm_action;
+
+pid_t send_task(struct rpc_desc *desc,
+		struct task_struct *tsk,
+		struct pt_regs *task_regs,
+		struct epm_action *action);
+struct task_struct *recv_task(struct rpc_desc *desc, struct epm_action *action);
+
+#endif /* __EPM_NETWORK_GHOST_H__ */
diff -ruN linux-2.6.29/kerrighed/epm/pid.c android_cluster/linux-2.6.29/kerrighed/epm/pid.c
--- linux-2.6.29/kerrighed/epm/pid.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/pid.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,787 @@
+/*
+ *  kerrighed/epm/pid.c
+ *
+ *  Copyright (C) 2006-2007 Pascal Gallard - Kerlabs, Louis Rilling - Kerlabs
+ */
+
+#include <linux/types.h>
+#include <linux/pid.h>
+#include <linux/pid_namespace.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/lockdep.h>
+#include <linux/rcupdate.h>
+#include <linux/workqueue.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/task.h>
+#include <kerrighed/workqueue.h>
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/hotplug.h>
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/ghost_helpers.h>
+#include <kerrighed/action.h>
+#include <kerrighed/application.h>
+#include <kerrighed/libproc.h>
+
+#include <linux/delay.h>
+
+#include "pid.h"
+
+struct pid_kddm_object {
+	struct list_head wq;
+	struct rcu_head rcu;
+	struct pid *pid;
+	int attach_pending;
+	int active;
+	struct task_kddm_object *task_obj;
+	/* This is the only field shared through kddm. */
+	int node_count;
+};
+
+static struct kmem_cache *pid_kddm_obj_cachep;
+static DEFINE_SPINLOCK(pid_kddm_lock);
+static struct kddm_set *pid_kddm_set;
+
+static LIST_HEAD(put_pid_wq_head);
+static DEFINE_SPINLOCK(put_pid_wq_lock);
+static struct work_struct put_pid_work;
+
+/*
+ * @author Pascal Gallard
+ */
+static int pid_alloc_object(struct kddm_obj *obj_entry,
+			    struct kddm_set *set, objid_t objid)
+{
+	struct pid_kddm_object *p;
+
+	p = kmem_cache_alloc(pid_kddm_obj_cachep, GFP_KERNEL);
+	if (!p)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&p->wq);
+	p->pid = NULL;
+	p->attach_pending = 0;
+	p->active = 0;
+	p->task_obj = NULL;
+	obj_entry->object = p;
+
+	return 0;
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int pid_first_touch(struct kddm_obj *obj_entry,
+			   struct kddm_set *set, objid_t objid, int flags)
+{
+	struct pid_kddm_object *obj;
+	int r;
+
+	BUG_ON(obj_entry->object);
+
+	r = pid_alloc_object(obj_entry, set, objid);
+	if (r)
+		return r;
+
+	obj = obj_entry->object;
+	/* Can be false in case of restart */
+	/* BUG_ON(ORIG_NODE(objid) != kerrighed_node_id); */
+	obj->node_count = 1; /* For the node having created the PID */
+	return 0;
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int pid_import_object(struct rpc_desc *desc,
+			     struct kddm_set *set,
+			     struct kddm_obj *obj_entry,
+			     objid_t objid,
+			     int flags)
+{
+	struct pid_kddm_object *obj = obj_entry->object;
+
+	return rpc_unpack_type(desc, obj->node_count);
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int pid_export_object(struct rpc_desc *desc,
+			     struct kddm_set *set,
+			     struct kddm_obj *obj_entry,
+			     objid_t objid,
+			     int flags)
+{
+	struct pid_kddm_object *obj = obj_entry->object;
+
+	return rpc_pack_type(desc, obj->node_count);
+}
+
+static void delayed_pid_free(struct rcu_head *rhp)
+{
+	struct pid_kddm_object *obj =
+		container_of(rhp, struct pid_kddm_object, rcu);
+
+	kmem_cache_free(pid_kddm_obj_cachep, obj);
+}
+
+static int pid_remove_object(void *object,
+			     struct kddm_set *set, objid_t objid)
+{
+	struct pid_kddm_object *obj = object;
+	struct pid *pid = obj->pid;
+
+	spin_lock(&pid_kddm_lock);
+	pid->kddm_obj = NULL;
+	obj->pid = NULL;
+	spin_unlock(&pid_kddm_lock);
+	free_pid(pid);
+
+	rcu_read_lock();
+	krg_pid_unlink_task(obj);
+	rcu_read_unlock();
+
+	call_rcu(&obj->rcu, delayed_pid_free);
+
+	return 0;
+}
+
+static struct iolinker_struct pid_io_linker = {
+	.first_touch   = pid_first_touch,
+	.linker_name   = "pid ",
+	.linker_id     = PID_LINKER,
+	.alloc_object  = pid_alloc_object,
+	.export_object = pid_export_object,
+	.import_object = pid_import_object,
+	.remove_object = pid_remove_object,
+	.default_owner = global_pid_default_owner,
+};
+
+static void __get_pid(struct pid_kddm_object *obj)
+{
+	obj->attach_pending++;
+	if (!obj->active) {
+		obj->node_count++;
+		obj->active = 1;
+	}
+}
+
+static struct pid *no_pid(int nr)
+{
+	struct pid_namespace *ns;
+	struct pid_kddm_object *obj;
+	struct pid *pid;
+
+	obj = _kddm_grab_object_no_ft(pid_kddm_set, nr);
+	if (IS_ERR(obj))
+		return NULL;
+	BUG_ON(!obj);
+
+	spin_lock(&pid_kddm_lock);
+	rcu_read_lock();
+	pid = find_kpid(nr); /* Double check once locked */
+	rcu_read_unlock();
+	/*
+	 * No need to get a reference on pid since we know that it is used on
+	 * another node: nobody will free it for the moment.
+	 */
+
+	if (!pid) {
+		ns = find_get_krg_pid_ns();
+		pid = __alloc_pid(ns, &nr);
+		put_pid_ns(ns);
+		if (!pid)
+			goto out_unlock;
+		obj->pid = pid;
+		pid->kddm_obj = obj;
+	}
+	BUG_ON(pid->kddm_obj != obj);
+
+	__get_pid(obj);
+
+out_unlock:
+	spin_unlock(&pid_kddm_lock);
+	_kddm_put_object(pid_kddm_set, nr);
+
+	return pid;
+}
+
+struct pid *krg_get_pid(int nr)
+{
+	struct pid_kddm_object *obj;
+	struct pid *pid;
+
+	rcu_read_lock();
+	pid = find_kpid(nr);
+	rcu_read_unlock();
+	/*
+	 * No need to get a reference on pid since we know that it is used on
+	 * another node: nobody will free it for the moment.
+	 */
+
+	if (!pid)
+		return no_pid(nr);
+
+	spin_lock(&pid_kddm_lock);
+	obj = pid->kddm_obj;
+	BUG_ON(!obj);
+	BUG_ON(obj->pid != pid);
+
+	if (likely(obj->active)) {
+		obj->attach_pending++;
+		spin_unlock(&pid_kddm_lock);
+		return pid;
+	}
+	/* Slow path: we must grab the kddm object. */
+	spin_unlock(&pid_kddm_lock);
+
+	obj = _kddm_grab_object_no_ft(pid_kddm_set, nr);
+	if (IS_ERR(obj))
+		return NULL;
+	BUG_ON(obj != pid->kddm_obj);
+	BUG_ON(obj->pid != pid);
+
+	spin_lock(&pid_kddm_lock);
+	__get_pid(obj);
+	spin_unlock(&pid_kddm_lock);
+
+	_kddm_put_object(pid_kddm_set, nr);
+
+	return pid;
+}
+
+void krg_end_get_pid(struct pid *pid)
+{
+	struct pid_kddm_object *obj = pid->kddm_obj;
+
+	spin_lock(&pid_kddm_lock);
+	BUG_ON(!obj);
+	obj->attach_pending--;
+	BUG_ON(obj->attach_pending < 0);
+	spin_unlock(&pid_kddm_lock);
+}
+
+static int may_put_pid(struct pid_kddm_object *obj)
+{
+	struct pid *pid = obj->pid;
+	int tmp;
+
+	if (obj->attach_pending || !obj->active)
+		return 0;
+	/* Check if this PID is used by a task struct on this node */
+	for (tmp = PIDTYPE_MAX; --tmp >= 0; )
+		if (!hlist_empty(&pid->tasks[tmp]))
+			return 0;
+
+	return 1;
+}
+
+static void __put_pid(struct pid_kddm_object *obj)
+{
+	struct pid *pid = obj->pid;
+	int nr = pid_knr(pid);
+	int may_put;
+	int grabbed = 0;
+
+	/* Try to avoid grabing the kddm object */
+	read_lock(&tasklist_lock);
+	spin_lock(&pid_kddm_lock);
+	may_put = may_put_pid(obj);
+	spin_unlock(&pid_kddm_lock);
+	if (!may_put)
+		goto release_work;
+	read_unlock(&tasklist_lock);
+
+	/* The pid seems to be unused locally. Have to check globally. */
+	/* Prevent pidmaps from changing host nodes. */
+	pidmap_map_read_lock();
+	fkddm_grab_object(kddm_def_ns, PID_KDDM_ID, nr,
+			  KDDM_NO_FT_REQ | KDDM_DONT_KILL);
+	grabbed = 1;
+
+	read_lock(&tasklist_lock);
+
+	spin_lock(&pid_kddm_lock);
+	may_put = may_put_pid(obj);
+	if (may_put) {
+		obj->active = 0;
+		obj->node_count--;
+		if (obj->node_count)
+			/* Still used elsewhere */
+			may_put = 0;
+	}
+	spin_unlock(&pid_kddm_lock);
+
+release_work:
+	spin_lock(&put_pid_wq_lock);
+	list_del_init(&obj->wq);
+	spin_unlock(&put_pid_wq_lock);
+
+	read_unlock(&tasklist_lock);
+
+	if (may_put) {
+		_kddm_remove_frozen_object(pid_kddm_set, nr);
+		pidmap_map_read_unlock();
+	} else if (grabbed) {
+		_kddm_put_object(pid_kddm_set, nr);
+		pidmap_map_read_unlock();
+	}
+}
+
+/* This worker cleans all PID related data on this node. */
+static void put_pid_worker(struct work_struct *work)
+{
+	LIST_HEAD(work_list);
+	struct pid_kddm_object *obj, *n;
+
+	/* Remove the current job-list */
+	spin_lock(&put_pid_wq_lock);
+	list_splice_init(&put_pid_wq_head, &work_list);
+	spin_unlock(&put_pid_wq_lock);
+
+	list_for_each_entry_safe(obj, n, &work_list, wq)
+		__put_pid(obj);
+}
+
+/*
+ * Only place where IRQs may be disabled. This induces lockdep to believe that
+ * deadlocks can occur whenever an IRQ handler takes pid_kddm_lock or
+ * put_pid_wq_lock, but we know that no IRQ handler can do this.
+ */
+void krg_put_pid(struct pid *pid)
+{
+	struct pid_kddm_object *obj;
+
+	lockdep_off();
+	spin_lock(&pid_kddm_lock);
+	obj = pid->kddm_obj;
+	spin_lock(&put_pid_wq_lock);
+	lockdep_on();
+
+	if (obj && obj->active && list_empty(&obj->wq)) {
+		BUG_ON(obj->pid != pid);
+		list_add_tail(&obj->wq, &put_pid_wq_head);
+		queue_work(krg_wq, &put_pid_work);
+	}
+
+	lockdep_off();
+	spin_unlock(&put_pid_wq_lock);
+	spin_unlock(&pid_kddm_lock);
+	lockdep_on();
+
+	if (!obj)
+		free_pid(pid);
+}
+
+static int create_pid_kddm_object(struct pid *pid, int early)
+{
+	int nr = pid_knr(pid);
+	struct pid_kddm_object *obj;
+	struct task_kddm_object *task_obj;
+
+	obj = _kddm_grab_object(pid_kddm_set, nr);
+	if (IS_ERR(obj)) {
+		_kddm_put_object(pid_kddm_set, nr);
+		return PTR_ERR(obj);
+	}
+	BUG_ON(!obj);
+	task_obj = krg_task_readlock(nr);
+
+	spin_lock(&pid_kddm_lock);
+	BUG_ON(early && pid->kddm_obj);
+	if (!pid->kddm_obj) {
+		obj->pid = pid;
+		obj->active = 1;
+		if (early)
+			obj->attach_pending = 1;
+		BUG_ON(obj->task_obj);
+		if (task_obj) {
+			BUG_ON(task_obj->pid_obj);
+			/*
+			 * These rcu_assign_pointer are not really needed,
+			 * but are cleaner :)
+			 */
+			rcu_assign_pointer(obj->task_obj, task_obj);
+			rcu_assign_pointer(obj->task_obj->pid_obj, obj);
+		}
+		pid->kddm_obj = obj;
+	}
+	BUG_ON(pid->kddm_obj != obj);
+	spin_unlock(&pid_kddm_lock);
+
+	krg_task_unlock(nr);
+	_kddm_put_object(pid_kddm_set, nr);
+
+	return 0;
+}
+
+int cr_create_pid_kddm_object(struct pid *pid)
+{
+	return create_pid_kddm_object(pid, 0);
+}
+
+int export_pid(struct epm_action *action,
+	       ghost_t *ghost, struct pid_link *link)
+{
+	struct pid *pid = link->pid;
+	int nr = pid_knr(pid);
+	int retval;
+
+	if (!(nr & GLOBAL_PID_MASK))
+		return -EPERM;
+
+	if (ORIG_NODE(nr) == kerrighed_node_id && !pid->kddm_obj
+	    && action->type != EPM_CHECKPOINT) {
+		retval = create_pid_kddm_object(pid, 0);
+		if (retval)
+			return retval;
+	}
+	return ghost_write(ghost, &nr, sizeof(nr));
+}
+
+int export_pid_namespace(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task)
+{
+	if (!is_krg_pid_ns_root(task_active_pid_ns(task))) {
+		PANIC("Cannot export processes"
+		      " using a non default PID namespace!\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/*--------------------------------------------------------------------------*
+ *--------------------------------------------------------------------------*/
+
+static int __reserve_pid(pid_t nr)
+{
+	kerrighed_node_t orig_node = ORIG_NODE(nr);
+	struct pid_namespace *pid_ns = find_get_krg_pid_ns();
+	struct pid_namespace *pidmap_ns;
+	struct pid *pid;
+	int r;
+
+	if (orig_node == kerrighed_node_id)
+		pidmap_ns = pid_ns;
+	else
+		pidmap_ns = node_pidmap(orig_node);
+	BUG_ON(!pidmap_ns);
+
+	r = reserve_pidmap(pidmap_ns, nr);
+	if (r) {
+		r = -E_CR_PIDBUSY;
+		goto out;
+	}
+
+	pid = __alloc_pid(pid_ns, &nr);
+	if (!pid) {
+		struct upid upid = {
+			.nr = nr,
+			.ns = pidmap_ns,
+		};
+
+		__free_pidmap(&upid);
+
+		r = -ENOMEM;
+		goto out;
+	}
+
+	/*
+	 * this is not always mandatory but really difficult
+	 * to know when it is or not
+	 */
+	r = create_pid_kddm_object(pid, 1);
+	if (r)
+		goto error;
+
+out:
+	put_pid_ns(pid_ns);
+	return r;
+
+error:
+	free_pid(pid);
+	goto out;
+}
+
+struct pid_reservation_msg {
+	kerrighed_node_t requester;
+	pid_t pid;
+};
+
+static int handle_reserve_pid(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	struct pid_reservation_msg *msg = _msg;
+	int r = __reserve_pid(msg->pid);
+	return r;
+}
+
+int reserve_pid(pid_t pid)
+{
+	int r;
+	kerrighed_node_t orig_node = ORIG_NODE(pid);
+	kerrighed_node_t host_node;
+	struct pid_reservation_msg msg;
+
+	msg.requester = kerrighed_node_id;
+	msg.pid = pid;
+
+	r = pidmap_map_read_lock();
+	if (r)
+		goto out;
+
+	host_node = pidmap_node(orig_node);
+	if (host_node == KERRIGHED_NODE_ID_NONE) {
+		pidmap_map_read_unlock();
+
+		r = pidmap_map_alloc(orig_node);
+		if (r)
+			goto out;
+
+		pidmap_map_read_lock();
+
+		host_node = pidmap_node(orig_node);
+		BUG_ON(host_node == KERRIGHED_NODE_ID_NONE);
+	}
+
+	r = rpc_sync(PROC_RESERVE_PID, host_node, &msg, sizeof(msg));
+
+	pidmap_map_read_unlock();
+
+out:
+	if (r)
+		ckpt_err(NULL, r, "Fail to reserve pid %d", pid);
+
+	return r;
+}
+
+static void __end_pid_reservation(int nr)
+{
+	struct pid *pid;
+
+	rcu_read_lock();
+	pid = find_kpid(nr);
+	BUG_ON(!pid);
+
+	krg_end_get_pid(pid);
+	krg_put_pid(pid);
+	rcu_read_unlock();
+}
+
+static int handle_end_pid_reservation(struct rpc_desc *desc, void *_msg,
+				      size_t size)
+{
+	struct pid_reservation_msg *msg = _msg;
+	__end_pid_reservation(msg->pid);
+	return 0;
+}
+
+int end_pid_reservation(pid_t pid)
+{
+	int r;
+	kerrighed_node_t host_node;
+	struct pid_reservation_msg msg;
+
+	msg.requester = kerrighed_node_id;
+	msg.pid = pid;
+
+	r = pidmap_map_read_lock();
+	if (r)
+		return r;
+
+	host_node = pidmap_node(ORIG_NODE(pid));
+	BUG_ON(host_node == KERRIGHED_NODE_ID_NONE);
+
+	r = rpc_sync(PROC_END_PID_RESERVATION, host_node, &msg, sizeof(msg));
+
+	pidmap_map_read_unlock();
+
+	return r;
+}
+
+/*--------------------------------------------------------------------------*
+ *--------------------------------------------------------------------------*/
+
+int import_pid(struct epm_action *action, ghost_t *ghost, struct pid_link *link,
+	       enum pid_type type)
+{
+	struct pid *pid;
+	int nr;
+	int retval;
+
+	retval = ghost_read(ghost, &nr, sizeof(nr));
+	if (retval)
+		return retval;
+
+	if (action->type == EPM_CHECKPOINT) {
+		if ((action->restart.flags & APP_REPLACE_PGRP)
+		    && type == PIDTYPE_PGID)
+			nr = action->restart.app->restart.substitution_pgrp;
+		else if ((action->restart.flags & APP_REPLACE_SID)
+			 && type == PIDTYPE_SID)
+			nr = action->restart.app->restart.substitution_sid;
+	}
+
+	pid = krg_get_pid(nr);
+	if (!pid)
+		return -ENOMEM;
+	INIT_HLIST_NODE(&link->node);
+	link->pid = pid;
+
+	return 0;
+}
+
+int import_pid_namespace(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *task)
+{
+	task->nsproxy->pid_ns = find_get_krg_pid_ns();
+
+	return 0;
+}
+
+void unimport_pid(struct pid_link *link)
+{
+	struct pid *pid = link->pid;
+
+	if (pid->kddm_obj)
+		krg_end_get_pid(pid);
+	krg_put_pid(pid);
+}
+
+/* Must be called under rcu_read_lock() */
+struct task_kddm_object *krg_pid_task(struct pid *pid)
+{
+	struct pid_kddm_object *obj;
+
+	obj = rcu_dereference(pid->kddm_obj);
+	if (obj)
+		return rcu_dereference(obj->task_obj);
+	return NULL;
+}
+
+/* Must be called under rcu_read_lock() */
+void krg_pid_unlink_task(struct pid_kddm_object *obj)
+{
+	struct task_kddm_object *task_obj;
+
+	if (obj) {
+		task_obj = rcu_dereference(obj->task_obj);
+		if (task_obj) {
+			rcu_assign_pointer(task_obj->pid_obj, NULL);
+			rcu_assign_pointer(obj->task_obj, NULL);
+		}
+	}
+}
+
+/*--------------------------------------------------------------------------*
+ *--------------------------------------------------------------------------*/
+
+struct pid_link_task_msg {
+	kerrighed_node_t requester;
+	pid_t pid;
+};
+
+int krg_pid_link_task(pid_t pid)
+{
+	struct pid_link_task_msg msg;
+	kerrighed_node_t host_node;
+	int r;
+
+	msg.requester = kerrighed_node_id;
+	msg.pid = pid;
+
+	r = pidmap_map_read_lock();
+	if (r)
+		return r;
+
+	host_node = pidmap_node(ORIG_NODE(pid));
+	BUG_ON(host_node == KERRIGHED_NODE_ID_NONE);
+
+	r = rpc_sync(PROC_PID_LINK_TASK, host_node, &msg, sizeof(msg));
+
+	pidmap_map_read_unlock();
+
+	return r;
+}
+
+static void __pid_link_task(struct pid *pid, struct task_kddm_object *task_obj)
+{
+	if (task_obj && pid && pid->kddm_obj) {
+		rcu_assign_pointer(pid->kddm_obj->task_obj, task_obj);
+		rcu_assign_pointer(task_obj->pid_obj, pid->kddm_obj);
+	}
+}
+
+int __krg_pid_link_task(pid_t nr)
+{
+	struct pid *pid;
+	struct task_kddm_object *task_obj;
+	int r = 0;
+
+	pid = krg_get_pid(nr);
+	if (!pid) {
+		r = -ENOMEM;
+		goto out;
+	}
+	task_obj = krg_task_readlock(nr);
+
+	__pid_link_task(pid, task_obj);
+
+	krg_task_unlock(nr);
+	krg_end_get_pid(pid);
+	krg_put_pid(pid);
+
+out:
+	return r;
+}
+
+static int handle_pid_link_task(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	struct pid_link_task_msg *msg = _msg;
+
+	return __krg_pid_link_task(msg->pid);
+}
+
+/*--------------------------------------------------------------------------*
+ *--------------------------------------------------------------------------*/
+
+void pid_wait_quiescent(void)
+{
+	flush_work(&put_pid_work);
+}
+
+void epm_pid_start(void)
+{
+	unsigned long cache_flags = SLAB_PANIC;
+
+#ifdef CONFIG_DEBUG_SLAB
+	cache_flags |= SLAB_POISON;
+#endif
+	pid_kddm_obj_cachep = KMEM_CACHE(pid_kddm_object, cache_flags);
+
+	INIT_WORK(&put_pid_work, put_pid_worker);
+
+	register_io_linker(PID_LINKER, &pid_io_linker);
+	pid_kddm_set = create_new_kddm_set(kddm_def_ns,
+					   PID_KDDM_ID,
+					   PID_LINKER,
+					   KDDM_CUSTOM_DEF_OWNER,
+					   0, 0);
+	if (IS_ERR(pid_kddm_set))
+		OOM;
+
+	rpc_register_int(PROC_RESERVE_PID, handle_reserve_pid, 0);
+	rpc_register_int(PROC_PID_LINK_TASK, handle_pid_link_task, 0);
+	rpc_register_int(PROC_END_PID_RESERVATION,
+			 handle_end_pid_reservation, 0);
+}
+
+void epm_pid_exit(void)
+{
+	return;
+}
diff -ruN linux-2.6.29/kerrighed/epm/pid.h android_cluster/linux-2.6.29/kerrighed/epm/pid.h
--- linux-2.6.29/kerrighed/epm/pid.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/pid.h	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,17 @@
+#ifndef __EPM_PID_H__
+#define __EPM_PID_H__
+
+#include <linux/types.h>
+#include <kerrighed/sys/types.h>
+
+/* Used by checkpoint/restart */
+int reserve_pid(pid_t pid);
+int krg_pid_link_task(pid_t pid);
+int __krg_pid_link_task(pid_t pid);
+int end_pid_reservation(pid_t pid);
+
+void pid_wait_quiescent(void);
+
+int pidmap_map_alloc(kerrighed_node_t node);
+
+#endif /* __EPM_PID_H__ */
diff -ruN linux-2.6.29/kerrighed/epm/pidmap.c android_cluster/linux-2.6.29/kerrighed/epm/pidmap.c
--- linux-2.6.29/kerrighed/epm/pidmap.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/pidmap.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,445 @@
+/*
+ *  kerrighed/epm/pidmap.c
+ *
+ *  Copyright (C) 2009 Louis Rilling - Kerlabs
+ */
+
+#include <linux/pid_namespace.h>
+#include <linux/pid.h>
+#include <linux/sched.h>
+#include <linux/gfp.h>
+#include <linux/rwsem.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/namespace.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/hotplug.h>
+#include <net/krgrpc/rpc.h>
+#include <net/krgrpc/rpcid.h>
+#include <kddm/kddm.h>
+
+#include "pid.h"
+
+#define BITS_PER_PAGE (PAGE_SIZE * 8)
+
+struct pidmap_map {
+	kerrighed_node_t host[KERRIGHED_MAX_NODES];
+};
+
+static struct kddm_set *pidmap_map_kddm_set;
+static struct pidmap_map pidmap_map;
+static DECLARE_RWSEM(pidmap_map_rwsem);
+static struct pid_namespace *foreign_pidmap[KERRIGHED_MAX_NODES];
+
+static int pidmap_map_alloc_object(struct kddm_obj *obj_entry,
+				   struct kddm_set *set, objid_t objid)
+{
+	BUG_ON(objid);
+	obj_entry->object = &pidmap_map;
+	return 0;
+}
+
+static int pidmap_map_first_touch(struct kddm_obj *obj_entry,
+				  struct kddm_set *set, objid_t objid,
+				  int flags)
+{
+	struct pidmap_map *map;
+	kerrighed_node_t n;
+	int err;
+
+	err = pidmap_map_alloc_object(obj_entry, set, objid);
+	if (err)
+		goto out;
+
+	map = obj_entry->object;
+	for (n = 0; n < KERRIGHED_MAX_NODES; n++)
+		map->host[n] = KERRIGHED_NODE_ID_NONE;
+
+out:
+	return 0;
+}
+
+static int pidmap_map_import_object(struct rpc_desc *desc, struct kddm_set *set,
+				    struct kddm_obj *obj_entry, objid_t objid,
+				    int flags)
+{
+	struct pidmap_map *map = obj_entry->object;
+
+	return rpc_unpack_type(desc, map->host);
+}
+
+static int pidmap_map_export_object(struct rpc_desc *desc, struct kddm_set *set,
+				    struct kddm_obj *obj_entry, objid_t objid,
+				    int flags)
+{
+	struct pidmap_map *map = obj_entry->object;
+
+	return rpc_pack_type(desc, map->host);
+}
+
+static int pidmap_map_remove_object(void *object,
+				    struct kddm_set *set, objid_t objid)
+{
+	return 0;
+}
+
+static struct iolinker_struct pidmap_map_io_linker = {
+	.first_touch   = pidmap_map_first_touch,
+	.linker_name   = "pidmap_map",
+	.linker_id     = PIDMAP_MAP_LINKER,
+	.alloc_object  = pidmap_map_alloc_object,
+	.export_object = pidmap_map_export_object,
+	.import_object = pidmap_map_import_object,
+	.remove_object = pidmap_map_remove_object,
+};
+
+int pidmap_map_read_lock(void)
+{
+	struct pidmap_map *map;
+	int err = 0;
+
+	map = _kddm_get_object(pidmap_map_kddm_set, 0);
+	BUG_ON(!map);
+	if (IS_ERR(map))
+		err = PTR_ERR(map);
+	else
+		down_read(&pidmap_map_rwsem);
+
+	return err;
+}
+
+void pidmap_map_read_unlock(void)
+{
+	up_read(&pidmap_map_rwsem);
+	_kddm_put_object(pidmap_map_kddm_set, 0);
+}
+
+int pidmap_map_write_lock(void)
+{
+	struct pidmap_map *map;
+	int err = 0;
+
+	map = _kddm_grab_object(pidmap_map_kddm_set, 0);
+	BUG_ON(!map);
+	if (IS_ERR(map))
+		err = PTR_ERR(map);
+	else
+		down_write(&pidmap_map_rwsem);
+
+	return err;
+}
+
+void pidmap_map_write_unlock(void)
+{
+	up_write(&pidmap_map_rwsem);
+	_kddm_put_object(pidmap_map_kddm_set, 0);
+}
+
+static struct pid_namespace *pidmap_alloc(void)
+{
+	struct pid_namespace *pidmap_ns;
+
+	pidmap_ns = create_pid_namespace(0);
+	if (IS_ERR(pidmap_ns))
+		return pidmap_ns;
+
+	set_bit(1, pidmap_ns->pidmap[0].page);
+	atomic_dec(&pidmap_ns->pidmap[0].nr_free);
+
+	return pidmap_ns;
+}
+
+int pidmap_map_alloc(kerrighed_node_t node)
+{
+	struct pid_namespace *pidmap_ns;
+	int err;
+
+	err = pidmap_map_write_lock();
+	if (err)
+		goto out;
+
+	if (pidmap_map.host[node] != KERRIGHED_NODE_ID_NONE)
+		goto unlock;
+
+	/*
+	 * Stupid policy: allocate here. We could do some load balancing if
+	 * required.
+	 */
+	pidmap_ns = pidmap_alloc();
+	if (IS_ERR(pidmap_ns)) {
+		err = PTR_ERR(pidmap_ns);
+		goto unlock;
+	}
+
+	foreign_pidmap[node] = pidmap_ns;
+	pidmap_map.host[node] = kerrighed_node_id;
+
+unlock:
+	pidmap_map_write_unlock();
+
+out:
+	return err;
+}
+
+kerrighed_node_t pidmap_node(kerrighed_node_t node)
+{
+	return pidmap_map.host[node];
+}
+
+struct pid_namespace *node_pidmap(kerrighed_node_t node)
+{
+	return foreign_pidmap[node];
+}
+
+void krg_free_pidmap(struct upid *upid)
+{
+	struct pid_namespace *pidmap_ns = node_pidmap(ORIG_NODE(upid->nr));
+	struct upid __upid = {
+		.nr = upid->nr,
+		.ns = pidmap_ns,
+	};
+
+	if (pidmap_ns)
+		__free_pidmap(&__upid);
+}
+
+void pidmap_map_cleanup(struct krg_namespace *krg_ns)
+{
+	kerrighed_node_t node;
+	struct pid_namespace *ns;
+
+	BUG_ON(num_online_krgnodes());
+
+	/*
+	 * Wait until all PIDs are ready to be reused
+	 * Restarted processes may have created pid kddm objects which logic
+	 * delays the actual free of the pidmap entry after the last user is
+	 * reaped.
+	 */
+	pid_wait_quiescent();
+
+	_kddm_remove_object(pidmap_map_kddm_set, 0);
+
+	for (node = 0; node < KERRIGHED_MAX_NODES; node++) {
+		ns = foreign_pidmap[node];
+		if (ns) {
+			BUG_ON(next_pidmap(ns, 1) >= 0);
+			put_pid_ns(ns);
+			foreign_pidmap[node] = NULL;
+		}
+	}
+}
+
+static int recv_pidmap(struct rpc_desc *desc,
+		       kerrighed_node_t node,
+		       struct pid_namespace *pidmap_ns)
+{
+	void *page;
+	struct pidmap *map;
+	int i, nr_pages, page_index;
+	int nr_free ;
+	int err;
+
+	page = (void *)__get_free_page(GFP_KERNEL);
+	if (!page)
+		return -ENOMEM;
+
+	err = rpc_unpack_type(desc, nr_pages);
+	if (err)
+		goto err;
+	BUG_ON(!nr_pages);
+
+	for (i = 0; i < nr_pages; i++) {
+		err = rpc_unpack_type(desc, page_index);
+		if (err)
+			goto err;
+		map = &pidmap_ns->pidmap[page_index];
+		if (!map->page) {
+			err = alloc_pidmap_page(map);
+			if (err)
+				goto err;
+		}
+		err = rpc_unpack(desc, 0, page, PAGE_SIZE);
+		if (err)
+			goto err;
+		err = rpc_unpack_type(desc, nr_free);
+		if (err)
+			goto err;
+		if (page_index == 0) {
+			/* Init's bit is set in map->page */
+			BUG_ON(!test_bit(1, map->page));
+			BUG_ON(atomic_read(&map->nr_free) != BITS_PER_PAGE - 2);
+			BUG_ON(!test_bit(0, page));
+			BUG_ON(!test_bit(1, page));
+		} else {
+			BUG_ON(atomic_read(&map->nr_free) != BITS_PER_PAGE);
+		}
+		memcpy(map->page, page, PAGE_SIZE);
+		atomic_set(&map->nr_free, nr_free);
+	}
+
+	i = 1;
+	while ((i = next_pidmap(pidmap_ns, i)) > 0) {
+		err = __krg_pid_link_task(GLOBAL_PID_NODE(i, node));
+		if (err)
+			goto err;
+	}
+
+	err = rpc_pack_type(desc, err);
+	if (err)
+		goto err;
+
+freepage:
+	free_page((unsigned long)page);
+
+	return err;
+
+err:
+	if (err > 0)
+		err = -EPIPE;
+	i = 1;
+	while ((i = next_pidmap(pidmap_ns, i)) > 0) {
+		struct upid upid = {
+			.nr = i,
+			.ns = pidmap_ns
+		};
+		__free_pidmap(&upid);
+	}
+	goto freepage;
+}
+
+static int send_pidmap(struct rpc_desc *desc, struct pid_namespace *pidmap_ns)
+{
+	struct pidmap *map;
+	int i, nr_pages;
+	int nr_free;
+	int err;
+
+	nr_pages = 0;
+	for (i = 0; i < PIDMAP_ENTRIES; i++)
+		if (atomic_read(&pidmap_ns->pidmap[i].nr_free) < BITS_PER_PAGE)
+			nr_pages++;
+	BUG_ON(!nr_pages);
+
+	err = rpc_pack_type(desc, nr_pages);
+	if (err)
+		goto out;
+
+	for (i = 0; i < PIDMAP_ENTRIES; i++) {
+		map = &pidmap_ns->pidmap[i];
+		nr_free = atomic_read(&map->nr_free);
+		if (nr_free == BITS_PER_PAGE)
+			continue;
+
+		err = rpc_pack_type(desc, i);
+		if (err)
+			goto out;
+		err = rpc_pack(desc, 0, map->page, PAGE_SIZE);
+		if (err)
+			goto out;
+		err = rpc_pack_type(desc, nr_free);
+		if (err)
+			goto out;
+	}
+
+	/* Make sure that the transfer went fine */
+	err = rpc_unpack_type(desc, err);
+	if (err > 0)
+		err = -EPIPE;
+
+out:
+	return err;
+}
+
+static void handle_pidmap_steal(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	kerrighed_node_t node = *(kerrighed_node_t *)_msg;
+	struct pid_namespace *pidmap_ns = foreign_pidmap[node];
+
+	if (send_pidmap(desc, pidmap_ns)) {
+		rpc_cancel(desc);
+		return;
+	}
+
+	foreign_pidmap[node] = NULL;
+	put_pid_ns(pidmap_ns);
+}
+
+int pidmap_map_add(struct hotplug_context *ctx)
+{
+	struct pid_namespace *ns = ctx->ns->root_nsproxy.pid_ns;
+	kerrighed_node_t host_node;
+	struct rpc_desc *desc;
+	int err;
+
+	if (!krgnode_isset(kerrighed_node_id, ctx->node_set.v))
+		return 0;
+
+	err = pidmap_map_read_lock();
+	if (err)
+		return err;
+	host_node = pidmap_node(kerrighed_node_id);
+	pidmap_map_read_unlock();
+
+	if (host_node == kerrighed_node_id)
+		return 0;
+
+	err = pidmap_map_write_lock();
+	if (err)
+		return err;
+
+	host_node = pidmap_node(kerrighed_node_id);
+	if (host_node == KERRIGHED_NODE_ID_NONE) {
+		pidmap_map.host[kerrighed_node_id] = kerrighed_node_id;
+		goto unlock;
+	}
+	BUG_ON(host_node == kerrighed_node_id);
+
+	err = -ENOMEM;
+	desc = rpc_begin(EPM_PIDMAP_STEAL, host_node);
+	if (!desc)
+		goto unlock;
+
+	err = rpc_pack_type(desc, kerrighed_node_id);
+	if (err)
+		goto cancel;
+
+	err = recv_pidmap(desc, kerrighed_node_id, ns);
+	if (err)
+		goto cancel;
+
+	pidmap_map.host[kerrighed_node_id] = kerrighed_node_id;
+
+end:
+	rpc_end(desc, 0);
+
+unlock:
+	pidmap_map_write_unlock();
+
+	return err;
+
+cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -EPIPE;
+	goto end;
+}
+
+void epm_pidmap_start(void)
+{
+	register_io_linker(PIDMAP_MAP_LINKER, &pidmap_map_io_linker);
+	pidmap_map_kddm_set = create_new_kddm_set(kddm_def_ns,
+						  PIDMAP_MAP_KDDM_ID,
+						  PIDMAP_MAP_LINKER,
+						  KDDM_RR_DEF_OWNER,
+						  0, 0);
+	if (IS_ERR(pidmap_map_kddm_set))
+		OOM;
+
+	rpc_register_void(EPM_PIDMAP_STEAL, handle_pidmap_steal, 0);
+}
+
+void epm_pidmap_exit(void)
+{
+	return;
+}
diff -ruN linux-2.6.29/kerrighed/epm/procfs.c android_cluster/linux-2.6.29/kerrighed/epm/procfs.c
--- linux-2.6.29/kerrighed/epm/procfs.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/procfs.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,391 @@
+/*
+ *  kerrighed/epm/procfs.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ */
+
+/**
+ *  /proc manager
+ *
+ *  @author Geoffroy Vallée.
+ */
+
+#include <linux/proc_fs.h>
+#include <linux/uaccess.h>
+#include <kerrighed/krg_syscalls.h>
+#include <kerrighed/krg_services.h>
+#include <kerrighed/procfs.h>
+#include <kerrighed/migration.h>
+#include "migration.h"
+#include "application/application_cr_api.h"
+
+static struct proc_dir_entry *proc_epm = NULL;
+
+/**
+ *  /proc function call to migrate a task
+ *  @author Geoffroy Vallée
+ *
+ *  @param arg	Migration arguments from user space.
+ */
+static int proc_migrate_process(void __user *arg)
+{
+	migration_infos_t migration_info;
+
+	if (copy_from_user(&migration_info, arg, sizeof(migration_info)))
+		return -EFAULT;
+
+	return sys_migrate_process(migration_info.process_to_migrate,
+				   migration_info.destination_node_id);
+}
+
+/**
+ *  /proc function call to migrate a thread
+ *  @author Geoffroy Vallée
+ *
+ *  @param arg	Migration arguments from user space.
+ */
+static int proc_migrate_thread(void __user *arg)
+{
+	migration_infos_t migration_info;
+
+	if (copy_from_user(&migration_info, arg, sizeof(migration_info)))
+		return -EFAULT;
+
+	return sys_migrate_thread(migration_info.thread_to_migrate,
+				  migration_info.destination_node_id);
+}
+
+/**
+ *  /proc function call to freeze an application.
+ *  @author Matthieu Fertré
+ */
+static int proc_app_freeze(void __user *arg)
+{
+	struct checkpoint_info ckpt_info;
+
+	if (copy_from_user(&ckpt_info, arg, sizeof(ckpt_info)))
+		return -EFAULT;
+
+	return sys_app_freeze(&ckpt_info);
+}
+
+/**
+ *  /proc function call to unfreeze an application.
+ *  @author Matthieu Fertré
+ */
+static int proc_app_unfreeze(void __user *arg)
+{
+	struct checkpoint_info ckpt_info;
+
+	if (copy_from_user(&ckpt_info, arg, sizeof(ckpt_info)))
+		return -EFAULT;
+
+	return sys_app_unfreeze(&ckpt_info);
+}
+
+static int copy_user_array(void **array, const void __user *from, int len)
+{
+	int res = 0;
+
+	*array = kmalloc(len, GFP_KERNEL);
+	if (!array)
+		return -ENOMEM;
+
+	if (copy_from_user(*array, from, len)) {
+		kfree(*array);
+		res = -EFAULT;
+	}
+
+	return res;
+}
+
+/**
+ *  /proc function call to checkpoint an application.
+ *  @author Matthieu Fertré
+ *
+ *  @param pid	Pid of one of the application processes
+ */
+static int proc_app_chkpt(void __user *arg)
+{
+	int res;
+	struct checkpoint_info ckpt_info;
+
+	if (copy_from_user(&ckpt_info, arg, sizeof(ckpt_info)))
+		return -EFAULT;
+
+	res = sys_app_chkpt(&ckpt_info);
+
+	if (copy_to_user(arg, &ckpt_info, sizeof(ckpt_info)))
+		return -EFAULT;
+
+	return res;
+}
+
+/**
+ *  /proc function call to restart a checkpointed application.
+ *  @author Matthieu Fertré
+ *
+ *  @param pid		Pid of one of the application processes
+ *  @param version	Version of checkpoint
+ */
+static int proc_app_restart(void __user *arg)
+{
+	int res;
+	unsigned int i = 0;
+	struct restart_request restart_req;
+
+	size_t file_str_len;
+	struct cr_subst_file *files = NULL;
+
+	if (copy_from_user(&restart_req, arg, sizeof(restart_req)))
+		return -EFAULT;
+
+	/* let's say that a user can not substitute more that 256 files */
+	if (restart_req.substitution.nr > 256) {
+		res = -E2BIG;
+		goto error;
+	}
+
+	/* first basic check about files substitution args */
+	if (!restart_req.substitution.nr) {
+		if (!restart_req.substitution.files)
+			goto call_restart;
+
+		res = -EINVAL;
+		goto error;
+	}
+
+	/* get the list of files to replace */
+	res = copy_user_array((void**)&files,
+			      restart_req.substitution.files,
+			      restart_req.substitution.nr *
+			      sizeof(struct cr_subst_file));
+	if (res)
+		goto error;
+
+	file_str_len = sizeof(kerrighed_node_t)*2 + sizeof(unsigned long)*2;
+
+	for (i = 0; i < restart_req.substitution.nr; i++) {
+
+		if (strlen(restart_req.substitution.files[i].file_id)
+		    == file_str_len)
+			res = copy_user_array(
+				(void**)&files[i].file_id,
+				restart_req.substitution.files[i].file_id,
+				file_str_len + 1);
+		else
+			res = -EINVAL;
+
+		if (res) {
+			files[i].file_id = NULL;
+			goto err_free_files;
+		}
+	}
+
+	restart_req.substitution.files = files;
+
+call_restart:
+	/* call the restart */
+	res = sys_app_restart(&restart_req);
+	if (res)
+		goto err_free_files;
+
+	if (copy_to_user(arg, &restart_req, sizeof(restart_req)))
+		res = -EFAULT;
+
+err_free_files:
+	for (i = 0; i < restart_req.substitution.nr; i++) {
+		if (!files[i].file_id)
+			break;
+
+		kfree(files[i].file_id);
+	}
+error:
+	return res;
+}
+
+static int proc_app_set_userdata(void __user *arg)
+{
+	int res;
+	__u64 data;
+
+	if (copy_from_user(&data, arg, sizeof(data)))
+		return -EFAULT;
+
+	res = sys_app_set_userdata(data);
+
+	return res;
+}
+
+static int proc_app_get_userdata(void __user *arg)
+{
+	int res;
+	struct app_userdata_request data_req;
+
+	if (copy_from_user(&data_req, arg, sizeof(data_req)))
+		return -EFAULT;
+
+	res = sys_app_get_userdata(&data_req);
+
+	if (copy_to_user(arg, &data_req, sizeof(data_req)))
+		return -EFAULT;
+
+	return res;
+}
+
+static int proc_app_cr_disable(void __user *arg)
+{
+	return sys_app_cr_disable();
+}
+
+static int proc_app_cr_enable(void __user *arg)
+{
+	return sys_app_cr_enable();
+}
+
+static int proc_app_cr_exclude(void __user *arg)
+{
+	struct cr_mm_region *first, *element, *next;
+	int r;
+
+	first = kzalloc(sizeof(struct cr_mm_region*), GFP_KERNEL);
+	if (!first)
+		return -ENOMEM;
+
+	if (copy_from_user(first, arg, sizeof(struct cr_mm_region))) {
+		r = -EFAULT;
+		goto error;
+	}
+
+	element = first;
+	while (element->next) {
+
+		element->next = NULL;
+
+		next = kzalloc(sizeof(struct cr_mm_region*), GFP_KERNEL);
+		if (!next) {
+			r = -ENOMEM;
+			goto error;
+		}
+
+		element->next = next;
+
+		if (copy_from_user(next, arg, sizeof(struct cr_mm_region))) {
+			r = -EFAULT;
+			next->next = NULL;
+			goto error;
+		}
+
+		element = next;
+	}
+
+	r = sys_app_cr_exclude(first);
+
+error:
+	element = first;
+	while (element) {
+		next = element->next;
+		kfree(element);
+		element = next;
+	}
+
+	return r;
+}
+
+int epm_procfs_start(void)
+{
+	int r;
+	int err = -EINVAL;
+
+	/* /proc/kerrighed/epm */
+
+	proc_epm = create_proc_entry("epm", S_IFDIR | 0755, proc_kerrighed);
+	if (!proc_epm)
+		return -ENOMEM;
+
+	r = register_proc_service(KSYS_PROCESS_MIGRATION, proc_migrate_process);
+	if (r)
+		goto err;
+
+	r = register_proc_service(KSYS_THREAD_MIGRATION, proc_migrate_thread);
+	if (r)
+		goto unreg_migrate_process;
+
+	r = register_proc_service(KSYS_APP_FREEZE, proc_app_freeze);
+	if (r)
+		goto unreg_migrate_thread;
+
+	r = register_proc_service(KSYS_APP_UNFREEZE, proc_app_unfreeze);
+	if (r)
+		goto unreg_app_freeze;
+
+	r = register_proc_service(KSYS_APP_CHKPT, proc_app_chkpt);
+	if (r)
+		goto unreg_app_unfreeze;
+
+	r = register_proc_service(KSYS_APP_RESTART, proc_app_restart);
+	if (r)
+		goto unreg_app_chkpt;
+
+	r = register_proc_service(KSYS_APP_SET_USERDATA, proc_app_set_userdata);
+	if (r)
+		goto unreg_app_restart;
+
+	r = register_proc_service(KSYS_APP_GET_USERDATA, proc_app_get_userdata);
+	if (r)
+		goto unreg_app_set_userdata;
+
+	r = register_proc_service(KSYS_APP_CR_DISABLE, proc_app_cr_disable);
+	if (r)
+		goto unreg_app_get_userdata;
+
+	r = register_proc_service(KSYS_APP_CR_ENABLE, proc_app_cr_enable);
+	if (r)
+		goto unreg_app_cr_disable;
+
+	r = register_proc_service(KSYS_APP_CR_EXCLUDE, proc_app_cr_exclude);
+	if (r)
+		goto unreg_app_cr_enable;
+
+	return 0;
+
+unreg_app_cr_enable:
+	unregister_proc_service(KSYS_APP_CR_ENABLE);
+unreg_app_cr_disable:
+	unregister_proc_service(KSYS_APP_CR_DISABLE);
+unreg_app_get_userdata:
+	unregister_proc_service(KSYS_APP_GET_USERDATA);
+unreg_app_set_userdata:
+	unregister_proc_service(KSYS_APP_SET_USERDATA);
+unreg_app_restart:
+	unregister_proc_service(KSYS_APP_RESTART);
+unreg_app_chkpt:
+	unregister_proc_service(KSYS_APP_CHKPT);
+unreg_app_unfreeze:
+	unregister_proc_service(KSYS_APP_UNFREEZE);
+unreg_app_freeze:
+	unregister_proc_service(KSYS_APP_FREEZE);
+unreg_migrate_thread:
+	unregister_proc_service(KSYS_THREAD_MIGRATION);
+unreg_migrate_process:
+	unregister_proc_service(KSYS_PROCESS_MIGRATION);
+err:
+	return err;
+}
+
+void epm_procfs_exit(void)
+{
+	unregister_proc_service(KSYS_PROCESS_MIGRATION);
+	unregister_proc_service(KSYS_THREAD_MIGRATION);
+	unregister_proc_service(KSYS_APP_FREEZE);
+	unregister_proc_service(KSYS_APP_UNFREEZE);
+	unregister_proc_service(KSYS_APP_CHKPT);
+	unregister_proc_service(KSYS_APP_RESTART);
+	unregister_proc_service(KSYS_APP_SET_USERDATA);
+	unregister_proc_service(KSYS_APP_GET_USERDATA);
+	unregister_proc_service(KSYS_APP_CR_DISABLE);
+	unregister_proc_service(KSYS_APP_CR_ENABLE);
+	unregister_proc_service(KSYS_APP_CR_EXCLUDE);
+
+	procfs_deltree(proc_epm);
+}
diff -ruN linux-2.6.29/kerrighed/epm/remote_clone.c android_cluster/linux-2.6.29/kerrighed/epm/remote_clone.c
--- linux-2.6.29/kerrighed/epm/remote_clone.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/remote_clone.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,298 @@
+/*
+ *  kerrighed/epm/remote_clone.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2006-2007 Pascal Gallard - Kerlabs, Louis Rilling - Kerlabs
+ *  Copyright (C) 2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/completion.h>
+#include <linux/freezer.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/action.h>
+#include <kerrighed/ghost.h>
+#ifdef CONFIG_KRG_SCHED
+#include <kerrighed/scheduler/placement.h>
+#endif
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include "network_ghost.h"
+
+struct vfork_done_proxy {
+	struct completion *waiter_vfork_done;
+	kerrighed_node_t waiter_node;
+};
+
+static struct kmem_cache *vfork_done_proxy_cachep;
+
+static void *cluster_started;
+
+int krg_do_fork(unsigned long clone_flags,
+		unsigned long stack_start,
+		struct pt_regs *regs,
+		unsigned long stack_size,
+		int __user *parent_tidptr,
+		int __user *child_tidptr,
+		int trace)
+{
+	struct task_struct *task = current;
+#ifdef CONFIG_KRG_SCHED
+	kerrighed_node_t distant_node;
+#else
+	static kerrighed_node_t distant_node = -1;
+#endif
+	struct epm_action remote_clone;
+	struct rpc_desc *desc;
+	struct completion vfork;
+	pid_t remote_pid = -1;
+	int retval = -ENOSYS;
+
+	if (!cluster_started)
+		goto out;
+
+	if ((clone_flags &
+	     ~(CSIGNAL |
+	       CLONE_CHILD_CLEARTID | CLONE_CHILD_SETTID |
+	       CLONE_VFORK | CLONE_SYSVSEM | CLONE_UNTRACED))
+	    || trace)
+		/* Unsupported clone flags are requested. Abort */
+		goto out;
+
+	if (!task->sighand->krg_objid || !task->signal->krg_objid
+	    || !task->task_obj || !task->children_obj) {
+		retval = -EPERM;
+		goto out;
+	}
+
+	retval = krg_action_start(task, EPM_REMOTE_CLONE);
+	if (retval)
+		goto out;
+
+#ifdef CONFIG_KRG_SCHED
+	distant_node = new_task_node(task);
+#else
+	if (distant_node < 0)
+		distant_node = kerrighed_node_id;
+	distant_node = krgnode_next_online_in_ring(distant_node);
+#endif
+	if (distant_node < 0 || distant_node == kerrighed_node_id)
+		goto out_action_stop;
+
+	retval = -ENOMEM;
+	desc = rpc_begin(RPC_EPM_REMOTE_CLONE, distant_node);
+	if (!desc)
+		goto out_action_stop;
+
+	remote_clone.type = EPM_REMOTE_CLONE;
+	remote_clone.remote_clone.target = distant_node;
+	remote_clone.remote_clone.clone_flags = clone_flags;
+	remote_clone.remote_clone.stack_start = stack_start;
+	remote_clone.remote_clone.stack_size = stack_size;
+	remote_clone.remote_clone.from_pid = task_pid_knr(task);
+	remote_clone.remote_clone.from_tgid = task_tgid_knr(task);
+	remote_clone.remote_clone.parent_tidptr = parent_tidptr;
+	remote_clone.remote_clone.child_tidptr = child_tidptr;
+	if (clone_flags & CLONE_VFORK) {
+		init_completion(&vfork);
+		remote_clone.remote_clone.vfork = &vfork;
+	}
+
+	remote_pid = send_task(desc, task, regs, &remote_clone);
+
+	if (remote_pid < 0)
+		rpc_cancel(desc);
+	rpc_end(desc, 0);
+
+	if (remote_pid > 0 && (clone_flags & CLONE_VFORK)) {
+		freezer_do_not_count();
+		wait_for_completion(&vfork);
+		freezer_count();
+	}
+
+out_action_stop:
+	krg_action_stop(task, EPM_REMOTE_CLONE);
+
+out:
+	return remote_pid;
+}
+
+static void handle_remote_clone(struct rpc_desc *desc, void *msg, size_t size)
+{
+	struct epm_action *action = msg;
+	struct task_struct *task;
+
+	task = recv_task(desc, action);
+	if (!task) {
+		rpc_cancel(desc);
+		return;
+	}
+
+	krg_action_stop(task, EPM_REMOTE_CLONE);
+
+	wake_up_new_task(task, CLONE_VM);
+}
+
+bool in_krg_do_fork(void)
+{
+	return task_tgid_knr(krg_current) != krg_current->signal->krg_objid;
+}
+
+static inline struct vfork_done_proxy *vfork_done_proxy_alloc(void)
+{
+	return kmem_cache_alloc(vfork_done_proxy_cachep, GFP_KERNEL);
+}
+
+static inline void vfork_done_proxy_free(struct vfork_done_proxy *proxy)
+{
+	kmem_cache_free(vfork_done_proxy_cachep, proxy);
+}
+
+int export_vfork_done(struct epm_action *action,
+		      ghost_t *ghost, struct task_struct *task)
+{
+	struct vfork_done_proxy proxy;
+	int retval = 0;
+
+	switch (action->type) {
+	case EPM_MIGRATE:
+		if (!task->vfork_done)
+			break;
+		if (task->remote_vfork_done) {
+			proxy = *(struct vfork_done_proxy *)task->vfork_done;
+		} else {
+			proxy.waiter_vfork_done = task->vfork_done;
+			proxy.waiter_node = kerrighed_node_id;
+		}
+		retval = ghost_write(ghost, &proxy, sizeof(proxy));
+		break;
+	case EPM_REMOTE_CLONE:
+		if (action->remote_clone.clone_flags & CLONE_VFORK) {
+			proxy.waiter_vfork_done = action->remote_clone.vfork;
+			proxy.waiter_node = kerrighed_node_id;
+			retval = ghost_write(ghost, &proxy, sizeof(proxy));
+		}
+		break;
+	default:
+		if (task->vfork_done)
+			retval = -ENOSYS;
+	}
+
+	return retval;
+}
+
+static int vfork_done_proxy_install(struct task_struct *task,
+				    struct vfork_done_proxy *proxy)
+{
+	struct vfork_done_proxy *p = vfork_done_proxy_alloc();
+	int retval = -ENOMEM;
+
+	if (!p)
+		goto out;
+	*p = *proxy;
+	task->vfork_done = (struct completion *)p;
+	task->remote_vfork_done = 1;
+	retval = 0;
+
+out:
+	return retval;
+}
+
+int import_vfork_done(struct epm_action *action,
+		      ghost_t *ghost, struct task_struct *task)
+{
+	struct vfork_done_proxy tmp_proxy;
+	int retval = 0;
+
+	switch (action->type) {
+	case EPM_MIGRATE:
+		if (!task->vfork_done)
+			break;
+
+		retval = ghost_read(ghost, &tmp_proxy, sizeof(tmp_proxy));
+		if (unlikely(retval))
+			goto out;
+
+		if (tmp_proxy.waiter_node == kerrighed_node_id) {
+			task->vfork_done = tmp_proxy.waiter_vfork_done;
+			task->remote_vfork_done = 0;
+			break;
+		}
+
+		retval = vfork_done_proxy_install(task, &tmp_proxy);
+		break;
+	case EPM_REMOTE_CLONE:
+		if (action->remote_clone.clone_flags & CLONE_VFORK) {
+			retval = ghost_read(ghost, &tmp_proxy, sizeof(tmp_proxy));
+			if (unlikely(retval))
+				goto out;
+			retval = vfork_done_proxy_install(task, &tmp_proxy);
+			break;
+		}
+		/* Fallthrough */
+	default:
+		task->vfork_done = NULL;
+	}
+
+out:
+	return retval;
+}
+
+void unimport_vfork_done(struct task_struct *task)
+{
+	struct completion *vfork_done = task->vfork_done;
+	if (vfork_done && task->remote_vfork_done)
+		vfork_done_proxy_free((struct vfork_done_proxy *)vfork_done);
+}
+
+/* Called after having successfuly migrated out task */
+void cleanup_vfork_done(struct task_struct *task)
+{
+	struct completion *vfork_done = task->vfork_done;
+	if (vfork_done) {
+		task->vfork_done = NULL;
+		if (task->remote_vfork_done)
+			vfork_done_proxy_free((struct vfork_done_proxy *)vfork_done);
+	}
+}
+
+static void handle_vfork_done(struct rpc_desc *desc, void *data, size_t size)
+{
+	struct completion *vfork_done = *(struct completion **)data;
+
+	complete(vfork_done);
+}
+
+void krg_vfork_done(struct completion *vfork_done)
+{
+	struct vfork_done_proxy *proxy = (struct vfork_done_proxy *)vfork_done;
+
+	rpc_async(PROC_VFORK_DONE, proxy->waiter_node,
+		  &proxy->waiter_vfork_done, sizeof(proxy->waiter_vfork_done));
+	vfork_done_proxy_free(proxy);
+}
+
+void register_remote_clone_hooks(void)
+{
+	hook_register(&cluster_started, (void *)true);
+}
+
+int epm_remote_clone_start(void)
+{
+	vfork_done_proxy_cachep = KMEM_CACHE(vfork_done_proxy, SLAB_PANIC);
+
+	if (rpc_register_void(RPC_EPM_REMOTE_CLONE, handle_remote_clone, 0))
+		BUG();
+	if (rpc_register_void(PROC_VFORK_DONE, handle_vfork_done, 0))
+		BUG();
+
+	return 0;
+}
+
+void epm_remote_clone_exit(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/epm/remote_clone.h android_cluster/linux-2.6.29/kerrighed/epm/remote_clone.h
--- linux-2.6.29/kerrighed/epm/remote_clone.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/remote_clone.h	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,9 @@
+#ifndef __REMOTE_CLONE_H__
+#define __REMOTE_CLONE_H__
+
+struct task_struct;
+
+/* Used by migration */
+void cleanup_vfork_done(struct task_struct *task);
+
+#endif /* __REMOTE_CLONE_H__ */
diff -ruN linux-2.6.29/kerrighed/epm/restart.c android_cluster/linux-2.6.29/kerrighed/epm/restart.c
--- linux-2.6.29/kerrighed/epm/restart.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/restart.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,165 @@
+/*
+ *  kerrighed/epm/restart.c
+ *
+ *  Copyright (C) 1999-2008 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2008-2009 Matthieu Fertré - Kerlabs
+ */
+
+/**
+ *  Process restart.
+ *  @file restart.c
+ *
+ *  @author Geoffroy Vallée, Matthieu Fertré
+ */
+
+#include <linux/sched.h>
+#include <linux/cred.h>
+#include <linux/pid_namespace.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/application.h>
+#include <kerrighed/action.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/ghost_helpers.h>
+#include "ghost.h"
+#include "pid.h"
+#include "restart.h"
+
+/**
+ *  Load the process information saved in a checkpoint-file
+ *  @author	Geoffroy Vallée, Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param action	Restart descriptor
+ *  @param pid		Pid of the task to restart
+ *  @param ghost	Ghost to restart the task from
+ *
+ *  @return		New Tasks's UNIX PID if success, NULL if failure
+ */
+static
+struct task_struct *restart_task_from_ghost(struct epm_action *action,
+					    pid_t pid,
+					    ghost_t *ghost)
+{
+	struct task_struct *newTsk = NULL;
+	int err;
+
+	/* Recreate the process */
+	newTsk = import_process(action, ghost);
+	if (IS_ERR(newTsk))
+		goto exit;
+	BUG_ON(!newTsk);
+
+	/* Link pid kddm object and task kddm obj */
+	err = krg_pid_link_task(pid);
+	if (err) {
+		newTsk = ERR_PTR(err);
+		goto exit;
+	}
+
+exit:
+	return newTsk;
+}
+
+/**
+ *  Load the process information saved in a checkpoint-file
+ *  @author       Matthieu Fertré
+ *
+ *  @param action	Restart descriptor
+ *  @param pid		Pid of the task to restart
+ *  @param app_id	Application id
+ *  @param chkpt_sn	Sequence number of the checkpoint
+ *
+ *  @return		New task if success, PTR_ERR if failure
+ */
+static
+struct task_struct *restart_task_from_disk(struct epm_action *action,
+					   pid_t pid,
+					   long app_id,
+					   int chkpt_sn)
+{
+	int r;
+	ghost_t *ghost;
+	struct task_struct *task;
+
+	ghost = create_file_ghost(GHOST_READ, app_id, chkpt_sn,
+				  "task_%d.bin", pid);
+
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		ckpt_err(action, r,
+			 "Fail to open file /var/chkpt/%ld/v%d/task_%d.bin to "
+			 "restart process %d",
+			 app_id, chkpt_sn, pid, pid);
+		return ERR_PTR(r);
+	}
+
+	/* Recreate the process */
+
+	task = restart_task_from_ghost(action, pid, ghost);
+
+	ghost_close(ghost);
+
+	return task;
+}
+
+/**
+ *  Load the process information saved
+ *  @author      Matthieu Fertré
+ *
+ *  @param action	Restart descriptor
+ *  @param pid		Pid of the task to restart
+ *  @param app_id	Application id
+ *  @param chkpt_sn	Sequence number of the checkpoint
+ *
+ *  @return		New task if success, PTR_ERR if failure
+ */
+static
+struct task_struct *restart_task(struct epm_action *action,
+				 pid_t pid, long app_id,
+				 int chkpt_sn)
+{
+	struct task_struct *task = NULL;
+	ghost_fs_t oldfs;
+
+	__set_ghost_fs(&oldfs);
+
+	task = restart_task_from_disk(action, pid, app_id, chkpt_sn);
+
+	unset_ghost_fs(&oldfs);
+	return task;
+}
+
+/**
+ *  Main kernel entry function to restart a checkpointed task.
+ *  @author Geoffroy Vallée, Matthieu Fertré
+ *
+ *  @param app          Application
+ *  @param pid		Pid of the task to restart
+ *  @param flags	Option flags
+ *
+ *  @return		New task if success, PTR_ERR if failure
+ */
+struct task_struct *restart_process(struct app_struct *app, pid_t pid,
+				    int flags)
+{
+	struct epm_action action;
+	struct task_struct *task;
+
+	/* Check if the process has not been already restarted */
+	if (find_task_by_kpid(pid) != NULL)
+		return ERR_PTR(-EALREADY);
+
+	action.type = EPM_CHECKPOINT;
+	action.restart.shared = CR_LINK_ONLY;
+	action.restart.app = app;
+	action.restart.flags = flags;
+
+	BUG_ON(!action.restart.app);
+
+	task = restart_task(&action, pid, app->app_id, app->chkpt_sn);
+	if (IS_ERR(task))
+		ckpt_err(&action, PTR_ERR(task),
+			 "Fail to restart process %d",
+			 pid);
+
+	return task;
+}
diff -ruN linux-2.6.29/kerrighed/epm/restart.h android_cluster/linux-2.6.29/kerrighed/epm/restart.h
--- linux-2.6.29/kerrighed/epm/restart.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/restart.h	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,26 @@
+/**
+ *  Process restart interface.
+ *  @file restart.h
+ *
+ *  Definition of process restart interface.
+ *  @author Geoffroy Vallée, Matthieu Fertré
+ */
+
+#ifndef __RESTART_H__
+#define __RESTART_H__
+
+#include <linux/types.h>
+
+struct task_struct;
+struct app_struct;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+struct task_struct *restart_process(struct app_struct *app, pid_t pid,
+				    int flags);
+
+#endif /* __RESTART_H__ */
diff -ruN linux-2.6.29/kerrighed/epm/sighand.c android_cluster/linux-2.6.29/kerrighed/epm/sighand.c
--- linux-2.6.29/kerrighed/epm/sighand.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/sighand.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,653 @@
+/*
+ *  kerrighed/epm/sighand.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2006-2007 Pascal Gallard - Kerlabs, Louis Rilling - Kerlabs
+ */
+
+#include <linux/types.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/rwsem.h>
+#include <linux/unique_id.h>
+#include <kerrighed/signal.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/task.h>
+#include <kerrighed/application.h>
+#include <kerrighed/app_shared.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/ghost_helpers.h>
+#include <kerrighed/action.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+
+struct sighand_struct_kddm_object {
+	struct sighand_struct *sighand;
+	atomic_t count;
+	int keep_on_remove;
+	struct rw_semaphore remove_sem;
+};
+
+static struct kmem_cache *sighand_struct_kddm_obj_cachep;
+
+/* Kddm set of 'struct sighand_struct' location */
+static struct kddm_set *sighand_struct_kddm_set = NULL;
+
+/* unique_id for sighand kddm objects */
+static unique_id_root_t sighand_struct_id_root;
+
+static struct sighand_struct_kddm_object *sighand_struct_kddm_object_alloc(void)
+{
+	struct sighand_struct_kddm_object *obj;
+
+	obj = kmem_cache_alloc(sighand_struct_kddm_obj_cachep, GFP_KERNEL);
+	if (obj) {
+		obj->sighand = NULL;
+		obj->keep_on_remove = 0;
+		init_rwsem(&obj->remove_sem);
+	}
+	return obj;
+}
+
+static struct sighand_struct *sighand_struct_alloc(void)
+{
+	return kmem_cache_alloc(sighand_cachep, GFP_KERNEL);
+}
+
+static void sighand_struct_attach_object(struct sighand_struct *sig,
+					 struct sighand_struct_kddm_object *obj,
+					 objid_t objid)
+{
+	sig->krg_objid = objid;
+	sig->kddm_obj = obj;
+	obj->sighand = sig;
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int sighand_struct_alloc_object(struct kddm_obj *obj_entry,
+				       struct kddm_set *set, objid_t objid)
+{
+	struct sighand_struct_kddm_object *obj;
+	struct sighand_struct *sig;
+
+	obj = sighand_struct_kddm_object_alloc();
+	if (!obj)
+		return -ENOMEM;
+
+	sig = sighand_struct_alloc();
+	if (!sig) {
+		kmem_cache_free(sighand_struct_kddm_obj_cachep, obj);
+		return -ENOMEM;
+	}
+
+	sighand_struct_attach_object(sig, obj, objid);
+
+	obj_entry->object = obj;
+
+	return 0;
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int sighand_struct_first_touch(struct kddm_obj *obj_entry,
+				      struct kddm_set *set, objid_t objid,
+				      int flags)
+{
+	struct sighand_struct_kddm_object *obj;
+
+	obj = sighand_struct_kddm_object_alloc();
+	if (!obj)
+		return -ENOMEM;
+	atomic_set(&obj->count, 1);
+
+	obj_entry->object = obj;
+
+	return 0;
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int sighand_struct_import_object(struct rpc_desc *desc,
+					struct kddm_set *set,
+					struct kddm_obj *obj_entry,
+					objid_t objid,
+					int flags)
+{
+	struct sighand_struct_kddm_object *obj = obj_entry->object;
+	struct sighand_struct *dest;
+	struct sighand_struct *tmp;
+	int retval;
+
+	tmp = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);
+	if (!tmp)
+		return -ENOMEM;
+
+	retval = rpc_unpack_type(desc, tmp->count);
+	if (likely(!retval))
+		retval = rpc_unpack_type(desc, tmp->action);
+	if (likely(!retval))
+		retval = rpc_unpack_type(desc, obj->count);
+
+	if (likely(!retval)) {
+		dest = obj->sighand;
+		spin_lock_irq(&dest->siglock);
+		/* This is safe since all changes are protected by grab, and
+		 * no thread can hold a grab during import */
+		atomic_set(&dest->count, atomic_read(&tmp->count));
+		memcpy(dest->action, tmp->action, sizeof(dest->action));
+		spin_unlock_irq(&dest->siglock);
+	}
+
+	kmem_cache_free(sighand_cachep, tmp);
+
+	return retval;
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int sighand_struct_export_object(struct rpc_desc *desc,
+					struct kddm_set *set,
+					struct kddm_obj *obj_entry,
+					objid_t objid,
+					int flags)
+{
+	struct sighand_struct_kddm_object *obj = obj_entry->object;
+	struct sighand_struct *src;
+	int retval;
+
+	src = obj->sighand;
+	retval = rpc_pack_type(desc, src->count);
+	if (likely(!retval))
+		retval = rpc_pack_type(desc, src->action);
+	if (likely(!retval))
+		retval = rpc_pack_type(desc, obj->count);
+
+	return retval;
+}
+
+void krg_sighand_pin(struct sighand_struct *sig)
+{
+	struct sighand_struct_kddm_object *obj = sig->kddm_obj;
+	BUG_ON(!obj);
+	down_read(&obj->remove_sem);
+}
+
+void krg_sighand_unpin(struct sighand_struct *sig)
+{
+	struct sighand_struct_kddm_object *obj = sig->kddm_obj;
+	BUG_ON(!obj);
+	up_read(&obj->remove_sem);
+}
+
+static int sighand_struct_remove_object(void *object,
+					struct kddm_set *set, objid_t objid)
+{
+	struct sighand_struct_kddm_object *obj = object;
+
+	/* Ensure that no thread uses this sighand_struct copy */
+	down_write(&obj->remove_sem);
+	up_write(&obj->remove_sem);
+
+	if (!obj->keep_on_remove) {
+		BUG_ON(waitqueue_active(&obj->sighand->signalfd_wqh));
+		kmem_cache_free(sighand_cachep, obj->sighand);
+	}
+	kmem_cache_free(sighand_struct_kddm_obj_cachep, obj);
+
+	return 0;
+}
+
+static struct iolinker_struct sighand_struct_io_linker = {
+	.first_touch   = sighand_struct_first_touch,
+	.linker_name   = "sigh ",
+	.linker_id     = SIGHAND_STRUCT_LINKER,
+	.alloc_object  = sighand_struct_alloc_object,
+	.export_object = sighand_struct_export_object,
+	.import_object = sighand_struct_import_object,
+	.remove_object = sighand_struct_remove_object,
+};
+
+/*
+ * Get and lock a sighand structure for a given process
+ * @author Pascal Gallard
+ */
+struct sighand_struct *krg_sighand_readlock(objid_t id)
+{
+	struct sighand_struct_kddm_object *obj;
+
+	obj = _kddm_get_object_no_ft(sighand_struct_kddm_set, id);
+	if (!obj) {
+		_kddm_put_object(sighand_struct_kddm_set, id);
+		return NULL;
+	}
+	BUG_ON(!obj->sighand);
+
+	return obj->sighand;
+}
+
+/*
+ * Grab and lock a sighand structure for a given process
+ * @author Pascal Gallard
+ */
+struct sighand_struct *krg_sighand_writelock(objid_t id)
+{
+	struct sighand_struct_kddm_object *obj;
+
+	obj = _kddm_grab_object_no_ft(sighand_struct_kddm_set, id);
+	if (!obj) {
+		_kddm_put_object(sighand_struct_kddm_set, id);
+		return NULL;
+	}
+	BUG_ON(!obj->sighand);
+
+	return obj->sighand;
+}
+
+/*
+ * unlock a sighand structure for a given process
+ * @author Pascal Gallard
+ */
+void krg_sighand_unlock(objid_t id)
+{
+	_kddm_put_object(sighand_struct_kddm_set, id);
+}
+
+static
+struct sighand_struct_kddm_object *
+____krg_sighand_alloc(struct sighand_struct *sig)
+{
+	struct sighand_struct_kddm_object *obj;
+	unique_id_t id;
+
+	id = get_unique_id(&sighand_struct_id_root);
+
+	/* Create the sighand object */
+	obj = _kddm_grab_object(sighand_struct_kddm_set, id);
+	BUG_ON(!obj);
+	/* Must be a first touch */
+	BUG_ON(obj->sighand);
+	sighand_struct_attach_object(sig, obj, id);
+
+	return obj;
+}
+
+/*
+ * Alloc a dedicated sighand_struct to task_struct task.
+ * @author Pascal Gallard
+ */
+static void __krg_sighand_alloc(struct task_struct *task,
+				struct sighand_struct *sig)
+{
+	struct sighand_struct_kddm_object *obj;
+
+	/*
+	 * Exclude kernel threads and local pids from using sighand_struct kddm
+	 * objects.
+	 */
+	/*
+	 * At this stage, task->mm may point to the mm of a
+	 * task being duplicated instead of the mm of task for which this struct
+	 * is being allocated, but we only need to know whether it is NULL or
+	 * not, which will be the same after copy_mm.
+	 */
+	if (!task->nsproxy->krg_ns
+	    || !(task_pid_knr(task) & GLOBAL_PID_MASK)
+	    || (task->flags & PF_KTHREAD)) {
+		BUG_ON(krg_current);
+		sig->krg_objid = 0;
+		sig->kddm_obj = NULL;
+		return;
+	}
+
+	obj = ____krg_sighand_alloc(sig);
+	BUG_ON(!obj);
+	krg_sighand_unlock(sig->krg_objid);
+}
+
+void krg_sighand_alloc(struct task_struct *task, unsigned long clone_flags)
+{
+	struct sighand_struct *sig = task->sighand;
+
+	if (krg_current && !in_krg_do_fork())
+		/*
+		 * This is a process migration or restart: sighand_struct is
+		 * already setup.
+		 */
+		return;
+
+	if (!krg_current && (clone_flags & CLONE_SIGHAND))
+		/* New thread: already done in copy_sighand() */
+		return;
+
+	__krg_sighand_alloc(task, sig);
+}
+
+void krg_sighand_alloc_unshared(struct task_struct *task,
+				struct sighand_struct *sig)
+{
+	__krg_sighand_alloc(task, sig);
+}
+
+struct sighand_struct *cr_sighand_alloc(void)
+{
+	struct sighand_struct_kddm_object *obj;
+	struct sighand_struct *sig;
+
+	sig = sighand_struct_alloc();
+	if (!sig)
+		return NULL;
+
+	obj = ____krg_sighand_alloc(sig);
+	BUG_ON(!obj);
+
+	return sig;
+}
+
+void cr_sighand_free(objid_t id)
+{
+	_kddm_remove_frozen_object(sighand_struct_kddm_set, id);
+}
+
+/* Assumes that the associated kddm object is write locked. */
+void krg_sighand_share(struct task_struct *task)
+{
+	struct sighand_struct_kddm_object *obj = task->sighand->kddm_obj;
+	int count;
+
+	count = atomic_inc_return(&obj->count);
+}
+
+objid_t krg_sighand_exit(struct sighand_struct *sig)
+{
+	struct sighand_struct_kddm_object *obj = sig->kddm_obj;
+	objid_t id = sig->krg_objid;
+	int count;
+
+	if (!obj)
+		return 0;
+
+	krg_sighand_writelock(id);
+	count = atomic_dec_return(&obj->count);
+	if (count == 0) {
+		krg_sighand_unlock(id);
+		BUG_ON(obj->keep_on_remove);
+		/* Free the kddm object but keep the sighand_struct so that
+		 * __exit_sighand releases it properly. */
+		obj->keep_on_remove = 1;
+		_kddm_remove_object(sighand_struct_kddm_set, id);
+
+		return 0;
+	}
+
+	return id;
+}
+
+void krg_sighand_cleanup(struct sighand_struct *sig)
+{
+	objid_t locked_id;
+
+	locked_id = krg_sighand_exit(sig);
+	__cleanup_sighand(sig);
+	if (locked_id)
+		krg_sighand_unlock(locked_id);
+}
+
+/* EPM actions */
+
+static int cr_export_later_sighand_struct(struct epm_action *action,
+					  ghost_t *ghost,
+					  struct task_struct *task)
+{
+	int r;
+	long key;
+
+	BUG_ON(action->type != EPM_CHECKPOINT);
+	BUG_ON(action->checkpoint.shared != CR_SAVE_LATER);
+
+	key = (long)(task->sighand);
+
+	r = ghost_write(ghost, &key, sizeof(long));
+	if (r)
+		goto err;
+
+	/*
+	 * WARNING, currently we do not really support sighand shared by
+	 * several nodes.
+	 */
+	r = add_to_shared_objects_list(task->application,
+				       SIGHAND_STRUCT, key, LOCAL_ONLY,
+				       task, NULL, 0);
+
+	if (r == -ENOKEY) /* the sighand_struct was already in the list. */
+		r = 0;
+err:
+	return r;
+}
+
+int export_sighand_struct(struct epm_action *action,
+			  ghost_t *ghost, struct task_struct *tsk)
+{
+	int r;
+
+	if (action->type == EPM_CHECKPOINT
+	    && action->checkpoint.shared == CR_SAVE_LATER) {
+		r = cr_export_later_sighand_struct(action, ghost, tsk);
+		return r;
+	}
+
+	r = ghost_write(ghost, &tsk->sighand->krg_objid,
+			sizeof(tsk->sighand->krg_objid));
+	if (r)
+		goto err_write;
+
+	if (action->type == EPM_CHECKPOINT)
+		r = ghost_write(ghost,
+				&tsk->sighand->action,
+				sizeof(tsk->sighand->action));
+
+err_write:
+	return r;
+}
+
+static int cr_link_to_sighand_struct(struct epm_action *action,
+				     ghost_t *ghost,
+				     struct task_struct *tsk)
+{
+	int r;
+	long key;
+	struct sighand_struct *sig;
+
+	r = ghost_read(ghost, &key, sizeof(long));
+	if (r)
+		goto err;
+
+	sig = get_imported_shared_object(action->restart.app,
+					 SIGHAND_STRUCT, key);
+
+	if (!sig) {
+		r = -E_CR_BADDATA;
+		goto err;
+	}
+	krg_sighand_writelock(sig->krg_objid);
+
+	atomic_inc(&sig->count);
+	tsk->sighand = sig;
+
+	krg_sighand_share(tsk);
+	krg_sighand_unlock(sig->krg_objid);
+err:
+	return r;
+}
+
+int import_sighand_struct(struct epm_action *action,
+			  ghost_t *ghost, struct task_struct *tsk)
+{
+	unsigned long krg_objid;
+	int r;
+
+	if (action->type == EPM_CHECKPOINT
+	    && action->restart.shared == CR_LINK_ONLY) {
+		r = cr_link_to_sighand_struct(action, ghost, tsk);
+		return r;
+	}
+
+	r = ghost_read(ghost, &krg_objid, sizeof(krg_objid));
+	if (r)
+		goto err_read;
+
+	switch (action->type) {
+	case EPM_MIGRATE:
+		tsk->sighand = krg_sighand_writelock(krg_objid);
+		BUG_ON(!tsk->sighand);
+		krg_sighand_unlock(krg_objid);
+		break;
+	case EPM_REMOTE_CLONE:
+		/*
+		 * The structure will be partly copied when creating the
+		 * active process.
+		 */
+		tsk->sighand = krg_sighand_readlock(krg_objid);
+		BUG_ON(!tsk->sighand);
+		krg_sighand_unlock(krg_objid);
+		break;
+	case EPM_CHECKPOINT:
+		tsk->sighand = cr_sighand_alloc();
+		krg_objid = tsk->sighand->krg_objid;
+
+		r = ghost_read(ghost,
+			       &tsk->sighand->action,
+			       sizeof(tsk->sighand->action));
+		if (r) {
+			cr_sighand_free(krg_objid);
+			goto err_read;
+		}
+		atomic_set(&tsk->sighand->count, 1);
+
+		krg_sighand_unlock(krg_objid);
+		break;
+	default:
+		PANIC("Case not supported: %d\n", action->type);
+	}
+
+err_read:
+	return r;
+}
+
+void unimport_sighand_struct(struct task_struct *task)
+{
+}
+
+static int cr_export_now_sighand_struct(struct epm_action *action,
+					ghost_t *ghost,
+					struct task_struct *task,
+					union export_args *args)
+{
+	int r;
+	r = export_sighand_struct(action, ghost, task);
+	if (r)
+		ckpt_err(action, r,
+			 "Fail to save struct sighand_struct "
+			 "of process %d (%s)",
+			 task_pid_knr(task), task->comm);
+	return r;
+}
+
+
+static int cr_import_now_sighand_struct(struct epm_action *action,
+					ghost_t *ghost,
+					struct task_struct *fake,
+					int local_only,
+					void **returned_data,
+					size_t *data_size)
+{
+	int r;
+	BUG_ON(*returned_data != NULL);
+
+	r = import_sighand_struct(action, ghost, fake);
+	if (r) {
+		ckpt_err(action, r,
+			 "App %d - Fail to restore a struct sighand_struct",
+			 action->restart.app->app_id);
+		goto err;
+	}
+
+	*returned_data = fake->sighand;
+err:
+	return r;
+}
+
+static int cr_import_complete_sighand_struct(struct task_struct *fake,
+					     void *_sig)
+{
+	unsigned long sighand_id;
+	struct sighand_struct *sig = _sig;
+	sighand_id = krg_sighand_exit(sig);
+	if (sighand_id)
+		krg_sighand_unlock(sighand_id);
+
+	BUG_ON(atomic_read(&sig->count) <= 1);
+	__cleanup_sighand(sig);
+
+	return 0;
+}
+
+static int cr_delete_sighand_struct(struct task_struct *fake, void *_sig)
+{
+	unsigned long sighand_id;
+	struct sighand_struct *sig = _sig;
+	sighand_id = krg_sighand_exit(sig);
+	if (sighand_id)
+		krg_sighand_unlock(sighand_id);
+
+	BUG_ON(atomic_read(&sig->count) != 1);
+	__cleanup_sighand(sig);
+
+	return 0;
+}
+
+struct shared_object_operations cr_shared_sighand_struct_ops = {
+	.export_now        = cr_export_now_sighand_struct,
+	.export_user_info  = NULL,
+	.import_now        = cr_import_now_sighand_struct,
+	.import_complete   = cr_import_complete_sighand_struct,
+	.delete            = cr_delete_sighand_struct,
+};
+
+int epm_sighand_start(void)
+{
+	unsigned long cache_flags = SLAB_PANIC;
+
+#ifdef CONFIG_DEBUG_SLAB
+	cache_flags |= SLAB_POISON;
+#endif
+	sighand_struct_kddm_obj_cachep = KMEM_CACHE(sighand_struct_kddm_object,
+						    cache_flags);
+
+	/*
+	 * Objid 0 is reserved to mark a sighand_struct having not been
+	 * linked to a kddm object yet.
+	 */
+	init_and_set_unique_id_root(&sighand_struct_id_root, 1);
+
+	register_io_linker(SIGHAND_STRUCT_LINKER, &sighand_struct_io_linker);
+
+	sighand_struct_kddm_set =
+		create_new_kddm_set(kddm_def_ns,
+				    SIGHAND_STRUCT_KDDM_ID,
+				    SIGHAND_STRUCT_LINKER,
+				    KDDM_UNIQUE_ID_DEF_OWNER,
+				    0, 0);
+	if (IS_ERR(sighand_struct_kddm_set))
+		OOM;
+
+	return 0;
+}
+
+void epm_sighand_exit(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/epm/signal.c android_cluster/linux-2.6.29/kerrighed/epm/signal.c
--- linux-2.6.29/kerrighed/epm/signal.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/epm/signal.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,1204 @@
+/*
+ *  kerrighed/epm/signal.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2006-2007 Pascal Gallard - Kerlabs, Louis Rilling - Kerlabs
+ */
+
+#include <linux/types.h>
+#include <linux/sched.h>
+#include <linux/signal.h>
+#include <linux/hrtimer.h>
+#include <linux/timer.h>
+#include <linux/posix-timers.h>
+#include <linux/slab.h>
+#ifdef CONFIG_TASKSTATS
+#include <linux/taskstats.h>
+#include <linux/taskstats_kern.h>
+#endif
+#include <linux/nsproxy.h>
+#include <linux/pid_namespace.h>
+#include <linux/pid.h>
+#include <linux/user_namespace.h>
+#include <linux/rwsem.h>
+#include <kerrighed/signal.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/namespace.h>
+#include <kerrighed/libproc.h>
+#include <kerrighed/application.h>
+#include <kerrighed/app_shared.h>
+#include <kerrighed/action.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/ghost_helpers.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+
+struct signal_struct_kddm_object {
+	struct signal_struct *signal;
+	atomic_t count;
+	int keep_on_remove;
+	struct rw_semaphore remove_sem;
+};
+
+static struct kmem_cache *signal_struct_kddm_obj_cachep;
+
+/* Kddm set of 'struct signal_struct' */
+static struct kddm_set *signal_struct_kddm_set;
+
+static struct signal_struct_kddm_object *signal_struct_kddm_object_alloc(void)
+{
+	struct signal_struct_kddm_object *obj;
+
+	obj = kmem_cache_alloc(signal_struct_kddm_obj_cachep, GFP_KERNEL);
+	if (obj) {
+		obj->signal = NULL;
+	/*	atomic_set(&obj->count, 1); */
+		obj->keep_on_remove = 0;
+		init_rwsem(&obj->remove_sem);
+	}
+	return obj;
+}
+
+static struct signal_struct *signal_struct_alloc(void)
+{
+	struct signal_struct *sig;
+
+	sig = kmem_cache_alloc(signal_cachep, GFP_KERNEL);
+	if (!sig)
+		return NULL;
+
+/*	atomic_set(&obj->signal->count, 1); */
+/*	atomic_set(&obj->signal->live, 1); */
+	init_waitqueue_head(&sig->wait_chldexit);
+/*	obj->signal->flags = 0; */
+/*	obj->signal->group_exit_code = 0; */
+	sig->group_exit_task = NULL;
+/*	obj->signal->group_stop_count = 0; */
+	sig->curr_target = NULL;
+	init_sigpending(&sig->shared_pending);
+
+	posix_cpu_timers_init_group(sig);
+	INIT_LIST_HEAD(&sig->posix_timers);
+
+	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	sig->real_timer.function = it_real_fn;
+	sig->leader_pid = NULL;
+
+	sig->tty_old_pgrp = NULL;
+	sig->tty = NULL;
+/*	obj->signal->leader = 0;	/\* session leadership doesn't inherit *\/ */
+#ifdef CONFIG_TASKSTATS
+	sig->stats = NULL;
+#endif
+#ifdef CONFIG_AUDIT
+	sig->tty_audit_buf = NULL;
+#endif
+
+	return sig;
+}
+
+static void signal_struct_attach_object(struct signal_struct *sig,
+					struct signal_struct_kddm_object *obj,
+					objid_t objid)
+{
+	sig->krg_objid = objid;
+	sig->kddm_obj = obj;
+	obj->signal = sig;
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int signal_struct_alloc_object(struct kddm_obj *obj_entry,
+				      struct kddm_set *set, objid_t objid)
+{
+	struct signal_struct_kddm_object *obj;
+	struct signal_struct *sig;
+
+	obj = signal_struct_kddm_object_alloc();
+	if (!obj)
+		return -ENOMEM;
+
+	sig = signal_struct_alloc();
+	if (!sig) {
+		kmem_cache_free(signal_struct_kddm_obj_cachep, obj);
+		return -ENOMEM;
+	}
+
+	signal_struct_attach_object(sig, obj, objid);
+
+	obj_entry->object = obj;
+
+	return 0;
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int signal_struct_first_touch(struct kddm_obj *obj_entry,
+				     struct kddm_set *set, objid_t objid,
+				     int flags)
+{
+	struct signal_struct_kddm_object *obj;
+
+	obj = signal_struct_kddm_object_alloc();
+	if (!obj)
+		return -ENOMEM;
+	atomic_set(&obj->count, 1);
+
+	obj_entry->object = obj;
+
+	return 0;
+}
+
+/*
+ * Lock on the dest signal_struct must be held. No other access
+ * to dest is allowed in the import time.
+ * @author Pascal Gallard
+ */
+static int signal_struct_import_object(struct rpc_desc *desc,
+				       struct kddm_set *set,
+				       struct kddm_obj *obj_entry,
+				       objid_t objid,
+				       int flags)
+{
+	struct signal_struct_kddm_object *obj = obj_entry->object;
+	struct signal_struct *dest = obj->signal;
+	struct signal_struct tmp_sig;
+	int retval;
+
+	retval = rpc_unpack_type(desc, tmp_sig);
+	if (retval)
+		return retval;
+#ifdef CONFIG_TASKSTATS
+	if (tmp_sig.stats) {
+		retval = -ENOMEM;
+		tmp_sig.stats = kmem_cache_alloc(taskstats_cache, GFP_KERNEL);
+		if (!tmp_sig.stats)
+			return retval;
+		retval = rpc_unpack_type(desc, *tmp_sig.stats);
+		if (retval) {
+			kmem_cache_free(taskstats_cache, tmp_sig.stats);
+			return retval;
+		}
+	}
+#endif
+	retval = rpc_unpack_type(desc, obj->count);
+	if (retval)
+		return retval;
+
+	/* We are only modifying a copy of the real signal struct. All pointers
+	 * should be left NULL. */
+	/* TODO: with distributed threads this will need more locking */
+	atomic_set(&dest->count, atomic_read(&tmp_sig.count));
+	atomic_set(&dest->live, atomic_read(&tmp_sig.live));
+
+	dest->group_exit_code = tmp_sig.group_exit_code;
+	dest->notify_count = tmp_sig.notify_count;
+	dest->group_stop_count = tmp_sig.group_stop_count;
+	dest->flags = tmp_sig.flags;
+
+	dest->it_real_incr = tmp_sig.it_real_incr;
+	dest->it_prof_expires = tmp_sig.it_prof_expires;
+	dest->it_virt_expires = tmp_sig.it_virt_expires;
+	dest->it_prof_incr = tmp_sig.it_prof_incr;
+	dest->it_virt_incr = tmp_sig.it_virt_incr;
+	dest->cputimer.cputime = tmp_sig.cputimer.cputime;
+	dest->cputimer.running = tmp_sig.cputimer.running;
+	dest->cputime_expires = tmp_sig.cputime_expires;
+
+	dest->leader = tmp_sig.leader;
+
+	dest->utime = tmp_sig.utime;
+	dest->stime = tmp_sig.stime;
+	dest->cutime = tmp_sig.cutime;
+	dest->cstime = tmp_sig.cstime;
+	dest->gtime = tmp_sig.gtime;
+	dest->cgtime = tmp_sig.cgtime;
+	dest->nvcsw = tmp_sig.nvcsw;
+	dest->nivcsw = tmp_sig.nivcsw;
+	dest->cnvcsw = tmp_sig.cnvcsw;
+	dest->cnivcsw = tmp_sig.cnivcsw;
+	dest->min_flt = tmp_sig.min_flt;
+	dest->maj_flt = tmp_sig.maj_flt;
+	dest->cmin_flt = tmp_sig.cmin_flt;
+	dest->cmaj_flt = tmp_sig.cmaj_flt;
+	dest->inblock = tmp_sig.inblock;
+	dest->oublock = tmp_sig.oublock;
+	dest->cinblock = tmp_sig.cinblock;
+	dest->coublock = tmp_sig.coublock;
+	/* ioac may be an empty struct */
+	if (sizeof(dest->ioac))
+		dest->ioac = tmp_sig.ioac;
+
+	dest->sum_sched_runtime = tmp_sig.sum_sched_runtime;
+
+	memcpy(dest->rlim, tmp_sig.rlim, sizeof(dest->rlim));
+#ifdef CONFIG_BSD_PROCESS_ACCT
+	dest->pacct = tmp_sig.pacct;
+#endif
+#ifdef CONFIG_TASKSTATS
+	if (tmp_sig.stats) {
+		if (dest->stats) {
+			memcpy(dest->stats, tmp_sig.stats,
+			       sizeof(*dest->stats));
+			kmem_cache_free(taskstats_cache, tmp_sig.stats);
+		} else {
+			dest->stats = tmp_sig.stats;
+		}
+	}
+#endif
+#ifdef CONFIG_AUDIT
+	dest->audit_tty = tmp_sig.audit_tty;
+#endif
+
+	return 0;
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int signal_struct_export_object(struct rpc_desc *desc,
+				       struct kddm_set *set,
+				       struct kddm_obj *obj_entry,
+				       objid_t objid,
+				       int _flags)
+{
+	struct signal_struct_kddm_object *obj = obj_entry->object;
+	struct task_struct *tsk;
+	unsigned long flags;
+	int retval;
+
+	rcu_read_lock();
+	tsk = find_task_by_kpid(obj->signal->krg_objid);
+	/*
+	 * We may find no task in the middle of a migration. In that case, kddm
+	 * locking is enough since neither userspace nor the kernel will access
+	 * this copy.
+	 */
+	if (tsk)
+		get_task_struct(tsk);
+	rcu_read_unlock();
+	if (tsk && !lock_task_sighand(tsk, &flags))
+		BUG();
+	retval = rpc_pack_type(desc, *obj->signal);
+#ifdef CONFIG_TASKSTATS
+	if (!retval && obj->signal->stats)
+		retval = rpc_pack_type(desc, *obj->signal->stats);
+#endif
+	if (tsk) {
+		unlock_task_sighand(tsk, &flags);
+		put_task_struct(tsk);
+	}
+	if (!retval)
+		retval = rpc_pack_type(desc, obj->count);
+
+	return retval;
+}
+
+void krg_signal_pin(struct signal_struct *sig)
+{
+	struct signal_struct_kddm_object *obj = sig->kddm_obj;
+	BUG_ON(!obj);
+	down_read(&obj->remove_sem);
+}
+
+void krg_signal_unpin(struct signal_struct *sig)
+{
+	struct signal_struct_kddm_object *obj = sig->kddm_obj;
+	BUG_ON(!obj);
+	up_read(&obj->remove_sem);
+}
+
+static int signal_struct_remove_object(void *object,
+				       struct kddm_set *set, objid_t objid)
+{
+	struct signal_struct_kddm_object *obj = object;
+
+	/* Ensure that no thread uses this signal_struct copy */
+	down_write(&obj->remove_sem);
+	up_write(&obj->remove_sem);
+
+	if (!obj->keep_on_remove) {
+		struct signal_struct *sig = obj->signal;
+
+		WARN_ON(!list_empty(&sig->shared_pending.list));
+		flush_sigqueue(&sig->shared_pending);
+#ifdef CONFIG_TASKSTATS
+		taskstats_tgid_free(sig);
+#endif
+#ifdef CONFIG_AUDIT
+		BUG_ON(sig->tty_audit_buf);
+#endif
+		put_pid(sig->tty_old_pgrp);
+		kmem_cache_free(signal_cachep, sig);
+	}
+	kmem_cache_free(signal_struct_kddm_obj_cachep, obj);
+
+	return 0;
+}
+
+static struct iolinker_struct signal_struct_io_linker = {
+	.first_touch   = signal_struct_first_touch,
+	.linker_name   = "sig ",
+	.linker_id     = SIGNAL_STRUCT_LINKER,
+	.alloc_object  = signal_struct_alloc_object,
+	.export_object = signal_struct_export_object,
+	.import_object = signal_struct_import_object,
+	.remove_object = signal_struct_remove_object,
+	.default_owner = global_pid_default_owner,
+};
+
+static
+struct signal_struct_kddm_object *
+____krg_signal_alloc(struct signal_struct *sig, objid_t id)
+{
+	struct signal_struct_kddm_object *obj;
+
+	/* Create the signal object */
+	obj = _kddm_grab_object(signal_struct_kddm_set, id);
+	BUG_ON(!obj);
+	/* Must be a first touch */
+	BUG_ON(obj->signal);
+	signal_struct_attach_object(sig, obj, id);
+
+	return obj;
+}
+
+static struct signal_struct *cr_signal_alloc(objid_t id)
+{
+	struct signal_struct_kddm_object *obj;
+	struct signal_struct *sig;
+
+	sig = signal_struct_alloc();
+	if (!sig)
+		return NULL;
+
+	obj = ____krg_signal_alloc(sig, id);
+	BUG_ON(!obj);
+
+	return sig;
+}
+
+static void cr_signal_free(struct signal_struct *sig)
+{
+	_kddm_remove_frozen_object(signal_struct_kddm_set, sig->krg_objid);
+}
+
+static void __krg_signal_alloc(struct task_struct *task, struct pid *pid)
+{
+	struct signal_struct_kddm_object *obj;
+	struct signal_struct *sig = task->signal;
+	pid_t tgid = pid_knr(pid);
+
+	/*
+	 * Exclude kernel threads and local pids from using signal_struct
+	 * kddm objects.
+	 */
+	/*
+	 * At this stage, task->mm may point to the mm of a
+	 * task being duplicated instead of the mm of task for which this struct
+	 * is being allocated, but we only need to know whether it is NULL or
+	 * not, which will be the same after copy_mm.
+	 */
+	if (!(tgid & GLOBAL_PID_MASK) || !task->mm) {
+		BUG_ON(krg_current);
+		sig->krg_objid = 0;
+		sig->kddm_obj = NULL;
+		return;
+	}
+
+	obj = ____krg_signal_alloc(sig, tgid);
+	BUG_ON(!obj);
+	krg_signal_unlock(sig);
+}
+
+/*
+ * Alloc a dedicated signal_struct to task_struct task.
+ * @author Pascal Gallard
+ */
+void krg_signal_alloc(struct task_struct *task, struct pid *pid,
+		      unsigned long clone_flags)
+{
+	if (!task->nsproxy->krg_ns)
+		return;
+
+	if (krg_current && !in_krg_do_fork())
+		/*
+		 * This is a process migration or restart: signal_struct is
+		 * already setup.
+		 */
+		return;
+
+	if (!krg_current && (clone_flags & CLONE_THREAD))
+		/* New thread: already done in copy_signal() */
+		return;
+
+	__krg_signal_alloc(task, pid);
+}
+
+/*
+ * Get and lock a signal structure for a given process
+ * @author Pascal Gallard
+ */
+static struct signal_struct_kddm_object *__krg_signal_readlock(objid_t id)
+{
+	struct signal_struct_kddm_object *obj;
+
+	obj = _kddm_get_object_no_ft(signal_struct_kddm_set, id);
+	if (!obj) {
+		_kddm_put_object(signal_struct_kddm_set, id);
+		return NULL;
+	}
+	BUG_ON(!obj->signal);
+
+	return obj;
+}
+
+struct signal_struct *krg_signal_readlock(struct signal_struct *sig)
+{
+	struct signal_struct_kddm_object *obj;
+	objid_t id = sig->krg_objid;
+
+	/* Filter well known cases of no signal_struct kddm object. */
+	if (!sig->kddm_obj)
+		return NULL;
+
+	obj = __krg_signal_readlock(id);
+	if (!obj)
+		return NULL;
+
+	return obj->signal;
+}
+
+static struct signal_struct_kddm_object *__krg_signal_writelock(objid_t id)
+{
+	struct signal_struct_kddm_object *obj;
+
+	obj = _kddm_grab_object_no_ft(signal_struct_kddm_set, id);
+	if (!obj) {
+		_kddm_put_object(signal_struct_kddm_set, id);
+		return NULL;
+	}
+	BUG_ON(!obj->signal);
+	return obj;
+}
+
+/*
+ * Grab and lock a signal structure for a given process
+ * @author Pascal Gallard
+ */
+struct signal_struct *krg_signal_writelock(struct signal_struct *sig)
+{
+	struct signal_struct_kddm_object *obj;
+	objid_t id = sig->krg_objid;
+
+	/* Filter well known cases of no signal_struct kddm object. */
+	if (!sig->kddm_obj)
+		return NULL;
+
+	obj = __krg_signal_writelock(id);
+	if (!obj)
+		return NULL;
+
+	return obj->signal;
+}
+
+/*
+ * unlock a signal structure for a given process
+ * @author Pascal Gallard
+ */
+void krg_signal_unlock(struct signal_struct *sig)
+{
+	if (sig)
+		_kddm_put_object(signal_struct_kddm_set, sig->krg_objid);
+}
+
+/* Assumes that the associated kddm object is write locked. */
+void krg_signal_share(struct signal_struct *sig)
+{
+	struct signal_struct_kddm_object *obj = sig->kddm_obj;
+	int count;
+
+	count = atomic_inc_return(&obj->count);
+}
+
+struct signal_struct *krg_signal_exit(struct signal_struct *sig)
+{
+	objid_t id = sig->krg_objid;
+	struct signal_struct_kddm_object *obj;
+	int count;
+
+	if (!sig->kddm_obj)
+		return NULL;
+
+	obj = __krg_signal_writelock(id);
+	BUG_ON(obj != sig->kddm_obj);
+	count = atomic_dec_return(&obj->count);
+	if (count == 0) {
+		krg_signal_unlock(sig);
+		BUG_ON(obj->keep_on_remove);
+		/* Free the kddm object but keep the signal_struct so that
+		 * __exit_signal releases it properly. */
+		obj->keep_on_remove = 1;
+		_kddm_remove_object(signal_struct_kddm_set, id);
+
+		return NULL;
+	}
+
+	return sig;
+}
+
+/* EPM actions */
+
+/* individual struct sigpending */
+
+static int export_sigqueue(ghost_t *ghost,
+			   struct task_struct *task,
+			   struct sigqueue *sig)
+{
+	int err = -EBUSY;
+
+	if (sig->user->user_ns != task->nsproxy->krg_ns->root_user_ns)
+		goto out;
+
+	err = ghost_write(ghost, &sig->info, sizeof(sig->info));
+	if (err)
+		goto out;
+	err = ghost_write(ghost, &sig->user->uid, sizeof(sig->user->uid));
+
+out:
+	return err;
+}
+
+static int import_sigqueue(ghost_t *ghost,
+			   struct task_struct *task,
+			   struct sigqueue *sig)
+{
+	struct user_struct *user;
+	uid_t uid;
+	int err;
+
+	err = ghost_read(ghost, &sig->info, sizeof(sig->info));
+	if (err)
+		goto out;
+
+	err = ghost_read(ghost, &uid, sizeof(uid));
+	if (err)
+		goto out;
+	user = alloc_uid(task->nsproxy->krg_ns->root_user_ns, uid);
+	if (!user) {
+		err = -ENOMEM;
+		goto out;
+	}
+	atomic_inc(&user->sigpending);
+
+	atomic_dec(&sig->user->sigpending);
+	free_uid(sig->user);
+
+	sig->user = user;
+
+out:
+	return err;
+}
+
+static int export_sigpending(ghost_t *ghost,
+			     struct task_struct *task,
+			     struct sigpending *pending)
+{
+	struct sigpending tmp_queue;
+	int nr_sig;
+	struct sigqueue *q;
+	unsigned long flags;
+	int err;
+
+	INIT_LIST_HEAD(&tmp_queue.list);
+	nr_sig = 0;
+	if (!lock_task_sighand(task, &flags))
+		BUG();
+	tmp_queue.signal = pending->signal;
+	list_for_each_entry(q, &pending->list, list) {
+		if (q->flags & SIGQUEUE_PREALLOC) {
+			unlock_task_sighand(task, &flags);
+			err = -EBUSY;
+			goto out;
+		}
+		nr_sig++;
+	}
+	list_splice_init(&pending->list, &tmp_queue.list);
+	unlock_task_sighand(task, &flags);
+
+	err = ghost_write(ghost, &tmp_queue.signal, sizeof(tmp_queue.signal));
+	if (err)
+		goto out_splice;
+
+	err = ghost_write(ghost, &nr_sig, sizeof(nr_sig));
+	if (err)
+		goto out_splice;
+
+	list_for_each_entry(q, &tmp_queue.list, list) {
+		err = export_sigqueue(ghost, task, q);
+		if (err)
+			goto out_splice;
+	}
+
+out_splice:
+	if (!lock_task_sighand(task, &flags))
+		BUG();
+	sigorsets(&pending->signal, &pending->signal, &tmp_queue.signal);
+	list_splice(&tmp_queue.list, &pending->list);
+	recalc_sigpending_tsk(task);
+	unlock_task_sighand(task, &flags);
+
+out:
+	return err;
+}
+
+static int import_sigpending(ghost_t *ghost,
+			     struct task_struct *task,
+			     struct sigpending *pending)
+{
+	int nr_sig;
+	struct sigqueue *q;
+	int i;
+	int err;
+
+	err = ghost_read(ghost, &pending->signal, sizeof(pending->signal));
+	if (err)
+		goto cleanup_queue;
+
+	err = ghost_read(ghost, &nr_sig, sizeof(nr_sig));
+	if (err)
+		goto cleanup_queue;
+
+	INIT_LIST_HEAD(&pending->list);
+	for (i = 0; i < nr_sig; i++) {
+		q = __sigqueue_alloc(current, GFP_KERNEL, 0);
+		if (!q) {
+			err = -ENOMEM;
+			goto free_queue;
+		}
+		err = import_sigqueue(ghost, task, q);
+		if (err) {
+			__sigqueue_free(q);
+			goto free_queue;
+		}
+		list_add_tail(&q->list, &pending->list);
+	}
+
+out:
+	return err;
+
+cleanup_queue:
+	init_sigpending(pending);
+	goto out;
+
+free_queue:
+	flush_sigqueue(pending);
+	goto out;
+}
+
+static void unimport_sigpending(struct task_struct *task,
+				struct sigpending *pending)
+{
+	flush_sigqueue(pending);
+}
+
+/* shared signals (struct signal_struct) */
+
+static int export_posix_timers(ghost_t *ghost, struct task_struct *task)
+{
+	int err = 0;
+	spin_lock_irq(&task->sighand->siglock);
+	if (!list_empty(&task->signal->posix_timers))
+		err = -EBUSY;
+	spin_unlock_irq(&task->sighand->siglock);
+	return err;
+}
+
+static int import_posix_timers(ghost_t *ghost, struct task_struct *task)
+{
+	BUG_ON(!list_empty(&task->signal->posix_timers));
+	return 0;
+}
+
+static void unimport_posix_timers(struct task_struct *task)
+{
+}
+
+#ifdef CONFIG_TASKSTATS
+static int cr_export_taskstats(ghost_t *ghost, struct signal_struct *sig)
+{
+	return ghost_write(ghost, sig->stats, sizeof(*sig->stats));
+}
+
+static int cr_import_taskstats(ghost_t *ghost, struct signal_struct *sig)
+{
+	struct taskstats *stats;
+	int err = -ENOMEM;
+
+	stats = kmem_cache_alloc(taskstats_cache, GFP_KERNEL);
+	if (!stats)
+		goto out;
+
+	err = ghost_read(ghost, stats, sizeof(*stats));
+	if (!err)
+		sig->stats = stats;
+	else
+		kmem_cache_free(taskstats_cache, stats);
+
+out:
+	return err;
+}
+#endif
+
+static int cr_export_later_signal_struct(struct epm_action *action,
+					 ghost_t *ghost,
+					 struct task_struct *task)
+{
+	int r;
+	long key;
+
+	BUG_ON(action->type != EPM_CHECKPOINT);
+	BUG_ON(action->checkpoint.shared != CR_SAVE_LATER);
+
+	key = (long)(task->signal->krg_objid);
+
+	r = ghost_write(ghost, &key, sizeof(long));
+	if (r)
+		goto err;
+
+	r = add_to_shared_objects_list(task->application,
+				       SIGNAL_STRUCT, key, LOCAL_ONLY,
+				       task, NULL, 0);
+
+	if (r == -ENOKEY) /* the signal_struct was already in the list */
+		r = 0;
+err:
+	return r;
+}
+
+int export_signal_struct(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *tsk)
+{
+	objid_t krg_objid = tsk->signal->krg_objid;
+	int r;
+
+	if (action->type == EPM_CHECKPOINT
+	    && action->checkpoint.shared == CR_SAVE_LATER) {
+		r = cr_export_later_signal_struct(action, ghost, tsk);
+		return r;
+	}
+
+	r = ghost_write(ghost, &krg_objid, sizeof(krg_objid));
+	if (r)
+		goto err_write;
+
+	switch (action->type) {
+	case EPM_MIGRATE:
+		r = export_sigpending(ghost, tsk, &tsk->signal->shared_pending);
+		if (r)
+			goto err_write;
+		r = export_posix_timers(ghost, tsk);
+		break;
+	case EPM_CHECKPOINT: {
+		struct signal_struct *sig =
+			krg_signal_readlock(tsk->signal);
+		r = ghost_write(ghost, sig, sizeof(*sig));
+		if (!r)
+			r = export_sigpending(ghost,
+					      tsk,
+					      &tsk->signal->shared_pending);
+#ifdef CONFIG_TASKSTATS
+		if (!r && sig->stats)
+			r = cr_export_taskstats(ghost, sig);
+#endif
+		if (!r)
+			r = export_posix_timers(ghost, tsk);
+		krg_signal_unlock(sig);
+		break;
+	} default:
+		break;
+	}
+
+err_write:
+	return r;
+}
+
+static int cr_link_to_signal_struct(struct epm_action *action,
+				    ghost_t *ghost,
+				    struct task_struct *tsk)
+{
+	int r;
+	long key;
+	struct signal_struct *sig;
+
+	r = ghost_read(ghost, &key, sizeof(long));
+	if (r)
+		goto err;
+
+	sig = get_imported_shared_object(action->restart.app,
+					 SIGNAL_STRUCT, key);
+
+	if (!sig) {
+		r = -E_CR_BADDATA;
+		goto err;
+	}
+
+	if (!sig->leader_pid) {
+		BUG_ON(tsk->tgid != tsk->pid);
+		sig->leader_pid = task_pid(tsk);
+	}
+
+	tsk->signal = sig;
+
+	krg_signal_writelock(sig);
+	atomic_inc(&tsk->signal->count);
+	atomic_inc(&tsk->signal->live);
+	krg_signal_share(sig);
+	krg_signal_unlock(sig);
+err:
+	return r;
+}
+
+int import_signal_struct(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *tsk)
+{
+	objid_t krg_objid;
+	struct signal_struct_kddm_object *obj;
+	struct signal_struct *sig;
+	int r;
+
+	if (action->type == EPM_CHECKPOINT
+	    && action->restart.shared == CR_LINK_ONLY) {
+		r = cr_link_to_signal_struct(action, ghost, tsk);
+		return r;
+	}
+
+	r = ghost_read(ghost, &krg_objid, sizeof(krg_objid));
+	if (r)
+		goto err_read;
+
+	switch (action->type) {
+	case EPM_MIGRATE:
+		/* TODO: this will need more locking with distributed threads */
+		obj = __krg_signal_writelock(krg_objid);
+		BUG_ON(!obj);
+		sig = obj->signal;
+		BUG_ON(!sig);
+
+		WARN_ON(sig->group_exit_task);
+		WARN_ON(!list_empty(&sig->shared_pending.list));
+		flush_sigqueue(&sig->shared_pending);
+		r = import_sigpending(ghost, tsk, &sig->shared_pending);
+		if (r)
+			goto out_mig_unlock;
+
+		if (!sig->leader_pid)
+			sig->leader_pid = task_pid(tsk);
+
+		/*
+		 * This will need proper tty handling once global control ttys
+		 * will exist.
+		 */
+		sig->tty = NULL;
+
+		tsk->signal = sig;
+
+		r = import_posix_timers(ghost, tsk);
+
+out_mig_unlock:
+		krg_signal_unlock(sig);
+		break;
+
+	case EPM_REMOTE_CLONE:
+		/*
+		 * The structure will be partly copied when creating the
+		 * active process.
+		 */
+		obj = __krg_signal_readlock(krg_objid);
+		BUG_ON(!obj);
+		sig = obj->signal;
+		BUG_ON(!sig);
+		krg_signal_unlock(sig);
+		tsk->signal = sig;
+		break;
+
+	case EPM_CHECKPOINT: {
+		struct signal_struct tmp_sig;
+
+		sig = cr_signal_alloc(krg_objid);
+
+		r = ghost_read(ghost, &tmp_sig, sizeof(tmp_sig));
+		if (r)
+			goto err_free_signal;
+
+		atomic_set(&sig->count, 1);
+		atomic_set(&sig->live, 1);
+
+		sig->group_exit_code = tmp_sig.group_exit_code;
+		WARN_ON(tmp_sig.group_exit_task);
+		sig->notify_count = tmp_sig.notify_count;
+		sig->group_stop_count = tmp_sig.group_stop_count;
+		sig->flags = tmp_sig.flags;
+
+		r = import_sigpending(ghost, tsk, &sig->shared_pending);
+		if (r)
+			goto err_free_signal;
+
+		sig->it_real_incr = tmp_sig.it_real_incr;
+		sig->it_prof_expires = tmp_sig.it_prof_expires;
+		sig->it_virt_expires = tmp_sig.it_virt_expires;
+		sig->it_prof_incr = tmp_sig.it_prof_incr;
+		sig->it_virt_incr = tmp_sig.it_virt_incr;
+		sig->cputimer.cputime = tmp_sig.cputimer.cputime;
+		sig->cputimer.running = tmp_sig.cputimer.running;
+		sig->cputime_expires = tmp_sig.cputime_expires;
+
+		/*
+		 * This will need proper tty handling once global control ttys
+		 * will exist.
+		 * The IO-linker already initialized those fields to NULL.
+		 */
+		/* sig->tty = NULL; */
+		/* sig->tty_old_pgrp = NULL; */
+
+		sig->leader = tmp_sig.leader;
+
+		sig->utime = tmp_sig.utime;
+		sig->stime = tmp_sig.stime;
+		sig->cutime = tmp_sig.cutime;
+		sig->cstime = tmp_sig.cstime;
+		sig->gtime = tmp_sig.gtime;
+		sig->cgtime = tmp_sig.cgtime;
+		sig->nvcsw = tmp_sig.nvcsw;
+		sig->nivcsw = tmp_sig.nivcsw;
+		sig->cnvcsw = tmp_sig.cnvcsw;
+		sig->cnivcsw = tmp_sig.cnivcsw;
+		sig->min_flt = tmp_sig.min_flt;
+		sig->maj_flt = tmp_sig.maj_flt;
+		sig->cmin_flt = tmp_sig.cmin_flt;
+		sig->cmaj_flt = tmp_sig.cmaj_flt;
+		sig->inblock = tmp_sig.inblock;
+		sig->oublock = tmp_sig.oublock;
+		sig->cinblock = tmp_sig.cinblock;
+		sig->coublock = tmp_sig.coublock;
+		/* ioac may be an empty struct */
+		if (sizeof(sig->ioac))
+			sig->ioac = tmp_sig.ioac;
+
+		sig->sum_sched_runtime = tmp_sig.sum_sched_runtime;
+
+		memcpy(sig->rlim, tmp_sig.rlim, sizeof(sig->rlim));
+#ifdef CONFIG_BSD_PROCESS_ACCT
+		sig->pacct = tmp_sig.pacct;
+#endif
+#ifdef CONFIG_TASKSTATS
+		if (tmp_sig.stats) {
+			r = cr_import_taskstats(ghost, sig);
+			if (r)
+				goto err_free_signal;
+		}
+#endif
+#ifdef CONFIG_AUDIT
+		sig->audit_tty = tmp_sig.audit_tty;
+#endif
+
+		tsk->signal = sig;
+
+		r = import_posix_timers(ghost, tsk);
+		if (r)
+			goto err_free_signal;
+
+		krg_signal_unlock(sig);
+		break;
+
+err_free_signal:
+		cr_signal_free(sig);
+		goto err_read;
+
+	} default:
+		PANIC("Case not supported: %d\n", action->type);
+	}
+
+err_read:
+	return r;
+}
+
+void unimport_signal_struct(struct task_struct *task)
+{
+	/*
+	 * TODO: for restart, we must free the created kddm signal_struct
+	 * object.
+	 */
+	unimport_posix_timers(task);
+	unimport_sigpending(task, &task->signal->shared_pending);
+}
+
+static int cr_export_now_signal_struct(struct epm_action *action,
+				       ghost_t *ghost,
+				       struct task_struct *task,
+				       union export_args *args)
+{
+	int r;
+	r = export_signal_struct(action, ghost, task);
+	if (r)
+		ckpt_err(action, r,
+			 "Fail to save struct signal_struct of process %d (%s)",
+			 task_pid_knr(task), task->comm);
+	return r;
+}
+
+static int cr_import_now_signal_struct(struct epm_action *action,
+				       ghost_t *ghost,
+				       struct task_struct *fake,
+				       int local_only,
+				       void **returned_data,
+				       size_t *data_size)
+{
+	int r;
+	BUG_ON(*returned_data != NULL);
+
+	r = import_signal_struct(action, ghost, fake);
+	if (r) {
+		ckpt_err(action, r,
+			 "App %ld - Fail to restore a struct signal_struct",
+			 action->restart.app->app_id);
+		goto err;
+	}
+
+	*returned_data = fake->signal;
+err:
+	return r;
+}
+
+static int cr_import_complete_signal_struct(struct task_struct *fake,
+					    void *_sig)
+{
+	struct signal_struct *sig = _sig;
+	struct signal_struct *locked_sig;
+
+	locked_sig = krg_signal_exit(sig);
+
+	atomic_dec(&sig->count);
+	atomic_dec(&sig->live);
+
+	krg_signal_unlock(locked_sig);
+
+	return 0;
+}
+
+static int cr_delete_signal_struct(struct task_struct *fake, void *_sig)
+{
+	struct signal_struct *sig = _sig;
+	struct signal_struct *locked_sig;
+
+	fake->signal = sig;
+	/*
+	 * Prevent
+	 * posix_cpu_timers_exit_group()
+	 *   thread_group_cputimer()
+	 *     thread_group_cputime()
+	 * from running through the thread group.
+	 */
+	fake->sighand = NULL;
+	INIT_LIST_HEAD(&fake->cpu_timers[0]);
+	INIT_LIST_HEAD(&fake->cpu_timers[1]);
+	INIT_LIST_HEAD(&fake->cpu_timers[2]);
+
+	locked_sig = krg_signal_exit(sig);
+
+	atomic_dec(&sig->count);
+	atomic_dec(&sig->live);
+
+	krg_signal_unlock(locked_sig);
+
+	posix_cpu_timers_exit_group(fake);
+
+	flush_sigqueue(&sig->shared_pending);
+	taskstats_tgid_free(sig);
+	__cleanup_signal(sig);
+
+	return 0;
+}
+
+struct shared_object_operations cr_shared_signal_struct_ops = {
+	.export_now        = cr_export_now_signal_struct,
+	.export_user_info  = NULL,
+	.import_now        = cr_import_now_signal_struct,
+	.import_complete   = cr_import_complete_signal_struct,
+	.delete            = cr_delete_signal_struct,
+};
+
+/* private signals */
+
+int export_private_signals(struct epm_action *action,
+			   ghost_t *ghost,
+			   struct task_struct *task)
+{
+	int err = 0;
+
+	switch (action->type) {
+	case EPM_MIGRATE:
+	case EPM_CHECKPOINT:
+		err = export_sigpending(ghost, task, &task->pending);
+		break;
+	default:
+		break;
+	}
+
+	return err;
+}
+
+int import_private_signals(struct epm_action *action,
+			   ghost_t *ghost,
+			   struct task_struct *task)
+{
+	int err = 0;
+
+	switch (action->type) {
+	case EPM_MIGRATE:
+	case EPM_CHECKPOINT:
+		err = import_sigpending(ghost, task, &task->pending);
+		break;
+	default:
+		init_sigpending(&task->pending);
+		break;
+	}
+
+	return err;
+}
+
+void unimport_private_signals(struct task_struct *task)
+{
+	unimport_sigpending(task, &task->pending);
+}
+
+int epm_signal_start(void)
+{
+	unsigned long cache_flags = SLAB_PANIC;
+
+#ifdef CONFIG_DEBUG_SLAB
+	cache_flags |= SLAB_POISON;
+#endif
+	signal_struct_kddm_obj_cachep = KMEM_CACHE(signal_struct_kddm_object,
+						   cache_flags);
+
+	register_io_linker(SIGNAL_STRUCT_LINKER, &signal_struct_io_linker);
+
+	signal_struct_kddm_set = create_new_kddm_set(kddm_def_ns,
+						     SIGNAL_STRUCT_KDDM_ID,
+						     SIGNAL_STRUCT_LINKER,
+						     KDDM_CUSTOM_DEF_OWNER,
+						     0,
+						     KDDM_LOCAL_EXCLUSIVE);
+	if (IS_ERR(signal_struct_kddm_set))
+		OOM;
+
+	return 0;
+}
+
+void epm_signal_exit(void)
+{
+	return;
+}
diff -ruN linux-2.6.29/kerrighed/fs/faf/faf.c android_cluster/linux-2.6.29/kerrighed/fs/faf/faf.c
--- linux-2.6.29/kerrighed/fs/faf/faf.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/faf/faf.c	2014-06-09 18:19:41.936331641 -0700
@@ -0,0 +1,283 @@
+/** Kerrighed Open File Access Forwarding System.
+ *  @file faf.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/file.h>
+#include <linux/fdtable.h>
+#include <linux/magic.h>
+#include <linux/socket.h>
+#include <linux/sched.h>
+
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/action.h>
+#include <kerrighed/faf.h>
+#include <kerrighed/file.h>
+#include "faf_internal.h"
+#include "faf_server.h"
+#include "faf_hooks.h"
+
+extern struct kmem_cache *faf_client_data_cachep;
+
+/*****************************************************************************/
+/*                                                                           */
+/*                             INTERFACE FUNCTIONS                           */
+/*                                                                           */
+/*****************************************************************************/
+
+/** Add a file in the FAF daemon.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file       The file to add in the FAF daemon
+ *
+ *  @return   0 if everything ok.
+ *            Negative value otherwise.
+ */
+int setup_faf_file(struct file *file)
+{
+	int server_fd = 0;
+	int res = 0;
+	struct files_struct *files = first_krgrpc->files;
+
+	/* Install the file in the destination task file array */
+	if (file->f_flags & O_FAF_SRV) {
+		res = -EALREADY;
+		goto out;
+	}
+
+	server_fd = __get_unused_fd(first_krgrpc);
+	if (server_fd < 0) {
+		res = server_fd;
+		goto out;
+	}
+
+	spin_lock(&files->file_lock);
+	if (unlikely(file->f_flags & O_FAF_SRV)) {
+		__put_unused_fd(files, server_fd);
+		res = -EALREADY;
+	} else {
+		file->f_flags |= O_FAF_SRV;
+		get_file(file);
+		file->f_faf_srv_index = server_fd;
+		__fd_install(files, server_fd, file);
+	}
+	spin_unlock(&files->file_lock);
+
+out:
+	return res;
+}
+
+/** Close a file in the FAF deamon.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file    The file to close.
+ */
+int close_faf_file(struct file * file)
+{
+        struct files_struct *files = first_krgrpc->files;
+        struct file * faf_file;
+        struct fdtable *fdt;
+        int fd = file->f_faf_srv_index;
+
+	BUG_ON (!(file->f_flags & O_FAF_SRV));
+	BUG_ON (file_count(file) != 1);
+
+	/* Remove the file from the FAF server file table */
+
+	spin_lock(&files->file_lock);
+
+        fdt = files_fdtable(files);
+        if (fd >= fdt->max_fds)
+                BUG();
+        faf_file = fdt->fd[fd];
+        if (!faf_file)
+                BUG();
+        BUG_ON (faf_file != file);
+
+        rcu_assign_pointer(fdt->fd[fd], NULL);
+        FD_CLR(fd, fdt->close_on_exec);
+        __put_unused_fd(files, fd);
+
+	spin_unlock(&files->file_lock);
+
+	/* Cleanup Kerrighed flags but not objid to pass through the regular
+	 * kernel close file code plus kh_put_file() only.
+	 */
+	file->f_flags = file->f_flags & (~O_FAF_SRV);
+
+        return filp_close(faf_file, files);
+}
+
+/** Check if we need to close a FAF server file.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file         The file to check.
+ *
+ *  We can close a FAF server file if local f_count == 1 and DVFS count == 1.
+ *  This means the FAF server is the last process cluster wide using the file.
+ */
+void check_close_faf_srv_file(struct file *file)
+{
+	struct dvfs_file_struct *dvfs_file;
+	unsigned long objid = file->f_objid;
+	int close_file = 0;
+
+	/* Pre-check the file count to avoid a useless call to get_dvfs */
+	if (file_count (file) != 1)
+		return;
+
+	dvfs_file = get_dvfs_file_struct(objid);
+	/* If dvfs file is NULL, someone else did the job before us */
+	if (dvfs_file->file == NULL)
+		goto done;
+	BUG_ON (dvfs_file->file != file);
+
+	/* Re-check f_count in case it changed during the get_dvfs */
+	if ((dvfs_file->count == 1) && (file_count (file) == 1)) {
+		/* The FAF server file is the last one used in the cluster.
+		 * We can now close it.
+		 */
+		close_file = 1;
+		dvfs_file->file = NULL;
+	}
+
+done:
+	put_dvfs_file_struct (objid);
+
+	if (close_file)
+		close_faf_file(file);
+}
+
+void free_faf_file_private_data(struct file *file)
+{
+	if (!(file->f_flags & O_FAF_CLT))
+		return;
+
+	kmem_cache_free (faf_client_data_cachep, file->private_data);
+	file->private_data = NULL;
+}
+
+/** Check if we are closing the last FAF client file.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file         The file attached to the DVFS struct.
+ *  @param dvfs_file    The DVFS file struct being put.
+ */
+void check_last_faf_client_close(struct file *file,
+				 struct dvfs_file_struct *dvfs_file)
+{
+	faf_client_data_t *data;
+	struct faf_notify_msg msg;
+
+	if(!(file->f_flags & O_FAF_CLT))
+		return;
+
+	/* If DVFS count == 1, there is no more FAF clients, the last count
+	 * being for the FAF server node. In this case, notify the FAF server
+	 * to let it check if it should close the FAF file or not.
+	 */
+	if (dvfs_file->count == 1) {
+		data = file->private_data;
+		msg.server_fd = data->server_fd;
+		msg.objid = file->f_objid;
+
+		rpc_async(RPC_FAF_NOTIFY_CLOSE, data->server_id,
+			  &msg, sizeof(msg));
+	}
+
+	free_faf_file_private_data(file);
+}
+
+int setup_faf_file_if_needed(struct file *file)
+{
+	int r = 0;
+
+	/* Check if the file is already a FAF file */
+	if (file->f_flags & (O_FAF_CLT | O_FAF_SRV))
+		goto exit;
+
+	/* Check if we can re-open the file */
+	if (file->f_dentry) {
+		umode_t i_mode = file->f_dentry->d_inode->i_mode;
+		unsigned long s_magic = file->f_dentry->d_sb->s_magic;
+
+		if (((s_magic == PROC_SUPER_MAGIC) ||
+		     (s_magic == NFS_SUPER_MAGIC)  ||
+		     (s_magic == OCFS2_SUPER_MAGIC)) &&
+		    (S_ISREG(i_mode) || S_ISDIR(i_mode) || S_ISLNK(i_mode)))
+			goto exit;
+	}
+
+	/* Ok, so, we cannot do something better then using the FAF.
+	 * Let's do it !
+	 */
+	r = setup_faf_file(file);
+exit:
+	return r;
+}
+
+int check_activate_faf(struct task_struct *tsk,
+		       int index,
+		       struct file *file,
+		       struct epm_action *action)
+{
+	int r = 0;
+
+	/* Index < 0 means a mapped file. We do not use FAF for this  */
+	if (index < 0)
+		goto done;
+
+	/* No need to activate FAF for a checkpoint */
+	if (action->type == EPM_CHECKPOINT)
+		goto done;
+
+/* 	if (file->f_dentry && */
+/* 	    file->f_dentry->d_inode && */
+/* 	    file->f_dentry->d_inode->i_mapping && */
+/* 	    file->f_dentry->d_inode->i_mapping->a_ctnr != NULL) */
+/* 		activate_faf = 0; */
+
+	r = setup_faf_file_if_needed(file);
+	if (r == -EALREADY)
+		r = 0;
+done:
+	return r;
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              INITIALIZATION                               */
+/*                                                                           */
+/*****************************************************************************/
+
+/* FAF Initialisation */
+
+extern int ruaccess_start(void);
+extern void ruaccess_exit(void);
+
+void faf_init ()
+{
+	printk("FAF: initialisation : start\n");
+
+	faf_client_data_cachep = kmem_cache_create("faf_client_data",
+						   sizeof(faf_client_data_t),
+						   0, SLAB_PANIC, NULL);
+
+	ruaccess_start();
+	faf_server_init();
+	faf_hooks_init();
+
+	printk("FAF: initialisation : done\n");
+}
+
+
+
+/* FAF Finalization */
+
+void faf_finalize ()
+{
+	faf_hooks_finalize();
+	faf_server_finalize();
+	ruaccess_exit();
+}
diff -ruN linux-2.6.29/kerrighed/fs/faf/faf_file_mgr.c android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_file_mgr.c
--- linux-2.6.29/kerrighed/fs/faf/faf_file_mgr.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_file_mgr.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,313 @@
+/** Global management of faf files.
+ *  @file faf_file_mgr.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/file.h>
+#include <linux/wait.h>
+#include <kddm/kddm.h>
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/file.h>
+#include <kerrighed/physical_fs.h>
+#include "../mobility.h"
+#include <kerrighed/action.h>
+#include <kerrighed/app_shared.h>
+
+#include "faf_internal.h"
+#include "faf_hooks.h"
+#include <kerrighed/regular_file_mgr.h>
+
+struct kmem_cache *faf_client_data_cachep;
+extern const struct file_operations tty_fops;
+extern const struct file_operations hung_up_tty_fops;
+
+/** Create a faf file struct from a Kerrighed file descriptor.
+ *  @author Renaud Lottiaux
+ *
+ *  @param task    Task to create the file for.
+ *  @param desc    Kerrighed file descriptor.
+ *
+ *  @return   0 if everything ok.
+ *            Negative value otherwise.
+ */
+struct file *create_faf_file_from_krg_desc (struct task_struct *task,
+                                            void *_desc)
+{
+	faf_client_data_t *desc = _desc, *data;
+	struct file *file = NULL;
+
+	data = kmem_cache_alloc (faf_client_data_cachep, GFP_KERNEL);
+	if (!data)
+		return NULL;
+
+	file = get_empty_filp ();
+
+	if (!file) {
+		kmem_cache_free (faf_client_data_cachep, data);
+		goto exit;
+	}
+
+	*data = *desc;
+	init_waitqueue_head(&data->poll_wq);
+
+	file->f_dentry = NULL;
+	file->f_op = &faf_file_ops;
+	file->f_flags = desc->f_flags | O_FAF_CLT;
+	file->f_mode = desc->f_mode;
+	file->f_pos = desc->f_pos;
+	file->private_data = data;
+
+exit:
+	return file;
+}
+
+void fill_faf_file_krg_desc(faf_client_data_t *data, struct file *file)
+{
+	unsigned int flags = file->f_flags & (~O_FAF_SRV);
+
+	if (file->f_op == &tty_fops
+	    || file->f_op == &hung_up_tty_fops)
+		flags |= O_FAF_TTY;
+
+	data->f_flags = flags;
+	data->f_mode = file->f_mode;
+	data->f_pos = file->f_pos;
+	data->server_id = kerrighed_node_id;
+	data->server_fd = file->f_faf_srv_index;
+	data->i_mode = file->f_dentry->d_inode->i_mode;
+
+	if (S_ISFIFO(file->f_dentry->d_inode->i_mode)
+	    && strlen(file->f_dentry->d_name.name))
+		data->is_named_pipe = 1;
+	else
+		data->is_named_pipe = 0;
+}
+
+
+/** Return a kerrighed descriptor corresponding to the given file.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file       The file to get a Kerrighed descriptor for.
+ *  @param desc       The returned descriptor.
+ *  @param desc_size  Size of the returned descriptor.
+ *
+ *  @return   0 if everything ok.
+ *            Negative value otherwise.
+ */
+int get_faf_file_krg_desc (struct file *file,
+                           void **desc,
+                           int *desc_size)
+{
+	faf_client_data_t *data, *ldata;
+
+	data = kmalloc(sizeof(faf_client_data_t), GFP_KERNEL);
+	if (data == NULL)
+		return -ENOMEM;
+
+	/* The file descriptor is already a FAF client desc */
+
+	if (file->f_flags & O_FAF_CLT) {
+		ldata = file->private_data;
+		*data = *ldata;
+		goto done;
+	}
+
+	BUG_ON (!(file->f_flags & O_FAF_SRV));
+
+	fill_faf_file_krg_desc(data, file);
+
+done:
+	*desc = data;
+	*desc_size = sizeof (faf_client_data_t);
+
+	return 0;
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                            FAF FILES IMPORT/EXPORT                        */
+/*                                                                           */
+/*****************************************************************************/
+
+/** Export a faf file descriptor into the given ghost.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost    The ghost to write data to.
+ *  @param tsk      Task we are exporting.
+ *  @parem index    Index of the exported file in the open files array.
+ *  @param file     The file to export.
+ *
+ *  @return   0 if everything ok.
+ *            Negative value otherwise.
+ */
+int faf_file_export (struct epm_action *action,
+		     ghost_t *ghost,
+		     struct task_struct *task,
+		     int index,
+		     struct file *file)
+{
+	void *desc;
+	int desc_size;
+	int r = 0;
+
+	BUG_ON(action->type == EPM_CHECKPOINT);
+
+	r = get_faf_file_krg_desc(file, &desc, &desc_size);
+	if (r)
+		goto error;
+
+	r = ghost_write_file_krg_desc(ghost, desc, desc_size);
+	kfree(desc);
+
+error:
+	return r;
+}
+
+/** Import a faf file descriptor from the given ghost.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost          The ghost to read data from.
+ *  @param task           The task data are imported for.
+ *  @param returned_file  The file struct where data should be imported to.
+ *
+ *  @return   0 if everything ok.
+ *            Negative value otherwise.
+ */
+int faf_file_import (struct epm_action *action,
+		     ghost_t *ghost,
+		     struct task_struct *task,
+		     struct file **returned_file)
+{
+	void *desc;
+	struct file *file;
+	int r, desc_size;
+
+	BUG_ON(action->type == EPM_CHECKPOINT);
+
+	r = ghost_read_file_krg_desc(ghost, &desc, &desc_size);
+	if (r)
+		goto exit;
+
+	file = create_faf_file_from_krg_desc (task, desc);
+
+	if (IS_ERR(file)) {
+		r = PTR_ERR(file);
+		goto exit_free_desc;
+	}
+	*returned_file = file;
+
+exit_free_desc:
+	kfree(desc);
+exit:
+	return r;
+}
+
+struct dvfs_mobility_operations dvfs_mobility_faf_ops = {
+	.file_export = faf_file_export,
+	.file_import = faf_file_import,
+};
+
+int __send_faf_file_desc(struct rpc_desc *desc, struct file *file)
+{
+	int r, fdesc_size;
+	void *fdesc;
+
+	BUG_ON(!file->f_objid);
+	BUG_ON(!(file->f_flags & (O_FAF_SRV|O_FAF_CLT)));
+
+	r = get_faf_file_krg_desc(file, &fdesc, &fdesc_size);
+	if (r)
+		goto out;
+
+	r = rpc_pack_type(desc, file->f_objid);
+	if (r)
+		goto out_free_fdesc;
+
+	r = rpc_pack_type(desc, fdesc_size);
+	if (r)
+		goto out_free_fdesc;
+
+	r = rpc_pack(desc, 0, fdesc, fdesc_size);
+	if (r)
+		goto out_free_fdesc;
+
+out_free_fdesc:
+	kfree(fdesc);
+
+out:
+	return r;
+}
+
+int send_faf_file_desc(struct rpc_desc *desc, struct file *file)
+{
+	int r;
+
+	if (!file->f_objid) {
+		r = create_kddm_file_object(file);
+		if (r)
+			goto out;
+	}
+
+	if (!(file->f_flags & (O_FAF_SRV|O_FAF_CLT))) {
+		r = setup_faf_file(file);
+		if (r && r != -EALREADY)
+			goto out;
+	}
+
+	r = __send_faf_file_desc(desc, file);
+
+out:
+	return r;
+}
+
+struct file *rcv_faf_file_desc(struct rpc_desc *desc)
+{
+	int r, first_import;
+	void *fdesc;
+	int fdesc_size;
+	long fobjid;
+	struct dvfs_file_struct *dvfs_file = NULL;
+	struct file *file = NULL;
+
+	r = rpc_unpack_type(desc, fobjid);
+	if (r)
+		goto error;
+
+	r = rpc_unpack_type(desc, fdesc_size);
+	if (r)
+		goto error;
+
+	fdesc = kmalloc(GFP_KERNEL, fdesc_size);
+	if (!fdesc) {
+		r = -ENOMEM;
+		goto error;
+	}
+
+	r = rpc_unpack(desc, 0, fdesc, fdesc_size);
+	if (r)
+		goto err_free_desc;
+
+	/* Check if the file struct is already present */
+	first_import = 0;
+
+	file = begin_import_dvfs_file(fobjid, &dvfs_file);
+
+	if (!file) {
+		file = create_faf_file_from_krg_desc(current, fdesc);
+		first_import = 1;
+	}
+
+	r = end_import_dvfs_file(fobjid, dvfs_file, file, first_import);
+	if (r)
+		goto err_free_desc;
+
+err_free_desc:
+	kfree(fdesc);
+
+error:
+	if (r)
+		file = ERR_PTR(r);
+
+	return file;
+}
diff -ruN linux-2.6.29/kerrighed/fs/faf/faf_hooks.c android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_hooks.c
--- linux-2.6.29/kerrighed/fs/faf/faf_hooks.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_hooks.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,1597 @@
+/** Kerrighed FAF Hooks.
+ *  @file file_hooks.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/fs.h>
+#include <linux/fs_struct.h>
+#include <linux/mount.h>
+#include <linux/file.h>
+#include <linux/uio.h>
+#include <linux/namei.h>
+#include <linux/socket.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/vmalloc.h>
+#include <linux/slab.h>
+#include <linux/poll.h>
+#include <linux/statfs.h>
+#include <linux/types.h>
+#include <linux/remote_sleep.h>
+#include <kerrighed/faf.h>
+#include <kerrighed/physical_fs.h>
+#include <kerrighed/remote_cred.h>
+#include <asm/uaccess.h>
+
+#include <kddm/kddm.h>
+#include <kerrighed/hotplug.h>
+#include <net/krgrpc/rpc.h>
+#include <net/krgrpc/rpcid.h>
+#include <kerrighed/file.h>
+#include "../file_struct_io_linker.h"
+
+#include "faf_internal.h"
+#include "faf_server.h"
+#include "faf_tools.h"
+#include <kerrighed/faf_file_mgr.h>
+#include "ruaccess.h"
+
+static DEFINE_MUTEX(faf_poll_mutex);
+
+static int pack_path(struct rpc_desc *desc, const struct path *path)
+{
+	char *tmp, *name;
+	struct path phys_root;
+	int len, err;
+
+	err = -EPERM;
+	get_physical_root(&phys_root);
+	if (path->mnt->mnt_ns != phys_root.mnt->mnt_ns)
+		/* path lives in a child mount namespace: not supported yet */
+		goto out;
+
+	err = -ENOMEM;
+	tmp = (char *)__get_free_page(GFP_KERNEL);
+	if (!tmp)
+		goto out;
+
+	err = -EINVAL;
+	name = physical_d_path(path, tmp, false);
+	if (!name)
+		goto out_free;
+	len = strlen(name) + 1;
+
+	err = rpc_pack_type(desc, len);
+	if (err)
+		goto out_free;
+	err = rpc_pack(desc, 0, name, len);
+
+out_free:
+	free_page((unsigned long)tmp);
+out:
+	path_put(&phys_root);
+
+	return err;
+}
+
+static int pack_root(struct rpc_desc *desc)
+{
+	struct path root;
+	int ret;
+
+	read_lock(&current->fs->lock);
+	root = current->fs->root;
+	path_get(&root);
+	read_unlock(&current->fs->lock);
+
+	ret = pack_path(desc, &root);
+
+	path_put(&root);
+
+	return ret;
+}
+
+static int pack_root_pwd(struct rpc_desc *desc)
+{
+	struct path root, pwd;
+	int ret;
+
+	read_lock(&current->fs->lock);
+	root = current->fs->root;
+	path_get(&root);
+	pwd = current->fs->pwd;
+	path_get(&pwd);
+	read_unlock(&current->fs->lock);
+
+	ret = pack_path(desc, &root);
+	if (!ret)
+		ret = pack_path(desc, &pwd);
+
+	path_put(&root);
+	path_put(&pwd);
+
+	return ret;
+}
+
+static int pack_context(struct rpc_desc *desc)
+{
+	int err;
+
+	err = pack_creds(desc, current_cred());
+	if (err)
+		goto out;
+	err = pack_root_pwd(desc);
+
+out:
+	return err;
+}
+
+/** Kerrighed kernel hook for FAF lseek function.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file    File to seek in.
+ *  @param offset  Offset to seek at.
+ *  @param origin  Origin of the seek.
+ */
+off_t krg_faf_lseek (struct file * file,
+		     off_t offset,
+		     unsigned int origin)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_seek_msg msg;
+	off_t r;
+	struct rpc_desc* desc;
+
+	msg.server_fd = data->server_fd;
+	msg.offset = offset;
+	msg.origin = origin;
+
+	desc = rpc_begin(RPC_FAF_LSEEK, data->server_id);
+
+	rpc_pack_type(desc, msg);
+
+	rpc_unpack_type(desc, r);
+
+	rpc_end(desc, 0);
+
+	return r;
+}
+
+/** Kerrighed kernel hook for FAF llseek function.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file          File to seek in.
+ *  @param offset_high   High part of the offset to seek at.
+ *  @param offset_low    Low part of the offset to seek at.
+ *  @param result        ...
+ *  @param origin        Origin of the seek.
+ */
+long krg_faf_llseek (struct file *file,
+		     unsigned long offset_high,
+		     unsigned long offset_low,
+		     loff_t * result,
+		     unsigned int origin)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_llseek_msg msg;
+	long r;
+	struct rpc_desc* desc;
+
+	msg.server_fd = data->server_fd;
+	msg.offset_high = offset_high;
+	msg.offset_low = offset_low;
+	msg.origin = origin;
+
+	desc = rpc_begin(RPC_FAF_LLSEEK, data->server_id);
+
+	rpc_pack_type(desc, msg);
+
+	rpc_unpack_type(desc, r);
+	rpc_unpack(desc, 0, result, sizeof(*result));
+
+	rpc_end(desc, 0);
+
+	return r;
+}
+
+/** Kerrighed kernel hook for FAF read function.
+ *  @author Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param file          File to read from.
+ *  @param buf           Buffer to store data in.
+ *  @param count         Number of bytes to read.
+ *  @param pos           Offset to read from (updated at the end).
+ */
+ssize_t krg_faf_read(struct file * file, char *buf, size_t count, loff_t *pos)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_rw_msg msg;
+	ssize_t nr;
+	ssize_t received = 0;
+	loff_t fpos;
+	char *kbuff;
+	int err;
+	struct rpc_desc *desc;
+
+	kbuff = kmalloc(PAGE_SIZE, GFP_KERNEL);
+	if (!kbuff)
+		return -ENOMEM;
+
+	msg.server_fd = data->server_fd;
+	msg.count = count;
+	msg.pos = *pos;
+
+	nr = -ENOMEM;
+	desc = rpc_begin(RPC_FAF_READ, data->server_id);
+	if (!desc)
+		goto out;
+
+	/* Send read request */
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto cancel;
+
+	err = unpack_remote_sleep_res_prepare(desc);
+	if (err)
+		goto cancel;
+
+	/* Get number of bytes to receive */
+	err = unpack_remote_sleep_res_type(desc, nr);
+	if (err)
+		goto cancel;
+
+	while (nr > 0) {
+		/* Receive file data */
+		err = rpc_unpack(desc, 0, kbuff, nr);
+		if (err)
+			goto cancel;
+		err = copy_to_user(&buf[received], kbuff, nr);
+		if (err) {
+			nr = -EFAULT;
+			break;
+		}
+		received += nr;
+		err = unpack_remote_sleep_res_type(desc, nr);
+		if (err)
+			goto cancel;
+	}
+
+	if (!nr)
+		/* no error occurs when reading */
+		nr = received;
+
+	/* Receive the updated offset */
+	err = rpc_unpack_type(desc, fpos);
+	if (err)
+		goto cancel;
+	*pos = fpos;
+
+out_end:
+	rpc_end(desc, 0);
+
+out:
+	kfree(kbuff);
+
+	return nr;
+
+cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -EPIPE;
+	nr = err;
+	goto out_end;
+}
+
+/** Kerrighed kernel hook for FAF write function.
+ *  @author Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param file          File to write to.
+ *  @param buf           Buffer of data to write.
+ *  @param count         Number of bytes to write.
+ *  @param pos           Offset to write from (updated at the end).
+ */
+ssize_t krg_faf_write(struct file * file, const char *buf,
+		      size_t count, loff_t *pos)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_rw_msg msg;
+	ssize_t buf_size = PAGE_SIZE, nr;
+	long offset = 0;
+	long to_send = count;
+	loff_t fpos;
+	char *kbuff;
+	int err;
+	struct rpc_desc *desc;
+
+	kbuff = kmalloc(PAGE_SIZE, GFP_KERNEL);
+	if (!kbuff)
+		return -ENOMEM;
+
+	msg.server_fd = data->server_fd;
+	msg.count = count;
+	msg.pos = *pos;
+
+	nr = -ENOMEM;
+	desc = rpc_begin(RPC_FAF_WRITE, data->server_id);
+	if (!desc)
+		goto out;
+
+	/* Send write request */
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto cancel;
+
+	err = unpack_remote_sleep_res_prepare(desc);
+	if (err)
+		goto cancel;
+
+	while (to_send > 0) {
+		if (to_send < PAGE_SIZE)
+			buf_size = to_send;
+
+		err = copy_from_user(kbuff, &buf[offset], buf_size);
+		if (err)
+			buf_size = -EFAULT;
+
+		err = rpc_pack_type(desc, buf_size);
+		if (err)
+			goto cancel;
+
+		if (buf_size < 0) /* copy_from_user has failed */
+			break;
+
+		err = rpc_pack(desc, 0, kbuff, buf_size);
+		if (err)
+			goto cancel;
+
+		to_send -= buf_size;
+		offset += buf_size;
+	}
+
+	err = unpack_remote_sleep_res_type(desc, nr);
+	if (err)
+		nr = err;
+	else if (nr == -EPIPE)
+		send_sig(SIGPIPE, current, 0);
+
+	/* Receive the updated offset */
+	err = rpc_unpack_type(desc, fpos);
+	if (err)
+		goto cancel;
+	*pos = fpos;
+
+out_end:
+	rpc_end(desc, 0);
+
+out:
+	kfree(kbuff);
+
+	return nr;
+
+cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -EPIPE;
+	nr = err;
+	goto out_end;
+}
+
+ssize_t krg_faf_readv(struct file *file, const struct iovec __user *vec,
+		      unsigned long vlen, loff_t *pos)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_rw_msg msg;
+	struct faf_rw_ret ret;
+	struct iovec iovstack[UIO_FASTIOV];
+	struct iovec *iov = iovstack;
+	int iovcnt;
+	size_t total_len;
+	struct rpc_desc *desc;
+	int err;
+
+	ret.ret = rw_copy_check_uvector(READ, vec, vlen,
+					ARRAY_SIZE(iovstack), iovstack, &iov);
+	if (ret.ret < 0)
+		return ret.ret;
+	iovcnt = vlen;
+	total_len = ret.ret;
+
+	ret.ret = -ENOMEM;
+	desc = rpc_begin(RPC_FAF_READV, data->server_id);
+	if (!desc)
+		goto out;
+
+	msg.server_fd = data->server_fd;
+	msg.count = total_len;
+	msg.pos = *pos;
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto cancel;
+
+	err = unpack_remote_sleep_res_prepare(desc);
+	if (err)
+		goto cancel;
+	err = unpack_remote_sleep_res_type(desc, ret);
+	if (err)
+		goto cancel;
+
+	*pos = ret.pos;
+	if (ret.ret <= 0)
+		goto out_end;
+
+	err = recv_iov(desc, iov, iovcnt, ret.ret, MSG_USER);
+	if (err)
+		goto cancel;
+
+out_end:
+	rpc_end(desc, 0);
+
+out:
+	if (iov != iovstack)
+		kfree(iov);
+
+	return ret.ret;
+
+cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -EPIPE;
+	ret.ret = err;
+	goto out_end;
+}
+
+ssize_t krg_faf_writev(struct file *file, const struct iovec __user *vec,
+		       unsigned long vlen, loff_t *pos)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_rw_msg msg;
+	struct faf_rw_ret ret;
+	struct iovec iovstack[UIO_FASTIOV];
+	struct iovec *iov = iovstack;
+	int iovcnt;
+	size_t total_len;
+	struct rpc_desc *desc;
+	int err;
+
+	ret.ret = rw_copy_check_uvector(WRITE, vec, vlen,
+					ARRAY_SIZE(iovstack), iovstack, &iov);
+	if (ret.ret < 0)
+		return ret.ret;
+	iovcnt = vlen;
+	total_len = ret.ret;
+
+	ret.ret = -ENOMEM;
+	desc = rpc_begin(RPC_FAF_WRITEV, data->server_id);
+	if (!desc)
+		goto out;
+
+	msg.server_fd = data->server_fd;
+	msg.count = total_len;
+	msg.pos = *pos;
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto cancel;
+
+	err = send_iov(desc, iov, iovcnt, total_len, MSG_USER);
+	if (err)
+		goto cancel;
+
+	err = unpack_remote_sleep_res_prepare(desc);
+	if (err)
+		goto cancel;
+	err = unpack_remote_sleep_res_type(desc, ret);
+	if (err)
+		goto cancel;
+
+	*pos = ret.pos;
+	if (ret.ret == -EPIPE)
+		send_sig(SIGPIPE, current, 0);
+
+out_end:
+	rpc_end(desc, 0);
+
+out:
+	if (iov != iovstack)
+		kfree(iov);
+
+	return ret.ret;
+
+cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -EPIPE;
+	ret.ret = err;
+	goto out_end;
+}
+
+/** Kerrighed kernel hook for FAF ioctl function.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file          File to do an ioctl to.
+ *  @param cmd           IOCTL command.
+ *  @param arg           IOCTL argument.
+ */
+long krg_faf_ioctl (struct file *file,
+		    unsigned int cmd,
+		    unsigned long arg)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_ctl_msg msg;
+	long r;
+	struct rpc_desc *desc;
+	int err;
+
+	msg.server_fd = data->server_fd;
+	msg.cmd = cmd;
+	msg.arg = arg;
+
+	err = -ENOMEM;
+	desc = rpc_begin(RPC_FAF_IOCTL, data->server_id);
+	if (!desc)
+		goto out_err;
+
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto out_cancel;
+	err = pack_context(desc);
+	if (err)
+		goto out_cancel;
+
+	err = unpack_remote_sleep_res_prepare(desc);
+	if (err)
+		goto out_cancel;
+	err = handle_ruaccess(desc);
+	if (err)
+		goto out_cancel;
+	err = unpack_remote_sleep_res_type(desc, r);
+	if (err)
+		goto out_cancel;
+	rpc_end(desc, 0);
+
+out:
+	return r;
+
+out_cancel:
+	rpc_cancel(desc);
+	rpc_end(desc, 0);
+	if (err > 0)
+		err = -ENOMEM;
+out_err:
+	r = err;
+	goto out;
+}
+
+/** Kerrighed kernel hook for FAF fcntl function.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file          File to do an fcntl to.
+ *  @param cmd           FCNTL command.
+ *  @param arg           FCNTL argument.
+ */
+long krg_faf_fcntl (struct file *file,
+		    unsigned int cmd,
+		    unsigned long arg)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_ctl_msg msg;
+	struct rpc_desc *desc;
+	int err;
+	long r;
+
+	msg.server_fd = data->server_fd;
+	msg.cmd = cmd;
+	r = -EFAULT;
+	if ((cmd == F_SETLK || cmd == F_SETLKW || cmd == F_GETLK)
+	    && copy_from_user(&msg.flock,
+			      (struct flock __user *) arg, sizeof(msg.flock)))
+			goto out;
+	else
+		msg.arg = arg;
+
+	r = -ENOLCK;
+	desc = rpc_begin(RPC_FAF_FCNTL, data->server_id);
+	if (unlikely(!desc))
+		goto out;
+
+	err = rpc_pack_type(desc, msg);
+	if (unlikely(err))
+		goto cancel;
+	err = pack_creds(desc, current_cred());
+	if (err)
+		goto cancel;
+
+	err = unpack_remote_sleep_res_prepare(desc);
+	if (err)
+		goto cancel;
+	err = unpack_remote_sleep_res_type(desc, r);
+	if (unlikely(err))
+		goto cancel;
+
+	if (!r && cmd == F_GETLK) {
+		err = rpc_unpack_type(desc, msg.flock);
+		if (unlikely(err))
+			goto cancel;
+		r = -EFAULT;
+		if (!copy_to_user((struct flock __user *) arg,
+				  &msg.flock, sizeof(msg.flock)))
+			r = 0;
+	}
+
+out_end:
+	rpc_end(desc, 0);
+
+out:
+	return r;
+
+cancel:
+	rpc_cancel(desc);
+	goto out_end;
+}
+
+#if BITS_PER_LONG == 32
+/** Kerrighed kernel hook for FAF fcntl64 function.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file          File to do an fcntl to.
+ *  @param cmd           FCNTL command.
+ *  @param arg           FCNTL argument.
+ */
+long krg_faf_fcntl64 (struct file *file,
+		      unsigned int cmd,
+		      unsigned long arg)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_ctl_msg msg;
+	long r;
+	struct rpc_desc* desc;
+	int err;
+
+	msg.server_fd = data->server_fd;
+	msg.cmd = cmd;
+	r = -EFAULT;
+	if ((cmd == F_GETLK64 || cmd == F_SETLK64 || cmd == F_SETLKW64)
+	    && copy_from_user(&msg.flock64,
+			      (struct flock64 __user *) arg, sizeof(msg.flock64)))
+			goto out;
+	else
+		msg.arg = arg;
+
+	r = -ENOLCK;
+	desc = rpc_begin(RPC_FAF_FCNTL64,
+			 data->server_id);
+	if (unlikely(!desc))
+		goto out;
+
+	err = rpc_pack_type(desc, msg);
+	if (unlikely(err))
+		goto cancel;
+	err = pack_creds(desc, current_cred());
+	if (err)
+		goto cancel;
+
+	err = unpack_remote_sleep_res_prepare(desc);
+	if (err)
+		goto cancel;
+	err = unpack_remote_sleep_res_type(desc, r);
+	if (unlikely(err))
+		goto cancel;
+
+	if (!r && cmd == F_GETLK64) {
+		err = rpc_unpack_type(desc, msg.flock64);
+		if (unlikely(err))
+			goto cancel;
+		r = -EFAULT;
+		if (!copy_to_user((struct flock64 __user *) arg,
+				  &msg.flock64, sizeof(msg.flock64)))
+			r = 0;
+	}
+
+out_end:
+	rpc_end(desc, 0);
+
+out:
+	return r;
+
+cancel:
+	rpc_cancel(desc);
+	goto out_end;
+}
+#endif
+
+/** Kerrighed kernel hook for FAF fstat function.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file          File to do an fcntl to.
+ *  @param statbuf       Kernel buffer to store file stats.
+ */
+long krg_faf_fstat (struct file *file,
+		    struct kstat *statbuf)
+{
+	struct kstat buffer;
+	faf_client_data_t *data = file->private_data;
+	struct faf_stat_msg msg;
+	long r;
+	struct rpc_desc* desc;
+
+	msg.server_fd = data->server_fd;
+
+	desc = rpc_begin(RPC_FAF_FSTAT, data->server_id);
+
+	rpc_pack_type(desc, msg);
+
+	rpc_unpack_type(desc, r);
+	rpc_unpack_type(desc, buffer);
+
+	rpc_end(desc, 0);
+
+	*statbuf = buffer;
+
+	return r;
+}
+
+/** Kerrighed kernel hook for FAF fstat function.
+ *  @author Matthieu Fertré
+ *
+ *  @param file          File to do an fcntl to.
+ *  @param statbuf       Kernel buffer to store file stats.
+ */
+long krg_faf_fstatfs(struct file *file,
+		     struct statfs *statfsbuf)
+{
+	struct statfs buffer;
+	faf_client_data_t *data = file->private_data;
+	struct faf_statfs_msg msg;
+	long r;
+	enum rpc_error err;
+	struct rpc_desc *desc;
+
+	msg.server_fd = data->server_fd;
+
+	desc = rpc_begin(RPC_FAF_FSTATFS, data->server_id);
+
+	r = rpc_pack_type(desc, msg);
+	if (r)
+		goto exit;
+
+	err = rpc_unpack_type(desc, r);
+	if (err)
+		goto err_rpc;
+
+	if (!r)
+		err = rpc_unpack_type(desc, buffer);
+
+	rpc_end(desc, 0);
+
+	*statfsbuf = buffer;
+
+exit:
+	return r;
+err_rpc:
+	r = -EPIPE;
+	goto exit;
+}
+
+/** Kerrighed kernel hook for FAF fsync function.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file          File to do a fsync to.
+ */
+long krg_faf_fsync (struct file *file)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_rw_msg msg;
+	long r;
+
+	msg.server_fd = data->server_fd;
+
+	r = rpc_sync(RPC_FAF_FSYNC, data->server_id, &msg, sizeof(msg));
+
+	return r;
+}
+
+/** Kerrighed kernel hook for FAF flock function.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file          File to do a flock to.
+ */
+long krg_faf_flock (struct file *file,
+		    unsigned int cmd)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_ctl_msg msg;
+	struct rpc_desc *desc;
+	long r;
+	int err;
+
+	msg.server_fd = data->server_fd;
+	msg.cmd = cmd;
+
+	desc = rpc_begin(RPC_FAF_FLOCK, data->server_id);
+	if (!desc)
+		return -ENOMEM;
+
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto cancel;
+	err = pack_creds(desc, current_cred());
+	if (err)
+		goto cancel;
+
+	err = unpack_remote_sleep_res_prepare(desc);
+	if (err)
+		goto cancel;
+	err = unpack_remote_sleep_res_type(desc, r);
+	if (err)
+		goto cancel;
+
+out_end:
+	rpc_end(desc, 0);
+	return r;
+
+cancel:
+	rpc_cancel(desc);
+	r = err;
+	goto out_end;
+}
+
+static char *__krg_faf_d_path(const struct path *root, const struct file *file,
+			      char *buff, int size, bool *deleted)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_d_path_msg msg;
+	struct rpc_desc* desc;
+	int len;
+	int err;
+
+	BUG_ON(file->f_flags & O_FAF_SRV);
+
+	msg.server_fd = data->server_fd;
+	msg.deleted = !!deleted;
+	msg.count = size;
+
+	desc = rpc_begin(RPC_FAF_D_PATH, data->server_id);
+	if (!desc)
+		return ERR_PTR(-ENOMEM);
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto err_cancel;
+	err = pack_creds(desc, current_cred());
+	if (err)
+		goto err_cancel;
+	err = pack_path(desc, root);
+	if (err)
+		goto err_cancel;
+
+	err = rpc_unpack_type(desc, len);
+	if (err)
+		goto err_cancel;
+	if (len >= 0) {
+		err = rpc_unpack(desc, 0, buff, len);
+		if (err)
+			goto err_cancel;
+		if (deleted) {
+			err = rpc_unpack_type(desc, *deleted);
+			if (err)
+				goto err_cancel;
+		}
+	} else {
+		buff = ERR_PTR(len);
+	}
+out_end:
+	rpc_end(desc, 0);
+
+	return buff;
+
+err_cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -EPIPE;
+	buff = ERR_PTR(err);
+	goto out_end;
+}
+
+char *krg_faf_phys_d_path(const struct file *file, char *buff, int size,
+			  bool *deleted)
+{
+	struct path root;
+	char *ret;
+
+	get_physical_root(&root);
+	ret = __krg_faf_d_path(&root, file, buff, size, deleted);
+	path_put(&root);
+
+	return ret;
+}
+
+/** Kerrighed FAF d_path function.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file     The file to get the path.
+ *  @param buff     Buffer to store the path in.
+ */
+char *
+krg_faf_d_path(const struct file *file, char *buff, int size, bool *deleted)
+{
+	struct path root;
+	char *ret;
+
+	read_lock(&current->fs->lock);
+	root = current->fs->root;
+	path_get(&root);
+	read_unlock(&current->fs->lock);
+
+	ret = __krg_faf_d_path(&root, file, buff, size, deleted);
+
+	path_put(&root);
+
+	return ret;
+}
+
+int krg_faf_do_path_lookup(struct file *file,
+			   const char *name,
+			   unsigned int flags,
+			   struct nameidata *nd)
+{
+	char *tmp = (char *) __get_free_page (GFP_KERNEL);
+	char *path;
+	bool deleted;
+	int len, err = 0;
+
+	path = krg_faf_d_path(file, tmp, PAGE_SIZE, &deleted);
+
+	if (IS_ERR(path)) {
+		err = PTR_ERR(path);
+		goto exit;
+	}
+	if (deleted) {
+		err = -ENOENT;
+		goto exit;
+	}
+
+
+	if (likely(path != tmp)) {
+		strncpy(tmp, path, PAGE_SIZE);
+		path = tmp;
+	}
+
+	len = strlen (path);
+	strncpy(&path[len], name, PAGE_SIZE - len);
+
+	err = path_lookup(path, flags, nd);
+exit:
+	free_page ((unsigned long) tmp);
+	return err;
+}
+
+long krg_faf_bind (struct file * file,
+		   struct sockaddr __user *umyaddr,
+		   int addrlen)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_bind_msg msg;
+	struct rpc_desc *desc;
+	int err, r;
+
+	msg.server_fd = data->server_fd;
+
+	r = move_addr_to_kernel(umyaddr, addrlen, (struct sockaddr *)&msg.sa);
+	if (r)
+		goto out;
+
+	msg.addrlen = addrlen;
+
+	r = -ENOMEM;
+	desc = rpc_begin(RPC_FAF_BIND, data->server_id);
+	if (!desc)
+		goto out;
+
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto cancel;
+	err = pack_context(desc);
+	if (err)
+		goto cancel;
+
+	err = rpc_unpack_type(desc, r);
+	if (err)
+		goto cancel;
+
+out_end:
+	rpc_end(desc, 0);
+out:
+	return r;
+
+cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -EPIPE;
+	r = err;
+	goto out_end;
+}
+
+
+
+long krg_faf_connect (struct file * file,
+		      struct sockaddr __user *uservaddr,
+		      int addrlen)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_bind_msg msg;
+	struct rpc_desc *desc;
+	int r, err;
+
+	msg.server_fd = data->server_fd;
+
+	r = move_addr_to_kernel(uservaddr, addrlen, (struct sockaddr *)&msg.sa);
+	if (r)
+		goto out;
+
+	msg.addrlen = addrlen;
+
+	desc = rpc_begin(RPC_FAF_CONNECT, data->server_id);
+	if (!desc) {
+		r = -ENOMEM;
+		goto out;
+	}
+
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto cancel;
+	err = pack_context(desc);
+	if (err)
+		goto cancel;
+
+	err = unpack_remote_sleep_res_prepare(desc);
+	if (err)
+		goto cancel;
+	err = unpack_remote_sleep_res_type(desc, r);
+	if (err)
+		goto cancel;
+
+out_end:
+	rpc_end(desc, 0);
+
+out:
+	return r;
+
+cancel:
+	rpc_cancel(desc);
+	r = err;
+	goto out_end;
+}
+
+long krg_faf_listen (struct file * file,
+		     int backlog)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_listen_msg msg;
+	int r;
+
+	msg.server_fd = data->server_fd;
+
+	msg.backlog = backlog;
+
+	r = rpc_sync(RPC_FAF_LISTEN, data->server_id, &msg, sizeof(msg));
+
+	return r;
+}
+
+long krg_faf_accept(struct file * file,
+		    struct sockaddr __user *upeer_sockaddr,
+		    int __user *upeer_addrlen)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_bind_msg msg;
+	int r, err;
+	struct sockaddr_storage sa;
+	int sa_len;
+	struct file *newfile;
+	int fd;
+	struct rpc_desc* desc;
+
+	BUG_ON (data->server_id == kerrighed_node_id);
+
+	fd = get_unused_fd();
+	if (fd < 0) {
+		r = fd;
+		goto out;
+	}
+
+	msg.server_fd = data->server_fd;
+
+	if (upeer_sockaddr) {
+		if (get_user(msg.addrlen, upeer_addrlen)) {
+			r = -EFAULT;
+			goto out_put_fd;
+		}
+	} else {
+		msg.addrlen = 0;
+	}
+
+	desc = rpc_begin(RPC_FAF_ACCEPT, data->server_id);
+	if (!desc)
+		goto out_put_fd;
+
+	r = rpc_pack_type(desc, msg);
+	if (r)
+		goto err_cancel;
+
+	r = unpack_remote_sleep_res_prepare(desc);
+	if (r)
+		goto err_cancel;
+	err = unpack_remote_sleep_res_type(desc, r);
+	if (err) {
+		r = err;
+		goto err_cancel;
+	}
+
+	if (r<0) {
+		rpc_end(desc, 0);
+		goto out_put_fd;
+	}
+
+	r = rpc_unpack_type(desc, sa_len);
+	if (r)
+		goto err_cancel;
+
+	r = rpc_unpack(desc, 0, &sa, sa_len);
+	if (r)
+		goto err_cancel;
+
+	newfile = rcv_faf_file_desc(desc);
+	if (IS_ERR(newfile)) {
+		r = PTR_ERR(newfile);
+		goto err_cancel;
+	}
+
+	/*
+	 * We have enough to clean up the new file ourself if needed. Tell it
+	 * to the server.
+	 */
+	r = rpc_pack_type(desc, fd);
+	if (r)
+		goto err_close_faf_file;
+
+	rpc_end(desc, 0);
+
+	if (upeer_sockaddr) {
+		r = move_addr_to_user((struct sockaddr *)&sa, sa_len,
+				      upeer_sockaddr, upeer_addrlen);
+		if (r)
+			goto err_close_faf_file;
+	}
+
+	fd_install(fd, newfile);
+	r = fd;
+
+out:
+	return r;
+
+err_cancel:
+	rpc_cancel(desc);
+	rpc_end(desc, 0);
+out_put_fd:
+	put_unused_fd(fd);
+	goto out;
+
+err_close_faf_file:
+	fput(newfile);
+	goto out_put_fd;
+}
+
+long krg_faf_getsockname (struct file * file,
+			  struct sockaddr __user *usockaddr,
+			  int __user *usockaddr_len)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_bind_msg msg;
+	struct sockaddr_storage sa;
+	int sa_len;
+	struct rpc_desc *desc;
+	int r = -EFAULT;
+
+	msg.server_fd = data->server_fd;
+	if (get_user(msg.addrlen, usockaddr_len))
+		goto out;
+
+	desc = rpc_begin(RPC_FAF_GETSOCKNAME, data->server_id);
+	rpc_pack_type(desc, msg);
+	pack_root(desc);
+
+	rpc_unpack_type(desc, r);
+	rpc_unpack_type(desc, sa_len);
+	rpc_unpack(desc, 0, &sa, sa_len);
+	rpc_end(desc, 0);
+
+	if (!r)
+		r = move_addr_to_user((struct sockaddr *)&sa, sa_len,
+				      usockaddr, usockaddr_len);
+
+out:
+	return r;
+}
+
+long krg_faf_getpeername (struct file * file,
+			  struct sockaddr __user *usockaddr,
+			  int __user *usockaddr_len)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_bind_msg msg;
+	struct sockaddr_storage sa;
+	int sa_len;
+	struct rpc_desc *desc;
+	int r;
+
+	msg.server_fd = data->server_fd;
+
+	if (get_user(msg.addrlen, usockaddr_len))
+		return -EFAULT;
+
+	desc = rpc_begin(RPC_FAF_GETPEERNAME, data->server_id);
+	rpc_pack_type(desc, msg);
+	pack_root(desc);
+	rpc_unpack_type(desc, r);
+	rpc_unpack_type(desc, sa_len);
+	rpc_unpack(desc, 0, &sa, sa_len);
+	rpc_end(desc, 0);
+
+	if (!r)
+		r = move_addr_to_user((struct sockaddr *)&sa, sa_len,
+				      usockaddr, usockaddr_len);
+
+	return r;
+}
+
+long krg_faf_shutdown (struct file * file,
+		       int how)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_shutdown_msg msg ;
+	int r;
+
+	msg.server_fd = data->server_fd;
+
+	msg.how = how;
+
+	r = rpc_sync(RPC_FAF_SHUTDOWN, data->server_id, &msg, sizeof(msg));
+
+	return r;
+}
+
+long krg_faf_setsockopt (struct file * file,
+			 int level,
+			 int optname,
+			 char __user *optval,
+			 int optlen)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_setsockopt_msg msg;
+	struct rpc_desc *desc;
+	int r, err;
+
+	msg.server_fd = data->server_fd;
+
+	msg.level = level;
+	msg.optname = optname;
+	msg.optval = optval;
+	msg.optlen = optlen;
+
+	desc = rpc_begin(RPC_FAF_SETSOCKOPT, data->server_id);
+	if (!desc) {
+		r = -ENOMEM;
+		goto out;
+	}
+
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto err_cancel;
+	err = pack_context(desc);
+	if (err)
+		goto err_cancel;
+	err = handle_ruaccess(desc);
+	if (err)
+		goto err_cancel;
+	err = rpc_unpack_type(desc, r);
+	if (err)
+		goto err_cancel;
+
+out_end:
+	rpc_end(desc, 0);
+
+out:
+	return r;
+
+err_cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -ENOMEM;
+	r = err;
+	goto out_end;
+}
+
+long krg_faf_getsockopt (struct file * file,
+			 int level,
+			 int optname,
+			 char __user *optval,
+			 int __user *optlen)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_getsockopt_msg msg;
+	int r, err;
+	struct rpc_desc *desc;
+
+	msg.server_fd = data->server_fd;
+
+	msg.level = level;
+	msg.optname = optname;
+	msg.optval = optval;
+	msg.optlen = optlen;
+
+	desc = rpc_begin(RPC_FAF_GETSOCKOPT, data->server_id);
+	if (!desc) {
+		r = -ENOMEM;
+		goto out;
+	}
+
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto err_cancel;
+	err = pack_context(desc);
+	if (err)
+		goto err_cancel;
+	err = handle_ruaccess(desc);
+	if (err)
+		goto err_cancel;
+	err = rpc_unpack_type(desc, r);
+	if (err)
+		goto err_cancel;
+
+out_end:
+	rpc_end(desc, 0);
+
+out:
+	return r;
+
+err_cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -ENOMEM;
+	r = err;
+	goto out_end;
+}
+
+ssize_t krg_faf_sendmsg(struct file *file, struct msghdr *msghdr,
+			size_t total_len)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_sendmsg_msg msg;
+	ssize_t r;
+	int err;
+	struct rpc_desc* desc;
+
+	msg.server_fd = data->server_fd;
+	msg.total_len = total_len;
+	msg.flags = msghdr->msg_flags;
+
+	desc = rpc_begin(RPC_FAF_SENDMSG, data->server_id);
+	if (!desc)
+		return -ENOMEM;
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto cancel;
+
+	err = send_msghdr(desc, msghdr, total_len, MSG_USER);
+	if (err)
+		goto cancel;
+
+	err = unpack_remote_sleep_res_prepare(desc);
+	if (err)
+		goto cancel;
+	err = unpack_remote_sleep_res_type(desc, r);
+	if (err)
+		goto cancel;
+	if (r == -EPIPE && !(msghdr->msg_flags & MSG_NOSIGNAL))
+		send_sig(SIGPIPE, current, 0);
+
+out_end:
+	rpc_end(desc, 0);
+
+	return r;
+
+cancel:
+	rpc_cancel(desc);
+	r = err;
+	goto out_end;
+}
+
+ssize_t krg_faf_recvmsg(struct file *file, struct msghdr *msghdr,
+			size_t total_len, unsigned int flags)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_sendmsg_msg msg;
+	ssize_t r;
+	int err;
+	struct rpc_desc* desc;
+
+	msg.server_fd = data->server_fd;
+	msg.total_len = total_len;
+	msg.flags = flags;
+
+	desc = rpc_begin(RPC_FAF_RECVMSG, data->server_id);
+	if (!desc)
+		return -ENOMEM;
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto cancel;
+
+	err = send_msghdr(desc, msghdr, total_len, MSG_USER|MSG_HDR_ONLY);
+	if (err)
+		goto cancel;
+
+	err = unpack_remote_sleep_res_prepare(desc);
+	if (err)
+		goto cancel;
+	err = unpack_remote_sleep_res_type(desc, r);
+	if (err)
+		goto cancel;
+
+	if (r < 0)
+		goto out_end;
+
+	err = recv_msghdr(desc, msghdr, r, MSG_USER);
+	if (err)
+		goto cancel;
+
+	/* Behave as sock_recvmsg() */
+	msghdr->msg_control += msghdr->msg_controllen;
+
+out_end:
+	rpc_end(desc, 0);
+
+	return r;
+
+cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -EPIPE;
+	r = err;
+	goto out_end;
+}
+
+void krg_faf_srv_close(struct file *file)
+{
+	check_close_faf_srv_file(file);
+}
+
+int krg_faf_poll_wait(struct file *file, int wait)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_poll_wait_msg msg;
+	struct rpc_desc *desc;
+	unsigned int revents;
+	int err = -ENOMEM, res = 0;
+	long old_state = current->state;
+
+	data->poll_revents = 0;
+
+	msg.server_fd = data->server_fd;
+	msg.objid = file->f_objid;
+	msg.wait = wait;
+
+	desc = rpc_begin(RPC_FAF_POLL_WAIT, data->server_id);
+	if (!desc)
+		goto out;
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto err_cancel;
+	if (wait) {
+		err = rpc_unpack_type(desc, res);
+		if (err)
+			goto err_cancel;
+	}
+	err = rpc_unpack_type(desc, revents);
+	if (err)
+		goto err_cancel;
+
+	if (res)
+		err = res;
+	data->poll_revents = revents;
+
+out_end:
+	rpc_end(desc, 0);
+
+out:
+	/*
+	 * after sleeping rpc_unpack() returns with
+	 * current->state == TASK_RUNNING
+	 */
+	set_current_state(old_state);
+	return err;
+
+err_cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -ENOMEM;
+	goto out_end;
+}
+
+void krg_faf_poll_dequeue(struct file *file)
+{
+	faf_client_data_t *data = file->private_data;
+	struct faf_notify_msg msg;
+	int err;
+
+	msg.server_fd = data->server_fd;
+	msg.objid = file->f_objid;
+	err = rpc_async(RPC_FAF_POLL_DEQUEUE, data->server_id,
+			&msg, sizeof(msg));
+	if (err)
+		printk("faf_poll: memory leak on server %d!\n", data->server_id);
+}
+
+/** Kerrighed kernel hook for FAF poll function.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file          File to do a poll to.
+ */
+unsigned int faf_poll (struct file *file,
+		       struct poll_table_struct *wait)
+{
+	faf_client_data_t *data = file->private_data;
+	unsigned int revents;
+	long old_state = current->state;
+
+	mutex_lock(&faf_poll_mutex);
+	/* Waking up from mutex_lock() sets current->state to TASK_RUNNING */
+	set_current_state(old_state);
+	poll_wait(file, &data->poll_wq, wait);
+	if (!wait)
+		krg_faf_poll_wait(file, 0);
+	revents = data->poll_revents;
+	mutex_unlock(&faf_poll_mutex);
+
+	return revents;
+}
+
+static void handle_faf_poll_notify(struct rpc_desc *desc,
+				   void *_msg,
+				   size_t size)
+{
+	unsigned long dvfs_id = *(unsigned long *)_msg;
+	struct dvfs_file_struct *dvfs_file;
+	faf_client_data_t *data;
+
+	dvfs_file = _kddm_get_object_no_ft(dvfs_file_struct_ctnr, dvfs_id);
+	if (dvfs_file && dvfs_file->file) {
+		/* TODO: still required? */
+		if (atomic_read (&dvfs_file->file->f_count) == 0)
+			dvfs_file->file = NULL;
+	}
+	if (!dvfs_file || !dvfs_file->file)
+		goto out_put_dvfs_file;
+
+	data = dvfs_file->file->private_data;
+	wake_up_interruptible_all(&data->poll_wq);
+
+out_put_dvfs_file:
+	_kddm_put_object(dvfs_file_struct_ctnr, dvfs_id);
+}
+
+struct file_operations faf_file_ops = {
+	poll: faf_poll,
+};
+
+
+
+/* FAF Hooks Initialisation */
+
+void faf_hooks_init (void)
+{
+	rpc_register_void(RPC_FAF_POLL_NOTIFY, handle_faf_poll_notify, 0);
+}
+
+/* FAF Hooks Finalization */
+void faf_hooks_finalize (void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/fs/faf/faf_hooks.h android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_hooks.h
--- linux-2.6.29/kerrighed/fs/faf/faf_hooks.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_hooks.h	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,27 @@
+/** Kerrighed Open File Access Forwarding System.
+ *  @file file_forwarding.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __FAF_HOOKS__
+#define __FAF_HOOKS__
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN VARIABLES                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+extern struct file_operations faf_file_ops;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+void faf_hooks_init (void);
+void faf_hooks_finalize (void);
+
+#endif // __FAF_HOOKS__
diff -ruN linux-2.6.29/kerrighed/fs/faf/faf_internal.h android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_internal.h
--- linux-2.6.29/kerrighed/fs/faf/faf_internal.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_internal.h	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,41 @@
+/** Kerrighed Open File Access Forwarding System.
+ *  @file faf_internal.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __FAF__
+#define __FAF__
+
+#include <linux/wait.h>
+#include <kerrighed/faf.h>
+
+struct epm_action;
+struct dvfs_file_struct;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                 MACROS                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+#define FAF_HASH_TABLE_SIZE 1024
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+void faf_init (void);
+void faf_finalize (void);
+
+int check_activate_faf(struct task_struct *tsk, int index, struct file *file,
+		       struct epm_action *action);
+
+void check_last_faf_client_close(struct file *file,
+				 struct dvfs_file_struct *dvfs_file);
+void check_close_faf_srv_file(struct file *file);
+void free_faf_file_private_data(struct file *file);
+
+#endif // __FAF__
diff -ruN linux-2.6.29/kerrighed/fs/faf/faf_server.c android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_server.c
--- linux-2.6.29/kerrighed/fs/faf/faf_server.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_server.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,1573 @@
+/** Kerrighed FAF servers.
+ *  @file faf_server.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/fs.h>
+#include <linux/fs_struct.h>
+#include <linux/file.h>
+#include <linux/fdtable.h>
+#include <linux/syscalls.h>
+#include <linux/socket.h>
+#include <linux/sched.h>
+#include <linux/vmalloc.h>
+#include <linux/slab.h>
+#include <linux/proc_fs.h>
+#include <linux/poll.h>
+#include <linux/eventpoll.h>
+#include <linux/list.h>
+#include <linux/hash.h>
+#include <linux/statfs.h>
+#include <linux/remote_sleep.h>
+
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/remote_cred.h>
+#include <kerrighed/physical_fs.h>
+#include <kerrighed/file.h>
+#include "../file_struct_io_linker.h"
+
+#include "faf_internal.h"
+#include "faf_server.h"
+#include "faf_tools.h"
+#include <kerrighed/faf_file_mgr.h>
+#include "ruaccess.h"
+
+
+/* Just a hint that must be > 0 */
+#define FAF_POLL_NR_FD 1
+static int faf_poll_epfd = -1;
+
+struct faf_polled_fd {
+	struct hlist_node list;
+	unsigned long dvfs_id;
+	struct hlist_head nodes;
+	int count;
+};
+
+struct faf_polled_fd_node {
+	struct hlist_node list;
+	int count;
+	kerrighed_node_t node_id;
+};
+
+#define FAF_POLLED_FD_HASH_SHIFT 8
+#define FAF_POLLED_FD_HASH_SIZE (1 << FAF_POLLED_FD_HASH_SHIFT)
+static struct hlist_head *faf_polled_fd_hash;
+static DEFINE_MUTEX(faf_polled_fd_mutex);
+
+#define FAF_POLL_EVENTS (POLLIN     | \
+			 POLLOUT    | \
+			 POLLPRI    | \
+			 POLLRDNORM | \
+			 POLLWRNORM | \
+			 POLLRDBAND | \
+			 POLLWRBAND | \
+			 POLLRDHUP  | \
+			 EPOLLET)
+#define FAF_POLL_MAXEVENTS 10
+
+static int unpack_path(struct rpc_desc *desc, struct path *path)
+{
+	char *tmp;
+	int len, err;
+
+	err = -ENOMEM;
+	tmp = (char *)__get_free_page(GFP_KERNEL);
+	if (!tmp)
+		goto out;
+
+	err = rpc_unpack_type(desc, len);
+	if (err)
+		goto out_free;
+	err = rpc_unpack(desc, 0, tmp, len);
+	if (err)
+		goto out_free;
+
+	err = kern_path(tmp, LOOKUP_FOLLOW | LOOKUP_DIRECTORY, path);
+
+out_free:
+	free_page((unsigned long)tmp);
+
+out:
+	return err;
+}
+
+static int unpack_root(struct rpc_desc *desc, struct prev_root *prev_root)
+{
+	struct path root, tmp_root;
+	int err;
+
+	chroot_to_physical_root(prev_root);
+
+	err = unpack_path(desc, &root);
+	if (err) {
+		chroot_to_prev_root(prev_root);
+		return err;
+	}
+
+	write_lock(&current->fs->lock);
+	tmp_root = current->fs->root;
+	current->fs->root = root;
+	write_unlock(&current->fs->lock);
+	path_put(&tmp_root);
+
+	return err;
+}
+
+static int unpack_root_pwd(struct rpc_desc *desc, struct prev_root *prev_root)
+{
+	struct path root, pwd, tmp_root, tmp_pwd;
+	int err;
+
+	chroot_to_physical_root(prev_root);
+
+	err = unpack_path(desc, &root);
+	if (err)
+		goto out_err;
+	err = unpack_path(desc, &pwd);
+	if (err)
+		goto out_err_pwd;
+
+	write_lock(&current->fs->lock);
+	tmp_root = current->fs->root;
+	current->fs->root = root;
+	tmp_pwd = current->fs->pwd;
+	current->fs->pwd = pwd;
+	write_unlock(&current->fs->lock);
+	path_put(&tmp_root);
+	path_put(&tmp_pwd);
+
+	return err;
+
+out_err_pwd:
+	path_put(&root);
+out_err:
+	chroot_to_prev_root(prev_root);
+	return err;
+}
+
+static int unpack_context(struct rpc_desc *desc, struct prev_root *prev_root,
+			  const struct cred **old_cred)
+{
+	int err;
+
+	*old_cred = unpack_override_creds(desc);
+	if (IS_ERR(*old_cred))
+		return PTR_ERR(*old_cred);
+
+	err = unpack_root_pwd(desc, prev_root);
+	if (err)
+		revert_creds(*old_cred);
+
+	return err;
+}
+
+static
+void
+restore_context(const struct prev_root *prev_root, const struct cred *old_cred)
+{
+	chroot_to_prev_root(prev_root);
+	revert_creds(old_cred);
+}
+
+/** Handler for reading in a FAF open file.
+ *  @author Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param from    Node sending the request
+ *  @param msgIn   Request message
+ */
+void handle_faf_read(struct rpc_desc* desc, void *msgIn, size_t size)
+{
+	struct faf_rw_msg *msg = msgIn;
+	struct file *file = NULL;
+	char *buf = NULL;
+	long buf_size = PAGE_SIZE;
+	ssize_t to_read, r;
+	loff_t fpos;
+	int err;
+
+	err = remote_sleep_prepare(desc);
+	if (err) {
+		rpc_cancel(desc);
+		return;
+	}
+
+	to_read = msg->count;
+	fpos = msg->pos;
+
+	r = -ENOMEM;
+	buf = kmalloc(PAGE_SIZE, GFP_KERNEL);
+	if (!buf)
+		goto error;
+
+	file = fget(msg->server_fd);
+
+	while (to_read > 0) {
+		if (to_read < PAGE_SIZE)
+			buf_size = to_read;
+
+		r = vfs_read(file, buf, buf_size, &fpos);
+
+		if (r > 0) {
+			err = rpc_pack_type(desc, r);
+			if (err)
+				goto cancel;
+			err = rpc_pack(desc, 0, buf, r);
+			if (err)
+				goto cancel;
+		}
+
+		/*
+		 * Check if we have reach the end of the file
+		 * or if there is an error
+		 */
+		if (r < buf_size)
+			break;
+
+		to_read -= r;
+	}
+
+error:
+	/*
+	 * Pack the end of transmission mark (0)
+	 * or the error returned by vfs_read()
+	 */
+	if (r > 0)
+		r = 0;
+	err = rpc_pack_type(desc, r);
+	if (err)
+		goto cancel;
+
+	/* send the updated file position */
+	err = rpc_pack_type(desc, fpos);
+	if (err)
+		goto cancel;
+
+out:
+	if (buf)
+		kfree(buf);
+	if (file)
+		fput(file);
+
+	remote_sleep_finish();
+	return;
+
+cancel:
+	rpc_cancel(desc);
+	goto out;
+}
+
+/** Handler for writing in a FAF open file.
+ *  @author Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param from    Node sending the request
+ *  @param msgIn   Request message
+ */
+void handle_faf_write(struct rpc_desc* desc, void *msgIn, size_t size)
+{
+	struct faf_rw_msg *msg = msgIn;
+	struct file *file = NULL;
+	long to_recv;
+	char *buf = NULL;
+	ssize_t buf_size = PAGE_SIZE;
+	ssize_t r, nr_received = -ENOMEM;
+	loff_t fpos;
+	int err;
+
+	r = remote_sleep_prepare(desc);
+	if (r) {
+		rpc_cancel(desc);
+		return;
+	}
+
+	to_recv = msg->count;
+	fpos = msg->pos;
+
+	buf = kmalloc(PAGE_SIZE, GFP_KERNEL);
+	if (!buf)
+		goto error;
+
+	nr_received = 0;
+
+	file = fget(msg->server_fd);
+
+	while (to_recv > 0) {
+		err = rpc_unpack_type(desc, buf_size);
+		if (err)
+			goto cancel;
+
+		/* copy_from_user has failed on the other side */
+		if (buf_size < 0) {
+			nr_received = buf_size;
+			break;
+		}
+
+		err = rpc_unpack(desc, 0, buf, buf_size);
+		if (err)
+			goto cancel;
+
+		r = vfs_write(file, buf, buf_size, &fpos);
+
+		/* The last write failed. Break the write sequence */
+		if (r < 0) {
+			nr_received = r;
+			break;
+		}
+		nr_received += r;
+		to_recv -= buf_size;
+	}
+
+error:
+	err = rpc_pack_type(desc, nr_received);
+	if (err)
+		goto cancel;
+
+	/* send the updated file position */
+	err = rpc_pack_type(desc, fpos);
+	if (err)
+		goto cancel;
+
+out:
+	if (buf)
+		kfree(buf);
+	if (file)
+		fput(file);
+
+	remote_sleep_finish();
+	return;
+
+cancel:
+	rpc_cancel(desc);
+	goto out;
+}
+
+static void handle_faf_readv(struct rpc_desc *desc, void *__msg, size_t size)
+{
+	struct faf_rw_msg *msg = __msg;
+	struct faf_rw_ret ret;
+	struct file *file;
+	struct iovec *iov;
+	int iovcnt, err;
+
+	err = alloc_iov(&iov, &iovcnt, msg->count);
+	if (err) {
+		ret.ret = err;
+		iov = NULL;
+	}
+
+	err = remote_sleep_prepare(desc);
+	if (err)
+		goto cancel;
+
+	ret.pos = msg->pos;
+	if (iov) {
+		file = fget(msg->server_fd);
+		ret.ret = vfs_readv(file, iov, iovcnt, &ret.pos);
+		fput(file);
+	}
+
+	remote_sleep_finish();
+
+	err = rpc_pack_type(desc, ret);
+	if (err)
+		goto cancel;
+	if (ret.ret <= 0)
+		goto out_free;
+
+	err = send_iov(desc, iov, iovcnt, ret.ret, 0);
+	if (err)
+		goto cancel;
+
+out_free:
+	if (iov)
+		free_iov(iov, iovcnt);
+
+	return;
+
+cancel:
+	rpc_cancel(desc);
+	goto out_free;
+}
+
+static void handle_faf_writev(struct rpc_desc *desc, void *__msg, size_t size)
+{
+	struct faf_rw_msg *msg = __msg;
+	struct faf_rw_ret ret;
+	struct file *file;
+	struct iovec *iov;
+	int iovcnt, err;
+
+	err = alloc_iov(&iov, &iovcnt, msg->count);
+	if (!err) {
+		err = recv_iov(desc, iov, iovcnt, msg->count, 0);
+		if (err)
+			goto cancel;
+	} else {
+		ret.ret = err;
+		iov = NULL;
+	}
+
+	err = remote_sleep_prepare(desc);
+	if (err)
+		goto cancel;
+
+	ret.pos = msg->pos;
+	if (iov) {
+		file = fget(msg->server_fd);
+		ret.ret = vfs_writev(file, iov, iovcnt, &ret.pos);
+		fput(file);
+	}
+
+	remote_sleep_finish();
+
+	err = rpc_pack_type(desc, ret);
+	if (err)
+		goto cancel;
+
+out_free:
+	if (iov)
+		free_iov(iov, iovcnt);
+
+	return;
+
+cancel:
+	rpc_cancel(desc);
+	goto out_free;
+}
+
+/** Handler for doing an IOCTL in a FAF open file.
+ *  @author Renaud Lottiaux
+ *
+ *  @param from    Node sending the request
+ *  @param msgIn   Request message
+ */
+void handle_faf_ioctl(struct rpc_desc *desc,
+		      void *msgIn, size_t size)
+{
+	struct faf_ctl_msg *msg = msgIn;
+	struct prev_root prev_root;
+	const struct cred *old_cred;
+	long r;
+	int err;
+
+	err = unpack_context(desc, &prev_root, &old_cred);
+	if (err) {
+		rpc_cancel(desc);
+		return;
+	}
+
+	err = remote_sleep_prepare(desc);
+	if (err)
+		goto out_err;
+
+	err = prepare_ruaccess(desc);
+	if (err)
+		goto out_sleep_finish;
+
+	r = sys_ioctl(msg->server_fd, msg->cmd, msg->arg);
+
+	err = cleanup_ruaccess(desc);
+	if (err)
+		goto out_sleep_finish;
+
+	err = rpc_pack_type(desc, r);
+
+out_sleep_finish:
+	remote_sleep_finish();
+	if (err)
+		goto out_err;
+
+out:
+	restore_context(&prev_root, old_cred);
+
+	return;
+
+out_err:
+	rpc_cancel(desc);
+	goto out;
+}
+
+/** Handler for doing an FCNTL in a FAF open file.
+ *  @author Renaud Lottiaux
+ *
+ *  @param from    Node sending the request
+ *  @param msgIn   Request message
+ */
+void handle_faf_fcntl (struct rpc_desc* desc,
+		       void *msgIn, size_t size)
+{
+	struct faf_ctl_msg *msg = msgIn;
+	const struct cred *old_cred;
+	unsigned long arg;
+	long r;
+	int err;
+
+	if (msg->cmd == F_GETLK || msg->cmd == F_SETLK || msg->cmd == F_SETLKW)
+		arg = (unsigned long) &msg->flock;
+	else
+		arg = msg->arg;
+
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred))
+		goto cancel;
+	err = remote_sleep_prepare(desc);
+	if (err) {
+		revert_creds(old_cred);
+		goto cancel;
+	}
+
+	r = sys_fcntl (msg->server_fd, msg->cmd, arg);
+
+	remote_sleep_finish();
+	revert_creds(old_cred);
+
+	err = rpc_pack_type(desc, r);
+	if (unlikely(err))
+		goto cancel;
+
+	if (!r && msg->cmd == F_GETLK) {
+		err = rpc_pack_type(desc, msg->flock);
+		if (unlikely(err))
+			goto cancel;
+	}
+
+	return;
+cancel:
+	rpc_cancel(desc);
+}
+
+#if BITS_PER_LONG == 32
+/** Handler for doing an FCNTL64 in a FAF open file.
+ *  @author Renaud Lottiaux
+ *
+ *  @param from    Node sending the request
+ *  @param msgIn   Request message
+ */
+void handle_faf_fcntl64 (struct rpc_desc* desc,
+			 void *msgIn, size_t size)
+{
+	struct faf_ctl_msg *msg = msgIn;
+	const struct cred *old_cred;
+	unsigned long arg;
+	long r;
+	int err;
+
+	if (msg->cmd == F_GETLK64 || msg->cmd == F_SETLK64 || msg->cmd == F_SETLKW64)
+		arg = (unsigned long) &msg->flock64;
+	else
+		arg = msg->arg;
+
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred))
+		goto cancel;
+	err = remote_sleep_prepare(desc);
+	if (err) {
+		revert_creds(old_cred);
+		goto cancel;
+	}
+
+	r = sys_fcntl64 (msg->server_fd, msg->cmd, arg);
+
+	remote_sleep_finish();
+	revert_creds(old_cred);
+
+	err = rpc_pack_type(desc, r);
+	if (unlikely(err))
+		goto cancel;
+
+	if (!r && msg->cmd == F_GETLK64) {
+		err = rpc_pack_type(desc, msg->flock64);
+		if (unlikely(err))
+			goto cancel;
+	}
+
+	return;
+cancel:
+	rpc_cancel(desc);
+}
+#endif
+
+/** Handler for doing an FSTAT in a FAF open file.
+ *  @author Renaud Lottiaux
+ *
+ *  @param from    Node sending the request
+ *  @param msgIn   Request message
+ */
+void handle_faf_fstat (struct rpc_desc* desc,
+		       void *msgIn, size_t size)
+{
+	struct kstat statbuf;
+	struct faf_stat_msg *msg = msgIn;
+	long r;
+
+	r = vfs_fstat (msg->server_fd, &statbuf);
+
+	rpc_pack_type(desc, r);
+	rpc_pack_type(desc, statbuf);
+}
+
+/** Handler for doing an FSTATFS in a FAF open file.
+ *  @author Matthieu Fertré
+ *
+ *  @param from    Node sending the request
+ *  @param msgIn   Request message
+ */
+static void handle_faf_fstatfs(struct rpc_desc* desc,
+			       void *msgIn, size_t size)
+{
+	struct statfs statbuf;
+	struct faf_statfs_msg *msg = msgIn;
+	long r;
+	int err_rpc;
+
+	r = sys_fstatfs(msg->server_fd, &statbuf);
+
+	err_rpc = rpc_pack_type(desc, r);
+	if (err_rpc)
+		goto err_rpc;
+
+	if (!r)
+		err_rpc = rpc_pack_type(desc, statbuf);
+err_rpc:
+	if (err_rpc)
+		rpc_cancel(desc);
+}
+
+/** Handler for seeking in a FAF open file.
+ *  @author Renaud Lottiaux
+ *
+ *  @param from    Node sending the request
+ *  @param msgIn   Request message
+ */
+void handle_faf_lseek (struct rpc_desc* desc,
+		       void *msgIn, size_t size)
+{
+	struct faf_seek_msg *msg = msgIn;
+	off_t r = -EINVAL;
+
+	r = sys_lseek (msg->server_fd, msg->offset, msg->origin);
+
+	rpc_pack_type(desc, r);
+}
+
+/** Handler for seeking in a FAF open file.
+ *  @author Renaud Lottiaux
+ *
+ *  @param from    Node sending the request
+ *  @param msgIn   Request message
+ */
+void handle_faf_llseek (struct rpc_desc* desc,
+			void *msgIn, size_t size)
+{
+	struct faf_llseek_msg *msg = msgIn;
+	long r = -EINVAL;
+	loff_t result;
+
+	r = sys_llseek (msg->server_fd, msg->offset_high, msg->offset_low,
+			&result, msg->origin);
+
+	rpc_pack_type(desc, r);
+	rpc_pack_type(desc, result);
+}
+
+/** Handler for syncing in a FAF open file.
+ *  @author Renaud Lottiaux
+ *
+ *  @param from    Node sending the request
+ *  @param msgIn   Request message
+ */
+int handle_faf_fsync (struct rpc_desc* desc,
+                      void *msgIn, size_t size)
+{
+	struct faf_rw_msg *msg = msgIn;
+	long r = -EINVAL;
+
+	r = sys_fsync (msg->server_fd);
+
+	return r;
+}
+
+/** Handler for locking in a FAF open file.
+ *  @author Renaud Lottiaux
+ *
+ *  @param from    Node sending the request
+ *  @param msgIn   Request message
+ */
+void handle_faf_flock(struct rpc_desc *desc,
+                      void *msgIn, size_t size)
+{
+	struct faf_ctl_msg *msg = msgIn;
+	const struct cred *old_cred;
+	long r = -EINVAL;
+	int err;
+
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred))
+		goto cancel;
+	r = remote_sleep_prepare(desc);
+	if (r) {
+		revert_creds(old_cred);
+		goto cancel;
+	}
+
+	r = sys_flock (msg->server_fd, msg->cmd);
+
+	remote_sleep_finish();
+	revert_creds(old_cred);
+
+	err = rpc_pack_type(desc, r);
+	if (err)
+		goto cancel;
+
+	return;
+
+cancel:
+	rpc_cancel(desc);
+}
+
+/*
+ * Handlers for polling a FAF open file.
+ * @author Louis Rilling
+ */
+static void faf_poll_notify_node(kerrighed_node_t node, unsigned long dvfs_id)
+{
+	int err;
+
+	err = rpc_async(RPC_FAF_POLL_NOTIFY, node, &dvfs_id, sizeof(dvfs_id));
+	if (err)
+		printk(KERN_WARNING "faf_poll_notify_node: "
+		       "failed to notify node %d for %lu\n",
+		       node, dvfs_id);
+}
+
+static struct faf_polled_fd *__faf_polled_fd_find(unsigned long dvfs_id);
+
+static void faf_poll_notify_nodes(unsigned long dvfs_id)
+{
+	struct dvfs_file_struct *dvfs_file;
+	struct faf_polled_fd *polled_fd;
+	struct faf_polled_fd_node *polled_fd_node;
+	struct hlist_node *pos;
+
+	dvfs_file = _kddm_get_object_no_ft(dvfs_file_struct_ctnr, dvfs_id);
+	if (dvfs_file && dvfs_file->file) {
+		/* TODO: still required? */
+		if (atomic_read (&dvfs_file->file->f_count) == 0)
+			dvfs_file->file = NULL;
+	}
+	if (!dvfs_file || !dvfs_file->file)
+		goto out_put_dvfs_file;
+
+	mutex_lock(&faf_polled_fd_mutex);
+
+	polled_fd = __faf_polled_fd_find(dvfs_id);
+	if (!polled_fd)
+		goto out_unlock;
+
+	hlist_for_each_entry(polled_fd_node, pos, &polled_fd->nodes, list)
+		faf_poll_notify_node(polled_fd_node->node_id, dvfs_id);
+
+out_unlock:
+	mutex_unlock(&faf_polled_fd_mutex);
+
+out_put_dvfs_file:
+	_kddm_put_object(dvfs_file_struct_ctnr, dvfs_id);
+}
+
+static int faf_poll_thread(void *arg)
+{
+	static struct epoll_event events[FAF_POLL_MAXEVENTS];
+	int epfd = (int)(long)arg;
+	long ret;
+	int i;
+
+	set_task_comm(current, "faf_poll");
+
+	for (;;) {
+		ret = sys_epoll_wait(epfd, events, FAF_POLL_MAXEVENTS, -1);
+		BUG_ON(ret < 0);
+
+		for (i = 0; i < ret; i++)
+			faf_poll_notify_nodes((unsigned long)events[i].data);
+	}
+
+	return 0;
+}
+
+static int faf_poll_init_epfd(void)
+{
+	long pid;
+	int epfd;
+
+	if (faf_poll_epfd >= 0)
+		goto out;
+
+	epfd = (int) sys_epoll_create(FAF_POLL_NR_FD);
+	if (epfd < 0)
+		goto err_epfd;
+
+	pid = kernel_thread(faf_poll_thread, (void *)(long)epfd, CLONE_FILES);
+	if (pid < 0)
+		goto err_thread;
+
+	faf_poll_epfd = epfd;
+
+out:
+	return 0;
+
+err_thread:
+	sys_close(epfd);
+	epfd = (int)pid;
+err_epfd:
+	return epfd;
+}
+
+static inline unsigned long faf_polled_fd_hashfn(unsigned long id)
+{
+	return hash_long(id, FAF_POLLED_FD_HASH_SHIFT);
+}
+
+static struct faf_polled_fd *__faf_polled_fd_find(unsigned long dvfs_id)
+{
+	struct faf_polled_fd *polled_fd;
+	struct hlist_head *hash_list;
+	struct hlist_node *pos;
+
+	hash_list = &faf_polled_fd_hash[faf_polled_fd_hashfn(dvfs_id)];
+	hlist_for_each_entry(polled_fd, pos, hash_list, list)
+		if (polled_fd->dvfs_id == dvfs_id)
+			return polled_fd;
+	return NULL;
+}
+
+static struct faf_polled_fd *faf_polled_fd_find(unsigned long dvfs_id)
+{
+	struct faf_polled_fd *polled_fd;
+
+	polled_fd = __faf_polled_fd_find(dvfs_id);
+	if (polled_fd)
+		goto out;
+
+	polled_fd = kmalloc(sizeof(*polled_fd), GFP_KERNEL);
+	if (!polled_fd)
+		goto out;
+
+	polled_fd->dvfs_id = dvfs_id;
+	INIT_HLIST_HEAD(&polled_fd->nodes);
+	polled_fd->count = 0;
+	hlist_add_head(&polled_fd->list,
+		       &faf_polled_fd_hash[faf_polled_fd_hashfn(dvfs_id)]);
+
+out:
+	return polled_fd;
+}
+
+static void faf_polled_fd_free(struct faf_polled_fd *polled_fd)
+{
+	BUG_ON(!hlist_empty(&polled_fd->nodes));
+	BUG_ON(polled_fd->count);
+	hlist_del(&polled_fd->list);
+	kfree(polled_fd);
+}
+
+static
+struct faf_polled_fd_node *
+__faf_polled_fd_find_node(struct faf_polled_fd *polled_fd,
+			  kerrighed_node_t node)
+{
+	struct faf_polled_fd_node *polled_fd_node;
+	struct hlist_node *pos;
+
+	hlist_for_each_entry(polled_fd_node, pos, &polled_fd->nodes, list)
+		if (polled_fd_node->node_id == node)
+			return polled_fd_node;
+	return NULL;
+}
+
+static
+struct faf_polled_fd_node *
+faf_polled_fd_find_node(struct faf_polled_fd *polled_fd, kerrighed_node_t node)
+{
+	struct faf_polled_fd_node *polled_fd_node;
+
+	polled_fd_node = __faf_polled_fd_find_node(polled_fd, node);
+	if (polled_fd_node)
+		goto out;
+
+	polled_fd_node = kmalloc(sizeof(*polled_fd_node), GFP_KERNEL);
+	if (!polled_fd_node)
+		goto out;
+
+	polled_fd_node->node_id = node;
+	polled_fd_node->count = 0;
+	hlist_add_head(&polled_fd_node->list, &polled_fd->nodes);
+	polled_fd->count++;
+
+out:
+	return polled_fd_node;
+}
+
+static void faf_polled_fd_node_free(struct faf_polled_fd *polled_fd,
+				    struct faf_polled_fd_node *polled_fd_node)
+{
+	BUG_ON(polled_fd_node->count);
+	hlist_del(&polled_fd_node->list);
+	polled_fd->count--;
+	kfree(polled_fd_node);
+}
+
+static int faf_polled_fd_add(kerrighed_node_t client,
+			     int server_fd,
+			     unsigned long dvfs_id)
+{
+	struct faf_polled_fd *polled_fd;
+	struct faf_polled_fd_node *polled_fd_node;
+	struct epoll_event event;
+	int err;
+
+	mutex_lock(&faf_polled_fd_mutex);
+	err = -ENOMEM;
+	polled_fd = faf_polled_fd_find(dvfs_id);
+	if (IS_ERR(polled_fd)) {
+		err = PTR_ERR(polled_fd);
+		goto err_polled_fd;
+	}
+	polled_fd_node = faf_polled_fd_find_node(polled_fd, client);
+	if (!polled_fd_node)
+		goto err_polled_fd_node;
+
+	err = 0;
+	polled_fd_node->count++;
+	if (polled_fd_node->count > 1)
+		/* Already polled by this node */
+		goto out_unlock_polled_fd;
+	if (polled_fd->count > 1)
+		/* Already polled by another node */
+		goto out_unlock_polled_fd;
+
+	err = faf_poll_init_epfd();
+	if (err)
+		goto err_epoll_ctl;
+	event.events = FAF_POLL_EVENTS;
+	event.data = dvfs_id;
+	err = sys_epoll_ctl(faf_poll_epfd, EPOLL_CTL_ADD, server_fd, &event);
+	if (err)
+		goto err_epoll_ctl;
+
+out_unlock_polled_fd:
+	mutex_unlock(&faf_polled_fd_mutex);
+
+	return err;
+
+err_epoll_ctl:
+	polled_fd_node->count--;
+	faf_polled_fd_node_free(polled_fd, polled_fd_node);
+err_polled_fd_node:
+	if (!polled_fd->count)
+		faf_polled_fd_free(polled_fd);
+err_polled_fd:
+	printk(KERN_WARNING
+	       "faf_polled_fd_add: failed to forward polling of %lu\n",
+	       dvfs_id);
+	goto out_unlock_polled_fd;
+}
+
+static int faf_polled_fd_remove(kerrighed_node_t client,
+				int server_fd,
+				unsigned long dvfs_id)
+{
+	struct dvfs_file_struct *dvfs_file;
+	struct faf_polled_fd *polled_fd;
+	struct faf_polled_fd_node *polled_fd_node;
+	int err;
+
+	dvfs_file = _kddm_get_object_no_ft(dvfs_file_struct_ctnr, dvfs_id);
+	if (dvfs_file && dvfs_file->file) {
+		/* TODO: still required? */
+		if (atomic_read (&dvfs_file->file->f_count) == 0)
+			dvfs_file->file = NULL;
+	}
+
+	mutex_lock(&faf_polled_fd_mutex);
+
+	polled_fd = __faf_polled_fd_find(dvfs_id);
+	BUG_ON(!polled_fd);
+	BUG_ON(!polled_fd->count);
+	polled_fd_node = __faf_polled_fd_find_node(polled_fd, client);
+	BUG_ON(!polled_fd_node);
+	BUG_ON(!polled_fd_node->count);
+
+	polled_fd_node->count--;
+	if (!polled_fd_node->count)
+		faf_polled_fd_node_free(polled_fd, polled_fd_node);
+	if (polled_fd->count)
+		goto out_unlock;
+
+	if (!dvfs_file || !dvfs_file->file)
+		/*
+		 * The file is already closed or about to be closed. The last
+		 * __fput() automatically removes it from the interest set of
+		 * faf_poll_epfd.
+		 */
+		goto free_polled_fd;
+
+	BUG_ON(faf_poll_epfd < 0);
+	err = sys_epoll_ctl(faf_poll_epfd, EPOLL_CTL_DEL, server_fd, NULL);
+	BUG_ON(err);
+
+free_polled_fd:
+	faf_polled_fd_free(polled_fd);
+
+out_unlock:
+	mutex_unlock(&faf_polled_fd_mutex);
+
+	_kddm_put_object(dvfs_file_struct_ctnr, dvfs_id);
+
+	return 0;
+}
+
+static
+void handle_faf_poll_wait(struct rpc_desc *desc, void *_msg, size_t size)
+{
+	struct faf_poll_wait_msg *msg = _msg;
+	struct file *file;
+	unsigned int revents;
+	int err, res = 0;
+
+	if (msg->wait) {
+		res = faf_polled_fd_add(desc->client,
+					msg->server_fd,
+					msg->objid);
+		err = rpc_pack_type(desc, res);
+		if (err)
+			goto err;
+	}
+
+	file = fget(msg->server_fd);
+	BUG_ON(!file);
+	revents = file->f_op->poll(file, NULL);
+	fput(file);
+
+	err = rpc_pack_type(desc, revents);
+	if (err)
+		goto err;
+
+	return;
+
+err:
+	if (msg->wait && !res)
+		faf_polled_fd_remove(desc->client, msg->server_fd, msg->objid);
+	rpc_cancel(desc);
+}
+
+static
+void handle_faf_poll_dequeue(struct rpc_desc* desc, void *_msg, size_t size)
+{
+	struct faf_notify_msg *msg = _msg;
+
+	faf_polled_fd_remove(desc->client, msg->server_fd, msg->objid);
+}
+
+static void faf_poll_init(void)
+{
+	int i;
+
+	faf_polled_fd_hash = kmalloc(FAF_POLLED_FD_HASH_SIZE *
+				     sizeof(*faf_polled_fd_hash),
+				     GFP_KERNEL);
+	if (!faf_polled_fd_hash)
+		panic("Couldn't allocate FAF poll descriptor table!\n");
+	for (i = 0; i < FAF_POLLED_FD_HASH_SIZE; i++)
+		INIT_HLIST_HEAD(&faf_polled_fd_hash[i]);
+
+	rpc_register_void(RPC_FAF_POLL_WAIT, handle_faf_poll_wait, 0);
+	rpc_register_void(RPC_FAF_POLL_DEQUEUE, handle_faf_poll_dequeue, 0);
+}
+
+
+
+/** Handler for d_path in a FAF open file.
+ *  @author Renaud Lottiaux
+ *
+ *  @param from    Node sending the request
+ *  @param msgIn   Request message
+ */
+void handle_faf_d_path (struct rpc_desc* desc,
+			void *msgIn, size_t size)
+{
+	struct faf_d_path_msg *msg = msgIn;
+	char *buff, *file_name = NULL;
+	struct file *file;
+	struct prev_root prev_root;
+	const struct cred *old_cred;
+	bool deleted = false;
+	int len;
+	int err;
+
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred)) {
+		rpc_cancel(desc);
+		return;
+	}
+	err = unpack_root(desc, &prev_root);
+	if (err) {
+		revert_creds(old_cred);
+		rpc_cancel(desc);
+		return;
+	}
+
+	buff = kmalloc (msg->count, GFP_KERNEL);
+
+	file = fcheck_files (current->files, msg->server_fd);
+	/* Remote caller holds a reference so it can't disappear. */
+	BUG_ON(!file);
+	if (msg->deleted)
+		file_name = d_path_check(&file->f_path, buff, msg->count, &deleted);
+	else
+		file_name = d_path(&file->f_path, buff, msg->count);
+	if (IS_ERR(file_name))
+		len = PTR_ERR(file_name);
+	else
+		len = strlen(file_name) + 1;
+
+	err = rpc_pack_type(desc, len);
+	if (err)
+		goto err_cancel;
+	if (len >= 0) {
+		err = rpc_pack(desc, 0, file_name, len);
+		if (err)
+			goto err_cancel;
+		if (msg->deleted) {
+			err = rpc_pack_type(desc, deleted);
+			if (err)
+				goto err_cancel;
+		}
+	}
+
+out:
+	kfree (buff);
+
+	chroot_to_prev_root(&prev_root);
+	revert_creds(old_cred);
+
+	return;
+
+err_cancel:
+	rpc_cancel(desc);
+	goto out;
+}
+
+
+
+int handle_faf_bind (struct rpc_desc* desc,
+                     void *msgIn, size_t size)
+{
+	struct faf_bind_msg *msg = msgIn;
+	struct prev_root prev_root;
+	const struct cred *old_cred;
+	int r;
+
+	r = unpack_context(desc, &prev_root, &old_cred);
+	if (r) {
+		rpc_cancel(desc);
+		return r;
+	}
+
+	r = sys_bind(msg->server_fd, (struct sockaddr *)&msg->sa, msg->addrlen);
+
+	restore_context(&prev_root, old_cred);
+
+	return r;
+}
+
+void handle_faf_connect(struct rpc_desc *desc,
+			void *msgIn, size_t size)
+{
+	struct faf_bind_msg *msg = msgIn;
+	struct prev_root prev_root;
+	const struct cred *old_cred;
+	int r, err;
+
+	r = unpack_context(desc, &prev_root, &old_cred);
+	if (r) {
+		rpc_cancel(desc);
+		return;
+	}
+
+	r = remote_sleep_prepare(desc);
+	if (r)
+		goto cancel;
+
+	r = sys_connect(msg->server_fd,
+			(struct sockaddr *)&msg->sa, msg->addrlen);
+
+	remote_sleep_finish();
+
+	err = rpc_pack_type(desc, r);
+	if (err)
+		goto cancel;
+
+out:
+	restore_context(&prev_root, old_cred);
+
+	return;
+
+cancel:
+	rpc_cancel(desc);
+	goto out;
+}
+
+int handle_faf_listen (struct rpc_desc* desc,
+		       void *msgIn, size_t size)
+{
+	struct faf_listen_msg *msg = msgIn;
+	int r;
+
+	r = sys_listen(msg->server_fd, msg->backlog);
+
+	return r;
+}
+
+void handle_faf_accept (struct rpc_desc *desc,
+		        void *msgIn, size_t size)
+{
+	struct faf_bind_msg *msg = msgIn;
+	int err, r;
+	struct file *file;
+
+	r = remote_sleep_prepare(desc);
+	if (r)
+		goto err_cancel;
+
+	r = sys_accept(msg->server_fd,
+		       (struct sockaddr *)&msg->sa, &msg->addrlen);
+
+	remote_sleep_finish();
+
+	err = rpc_pack_type(desc, r);
+	if (err)
+		goto err_close_file;
+
+	if (r < 0)
+		return;
+
+	file = fcheck_files(current->files, r);
+
+	if (!file->f_objid) {
+		err = create_kddm_file_object(file);
+		if (err)
+			goto err_close_file;
+	}
+
+	file->f_flags |= O_FAF_SRV;
+	file->f_faf_srv_index = r;
+
+	/* Increment the DVFS count for the client node */
+	get_dvfs_file(r, file->f_objid);
+
+	err = rpc_pack_type(desc, msg->addrlen);
+	if (err)
+		goto err_close_faf_file;
+
+	err = rpc_pack(desc, 0, &msg->sa, msg->addrlen);
+	if (err)
+		goto err_close_faf_file;
+
+	err = __send_faf_file_desc(desc, file);
+	if (err)
+		goto err_close_faf_file;
+
+	err = rpc_unpack_type(desc, r);
+	if (err)
+		goto err_close_faf_file;
+
+out:
+	return;
+
+err_cancel:
+	rpc_cancel(desc);
+	goto out;
+
+err_close_faf_file:
+	/* The client couldn't setup a FAF client file. */
+	put_dvfs_file(file->f_faf_srv_index, file);
+	check_close_faf_srv_file(file);
+	goto err_cancel;
+
+err_close_file:
+	if (r >= 0)
+		sys_close(r);
+	goto err_cancel;
+}
+
+int handle_faf_getsockname (struct rpc_desc* desc,
+			    void *msgIn, size_t size)
+{
+	struct faf_bind_msg *msg = msgIn;
+	struct prev_root prev_root;
+	int r;
+
+	unpack_root(desc, &prev_root);
+
+	r = sys_getsockname(msg->server_fd,
+			    (struct sockaddr *)&msg->sa, &msg->addrlen);
+
+	rpc_pack_type(desc, msg->addrlen);
+	rpc_pack(desc, 0, &msg->sa, msg->addrlen);
+
+	chroot_to_prev_root(&prev_root);
+
+	return r;
+}
+
+int handle_faf_getpeername (struct rpc_desc* desc,
+			    void *msgIn, size_t size)
+{
+	struct faf_bind_msg *msg = msgIn;
+	struct prev_root prev_root;
+	int r;
+
+	unpack_root(desc, &prev_root);
+
+	r = sys_getpeername(msg->server_fd,
+			    (struct sockaddr *)&msg->sa, &msg->addrlen);
+
+	rpc_pack_type(desc, msg->addrlen);
+	rpc_pack(desc, 0, &msg->sa, msg->addrlen);
+
+	chroot_to_prev_root(&prev_root);
+
+	return r;
+}
+
+int handle_faf_shutdown (struct rpc_desc* desc,
+                     void *msgIn, size_t size)
+{
+	struct faf_shutdown_msg *msg = msgIn;
+	int r;
+
+	r = sys_shutdown(msg->server_fd, msg->how);
+
+	return r;
+}
+
+void handle_faf_setsockopt (struct rpc_desc *desc,
+			    void *msgIn, size_t size)
+{
+	struct faf_setsockopt_msg *msg = msgIn;
+	struct prev_root prev_root;
+	const struct cred *old_cred;
+	int r, err;
+
+	err = unpack_context(desc, &prev_root, &old_cred);
+	if (err) {
+		rpc_cancel(desc);
+		return;
+	}
+
+	err = prepare_ruaccess(desc);
+	if (err)
+		goto out_err;
+	r = sys_setsockopt(msg->server_fd, msg->level, msg->optname,
+			   msg->optval, msg->optlen);
+	err = cleanup_ruaccess(desc);
+	if (err)
+		goto out_err;
+	err = rpc_pack_type(desc, r);
+	if (err)
+		goto out_err;
+
+exit:
+	restore_context(&prev_root, old_cred);
+
+	return;
+
+out_err:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -ENOMEM;
+	r = err;
+	goto exit;
+}
+
+void handle_faf_getsockopt (struct rpc_desc *desc,
+			    void *msgIn, size_t size)
+{
+	struct faf_getsockopt_msg *msg = msgIn;
+	struct prev_root prev_root;
+	const struct cred *old_cred;
+	int r, err;
+
+	err = unpack_context(desc, &prev_root, &old_cred);
+	if (err) {
+		rpc_cancel(desc);
+		return;
+	}
+
+	err = prepare_ruaccess(desc);
+	if (err)
+		goto out_err;
+	r = sys_getsockopt(msg->server_fd, msg->level, msg->optname,
+			   msg->optval, msg->optlen);
+	err = cleanup_ruaccess(desc);
+	if (err)
+		goto out_err;
+	err = rpc_pack_type(desc, r);
+		goto out_err;
+
+exit:
+	restore_context(&prev_root, old_cred);
+
+	return;
+
+out_err:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -ENOMEM;
+	r = err;
+	goto exit;
+}
+
+void handle_faf_sendmsg(struct rpc_desc *desc,
+			void *msgIn, size_t size)
+{
+	struct faf_sendmsg_msg *msg = msgIn;
+	ssize_t r;
+	int err;
+	struct msghdr msghdr;
+
+	err = recv_msghdr(desc, &msghdr, msg->total_len, 0);
+	if (err) {
+		rpc_cancel(desc);
+		return;
+	}
+
+	err = remote_sleep_prepare(desc);
+	if (err)
+		goto cancel;
+
+	r = sys_sendmsg (msg->server_fd, &msghdr, msg->flags);
+
+	remote_sleep_finish();
+
+	err = rpc_pack_type(desc, r);
+	if (err)
+		goto cancel;
+
+out_free:
+	free_msghdr(&msghdr);
+
+	return;
+
+cancel:
+	rpc_cancel(desc);
+	goto out_free;
+}
+
+void handle_faf_recvmsg(struct rpc_desc *desc,
+			void *msgIn, size_t size)
+{
+	struct faf_sendmsg_msg *msg = msgIn;
+	ssize_t r;
+	int err;
+	struct msghdr msghdr;
+
+	err = recv_msghdr(desc, &msghdr, msg->total_len, MSG_HDR_ONLY);
+	if (err) {
+		rpc_cancel(desc);
+		return;
+	}
+
+	err = remote_sleep_prepare(desc);
+	if (err)
+		goto cancel;
+
+	r = sys_recvmsg(msg->server_fd, &msghdr, msg->flags);
+
+	remote_sleep_finish();
+
+	err = rpc_pack_type(desc, r);
+	if (err)
+		goto cancel;
+
+	if (r < 0)
+		goto out_free;
+
+	err = send_msghdr(desc, &msghdr, r, 0);
+	if (err)
+		goto cancel;
+
+out_free:
+	free_msghdr(&msghdr);
+
+	return;
+
+cancel:
+	rpc_cancel(desc);
+	goto out_free;
+}
+
+int handle_faf_notify_close (struct rpc_desc* desc,
+			     void *msgIn, size_t size)
+{
+	struct faf_notify_msg *msg = msgIn;
+	struct file *file;
+
+	file = fcheck_files(current->files, msg->server_fd);
+	/* Check if the file has been closed locally before we receive the
+	 * notification message.
+	 */
+	if (file == NULL)
+		return 0;
+	if (file->f_objid != msg->objid)
+		return 0;
+	BUG_ON (!(file->f_flags & O_FAF_SRV));
+
+	check_close_faf_srv_file(file);
+
+	return 0;
+}
+
+/* FAF handler Initialisation */
+void faf_server_init (void)
+{
+	rpc_register_void(RPC_FAF_READ, handle_faf_read, 0);
+	rpc_register_void(RPC_FAF_WRITE, handle_faf_write, 0);
+	rpc_register_void(RPC_FAF_READV, handle_faf_readv, 0);
+	rpc_register_void(RPC_FAF_WRITEV, handle_faf_writev, 0);
+	faf_poll_init();
+	rpc_register_void(RPC_FAF_IOCTL, handle_faf_ioctl, 0);
+	rpc_register_void(RPC_FAF_FCNTL, handle_faf_fcntl, 0);
+
+#if BITS_PER_LONG == 32
+	rpc_register_void(RPC_FAF_FCNTL64, handle_faf_fcntl64, 0);
+#endif
+
+	rpc_register_void(RPC_FAF_FSTAT, handle_faf_fstat, 0);
+	rpc_register_void(RPC_FAF_FSTATFS, handle_faf_fstatfs, 0);
+	rpc_register_int(RPC_FAF_FSYNC, handle_faf_fsync, 0);
+	rpc_register_void(RPC_FAF_FLOCK, handle_faf_flock, 0);
+	rpc_register_void(RPC_FAF_LSEEK, handle_faf_lseek, 0);
+	rpc_register_void(RPC_FAF_LLSEEK, handle_faf_llseek, 0);
+	rpc_register_void(RPC_FAF_D_PATH, handle_faf_d_path, 0);
+
+	rpc_register_int(RPC_FAF_BIND, handle_faf_bind, 0);
+	rpc_register_void(RPC_FAF_CONNECT, handle_faf_connect, 0);
+	rpc_register_int(RPC_FAF_LISTEN, handle_faf_listen, 0);
+	rpc_register_void(RPC_FAF_ACCEPT, handle_faf_accept, 0);
+	rpc_register_int(RPC_FAF_GETSOCKNAME, handle_faf_getsockname, 0);
+	rpc_register_int(RPC_FAF_GETPEERNAME, handle_faf_getpeername, 0);
+	rpc_register_int(RPC_FAF_SHUTDOWN, handle_faf_shutdown, 0);
+	rpc_register_void(RPC_FAF_SETSOCKOPT, handle_faf_setsockopt, 0);
+	rpc_register_void(RPC_FAF_GETSOCKOPT, handle_faf_getsockopt, 0);
+	rpc_register_void(RPC_FAF_SENDMSG, handle_faf_sendmsg, 0);
+	rpc_register_void(RPC_FAF_RECVMSG, handle_faf_recvmsg, 0);
+	rpc_register_int(RPC_FAF_NOTIFY_CLOSE, handle_faf_notify_close, 0);
+}
+
+/* FAF server Finalization */
+void faf_server_finalize (void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/fs/faf/faf_server.h android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_server.h
--- linux-2.6.29/kerrighed/fs/faf/faf_server.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_server.h	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,128 @@
+/** Kerrighed FAF Server.
+ *  @file faf_server.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __FAF_SERVER__
+#define __FAF_SERVER__
+
+#include <linux/socket.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+struct faf_rw_msg {
+	int server_fd;
+	size_t count;
+	loff_t pos;
+};
+
+struct faf_rw_ret {
+	ssize_t ret;
+	loff_t pos;
+};
+
+struct faf_d_path_msg {
+	int server_fd;
+	int deleted;
+	size_t count;
+};
+
+struct faf_notify_msg {
+	int server_fd;
+	unsigned long objid;
+};
+
+struct faf_stat_msg {
+	int server_fd;
+	long flags;
+};
+
+struct faf_statfs_msg {
+	int server_fd;
+};
+
+struct faf_ctl_msg {
+	int server_fd;
+	unsigned int cmd;
+	union {
+		unsigned long arg;
+		struct flock flock;
+#if BITS_PER_LONG == 32
+		struct flock64 flock64;
+#endif
+	};
+};
+
+struct faf_seek_msg {
+	int server_fd;
+	off_t offset;
+	unsigned int origin;
+};
+
+struct faf_llseek_msg {
+	int server_fd;
+	unsigned long offset_high;
+	unsigned long offset_low;
+	unsigned int origin;
+};
+
+struct faf_bind_msg {
+	int server_fd;
+	int addrlen;
+	struct sockaddr_storage sa;
+};
+
+struct faf_listen_msg {
+	int server_fd;
+	int sub_chan;
+	int backlog;
+};
+
+struct faf_shutdown_msg {
+	int server_fd;
+	int how;
+};
+
+struct faf_setsockopt_msg {
+	int server_fd;
+	int level;
+	int optname;
+	char __user *optval;
+	int optlen;
+};
+
+struct faf_getsockopt_msg {
+	int server_fd;
+	int level;
+	int optname;
+	char __user *optval;
+	int __user *optlen;
+};
+
+struct faf_sendmsg_msg {
+	int server_fd;
+	unsigned int flags;
+	size_t total_len;
+};
+
+struct faf_poll_wait_msg {
+	int server_fd;
+	unsigned long objid;
+	int wait;
+};
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+void faf_server_init (void);
+void faf_server_finalize (void);
+
+#endif // __FAF_SERVER__
diff -ruN linux-2.6.29/kerrighed/fs/faf/faf_tools.c android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_tools.c
--- linux-2.6.29/kerrighed/fs/faf/faf_tools.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_tools.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,356 @@
+/** Kerrighed FAF Tools.
+ *  @file faf_tools.c
+ *
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+#include <linux/socket.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <net/krgrpc/rpc.h>
+#include "faf_tools.h"
+
+static
+int send_user_iov(struct rpc_desc *desc, struct iovec *iov, int iovcnt, size_t total_len)
+{
+	void *page;
+	void __user *iov_base;
+	size_t iov_len, iov_offset, sent;
+	int i, page_offset, max_page_offset, err = 0;
+
+	page = (void *)__get_free_page(GFP_KERNEL);
+	if (!page)
+		return -ENOMEM;
+
+	i = 0;
+	iov_offset = 0;
+	for (sent = 0; sent < total_len; sent += PAGE_SIZE) {
+		max_page_offset = PAGE_SIZE;
+		if (sent + max_page_offset > total_len)
+			max_page_offset = total_len - sent;
+
+		page_offset = 0;
+		while (page_offset < max_page_offset) {
+			BUG_ON(i >= iovcnt);
+
+			iov_base = (__force void __user *)iov[i].iov_base
+				+ iov_offset;
+			iov_len = iov[i].iov_len - iov_offset;
+
+			if (iov_len > max_page_offset - page_offset) {
+				iov_len = max_page_offset - page_offset;
+				iov_offset += iov_len;
+			} else {
+				i++;
+				iov_offset = 0;
+			}
+
+			if (copy_from_user(page + page_offset,
+					   iov_base, iov_len)) {
+				err = -EFAULT;
+				goto out_free;
+			}
+
+			page_offset += iov_len;
+		}
+
+		err = rpc_pack(desc, 0, page, max_page_offset);
+		if (err)
+			break;
+	}
+
+out_free:
+	free_page((unsigned long)page);
+
+	return err;
+}
+
+static
+int send_kernel_iov(struct rpc_desc *desc, struct iovec *iov, int iovcnt, size_t total_len)
+{
+	size_t iov_len, sent;
+	int i, err = 0;
+
+	/* FAF server is supposed to have page-backed iovecs */
+	for (sent = 0, i = 0; sent < total_len; sent += PAGE_SIZE, i++) {
+		BUG_ON(i >= iovcnt);
+
+		iov_len = iov[i].iov_len;
+		BUG_ON(iov_len != PAGE_SIZE && sent + iov_len < total_len);
+
+		if (sent + iov_len > total_len)
+			iov_len = total_len - sent;
+		err = rpc_pack(desc, 0, iov[i].iov_base, iov_len);
+		if (err)
+			break;
+	}
+
+	return err;
+}
+
+int
+send_iov(struct rpc_desc *desc, struct iovec *iov, int iovcnt, size_t total_len, int flags)
+{
+	int err;
+
+	if (flags & MSG_USER)
+		err = send_user_iov(desc, iov, iovcnt, total_len);
+	else
+		err = send_kernel_iov(desc, iov, iovcnt, total_len);
+
+	return err;
+}
+
+static
+int recv_user_iov(struct rpc_desc *desc, struct iovec *iov, int iovcnt, size_t total_len)
+{
+	void *page;
+	void __user *iov_base;
+	size_t iov_len, iov_offset, rcvd;
+	int i, page_offset, max_page_offset, err = 0;
+
+	page = (void *)__get_free_page(GFP_KERNEL);
+	if (!page)
+		return -ENOMEM;
+
+	i = 0;
+	iov_offset = 0;
+	for (rcvd = 0; rcvd < total_len; rcvd += PAGE_SIZE) {
+		max_page_offset = PAGE_SIZE;
+		if (rcvd + max_page_offset > total_len)
+			max_page_offset = total_len - rcvd;
+
+		err = rpc_unpack(desc, 0, page, max_page_offset);
+		if (err) {
+			if (err > 0)
+				err = -EPIPE;
+			break;
+		}
+
+		page_offset = 0;
+		while (page_offset < max_page_offset) {
+			BUG_ON(i >= iovcnt);
+
+			iov_base = (__force void __user *)iov[i].iov_base
+				+ iov_offset;
+			iov_len = iov[i].iov_len - iov_offset;
+
+			if (iov_len > max_page_offset - page_offset) {
+				iov_len = max_page_offset - page_offset;
+				iov_offset += iov_len;
+			} else {
+				i++;
+				iov_offset = 0;
+			}
+
+			if (copy_to_user(iov_base, page + page_offset,
+					 iov_len)) {
+				err = -EFAULT;
+				goto out_free;
+			}
+
+			page_offset += iov_len;
+		}
+	}
+
+out_free:
+	free_page((unsigned long)page);
+
+	return err;
+}
+
+static
+int recv_kernel_iov(struct rpc_desc *desc, struct iovec *iov, int iovcnt, size_t total_len)
+{
+	size_t iov_len, rcvd;
+	int i, err = 0;
+
+	/* FAF server is supposed to have page-backed iovecs */
+	for (rcvd = 0, i = 0; rcvd < total_len; rcvd += PAGE_SIZE, i++) {
+		BUG_ON(i >= iovcnt);
+
+		iov_len = iov[i].iov_len;
+		BUG_ON(iov_len != PAGE_SIZE && rcvd + iov_len < total_len);
+
+		if (rcvd + iov_len > total_len)
+			iov_len = total_len - rcvd;
+		err = rpc_unpack(desc, 0, iov[i].iov_base, iov_len);
+		if (err) {
+			if (err > 0)
+				err = -EPIPE;
+			break;
+		}
+	}
+
+	return err;
+}
+
+int alloc_iov(struct iovec **iov, int *iovcnt, size_t total_len)
+{
+	struct iovec *__iov;
+	int i, iovlen;
+
+	iovlen = DIV_ROUND_UP(total_len, PAGE_SIZE);
+	__iov = kmalloc(sizeof(*__iov) * iovlen, GFP_KERNEL);
+	if (!__iov)
+		return -ENOMEM;
+
+	*iov = __iov;
+	*iovcnt = iovlen;
+
+	if (!iovlen)
+		return 0;
+
+	for (i = 0; i < iovlen; i++) {
+		__iov[i].iov_base = (void *)__get_free_page(GFP_KERNEL);
+		if (!__iov[i].iov_base)
+			goto out_free;
+		__iov[i].iov_len = PAGE_SIZE;
+	}
+	__iov[iovlen - 1].iov_len = total_len - (iovlen - 1) * PAGE_SIZE;
+
+	return 0;
+
+out_free:
+	for (i--; i >= 0; i--)
+		free_page((unsigned long)__iov[i].iov_base);
+	kfree(__iov);
+	return -ENOMEM;
+}
+
+void free_iov(struct iovec *iov, int iovcnt)
+{
+	int i;
+
+	for (i = 0; i < iovcnt; i++)
+		free_page((unsigned long)iov[i].iov_base);
+	kfree(iov);
+}
+
+int recv_iov(struct rpc_desc *desc, struct iovec *iov, int iovcnt, size_t total_len, int flags)
+{
+	int err;
+
+	if (flags & MSG_USER)
+		err = recv_user_iov(desc, iov, iovcnt, total_len);
+	else
+		err = recv_kernel_iov(desc, iov, iovcnt, total_len);
+
+	return err;
+}
+
+int send_msghdr(struct rpc_desc* desc,
+		struct msghdr *msghdr,
+		size_t total_len,
+		int flags)
+{
+	int err;
+
+	err = rpc_pack(desc, 0, msghdr, sizeof(*msghdr));
+	if (err)
+		return err;
+	if (!(flags & MSG_HDR_ONLY)) {
+		err = rpc_pack(desc, 0, msghdr->msg_name, msghdr->msg_namelen);
+		if (err)
+			return err;
+		err = rpc_pack(desc, 0, msghdr->msg_control, msghdr->msg_controllen);
+		if (err)
+			return err;
+		err = send_iov(desc, msghdr->msg_iov, msghdr->msg_iovlen, total_len, flags);
+	}
+
+	return err;
+}
+
+int recv_msghdr(struct rpc_desc* desc,
+		struct msghdr *msghdr,
+		size_t total_len,
+		int flags)
+{
+	struct msghdr tmp_msg;
+	struct msghdr *msg = (flags & MSG_USER) ? &tmp_msg : msghdr;
+	int err;
+
+	BUG_ON((flags & MSG_USER) && (flags & MSG_HDR_ONLY));
+
+	err = rpc_unpack(desc, 0, msg, sizeof(*msg));
+	if (err)
+		goto out_err;
+
+	err = -ENOMEM;
+	if (flags & MSG_USER) {
+		msg->msg_name = msghdr->msg_name;
+
+		msg->msg_iov = msghdr->msg_iov;
+		msg->msg_iovlen = msghdr->msg_iovlen;
+	} else {
+		int msg_iovlen;
+
+		msg->msg_name = kmalloc(msg->msg_namelen, GFP_KERNEL);
+		if (!msg->msg_name)
+			goto out_err;
+
+		err = alloc_iov(&msg->msg_iov, &msg_iovlen, total_len);
+		if (err)
+			goto err_free_name;
+		msg->msg_iovlen = msg_iovlen;
+	}
+
+	/*
+	 * FAF server always wants to allocate buffers,
+	 * and FAF client always wants to receive data.
+	 */
+	msg->msg_control = kmalloc(msg->msg_controllen, GFP_KERNEL);
+	if (!msg->msg_control)
+		goto err_free_iov;
+
+	if (!(flags & MSG_HDR_ONLY)) {
+		err = rpc_unpack(desc, 0, msg->msg_name, msg->msg_namelen);
+		if (err)
+			goto err_free_control;
+		err = rpc_unpack(desc, 0, msg->msg_control, msg->msg_controllen);
+		if (err)
+			goto err_free_control;
+		err = recv_iov(desc, msg->msg_iov, msg->msg_iovlen, total_len, flags);
+		if (err)
+			goto err_free_control;
+	}
+
+	if (flags & MSG_USER) {
+		if (msg->msg_namelen < msghdr->msg_namelen)
+			msghdr->msg_namelen = msg->msg_namelen;
+
+		if (msg->msg_controllen < msghdr->msg_controllen)
+			msghdr->msg_controllen = msg->msg_controllen;
+		if (copy_to_user(msghdr->msg_control, msg->msg_control,
+				 msghdr->msg_controllen)) {
+			err = -EFAULT;
+			goto err_free_control;
+		}
+		kfree(msg->msg_control);
+
+		msghdr->msg_flags = msg->msg_flags;
+	}
+
+	return 0;
+
+err_free_control:
+	kfree(msg->msg_control);
+err_free_iov:
+	if (!(flags & MSG_USER))
+		free_iov(msg->msg_iov, msg->msg_iovlen);
+err_free_name:
+	if (!(flags & MSG_USER))
+		kfree(msg->msg_name);
+out_err:
+	if (err > 0)
+		err = -EPIPE;
+	return err;
+}
+
+void free_msghdr(struct msghdr *msghdr)
+{
+	kfree(msghdr->msg_name);
+	kfree(msghdr->msg_control);
+
+	free_iov(msghdr->msg_iov, msghdr->msg_iovlen);
+}
diff -ruN linux-2.6.29/kerrighed/fs/faf/faf_tools.h android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_tools.h
--- linux-2.6.29/kerrighed/fs/faf/faf_tools.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/faf/faf_tools.h	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,26 @@
+/** Kerrighed FAF tools.
+ *  @file faf_tools.h
+ *  
+ *  @author Pascal Gallard
+ */
+
+#ifndef __FAF_TOOLS__
+#define __FAF_TOOLS__
+
+enum {
+	MSG_USER     = 1,
+	MSG_HDR_ONLY = 2,
+};
+
+int
+send_iov(struct rpc_desc *desc, struct iovec *iov, int iovcnt, size_t total_len, int flags);
+int recv_iov(struct rpc_desc *desc, struct iovec *iov, int iovcnt, size_t total_len, int flags);
+int alloc_iov(struct iovec **iov, int *iovcnt, size_t total_len);
+void free_iov(struct iovec *iov, int iovcnt);
+
+int send_msghdr(struct rpc_desc *desc, struct msghdr *msghdr,
+		size_t total_len, int flags);
+int recv_msghdr(struct rpc_desc *desc, struct msghdr *msghdr, size_t total_len, int flags);
+void free_msghdr(struct msghdr *msghdr);
+
+#endif // __FAF_TOOLS__
diff -ruN linux-2.6.29/kerrighed/fs/faf/Makefile android_cluster/linux-2.6.29/kerrighed/fs/faf/Makefile
--- linux-2.6.29/kerrighed/fs/faf/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/faf/Makefile	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,6 @@
+obj-$(CONFIG_KRG_FAF) = krg_faf.o
+
+krg_faf-y :=  ruaccess.o faf_tools.o faf.o faf_hooks.o faf_server.o faf_file_mgr.o
+
+EXTRA_CFLAGS += -I$(M) -Wall -Werror
+
diff -ruN linux-2.6.29/kerrighed/fs/faf/ruaccess.c android_cluster/linux-2.6.29/kerrighed/fs/faf/ruaccess.c
--- linux-2.6.29/kerrighed/fs/faf/ruaccess.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/faf/ruaccess.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,473 @@
+/*
+ *  Copyright (C) 2008, Louis Rilling - Kerlabs.
+ */
+
+#include <linux/uaccess.h>
+#include <linux/hardirq.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/hash.h>
+#include <linux/rculist.h>
+#include <linux/spinlock.h>
+#include <linux/rcupdate.h>
+#include <linux/mm.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/errno.h>
+#include <kerrighed/faf.h>
+#include <kerrighed/hotplug.h>
+#include <net/krgrpc/rpc.h>
+
+
+enum ruaccess_op {
+	RUACCESS_COPY = 0,
+	RUACCESS_STRNCPY,
+	RUACCESS_STRNLEN,
+	RUACCESS_CLEAR,
+	RUACCESS_END
+};
+
+enum ruaccess_type {
+	RUACCESS_TO,
+	RUACCESS_FROM,
+	RUACCESS_IN
+};
+
+struct ruaccess_req {
+	enum ruaccess_op op;
+	enum ruaccess_type type;
+	unsigned long len;
+	union {
+		struct {
+			void *to;
+			const void *from;
+			int zerorest;
+		} copy;
+		struct {
+			const char *src;
+		} strncpy;
+		struct {
+			const char *str;
+			unsigned long n;
+		} strnlen;
+		struct {
+			void *mem;
+			unsigned long len;
+		} clear;
+	} u;
+};
+
+typedef void (*ruaccess_handler_t)(const struct ruaccess_req *req, void *buf,
+				   unsigned long *ret, unsigned long *count);
+
+struct ruaccess_desc {
+	struct hlist_node list;
+	struct rpc_desc *desc;
+	struct task_struct *thread;
+	mm_segment_t old_fs;
+	int err;
+	struct rcu_head rcu;
+};
+
+#define RUACCESS_DESC_BITS 8
+#define RUACCESS_DESC_BUCKETS (1 << RUACCESS_DESC_BITS)
+
+static struct hlist_head desc_table[RUACCESS_DESC_BUCKETS];
+static DEFINE_SPINLOCK(desc_table_lock);
+
+static struct ruaccess_desc *ruaccess_desc_alloc(struct rpc_desc *desc)
+{
+	struct ruaccess_desc *ru_desc;
+	struct task_struct *tsk = current;
+	unsigned long hash;
+
+	ru_desc = kmalloc(sizeof(*ru_desc), GFP_KERNEL);
+	if (!ru_desc)
+		goto out;
+	ru_desc->desc = desc;
+	ru_desc->thread = tsk;
+	ru_desc->err = 0;
+
+	hash = hash_ptr(tsk, RUACCESS_DESC_BITS);
+	spin_lock(&desc_table_lock);
+	hlist_add_head_rcu(&ru_desc->list, &desc_table[hash]);
+	spin_unlock(&desc_table_lock);
+
+out:
+	return ru_desc;
+}
+
+static struct ruaccess_desc *ruaccess_desc_find(void)
+{
+	struct ruaccess_desc *desc;
+	struct hlist_node *node;
+	struct task_struct *tsk = current;
+	unsigned long hash;
+
+	hash = hash_ptr(tsk, RUACCESS_DESC_BITS);
+	rcu_read_lock();
+	hlist_for_each_entry_rcu(desc, node, &desc_table[hash], list)
+		if (desc->thread == tsk)
+			goto out;
+	desc = NULL;
+out:
+	rcu_read_unlock();
+	return desc;
+}
+
+static void ruaccess_desc_delayed_free(struct rcu_head *rcu)
+{
+	kfree(container_of(rcu, struct ruaccess_desc, rcu));
+}
+
+static void ruaccess_desc_free(struct ruaccess_desc *desc)
+{
+	spin_lock(&desc_table_lock);
+	hlist_del_rcu(&desc->list);
+	spin_unlock(&desc_table_lock);
+	call_rcu(&desc->rcu, ruaccess_desc_delayed_free);
+}
+
+int prepare_ruaccess(struct rpc_desc *desc)
+{
+	struct ruaccess_desc *ru_desc;
+	int err;
+
+#ifndef ARCH_HAS_RUACCESS
+	return -ENOSYS;
+#endif
+
+	err = -ENOMEM;
+	ru_desc = ruaccess_desc_alloc(desc);
+	if (!ru_desc)
+		goto out;
+
+#ifdef ARCH_HAS_RUACCESS_FIXUP
+	use_mm(&init_mm);
+#endif
+	ru_desc->old_fs = get_fs();
+	set_fs(USER_DS);
+	set_thread_flag(TIF_RUACCESS);
+	err = 0;
+
+out:
+	return err;
+}
+
+int cleanup_ruaccess(struct rpc_desc *desc)
+{
+	struct ruaccess_req req = { .op = RUACCESS_END };
+	struct ruaccess_desc *ru_desc;
+	int err;
+
+	ru_desc = ruaccess_desc_find();
+	BUG_ON(!desc);
+
+	clear_thread_flag(TIF_RUACCESS);
+	set_fs(ru_desc->old_fs);
+#ifdef ARCH_HAS_RUACCESS_FIXUP
+	unuse_mm(&init_mm);
+#endif
+
+	err = ru_desc->err;
+	if (err)
+		goto out;
+	err = rpc_pack_type(desc, req);
+out:
+	ruaccess_desc_free(ru_desc);
+	return err;
+}
+
+static int do_ruaccess(struct ruaccess_req *req, unsigned long *ret, void *buf)
+{
+	struct ruaccess_desc *ru_desc;
+	struct rpc_desc *desc;
+	unsigned long count;
+	int err;
+
+	ru_desc = ruaccess_desc_find();
+	BUG_ON(!ru_desc);
+	err = ru_desc->err;
+	if (err)
+		goto out;
+	desc = ru_desc->desc;
+
+	err = rpc_pack_type(desc, *req);
+	if (err)
+		goto out_err;
+	if (req->type == RUACCESS_TO) {
+		err = rpc_pack(desc, 0, buf, req->len);
+		if (err)
+			goto out_err;
+	}
+	if (req->type == RUACCESS_FROM) {
+		err = rpc_unpack_type(desc, count);
+		if (err)
+			goto out_err;
+		if (count) {
+			BUG_ON(count > req->len);
+			err = rpc_unpack(desc, 0, buf, count);
+			if (err)
+				goto out_err;
+		}
+	}
+	err = rpc_unpack_type(desc, *ret);
+	if (err)
+		goto out_err;
+
+out:
+	return err;
+
+out_err:
+	printk(KERN_ERR "access to remote user memory failed!\n");
+	ru_desc->err = err;
+	goto out;
+}
+
+#define DO_RUACCESS(req, ret) do_ruaccess(&req, &ret, NULL)
+
+unsigned long krg_copy_user_generic(void *to, const void *from,
+				    unsigned long n, int zerorest)
+{
+	struct ruaccess_req req = {
+		.op = RUACCESS_COPY,
+		.len = n,
+		.u.copy = {
+			.to = to,
+			.from = from,
+			.zerorest = zerorest
+		},
+	};
+	void *buf;
+
+	if (in_atomic())
+		goto out;
+	BUG_ON(!segment_eq(get_fs(), USER_DS));
+
+	if ((unsigned long)to >= TASK_SIZE) {
+		req.type = RUACCESS_FROM;
+		buf = (void *)to;
+	} else if ((unsigned long)from >= TASK_SIZE) {
+		req.type = RUACCESS_TO;
+		buf = (void *)from;
+	} else {
+#ifndef CONFIG_COMPAT
+		BUG();
+#endif
+		req.type = RUACCESS_IN;
+		buf = NULL;
+	}
+
+	do_ruaccess(&req, &n, buf);
+
+out:
+	return n;
+}
+
+static void handle_copy(const struct ruaccess_req *req, void *buf,
+			unsigned long *ret, unsigned long *count)
+{
+	unsigned long n = req->len;
+
+	switch (req->type) {
+	case RUACCESS_TO:
+		n = __copy_to_user(req->u.copy.to, buf, n);
+		break;
+	case RUACCESS_FROM:
+		if (req->u.copy.zerorest) {
+			n = __copy_from_user(buf, req->u.copy.from, n);
+			*count = req->len;
+		} else {
+			n = __copy_from_user_inatomic(buf, req->u.copy.from, n);
+			*count = req->len - n;
+		}
+		break;
+	case RUACCESS_IN:
+#ifdef CONFIG_COMPAT
+		n = __copy_in_user(req->u.copy.to, req->u.copy.from, n);
+#else
+		BUG();
+#endif
+		break;
+	};
+
+	*ret = n;
+}
+
+long krg___strncpy_from_user(char *dst, const char __user *src,
+			     unsigned long count)
+{
+	struct ruaccess_req req = {
+		.op = RUACCESS_STRNCPY,
+		.type = RUACCESS_FROM,
+		.len = count,
+		.u.strncpy = {
+			.src = src,
+		}
+	};
+	long res = -EFAULT;
+
+	if (in_atomic()) {
+		printk("__strncpy_from_user() called in atomic!\n");
+		goto out;
+	}
+	BUG_ON(!segment_eq(get_fs(), USER_DS));
+
+	do_ruaccess(&req, (unsigned long *)&res, dst);
+
+out:
+	return res;
+}
+
+static void handle_strncpy(const struct ruaccess_req *req, void *buf,
+			   unsigned long *ret, unsigned long *count)
+{
+	long res;
+
+	res = __strncpy_from_user(buf, req->u.strncpy.src, req->len);
+	*count = (res > 0) ? min((unsigned long)res + 1, req->len) : 0;
+	*ret = (unsigned long)res;
+}
+
+unsigned long krg___strnlen_user(const char __user *str, unsigned long n)
+{
+	struct ruaccess_req req = {
+		.op = RUACCESS_STRNLEN,
+		.type = RUACCESS_IN,
+		.u.strnlen = {
+			.str = str,
+			.n = n
+		}
+	};
+	unsigned long len = 0;
+
+	if (in_atomic()) {
+		printk("__strnlen_user() called in atomic!\n");
+		goto out;
+	}
+	BUG_ON(!segment_eq(get_fs(), USER_DS));
+
+	DO_RUACCESS(req, len);
+
+out:
+	return len;
+}
+
+static void handle_strnlen(const struct ruaccess_req *req, void *buf,
+			   unsigned long *ret, unsigned long *count)
+{
+	const char __user *str = req->u.strnlen.str;
+	unsigned long n = req->u.strnlen.n;
+
+	*count = 0;
+	if (n == ~0UL)
+		*ret = strlen_user(str);
+	else
+		*ret = strnlen_user(str, n);
+}
+
+unsigned long krg___clear_user(void __user *mem, unsigned long len)
+{
+	struct ruaccess_req req = {
+		.op = RUACCESS_CLEAR,
+		.type = RUACCESS_IN,
+		.u.clear = {
+			.mem = mem,
+			.len = len
+		}
+	};
+
+	if (in_atomic()) {
+		printk("__clear_user() called in atomic!\n");
+		goto out;
+	}
+	BUG_ON(!segment_eq(get_fs(), USER_DS));
+
+	DO_RUACCESS(req, len);
+
+out:
+	return len;
+}
+
+static void handle_clear(const struct ruaccess_req *req, void *buf,
+			 unsigned long *ret, unsigned long *count)
+{
+	*count = 0;
+	*ret = __clear_user(req->u.clear.mem, req->u.clear.len);
+}
+
+static ruaccess_handler_t ruaccess_handler[] = {
+	[RUACCESS_COPY] = handle_copy,
+	[RUACCESS_STRNCPY] = handle_strncpy,
+	[RUACCESS_STRNLEN] = handle_strnlen,
+	[RUACCESS_CLEAR] = handle_clear
+};
+
+static int handle_ruaccess_req(struct rpc_desc *desc, const struct ruaccess_req *req)
+{
+	void *buf = NULL;
+	unsigned long ret;
+	unsigned long count;
+	int err;
+
+	if (req->type == RUACCESS_TO || req->type == RUACCESS_FROM) {
+		err = -ENOMEM;
+		buf = kmalloc(req->len, GFP_KERNEL);
+		if (!buf)
+			goto out;
+	}
+	if (req->type == RUACCESS_TO) {
+		err = rpc_unpack(desc, 0, buf, req->len);
+		if (err)
+			goto out;
+	}
+
+	ruaccess_handler[req->op](req, buf, &ret, &count);
+
+	if (req->type == RUACCESS_FROM) {
+		BUG_ON(count > req->len);
+		err = rpc_pack_type(desc, count);
+		if (err)
+			goto out;
+		if (count) {
+			err = rpc_pack(desc, 0, buf, count);
+			if (err)
+				goto out;
+		}
+	}
+	err = rpc_pack_type(desc, ret);
+
+out:
+	kfree(buf);
+	return err;
+}
+
+int handle_ruaccess(struct rpc_desc *desc)
+{
+	struct ruaccess_req req;
+	int err;
+
+	BUG_ON(!segment_eq(get_fs(), USER_DS));
+
+	for (;;) {
+		err = rpc_unpack_type(desc, req);
+		if (err)
+			break;
+		if (req.op == RUACCESS_END)
+			break;
+		err = handle_ruaccess_req(desc, &req);
+		if (err)
+			break;
+	}
+
+	return err;
+}
+
+int ruaccess_start(void)
+{
+	return 0;
+}
+
+void ruaccess_exit(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/fs/faf/ruaccess.h android_cluster/linux-2.6.29/kerrighed/fs/faf/ruaccess.h
--- linux-2.6.29/kerrighed/fs/faf/ruaccess.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/faf/ruaccess.h	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,11 @@
+#ifndef __RUACCESS_H__
+#define __RUACCESS_H__
+
+struct rpc_desc;
+
+int prepare_ruaccess(struct rpc_desc *desc);
+int cleanup_ruaccess(struct rpc_desc *desc);
+
+int handle_ruaccess(struct rpc_desc *desc);
+
+#endif /* __RUACCESS_H__ */
diff -ruN linux-2.6.29/kerrighed/fs/file.c android_cluster/linux-2.6.29/kerrighed/fs/file.c
--- linux-2.6.29/kerrighed/fs/file.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/file.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,219 @@
+/** DVFS Level 3 - File struct sharing management.
+ *  @file file.c
+ *
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/file.h>
+#include <linux/unique_id.h>
+#include <linux/sched.h>
+#include <kerrighed/dvfs.h>
+
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf_file_mgr.h>
+#include "faf/faf_hooks.h"
+#include "faf/faf_internal.h"
+#endif
+
+#include <kddm/kddm.h>
+#include <kerrighed/hotplug.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/action.h>
+#endif
+#include <kerrighed/file.h>
+#include "file_struct_io_linker.h"
+
+/* Unique DVFS file struct id generator root */
+unique_id_root_t file_struct_unique_id_root;
+
+/* DVFS file struct container */
+struct kddm_set *dvfs_file_struct_ctnr = NULL;
+
+int create_kddm_file_object(struct file *file)
+{
+	struct dvfs_file_struct *dvfs_file;
+	unsigned long file_id;
+
+	file_id = get_unique_id (&file_struct_unique_id_root);
+
+	dvfs_file = grab_dvfs_file_struct(file_id);
+	BUG_ON (dvfs_file->file != NULL);
+
+	dvfs_file->f_pos = file->f_pos;
+	dvfs_file->count = 1;
+	dvfs_file->file = NULL;
+
+	/* Make sure we don't put the same file struct in 2 different objects.
+	 * The first writing in file->f_objid wins.
+	 * The second one is destroyed. We assume this is really unlikely.
+	 */
+	if (cmpxchg (&file->f_objid, 0, file_id) != 0)
+		_kddm_remove_frozen_object(dvfs_file_struct_ctnr, file_id);
+	else {
+		dvfs_file->file = file;
+		put_dvfs_file_struct (file_id);
+	}
+
+	return 0;
+}
+
+#ifdef CONFIG_KRG_EPM
+/** Check if we need to share a file struct cluster wide and do whatever needed
+ *  @author Renaud Lottiaux
+ *
+ *  @param file    Struct of the file to check the sharing.
+ */
+void check_file_struct_sharing (int index, struct file *file,
+				struct epm_action *action)
+{
+	/* Do not share the file struct for FAF files or already shared files*/
+	if (file->f_flags & (O_FAF_CLT | O_FAF_SRV | O_KRG_SHARED))
+		goto done;
+
+#ifdef CONFIG_KRG_IPC
+	BUG_ON(file->f_op == &krg_shm_file_operations);
+
+	/* Do not share the file struct for Kerrighed SHM files */
+	if (file->f_op == &shm_file_operations)
+		goto done;
+#endif
+
+	switch (action->type) {
+	  case EPM_CHECKPOINT:
+		  goto done;
+
+	  case EPM_REMOTE_CLONE:
+		  goto share;
+
+	  case EPM_MIGRATE:
+		  if (file_count(file) == 1)
+			  goto done;
+		  break;
+
+	  default:
+		  BUG();
+	}
+
+share:
+	file->f_flags |= O_KRG_SHARED;
+
+done:
+	return;
+}
+#endif
+
+void get_dvfs_file(int index, unsigned long objid)
+{
+	struct dvfs_file_struct *dvfs_file;
+	struct file *file;
+
+	dvfs_file = grab_dvfs_file_struct(objid);
+	file = dvfs_file->file;
+
+	dvfs_file->count++;
+
+	put_dvfs_file_struct (objid);
+}
+
+void put_dvfs_file(int index, struct file *file)
+{
+	struct dvfs_file_struct *dvfs_file;
+	unsigned long objid = file->f_objid;
+
+	dvfs_file = grab_dvfs_file_struct(objid);
+	dvfs_file->count--;
+
+#ifdef CONFIG_KRG_FAF
+	check_last_faf_client_close(file, dvfs_file);
+#endif
+
+	/* else someone has allocated a new structure during the grab */
+
+	if (dvfs_file->count == 0)
+		_kddm_remove_frozen_object (dvfs_file_struct_ctnr, objid);
+	else
+		put_dvfs_file_struct (objid);
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                                KERNEL HOOKS                               */
+/*                                                                           */
+/*****************************************************************************/
+
+/** Get fresh position value for the given file struct.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file    Struct of the file to get the position value.
+ */
+loff_t krg_file_pos_read(struct file *file)
+{
+	struct dvfs_file_struct *dvfs_file;
+	loff_t pos;
+
+	dvfs_file = get_dvfs_file_struct (file->f_objid);
+
+	pos = dvfs_file->f_pos;
+
+	put_dvfs_file_struct (file->f_objid);
+
+	return pos;
+}
+
+/** Write the new file position in the file container.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file    Struct of the file to write position value.
+ */
+void krg_file_pos_write(struct file *file, loff_t pos)
+{
+	struct dvfs_file_struct *dvfs_file;
+
+	dvfs_file = grab_dvfs_file_struct (file->f_objid);
+
+	dvfs_file->f_pos = pos;
+
+	put_dvfs_file_struct (file->f_objid);
+}
+
+/** Decrease usage count on a dvfs file struct.
+ *  @author Renaud Lottiaux
+ *
+ *  @param file    Struct of the file to decrease usage counter.
+ */
+void krg_put_file(struct file *file)
+{
+	BUG_ON (file->f_objid == 0);
+
+	put_dvfs_file(-1, file);
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              INITIALIZATION                               */
+/*                                                                           */
+/*****************************************************************************/
+
+int dvfs_file_init(void)
+{
+	init_and_set_unique_id_root (&file_struct_unique_id_root, 1);
+
+	/* Create the DVFS file struct container */
+
+	dvfs_file_struct_ctnr = create_new_kddm_set(
+		kddm_def_ns,
+		DVFS_FILE_STRUCT_KDDM_ID,
+		DVFS_FILE_STRUCT_LINKER,
+		KDDM_UNIQUE_ID_DEF_OWNER,
+		sizeof (struct dvfs_file_struct),
+		KDDM_LOCAL_EXCLUSIVE);
+
+	if (IS_ERR(dvfs_file_struct_ctnr))
+		OOM;
+
+	return 0;
+}
+
+void dvfs_file_finalize (void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/fs/file_stat.c android_cluster/linux-2.6.29/kerrighed/fs/file_stat.c
--- linux-2.6.29/kerrighed/fs/file_stat.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/file_stat.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,202 @@
+/*
+ * Get information about file
+ *
+ * Copyright (C) 2009, Matthieu Fertré, Kerlabs.
+ */
+
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/stat.h>
+#include <kerrighed/fcntl.h>
+#include <kerrighed/file.h>
+#include <kerrighed/file_stat.h>
+#include <kerrighed/physical_fs.h>
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#include "faf/faf_internal.h"
+#endif
+
+static inline umode_t get_inode_mode(const struct file *file)
+{
+	umode_t i_mode;
+
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & O_FAF_CLT)
+		i_mode = ((struct faf_client_data*)file->private_data)->i_mode;
+	else
+#endif
+		i_mode = file->f_dentry->d_inode->i_mode;
+
+	return i_mode;
+}
+
+int is_pipe(const struct file *file)
+{
+	return S_ISFIFO(get_inode_mode(file));
+}
+
+static int __is_pipe_named(const struct file *file)
+{
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & O_FAF_CLT)
+		return ((struct faf_client_data*)file->private_data)->is_named_pipe;
+#endif
+
+	return strlen(file->f_dentry->d_name.name);
+}
+
+int is_anonymous_pipe(const struct file *file)
+{
+	if (!is_pipe(file))
+		return 0;
+
+	return (!__is_pipe_named(file));
+}
+
+int is_named_pipe(const struct file *file)
+{
+	if (!is_pipe(file))
+		return 0;
+
+	return (__is_pipe_named(file));
+}
+
+int is_socket(const struct file *file)
+{
+	return S_ISSOCK(get_inode_mode(file));
+}
+
+int is_shm(const struct file *file)
+{
+	return (file->f_op == &shm_file_operations);
+}
+
+int is_char_device(const struct file *file)
+{
+	return S_ISCHR(get_inode_mode(file));
+}
+
+int is_block_device(const struct file *file)
+{
+	return S_ISBLK(get_inode_mode(file));
+}
+
+int is_directory(const struct file *file)
+{
+	return S_ISDIR(get_inode_mode(file));
+}
+
+int is_link(const struct file *file)
+{
+	return S_ISLNK(get_inode_mode(file));
+}
+
+int is_anon_shared_mmap(const struct file *file)
+{
+	if (file->f_flags)
+		return 0;
+
+	if (file->f_op != &shmem_file_operations)
+		return 0;
+
+	if (strcmp("dev/zero", file->f_dentry->d_name.name) == 0)
+		return 1;
+
+	return 0;
+}
+
+extern const struct file_operations tty_fops;
+extern const struct file_operations hung_up_tty_fops;
+
+int is_tty(const struct file *file)
+{
+	int r = 0;
+
+	if (!file)
+		return 0;
+
+	if (file->f_flags & O_FAF_CLT) {
+		if (file->f_flags & O_FAF_TTY)
+			r = 1;
+	} else if (file->f_op == &tty_fops
+		   || file->f_op == &hung_up_tty_fops)
+		r = 1;
+
+	return r;
+}
+
+char *get_phys_filename(struct file *file, char *buffer, bool del_ok)
+{
+	char *filename;
+
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & O_FAF_CLT) {
+		bool deleted = false;
+		bool *deleted_param = del_ok ? NULL : &deleted;
+
+		filename = krg_faf_phys_d_path(file, buffer, PAGE_SIZE, deleted_param);
+		if ((!del_ok && deleted) || IS_ERR(filename))
+			filename = NULL;
+	} else
+#endif
+		filename = physical_d_path(&file->f_path, buffer, del_ok);
+
+	return filename;
+}
+
+char *get_filename(struct file *file, char *buffer)
+{
+	char *filename;
+
+	if (file->f_path.dentry && file->f_path.dentry->d_op
+	    && file->f_path.dentry->d_op->d_dname) {
+		filename = file->f_path.dentry->d_op->d_dname(
+				file->f_path.dentry, buffer, PAGE_SIZE);
+		if (IS_ERR(filename))
+			filename = NULL;
+		return filename;
+	}
+
+	return get_phys_filename(file, buffer, true);
+}
+
+char *alloc_filename(struct file *file, char **buffer)
+{
+	char *file_name;
+
+	*buffer = (char *)__get_free_page(GFP_KERNEL);
+	if (!*buffer) {
+		file_name = ERR_PTR(-ENOMEM);
+		goto exit;
+	}
+
+	file_name = get_filename(file, *buffer);
+	if (!file_name) {
+		file_name = *buffer;
+		sprintf(file_name, "?");
+	}
+
+exit:
+	return file_name;
+}
+
+void free_filename(char *buffer)
+{
+	free_page((unsigned long)buffer);
+}
+
+int can_checkpoint_file(const struct file *file)
+{
+	if (is_socket(file)) {
+		printk("Checkpoint of socket file is not supported\n");
+		return 0;
+	} else if (is_named_pipe(file)) {
+		printk("Checkpoint of FIFO file (nammed pipe) is not supported\n");
+		return 0;
+	} else if (is_anon_shared_mmap(file)) {
+		printk("Checkpoint of anonymous shared mmap file is not supported\n");
+	}
+
+	return 1;
+}
diff -ruN linux-2.6.29/kerrighed/fs/file_struct_io_linker.c android_cluster/linux-2.6.29/kerrighed/fs/file_struct_io_linker.c
--- linux-2.6.29/kerrighed/fs/file_struct_io_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/file_struct_io_linker.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,121 @@
+/** DVFS level 3 - File Struct Linker.
+ *  @file file_struct_io_linker.c
+ *
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+#include <kerrighed/file.h>
+#include "file_struct_io_linker.h"
+
+struct kmem_cache *dvfs_file_cachep;
+
+/*****************************************************************************/
+/*                                                                           */
+/*                     FILE_STRUCT CONTAINER IO FUNCTIONS                    */
+/*                                                                           */
+/*****************************************************************************/
+
+int file_alloc_object (struct kddm_obj * obj_entry,
+		       struct kddm_set * ctnr,
+		       objid_t objid)
+{
+	struct dvfs_file_struct *dvfs_file;
+
+	dvfs_file = kmem_cache_alloc (dvfs_file_cachep, GFP_KERNEL);
+	if (dvfs_file == NULL)
+		return -ENOMEM;
+
+	dvfs_file->file = NULL;
+	obj_entry->object = dvfs_file;
+
+	return 0;
+}
+
+int file_first_touch (struct kddm_obj * obj_entry,
+		      struct kddm_set * ctnr,
+		      objid_t objid,
+		      int flags)
+{
+	return file_alloc_object(obj_entry, ctnr, objid);
+}
+
+/** Handle a container object remove.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  obj_entry  Descriptor of the object to remove.
+ *  @param  ctnr      Container descriptor.
+ *  @param  objid     Id of the object to remove.
+ */
+int file_remove_object (void *object,
+			struct kddm_set * ctnr,
+			objid_t objid)
+{
+	struct dvfs_file_struct *dvfs_file;
+
+	dvfs_file = object;
+
+	if (dvfs_file != NULL) {
+		BUG_ON(dvfs_file->file != NULL);
+		kmem_cache_free (dvfs_file_cachep, dvfs_file);
+	}
+
+	return 0;
+}
+
+/** Export an file object
+ *  @author Renaud Lottiaux
+ *
+ *  @param  buffer    Buffer to export object data in.
+ *  @param  obj_entry  Object entry of the object to export.
+ */
+int file_export_object (struct rpc_desc *desc,
+			struct kddm_set *set,
+			struct kddm_obj *obj_entry,
+			objid_t objid,
+			int flags)
+{
+	struct dvfs_file_struct *dvfs_file;
+
+	dvfs_file = obj_entry->object;
+	rpc_pack(desc, 0, dvfs_file, sizeof(struct dvfs_file_struct));
+
+	return 0;
+}
+
+/** Import an file object
+ *  @author Renaud Lottiaux
+ *
+ *  @param  obj_entry  Object entry of the object to import.
+ *  @param  _buffer   Data to import in the object.
+ */
+int file_import_object (struct rpc_desc *desc,
+			struct kddm_set *set,
+			struct kddm_obj *obj_entry,
+			objid_t objid,
+			int flags)
+{
+	struct dvfs_file_struct *dvfs_file, buffer;
+
+	dvfs_file = obj_entry->object;
+	rpc_unpack(desc, 0, &buffer, sizeof(struct dvfs_file_struct));
+
+	dvfs_file->f_pos = buffer.f_pos;
+	dvfs_file->count = buffer.count;
+
+	return 0;
+}
+
+/****************************************************************************/
+/* Init the file_struct IO linker */
+
+struct iolinker_struct dvfs_file_struct_io_linker = {
+	alloc_object:	file_alloc_object,
+	first_touch:	file_first_touch,
+	export_object:	file_export_object,
+	import_object:	file_import_object,
+	remove_object:	file_remove_object,
+	linker_name:	"DVFS ",
+	linker_id:	DVFS_FILE_STRUCT_LINKER,
+};
diff -ruN linux-2.6.29/kerrighed/fs/file_struct_io_linker.h android_cluster/linux-2.6.29/kerrighed/fs/file_struct_io_linker.h
--- linux-2.6.29/kerrighed/fs/file_struct_io_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/file_struct_io_linker.h	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,19 @@
+/** DVFS level 3 - File Struct Linker.
+ *  @file file_struct_io_linker.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __DVFS_FILE_STRUCT_LINKER__
+#define __DVFS_FILE_STRUCT_LINKER__
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+extern struct iolinker_struct dvfs_file_struct_io_linker;
+extern struct kmem_cache *dvfs_file_cachep;
+
+#endif // __FILE_STRUCT_LINKER__
diff -ruN linux-2.6.29/kerrighed/fs/fs.c android_cluster/linux-2.6.29/kerrighed/fs/fs.c
--- linux-2.6.29/kerrighed/fs/fs.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/fs.c	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,73 @@
+/** Kerfs module initialization.
+ *  @file module.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ *
+ *  Implementation of functions used to initialize and finalize the
+ *  kerfs module.
+ */
+#include <linux/module.h>
+#include <linux/proc_fs.h>
+
+#include <kddm/kddm.h>
+#include <kerrighed/file.h>
+#include "file_struct_io_linker.h"
+#ifdef CONFIG_KRG_EPM
+#include "mobility.h"
+#include <kerrighed/regular_file_mgr.h>
+#endif
+#ifdef CONFIG_KRG_FAF
+#include "faf/faf_internal.h"
+#endif
+
+
+/** Initialisation of the DVFS module.
+ *  @author Renaud Lottiaux
+ *
+ *  Start DVFS server.
+ */
+int init_dvfs (void)
+{
+	printk ("DVFS initialisation : start\n");
+
+	dvfs_file_cachep = kmem_cache_create("dvfs_file",
+					     sizeof(struct dvfs_file_struct),
+					     0, SLAB_PANIC, NULL);
+
+	register_io_linker (DVFS_FILE_STRUCT_LINKER,
+			    &dvfs_file_struct_io_linker);
+
+#ifdef CONFIG_KRG_EPM
+	dvfs_mobility_init();
+#endif
+#ifdef CONFIG_KRG_FAF
+	faf_init();
+#endif
+	dvfs_file_init();
+
+	printk ("DVFS initialisation done\n");
+
+	return 0;
+}
+
+
+
+/** Cleanup of the DVFS module.
+ *  @author Renaud Lottiaux
+ *
+ *  Kill DVFS server.
+ */
+void cleanup_dvfs (void)
+{
+	printk ("DVFS termination : start\n");
+
+#ifdef CONFIG_KRG_FAF
+	faf_finalize() ;
+#endif
+	dvfs_file_finalize();
+#ifdef CONFIG_KRG_EPM
+	dvfs_mobility_finalize();
+#endif
+	printk ("DVFS termination done\n");
+}
diff -ruN linux-2.6.29/kerrighed/fs/Makefile android_cluster/linux-2.6.29/kerrighed/fs/Makefile
--- linux-2.6.29/kerrighed/fs/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/Makefile	2014-05-27 23:04:10.466027093 -0700
@@ -0,0 +1,10 @@
+obj-$(CONFIG_KRG_DVFS) := krg_fs.o
+obj-$(CONFIG_KRG_FAF) += faf/
+
+krg_fs-y := file_struct_io_linker.o physical_fs.o file.o fs.o file_stat.o
+krg_fs-$(CONFIG_KRG_EPM) += regular_file_mgr.o mobility.o
+
+krg_fs-$(CONFIG_KRG_STREAM) +=
+#krg_fs-$(CONFIG_KRG_DEBUG) += debug_fs.o
+
+EXTRA_CFLAGS += -Wall -Werror
diff -ruN linux-2.6.29/kerrighed/fs/mobility.c android_cluster/linux-2.6.29/kerrighed/fs/mobility.c
--- linux-2.6.29/kerrighed/fs/mobility.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/mobility.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,1634 @@
+/** Implementation of DFS mobility mechanisms.
+ *  @file dfs_mobility.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ *
+ *  Implementation of functions used to migrate, duplicate and checkpoint
+ *  DFS data, process memory and file structures.
+ */
+#include <linux/sched.h>
+#include <linux/fs.h>
+#include <linux/file.h>
+#include <linux/fdtable.h>
+#include <linux/fs_struct.h>
+#include <linux/dcache.h>
+#include <linux/namei.h>
+#include <linux/mount.h>
+#include <linux/mnt_namespace.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+#include <linux/rcupdate.h>
+
+#include <kddm/kddm.h>
+#include <kerrighed/namespace.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/ghost_helpers.h>
+#include <kerrighed/action.h>
+#include <kerrighed/application.h>
+#include <kerrighed/app_shared.h>
+#include <kerrighed/file.h>
+#include <kerrighed/file_stat.h>
+#include "mobility.h"
+#include <kerrighed/regular_file_mgr.h>
+#include <kerrighed/physical_fs.h>
+#include <kerrighed/pid.h>
+#include "file_struct_io_linker.h"
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#include <kerrighed/faf_file_mgr.h>
+#include "faf/faf_internal.h"
+#include "faf/faf_hooks.h"
+#endif
+
+#define VM_FILE_NONE 0
+#define VM_FILE_PHYS 1
+
+#define MMAPPED_FILE -1
+
+void free_ghost_files (struct task_struct *ghost)
+{
+	struct fdtable *fdt;
+
+	BUG_ON (ghost->files == NULL);
+
+	fdt = files_fdtable(ghost->files);
+
+	BUG_ON (fdt->close_on_exec == NULL);
+	BUG_ON (fdt->open_fds == NULL);
+
+	exit_files(ghost);
+	exit_fs(ghost);
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              HELPER FUNCTIONS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+static inline int populate_fs_struct (ghost_t * ghost,
+				      char *buffer,
+				      struct path *path)
+{
+	struct prev_root root;
+	int len, r;
+
+	r = ghost_read (ghost, &len, sizeof (int));
+	if (r)
+		goto error;
+
+	if (len == 0) {
+		path->dentry = NULL;
+		path->mnt = NULL;
+		return 0;
+	}
+
+	r = ghost_read(ghost, buffer, len);
+	if (r)
+		goto error;
+
+	chroot_to_physical_root(&root);
+	r = kern_path(buffer, LOOKUP_FOLLOW | LOOKUP_DIRECTORY, path);
+	chroot_to_prev_root(&root);
+	if (r)
+		goto error;
+
+error:
+	return r;
+}
+
+
+static struct dvfs_mobility_operations *get_dvfs_mobility_ops(struct file *file)
+{
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & (O_FAF_SRV | O_FAF_CLT))
+		return &dvfs_mobility_faf_ops;
+#endif
+
+	return &dvfs_mobility_regular_ops;
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              EXPORT FUNCTIONS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+/** Generic function to export an open file into a ghost.
+ *  Not used by Checkpoint.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost    Ghost where data should be stored.
+ *  @param tsk      Task we are exporting.
+ *  @parem index    Index of the exported file in the open files array.
+ *  @param file     Struct of the file to export.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int export_one_open_file (struct epm_action *action,
+			  ghost_t *ghost,
+                          struct task_struct *tsk,
+                          int index,
+                          struct file *file)
+{
+	struct dvfs_mobility_operations *ops;
+	krgsyms_val_t dvfs_ops_type;
+	int r;
+
+	BUG_ON(action->type == EPM_CHECKPOINT);
+
+	if (action->type != EPM_CHECKPOINT
+	    && index != MMAPPED_FILE) {
+		if (!file->f_objid)
+			create_kddm_file_object(file);
+		check_file_struct_sharing (index, file, action);
+	}
+
+#ifdef CONFIG_KRG_FAF
+	r = check_activate_faf(tsk, index, file, action);
+	if (r)
+		goto err;
+#endif
+	ops = get_dvfs_mobility_ops(file);
+
+	dvfs_ops_type = krgsyms_export(ops);
+
+	r = ghost_write(ghost, &dvfs_ops_type, sizeof(krgsyms_val_t));
+	if (r)
+		goto err;
+
+	r = ghost_write(ghost, &file->f_objid,
+			sizeof (unsigned long));
+	if (r)
+		goto err;
+
+	r = ops->file_export (action, ghost, tsk, index, file);
+
+err:
+	return r;
+}
+
+static int get_file_size(struct file *file, loff_t *size)
+{
+	int r = 0;
+
+#ifdef CONFIG_KRG_FAF
+	if (file->f_flags & O_FAF_CLT) {
+		struct kstat stat;
+		r = krg_faf_fstat(file, &stat);
+		if (r)
+			goto exit;
+
+		*size = stat.size;
+	} else
+#endif
+		*size = i_size_read(file->f_dentry->d_inode);
+#ifdef CONFIG_KRG_FAF
+exit:
+#endif
+	return r;
+}
+
+static int _cr_get_file_type_and_key(const struct file *file,
+				     enum shared_obj_type *type,
+				     long *key,
+				     enum object_locality *locality,
+				     int allow_unsupported)
+{
+	if (!can_checkpoint_file(file)
+	    && !allow_unsupported)
+		return -ENOSYS;
+
+	if (file->f_objid) {
+		*type = DVFS_FILE;
+		*key = file->f_objid;
+		if (locality) {
+			if (is_anonymous_pipe(file)) {
+				if (file->f_flags & O_FAF_CLT)
+					*locality = SHARED_SLAVE;
+				else
+					*locality = SHARED_MASTER;
+			} else
+				*locality = SHARED_ANY;
+		}
+	} else {
+		*type = LOCAL_FILE;
+		*key = (long)file;
+		if (locality)
+			*locality = LOCAL_ONLY;
+	}
+
+	return 0;
+}
+
+void cr_get_file_type_and_key(const struct file *file,
+			      enum shared_obj_type *type,
+			      long *key,
+			      int allow_unsupported)
+{
+	int r;
+	r = _cr_get_file_type_and_key(file, type, key, NULL, allow_unsupported);
+
+	/* normally, the error have been catched before */
+	BUG_ON(r);
+}
+
+static int cr_ghost_write_file_id(ghost_t *ghost, struct file *file,
+				  int allow_unsupported)
+{
+	int r;
+	long key;
+	enum shared_obj_type type;
+
+	cr_get_file_type_and_key(file, &type, &key, allow_unsupported);
+
+	r = ghost_write(ghost, &type, sizeof(enum shared_obj_type));
+	if (r)
+		goto error;
+
+	r = ghost_write(ghost, &key, sizeof(long));
+	if (r)
+		goto error;
+
+error:
+	return r;
+}
+
+static int cr_write_vma_phys_file_id(ghost_t *ghost, struct vm_area_struct *vma)
+{
+	int r;
+	int anon_shared;
+
+	anon_shared = is_anon_shared_mmap(vma->vm_file);
+
+	r = ghost_write_type(ghost, anon_shared);
+	if (r || anon_shared)
+		goto exit;
+
+	r = cr_ghost_write_file_id(ghost, vma->vm_file, 0);
+	if (r)
+		goto exit;
+
+	if (vma->vm_flags & VM_EXEC) {
+
+		/* to check it is the same file when restarting */
+		loff_t file_size;
+		r = get_file_size(vma->vm_file, &file_size);
+		if (r)
+			goto exit;
+
+		r = ghost_write(ghost, &file_size, sizeof(loff_t));
+		if (r)
+			goto exit;
+	}
+exit:
+	return r;
+}
+
+static int export_vma_phys_file(struct epm_action *action,
+				ghost_t *ghost,
+				struct task_struct *tsk,
+				struct vm_area_struct *vma,
+				hashtable_t *file_table)
+{
+	struct file *file = vma->vm_file;
+	unsigned long key = (unsigned long)file;
+	int export_file = 1;
+	int r;
+
+	if (action->type == EPM_CHECKPOINT) {
+		BUG_ON(action->checkpoint.shared != CR_SAVE_NOW);
+		r = cr_write_vma_phys_file_id(ghost, vma);
+		goto done;
+	}
+
+	/* Don't try to share SHM files */
+	if (file->f_op == &krg_shm_file_operations)
+		goto export_file;
+
+	if (__hashtable_find(file_table, key))
+		/* File already exported. Don't export it again */
+		export_file = 0;
+	else {
+		/* First export ? Add the file in the export file table */
+		r = __hashtable_add(file_table, key, file);
+		if (r)
+			goto done;
+	}
+
+export_file:
+	r = ghost_write(ghost, &key, sizeof(unsigned long));
+	if (r)
+		goto done;
+
+	r = ghost_write(ghost, &export_file, sizeof(int));
+	if (r)
+		goto done;
+
+	/* Only export the file once */
+	if (export_file)
+		r = export_one_open_file(action, ghost, tsk,
+					 MMAPPED_FILE, file);
+
+done:
+	return r;
+}
+
+/** Export the file associated to a VMA.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost    Ghost where data should be stored.
+ *  @param vma      The VMA hosting the file to export.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int export_vma_file (struct epm_action *action,
+		     ghost_t *ghost,
+		     struct task_struct *tsk,
+                     struct vm_area_struct *vma,
+		     hashtable_t *file_table)
+{
+	int vm_file_type;
+	int r;
+
+	/* Creation of the vm_file ghost */
+
+	if (vma->vm_file == NULL)
+		vm_file_type = VM_FILE_NONE;
+	else
+		vm_file_type = VM_FILE_PHYS;
+
+	r = ghost_write (ghost, &vm_file_type, sizeof (int));
+	if (r)
+		goto err;
+
+	switch (vm_file_type) {
+	case VM_FILE_NONE:
+		break;
+	case VM_FILE_PHYS:
+		  r = export_vma_phys_file(action, ghost, tsk, vma,
+					   file_table);
+		break;
+	default:
+		BUG();
+	}
+
+err:
+	return r;
+}
+
+int export_mm_exe_file(struct epm_action *action, ghost_t *ghost,
+		       struct task_struct *tsk)
+{
+	int dump = 0, r = 0;
+
+#ifdef CONFIG_PROC_FS
+	if (tsk->mm->exe_file) {
+		dump = 1;
+		r = ghost_write(ghost, &dump, sizeof(int));
+		if (r)
+			goto exit;
+
+		if (action->type == EPM_CHECKPOINT) {
+			BUG_ON(action->checkpoint.shared != CR_SAVE_NOW);
+			r = cr_ghost_write_file_id(ghost, tsk->mm->exe_file, 0);
+		} else
+			r = export_one_open_file(action, ghost, tsk, -1,
+						 tsk->mm->exe_file);
+	} else
+		r = ghost_write(ghost, &dump, sizeof(int));
+
+exit:
+#endif
+	return r;
+}
+
+/** Export the open files array of a process
+ *  Not used by Checkpoint
+ *  @author  Geoffroy Vallee, Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where file data should be stored.
+ *  @param tsk    Task to export file data from.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int export_open_files (struct epm_action *action,
+		       ghost_t *ghost,
+                       struct task_struct *tsk,
+		       struct fdtable *fdt,
+		       int last_open_fd)
+{
+	struct file *file;
+	int i, r = 0;
+
+	BUG_ON (!tsk);
+	BUG_ON(action->type == EPM_CHECKPOINT);
+
+	/* Export files opened by the process */
+	for (i = 0; i < last_open_fd; i++) {
+		if (FD_ISSET (i, fdt->open_fds)) {
+			BUG_ON (!fdt->fd[i]);
+			file = fdt->fd[i];
+
+			r = export_one_open_file (action, ghost, tsk, i, file);
+
+			if (r != 0)
+				goto exit;
+		} else {
+			if (fdt->fd[i] != NULL)
+				printk ("Entry %d : %p\n", i, fdt->fd[i]);
+			BUG_ON (fdt->fd[i] != NULL);
+		}
+	}
+
+exit:
+	return r;
+}
+
+static int cr_write_open_files_id(ghost_t *ghost,
+				  struct task_struct *tsk,
+				  struct fdtable *fdt,
+				  int last_open_fd)
+{
+	struct file *file;
+	int i, allow_unsupported, r = 0;
+
+	BUG_ON (!tsk);
+
+	if (tsk->application->checkpoint.flags & CKPT_W_UNSUPPORTED_FILE)
+		allow_unsupported = 1;
+	else
+		allow_unsupported = 0;
+
+	/* Write id of files opened by the process */
+	for (i = 0; i < last_open_fd; i++) {
+		if (FD_ISSET (i, fdt->open_fds)) {
+			BUG_ON (!fdt->fd[i]);
+			file = fdt->fd[i];
+
+			r = cr_ghost_write_file_id(ghost, file,
+						   allow_unsupported);
+
+			if (r != 0)
+				goto exit;
+		} else {
+			if (fdt->fd[i] != NULL)
+				printk ("Entry %d : %p\n", i, fdt->fd[i]);
+			BUG_ON (fdt->fd[i] != NULL);
+		}
+	}
+
+exit:
+	return r;
+}
+
+int _cr_add_file_to_shared_table(struct task_struct *task,
+				 int index, struct file *file,
+				 int allow_unsupported)
+{
+	int r, force;
+	long key;
+	enum shared_obj_type type;
+	enum object_locality locality;
+	union export_args args;
+
+	r = _cr_get_file_type_and_key(file, &type, &key, &locality,
+				      allow_unsupported);
+	if (r)
+		goto error;
+
+	args.file_args.index = index;
+	args.file_args.file = file;
+
+	if (index == -1)
+		force = 0;
+	else
+		force = 1;
+
+	r = add_to_shared_objects_list(task->application,
+				       type, key, locality, task,
+				       &args, force);
+
+	if (r == -ENOKEY) /* the file was already in the list */
+               r = 0;
+
+error:
+	return r;
+}
+
+int cr_add_file_to_shared_table(struct task_struct *task,
+				int index, struct file *file,
+				int allow_unsupported)
+{
+	int r;
+
+	r = _cr_add_file_to_shared_table(task, index, file,
+					 allow_unsupported);
+	if (r)
+		goto error;
+
+	if (!(file->f_flags & O_FAF_CLT) && is_anonymous_pipe(file)) {
+		r = cr_add_pipe_inode_to_shared_table(task, file);
+		if (r)
+			goto error;
+	}
+
+error:
+	return r;
+}
+
+static int cr_add_files_to_shared_table(struct task_struct *tsk,
+					struct fdtable *fdt,
+					int last_open_fd)
+{
+	struct file *file;
+	int i, allow_unsupported, r = 0;
+
+	BUG_ON (!tsk);
+
+	if (tsk->application->checkpoint.flags & CKPT_W_UNSUPPORTED_FILE)
+		allow_unsupported = 1;
+	else
+		allow_unsupported = 0;
+
+	/* Write id of files opened by the process */
+	for (i = 0; i < last_open_fd; i++) {
+		if (FD_ISSET (i, fdt->open_fds)) {
+			BUG_ON (!fdt->fd[i]);
+			file = fdt->fd[i];
+
+			r = cr_add_file_to_shared_table(tsk, i, file,
+							allow_unsupported);
+
+			if (r != 0)
+				goto exit;
+		} else {
+			if (fdt->fd[i] != NULL)
+				printk ("Entry %d : %p\n", i, fdt->fd[i]);
+			BUG_ON (fdt->fd[i] != NULL);
+		}
+	}
+
+exit:
+	return r;
+}
+
+static int cr_export_later_files_struct(ghost_t *ghost,
+					struct task_struct *task)
+{
+	int r;
+	long key;
+	int last_open_fd;
+	struct fdtable *fdt;
+
+	key = (long)(task->files);
+
+	r = ghost_write(ghost, &key, sizeof(long));
+	if (r)
+		goto err;
+
+	r = add_to_shared_objects_list(task->application,
+				       FILES_STRUCT, key, LOCAL_ONLY,
+				       task, NULL, 0);
+	if (r && r != -ENOKEY)
+		goto err;
+
+	/*
+	 * we need to check the files to see if they are shared even if
+	 * the files_struct itself is shared. These is needed to export
+	 * valid information to user to help file substitution.
+	 */
+	rcu_read_lock();
+	fdt = files_fdtable(task->files);
+
+	last_open_fd = count_open_files(fdt);
+	r = cr_add_files_to_shared_table(task, fdt, last_open_fd);
+	rcu_read_unlock();
+
+err:
+	return r;
+}
+
+/** Export the files_struct of a process
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where file data should be stored.
+ *  @param tsk    Task to export file data from.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int export_files_struct (struct epm_action *action,
+			 ghost_t *ghost,
+                         struct task_struct *tsk)
+{
+	int r = 0, export_fdt;
+	int last_open_fd;
+	struct fdtable *fdt;
+	struct files_struct *exported_files;
+
+	BUG_ON (!tsk);
+
+	{
+		int magic = 780574;
+
+		r = ghost_write (ghost, &magic, sizeof (int));
+		if (r)
+			goto err;
+	}
+
+	if (action->type == EPM_CHECKPOINT &&
+	    action->checkpoint.shared == CR_SAVE_LATER) {
+
+		r = cr_export_later_files_struct(ghost, tsk);
+		return r;
+	}
+
+	/* Export the main files structure */
+
+	exported_files = dup_fd (tsk->files, &r);
+	if (!exported_files)
+		goto err;
+
+	r = ghost_write (ghost, exported_files, sizeof (struct files_struct));
+	if (r)
+		goto exit_put_files;
+
+	/* Export the bit vector close_on_exec */
+
+	fdt = files_fdtable(exported_files);
+
+	last_open_fd = count_open_files(fdt);
+	r = ghost_write (ghost, &last_open_fd, sizeof (int));
+	if (r)
+		goto exit_put_files;
+
+	export_fdt = (fdt != &exported_files->fdtab);
+	r = ghost_write (ghost, &export_fdt, sizeof (int));
+	if (r)
+		goto exit_put_files;
+
+	if (export_fdt) {
+		int nr = last_open_fd / BITS_PER_BYTE;
+		r = ghost_write (ghost, fdt->close_on_exec, nr);
+		if (r)
+			goto exit_put_files;
+
+		r = ghost_write (ghost, fdt->open_fds, nr);
+		if (r)
+			goto exit_put_files;
+
+	}
+
+	{
+		int magic = 280574;
+
+		r = ghost_write (ghost, &magic, sizeof (int));
+		if (r)
+			goto exit_put_files;
+	}
+
+	if (action->type == EPM_CHECKPOINT) {
+		BUG_ON(action->checkpoint.shared != CR_SAVE_NOW);
+		r = cr_write_open_files_id(ghost, tsk, fdt, last_open_fd);
+	} else
+		r = export_open_files (action, ghost, tsk, fdt, last_open_fd);
+
+	if (r)
+		goto exit_put_files;
+
+	{
+		int magic = 380574;
+
+		r = ghost_write (ghost, &magic, sizeof (int));
+	}
+
+exit_put_files:
+	put_files_struct (exported_files);
+
+err:
+	return r;
+}
+
+static int cr_export_later_fs_struct(struct epm_action *action,
+				     ghost_t *ghost,
+				     struct task_struct *task)
+{
+	int r;
+	long key;
+
+	BUG_ON(action->type != EPM_CHECKPOINT);
+	BUG_ON(action->checkpoint.shared != CR_SAVE_LATER);
+
+	key = (long)(task->fs);
+
+	r = ghost_write(ghost, &key, sizeof(long));
+	if (r)
+		goto err;
+
+	r = add_to_shared_objects_list(task->application,
+				       FS_STRUCT, key, LOCAL_ONLY, task,
+				       NULL, 0);
+
+	if (r == -ENOKEY) /* the fs_struct was already in the list */
+		r = 0;
+err:
+	return r;
+}
+
+/** Export the fs_struct of a process
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where file data should be stored.
+ *  @param tsk    Task to export file data from.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int export_fs_struct (struct epm_action *action,
+		      ghost_t *ghost,
+                      struct task_struct *tsk)
+{
+	char *tmp, *file_name;
+	int r, len;
+
+	if (action->type == EPM_CHECKPOINT &&
+	    action->checkpoint.shared == CR_SAVE_LATER) {
+		int r;
+		r = cr_export_later_fs_struct(action, ghost, tsk);
+		return r;
+	}
+
+	r = -ENOMEM;
+	tmp = (char *) __get_free_page (GFP_KERNEL);
+	if (!tmp)
+		goto err_write;
+
+	{
+		int magic = 55611;
+
+		r = ghost_write (ghost, &magic, sizeof (int));
+		if (r)
+			goto err_write;
+	}
+
+	/* Export the umask value */
+
+	r = ghost_write (ghost, &tsk->fs->umask, sizeof (int));
+	if (r)
+			goto err_write;
+
+	/* Export the root path name */
+
+	file_name = physical_d_path(&tsk->fs->root, tmp, false);
+	if (!file_name) {
+		r = -ENOENT;
+		goto err_write;
+	}
+
+	len = strlen (file_name) + 1;
+	r = ghost_write (ghost, &len, sizeof (int));
+	if (r)
+			goto err_write;
+	r = ghost_write (ghost, file_name, len);
+	if (r)
+			goto err_write;
+
+	/* Export the pwd path name */
+
+	file_name = physical_d_path(&tsk->fs->pwd, tmp, false);
+	if (!file_name) {
+		r = -ENOENT;
+		goto err_write;
+	}
+
+	len = strlen (file_name) + 1;
+	r = ghost_write (ghost, &len, sizeof (int));
+	if (r)
+			goto err_write;
+	r = ghost_write (ghost, file_name, len);
+	if (r)
+			goto err_write;
+
+	{
+		int magic = 180574;
+
+		r = ghost_write (ghost, &magic, sizeof (int));
+	}
+
+err_write:
+	free_page ((unsigned long) tmp);
+
+	return r;
+}
+
+int export_mnt_namespace(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *tsk)
+{
+	/* Nothing done right now... */
+	if (tsk->nsproxy->mnt_ns != tsk->nsproxy->krg_ns->root_nsproxy.mnt_ns)
+		return -EPERM;
+	return 0;
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              IMPORT FUNCTIONS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+/** Generic function to import an open file from a ghost.
+ *  Not used by Restart.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost   Ghost where data should be read from.
+ *  @param task    the task to import the file for.
+ *  @param file    The resulting imported file structure.
+ *
+ *  @return   0 if everything ok.
+ *            Negative value otherwise.
+ */
+int import_one_open_file (struct epm_action *action,
+			  ghost_t *ghost,
+                          struct task_struct *task,
+			  int index,
+                          struct file **returned_file)
+{
+	struct dvfs_file_struct *dvfs_file = NULL;
+	struct dvfs_mobility_operations *ops;
+	struct file *file = NULL, *imported_file = NULL;
+	krgsyms_val_t dvfs_ops_type;
+	unsigned long objid;
+	int first_import = 0;
+	int r = 0;
+
+	BUG_ON(action->type == EPM_CHECKPOINT);
+
+	*returned_file = NULL;
+
+	r = ghost_read(ghost, &dvfs_ops_type, sizeof (dvfs_ops_type));
+	if (r)
+		goto err_read;
+	r = ghost_read(ghost, &objid, sizeof (unsigned long));
+	if (r)
+		goto err_read;
+
+	ops = krgsyms_import(dvfs_ops_type);
+
+	/* We need to import the file, to avoid leaving unused data in
+	 * the ghost... We can probably do better...
+	 */
+	r = ops->file_import (action, ghost, task, &imported_file);
+	if (r)
+		goto err_read;
+
+	if (index == MMAPPED_FILE) {
+		*returned_file = imported_file;
+		goto exit;
+	}
+
+	/* Check if the file struct is already present */
+	file = begin_import_dvfs_file(objid, &dvfs_file);
+
+	/* If a file struct was alreay present, use it and discard the one we
+	 * have just created. If f_count == 0, someone else is being freeing
+	 * the structure.
+	 */
+	if (file) {
+		/* The file has already been imported on this node */
+#ifdef CONFIG_KRG_FAF
+		free_faf_file_private_data(imported_file);
+#endif
+		fput(imported_file);
+		*returned_file = file;
+	}
+	else {
+		*returned_file = imported_file;
+		first_import = 1;
+	}
+
+	r = end_import_dvfs_file(objid, dvfs_file, imported_file, first_import);
+	if (!r)
+		goto exit;
+
+	if (!first_import) {
+#ifdef CONFIG_KRG_FAF
+		free_faf_file_private_data(imported_file);
+#endif
+		fput(imported_file);
+		*returned_file = NULL;
+	}
+
+exit:
+err_read:
+	return r;
+}
+
+/** Imports the open files of the process
+ *  Not used by Restart.
+ *  @author  Geoffroy Vallee, Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where open files data are stored.
+ *  @param tsk    Task to load open files data in.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int import_open_files (struct epm_action *action,
+		       ghost_t *ghost,
+                       struct task_struct *tsk,
+		       struct files_struct *files,
+		       struct fdtable *fdt,
+		       int last_open_fd)
+{
+	int i, j, r = 0;
+
+	BUG_ON(action->type == EPM_CHECKPOINT);
+
+	/* Reception of the files list and their names */
+	for (i = 0; i < last_open_fd; i++) {
+		if (FD_ISSET (i, fdt->open_fds)) {
+			r = import_one_open_file (action, ghost, tsk, i,
+						  (void *) &fdt->fd[i]);
+			if (r != 0)
+				goto err;
+			BUG_ON (!fdt->fd[i]);
+		}
+		else
+			fdt->fd[i] = NULL;
+	}
+exit:
+	return r;
+
+err:
+	for (j = 0; j < i; j++) {
+		if (fdt->fd[j])
+			filp_close(fdt->fd[j], files);
+	}
+
+	goto exit;
+}
+
+static int cr_link_to_open_files(struct epm_action *action,
+				 ghost_t *ghost,
+				 struct task_struct *tsk,
+				 struct files_struct *files,
+				 struct fdtable *fdt,
+				 int last_open_fd)
+{
+	int i, r = 0;
+
+	BUG_ON(action->type == EPM_CHECKPOINT
+	       && action->restart.shared == CR_LINK_ONLY);
+
+	/* Linking the files in the files_struct */
+	for (i = 0; i < last_open_fd; i++) {
+		if (FD_ISSET (i, fdt->open_fds)) {
+			r = cr_link_to_file(action, ghost, tsk,
+					    (void *) &fdt->fd[i]);
+			if (r != 0)
+				goto exit;
+
+			/* in case of unsupported files and related option
+			 * cr_link_to_file may return r==0 with
+			 * fdt->fd[i] == NULL
+			 */
+			if (!fdt->fd[i])
+				FD_CLR(i, fdt->open_fds);
+		}
+		else
+			fdt->fd[i] = NULL;
+	}
+
+exit:
+	return r;
+}
+
+static int cr_link_to_files_struct(struct epm_action *action,
+				   ghost_t *ghost,
+				   struct task_struct *tsk)
+{
+	int r;
+	long key;
+	struct files_struct *files;
+
+	r = ghost_read(ghost, &key, sizeof(long));
+	if (r)
+		goto err;
+
+	files = get_imported_shared_object(action->restart.app,
+					   FILES_STRUCT, key);
+
+	if (!files) {
+		r = -E_CR_BADDATA;
+		goto err;
+	}
+
+	/* the task is not yet hashed, no need to lock */
+	atomic_inc(&files->count);
+	tsk->files = files;
+err:
+	return r;
+}
+
+/** Imports the files informations of the process
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where files data are stored.
+ *  @param tsk    Task to load files data in.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int import_files_struct (struct epm_action *action,
+			 ghost_t *ghost,
+                         struct task_struct *tsk)
+{
+	int import_fdt;
+	int last_open_fd;
+	int r = -ENOMEM;
+	struct files_struct *files;
+	struct fdtable *fdt;
+
+	{
+		int magic = 0;
+
+		r = ghost_read (ghost, &magic, sizeof (int));
+
+		BUG_ON (!r && magic != 780574);
+	}
+
+	if (action->type == EPM_CHECKPOINT
+	    && action->restart.shared == CR_LINK_ONLY) {
+		r = cr_link_to_files_struct(action, ghost, tsk);
+		return r;
+	}
+
+	/* Import the main files structure */
+
+	files = kmem_cache_alloc (files_cachep, GFP_KERNEL);
+	if (files == NULL)
+		return -ENOMEM;
+
+	r = ghost_read (ghost, files, sizeof (struct files_struct));
+	if (r)
+		goto exit_free_files;
+
+	atomic_set (&files->count, 1);
+	spin_lock_init (&files->file_lock);
+
+	r = ghost_read (ghost, &last_open_fd, sizeof (int));
+	if (r)
+		goto exit_free_files;
+	r = ghost_read (ghost, &import_fdt, sizeof (int));
+	if (r)
+		goto exit_free_files;
+
+	/* Import the open files table structure */
+
+	if (import_fdt) {
+		unsigned int cpy, set;
+
+		fdt = alloc_fdtable(last_open_fd);
+		if (fdt == NULL)
+			goto exit_free_files;
+
+		cpy = last_open_fd * sizeof(struct file *);
+		set = (fdt->max_fds - last_open_fd) * sizeof(struct file *);
+		memset((char *)(fdt->fd) + cpy, 0, set);
+
+		cpy = last_open_fd / BITS_PER_BYTE;
+		set = (fdt->max_fds - last_open_fd) / BITS_PER_BYTE;
+
+		r = ghost_read (ghost, fdt->close_on_exec, cpy);
+		if (r)
+			goto exit_free_files;
+		memset((char *)(fdt->close_on_exec) + cpy, 0, set);
+
+		r = ghost_read (ghost, fdt->open_fds, cpy);
+		if (r)
+			goto exit_free_files;
+		memset((char *)(fdt->open_fds) + cpy, 0, set);
+	}
+	else {
+		fdt = &files->fdtab;
+		INIT_RCU_HEAD(&fdt->rcu);
+		fdt->next = NULL;
+		fdt->close_on_exec = (fd_set *)&files->close_on_exec_init;
+		fdt->open_fds = (fd_set *)&files->open_fds_init;
+		fdt->fd = &files->fd_array[0];
+	}
+
+	rcu_assign_pointer(files->fdt, fdt);
+
+	tsk->files = files;
+
+	{
+		int magic = 0;
+
+		r = ghost_read (ghost, &magic, sizeof (int));
+
+		BUG_ON (!r && magic != 280574);
+	}
+
+	if (action->type == EPM_CHECKPOINT)
+		r = cr_link_to_open_files(action, ghost, tsk, files,
+					  fdt, last_open_fd);
+	else
+		r = import_open_files(action, ghost, tsk, files, fdt,
+				      last_open_fd);
+
+	if (r)
+		goto exit_free_fdt;
+
+	{
+		int magic = 0;
+
+		r = ghost_read (ghost, &magic, sizeof (int));
+
+		BUG_ON (!r && magic != 380574);
+	}
+
+	return 0;
+
+exit_free_fdt:
+	if (import_fdt)
+		free_fdtable(fdt);
+
+exit_free_files:
+	kmem_cache_free(files_cachep, files);
+	return r;
+}
+
+static int cr_link_to_vma_phys_file(struct epm_action *action,
+				    ghost_t *ghost,
+				    struct task_struct *tsk,
+				    struct vm_area_struct *vma,
+				    struct file **file)
+{
+	int r;
+	int anon_shared;
+
+	r = ghost_read_type(ghost, anon_shared);
+	if (r)
+		goto exit;
+
+	if (anon_shared) {
+		vma->vm_file = NULL;
+		r = shmem_zero_setup(vma);
+		*file = vma->vm_file;
+		goto exit;
+	}
+
+	r = cr_link_to_file(action, ghost, tsk, file);
+	if (r)
+		goto exit;
+
+	if (vma->vm_flags & VM_EXEC) {
+
+		/* to check it is the same file */
+
+		loff_t old_file_size;
+		loff_t current_file_size;
+
+		r = ghost_read(ghost, &old_file_size, sizeof(loff_t));
+		if (r)
+			goto exit;
+
+		r = get_file_size(*file, &current_file_size);
+		if (r)
+			goto exit;
+
+		if (old_file_size != current_file_size) {
+			printk("The application binary or libraries may have "
+			       "changed since the checkpoint (%llu != %llu)\n",
+			       old_file_size, current_file_size);
+			r = -ENOEXEC;
+		}
+	}
+
+exit:
+	return r;
+}
+
+int import_vma_phys_file(struct epm_action *action,
+			 ghost_t *ghost,
+			 struct task_struct *tsk,
+			 struct vm_area_struct *vma,
+			 hashtable_t *file_table)
+{
+	unsigned long key;
+	struct file *file;
+	int import_file;
+	int r;
+
+	if (action->type == EPM_CHECKPOINT) {
+		r = cr_link_to_vma_phys_file(action, ghost, tsk, vma, &file);
+		if (r || is_anon_shared_mmap(file))
+			goto err;
+		goto map_file;
+	}
+
+	r = ghost_read(ghost, &key, sizeof(unsigned long));
+	if (r)
+		goto err;
+
+	r = ghost_read(ghost, &import_file, sizeof(int));
+	if (r)
+		goto err;
+
+	if (import_file) {
+		/* First import ? Let's do the job ! */
+		r = import_one_open_file(action, ghost, tsk,
+					 MMAPPED_FILE, &file);
+		if (r)
+			goto err;
+
+		r = __hashtable_add(file_table, key, file);
+		if (r)
+			goto err;
+
+		/* Get a reference until mm import is done */
+		get_file(file);
+	}
+	else {
+		/* File has already been imported for another VMA: reuse it. */
+		file = __hashtable_find(file_table, key);
+		BUG_ON(file == NULL);
+		get_file(file);
+	}
+
+map_file:
+	vma->vm_file = file;
+	if (file->f_op && file->f_op->mmap)
+		r = file->f_op->mmap(file, vma);
+	if (r)
+		goto err;
+
+err:
+	return r;
+}
+
+/** Import the file associated to a VMA.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost    Ghost where data are be stored.
+ *  @param tsk      The task to import VMA for.
+ *  @param vma      The VMA to import the file in.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int import_vma_file (struct epm_action *action,
+		     ghost_t *ghost,
+                     struct task_struct *tsk,
+                     struct vm_area_struct *vma,
+		     hashtable_t *file_table)
+{
+	int vm_file_type;
+	int r;
+
+	/* Import the file type flag */
+	r = ghost_read (ghost, &vm_file_type, sizeof (int));
+	if (r)
+		goto err_read;
+
+	switch (vm_file_type) {
+	  case VM_FILE_NONE:
+		  vma->vm_file = NULL;
+		  break;
+
+	  case VM_FILE_PHYS:
+		  r = import_vma_phys_file(action, ghost, tsk, vma,
+					   file_table);
+		  if (r)
+			  goto err_read;
+		  BUG_ON (!vma->vm_file);
+		  break;
+
+	  default:
+		  BUG();
+	}
+
+err_read:
+	return r;
+}
+
+int import_mm_exe_file(struct epm_action *action, ghost_t *ghost,
+		       struct task_struct *tsk)
+{
+	int dump, r = 0;
+
+	BUG_ON(action->type == EPM_CHECKPOINT
+	       && action->restart.shared == CR_LINK_ONLY);
+
+#ifdef CONFIG_PROC_FS
+	r = ghost_read(ghost, &dump, sizeof(int));
+	if (r)
+		goto exit;
+
+	if (dump) {
+		if (action->type == EPM_CHECKPOINT)
+			r = cr_link_to_file(action, ghost, tsk,
+					    &tsk->mm->exe_file);
+		else
+			r = import_one_open_file(action, ghost, tsk, -1,
+						 &tsk->mm->exe_file);
+	}
+exit:
+#endif
+	return r;
+}
+
+static int cr_link_to_fs_struct(struct epm_action *action,
+				ghost_t *ghost,
+				struct task_struct *tsk)
+{
+	int r;
+	long key;
+	struct fs_struct *fs;
+
+	r = ghost_read(ghost, &key, sizeof(long));
+	if (r)
+		goto err;
+
+	fs = get_imported_shared_object(action->restart.app,
+					FS_STRUCT, key);
+
+	if (!fs) {
+		r = -E_CR_BADDATA;
+		goto err;
+	}
+
+	/* the task is not yet hashed, no need to lock */
+//	fs->users++;
+	tsk->fs = fs;
+err:
+	return r;
+}
+
+/** Import the fs_struct of a process
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where file data are stored.
+ *  @param tsk    Task to import file data in.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int import_fs_struct (struct epm_action *action,
+		      ghost_t *ghost,
+                      struct task_struct *tsk)
+{
+	struct fs_struct *fs;
+	char *buffer;
+	int r;
+
+	if (action->type == EPM_CHECKPOINT
+	    && action->restart.shared == CR_LINK_ONLY) {
+		r = cr_link_to_fs_struct(action, ghost, tsk);
+		return r;
+	}
+
+	buffer = (char *)__get_free_page(GFP_KERNEL);
+	if (!buffer)
+		return -ENOMEM;
+
+	{
+		int magic = 0;
+
+		r = ghost_read (ghost, &magic, sizeof (int));
+		BUG_ON (!r && magic != 55611);
+	}
+
+	r = -ENOMEM;
+	fs = kmem_cache_alloc(fs_cachep, GFP_KERNEL);
+	if (fs == NULL)
+		goto exit;
+
+//	fs->users = 1;
+//	fs->in_exec = 0;
+	atomic_inc(&fs->count);
+	rwlock_init (&fs->lock);
+
+	/* Import the umask value */
+
+	r = ghost_read(ghost, &fs->umask, sizeof (int));
+	if (r)
+		goto exit_free_fs;
+
+	/* Import the root path name */
+
+	r = populate_fs_struct(ghost, buffer, &fs->root);
+	if (r)
+		goto exit_free_fs;
+
+	/* Import the pwd path name */
+
+	r = populate_fs_struct(ghost, buffer, &fs->pwd);
+	if (r)
+		goto exit_put_root;
+
+	{
+		int magic = 0;
+
+		r = ghost_read (ghost, &magic, sizeof (int));
+		BUG_ON (!r && magic != 180574);
+	}
+
+	tsk->fs = fs;
+
+exit:
+	free_page ((unsigned long) buffer);
+
+	return r;
+
+exit_put_root:
+	path_put(&fs->root);
+
+exit_free_fs:
+	kmem_cache_free (fs_cachep, fs);
+	goto exit;
+}
+
+int import_mnt_namespace(struct epm_action *action,
+			 ghost_t *ghost, struct task_struct *tsk)
+{
+	/* TODO */
+	tsk->nsproxy->mnt_ns = tsk->nsproxy->krg_ns->root_nsproxy.mnt_ns;
+	get_mnt_ns(tsk->nsproxy->mnt_ns);
+
+	return 0;
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                            UNIMPORT FUNCTIONS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+void unimport_files_struct(struct task_struct *tsk)
+{
+	exit_files(tsk);
+}
+
+void unimport_fs_struct(struct task_struct *tsk)
+{
+	exit_fs(tsk);
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              INITIALIZATION                               */
+/*                                                                           */
+/*****************************************************************************/
+
+int dvfs_mobility_init(void)
+{
+#ifdef CONFIG_KRG_FAF
+	krgsyms_register(KRGSYMS_DVFS_MOBILITY_FAF_OPS,
+			 &dvfs_mobility_faf_ops);
+#endif
+	krgsyms_register(KRGSYMS_DVFS_MOBILITY_REGULAR_OPS,
+			 &dvfs_mobility_regular_ops);
+
+	return 0;
+}
+
+void dvfs_mobility_finalize (void)
+{
+	krgsyms_unregister(KRGSYMS_DVFS_MOBILITY_REGULAR_OPS);
+#ifdef CONFIG_KRG_FAF
+	krgsyms_unregister(KRGSYMS_DVFS_MOBILITY_FAF_OPS);
+#endif
+}
+
+static int cr_export_now_files_struct(struct epm_action *action, ghost_t *ghost,
+				      struct task_struct *task,
+				      union export_args *args)
+{
+	int r;
+	r = export_files_struct(action, ghost, task);
+	if (r)
+		ckpt_err(action, r,
+			 "Fail to save struct files_struct of process %d (%s)",
+			 task_pid_knr(task), task->comm);
+
+	return r;
+}
+
+static int cr_import_now_files_struct(struct epm_action *action, ghost_t *ghost,
+				      struct task_struct *fake, int local_only,
+				      void ** returned_data, size_t *data_size)
+{
+	int r;
+	BUG_ON(*returned_data != NULL);
+
+	r = import_files_struct(action, ghost, fake);
+	if (r) {
+		ckpt_err(action, r,
+			 "Fail to restore a struct files_struct",
+			 action->restart.app->app_id);
+		goto err;
+	}
+
+	*returned_data = fake->files;
+err:
+	return r;
+}
+
+static int cr_import_complete_files_struct(struct task_struct *fake,
+					   void *_files)
+{
+	struct files_struct *files = _files;
+
+	fake->files = files;
+	exit_files(fake);
+
+	return 0;
+}
+
+static int cr_delete_files_struct(struct task_struct *fake, void *_files)
+{
+	struct files_struct *files = _files;
+
+	fake->files = files;
+	exit_files(fake);
+
+	return 0;
+}
+
+struct shared_object_operations cr_shared_files_struct_ops = {
+        .export_now        = cr_export_now_files_struct,
+	.export_user_info  = NULL,
+	.import_now        = cr_import_now_files_struct,
+	.import_complete   = cr_import_complete_files_struct,
+	.delete            = cr_delete_files_struct,
+};
+
+static int cr_export_now_fs_struct(struct epm_action *action, ghost_t *ghost,
+				   struct task_struct *task,
+				   union export_args *args)
+{
+	int r;
+	r = export_fs_struct(action, ghost, task);
+	if (r)
+		ckpt_err(action, r,
+			 "Fail to save struct fs_struct of process %d (%s)",
+			 task_pid_knr(task), task->comm);
+
+	return r;
+}
+
+static int cr_import_now_fs_struct(struct epm_action *action, ghost_t *ghost,
+				   struct task_struct *fake, int local_only,
+				   void ** returned_data, size_t *data_size)
+{
+	int r;
+	BUG_ON(*returned_data != NULL);
+
+	r = import_fs_struct(action, ghost, fake);
+	if (r) {
+		ckpt_err(action, r,
+			 "App %ld - Fail to restore a struct fs_struct",
+			 action->restart.app->app_id);
+		goto err;
+	}
+
+	*returned_data = fake->fs;
+err:
+	return r;
+}
+
+static int cr_import_complete_fs_struct(struct task_struct *fake, void *_fs)
+{
+	struct fs_struct *fs = _fs;
+
+	fake->fs = fs;
+	exit_fs(fake);
+
+	return 0;
+}
+
+static int cr_delete_fs_struct(struct task_struct *fake, void *_fs)
+{
+	struct fs_struct *fs = _fs;
+
+	fake->fs = fs;
+	exit_fs(fake);
+
+	return 0;
+}
+
+struct shared_object_operations cr_shared_fs_struct_ops = {
+        .export_now        = cr_export_now_fs_struct,
+	.export_user_info  = NULL,
+	.import_now        = cr_import_now_fs_struct,
+	.import_complete   = cr_import_complete_fs_struct,
+	.delete            = cr_delete_fs_struct,
+};
diff -ruN linux-2.6.29/kerrighed/fs/mobility.h android_cluster/linux-2.6.29/kerrighed/fs/mobility.h
--- linux-2.6.29/kerrighed/fs/mobility.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/mobility.h	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,39 @@
+/** DVFS mobililty interface
+ *  @file mobility.h
+ *
+ *  Definition of DVFS mobility function interface.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __MOBILITY_H__
+#define __MOBILITY_H__
+
+#include <kerrighed/ghost_types.h>
+#include <linux/hashtable.h>
+
+struct epm_action;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+struct dvfs_mobility_operations {
+  int (*file_export) (struct epm_action *,
+		      ghost_t *, struct task_struct *, int, struct file *);
+  int (*file_import) (struct epm_action *,
+		      ghost_t *, struct task_struct *, struct file **);
+};
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+int dvfs_mobility_init(void) ;
+
+void dvfs_mobility_finalize (void) ;
+
+#endif // __MOBILITY_H__
diff -ruN linux-2.6.29/kerrighed/fs/physical_fs.c android_cluster/linux-2.6.29/kerrighed/fs/physical_fs.c
--- linux-2.6.29/kerrighed/fs/physical_fs.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/physical_fs.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,178 @@
+/** Access to Physical File System management.
+ *  @file physical_fs.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#include <linux/fs.h>
+#include <linux/mount.h>
+#include <linux/mnt_namespace.h>
+#include <linux/dcache.h>
+#include <linux/fs_struct.h>
+#include <linux/sched.h>
+#include <linux/module.h>
+#ifdef CONFIG_X86_64
+#include <asm/ia32.h>
+#endif
+#include <linux/file.h>
+#include <linux/namei.h>
+#include <kerrighed/physical_fs.h>
+#include <kerrighed/namespace.h>
+
+char *physical_d_path(const struct path *path, char *tmp, bool del_ok)
+{
+	struct path ns_root;
+	char *pathname;
+	bool deleted;
+
+	/* Mnt namespace is already pinned by path->mnt */
+	if (!path->mnt->mnt_ns)
+		/* Not exportable */
+		return NULL;
+
+	ns_root.mnt = path->mnt->mnt_ns->root;
+	ns_root.dentry = ns_root.mnt->mnt_root;
+	spin_lock(&dcache_lock);
+	pathname = ____d_path(path, &ns_root, tmp, PAGE_SIZE, &deleted);
+	spin_unlock(&dcache_lock);
+	BUG_ON(ns_root.mnt != path->mnt->mnt_ns->root
+	       || ns_root.dentry != ns_root.mnt->mnt_root);
+
+	if ((deleted && !del_ok) || IS_ERR(pathname))
+		return NULL;
+
+	return pathname;
+}
+
+void get_physical_root(struct path *root)
+{
+	struct krg_namespace *krg_ns = find_get_krg_ns();
+
+	BUG_ON(!krg_ns);
+	root->mnt = krg_ns->root_nsproxy.mnt_ns->root;
+	root->dentry = root->mnt->mnt_root;
+	path_get(root);
+	put_krg_ns(krg_ns);
+
+	while (d_mountpoint(root->dentry) &&
+	       follow_down(&root->mnt, &root->dentry))
+		;
+}
+
+void chroot_to_physical_root(struct prev_root *prev_root)
+{
+	struct krg_namespace *krg_ns = find_get_krg_ns();
+	struct fs_struct *fs = current->fs;
+	struct path root, prev_pwd;
+
+	BUG_ON(!krg_ns);
+	put_krg_ns(krg_ns);
+//	BUG_ON(fs->users != 1);
+
+	get_physical_root(&root);
+	write_lock(&fs->lock);
+	prev_root->path = fs->root;
+	fs->root = root;
+	path_get(&root);
+	prev_pwd = fs->pwd;
+	fs->pwd = root;
+	write_unlock(&fs->lock);
+	path_put(&prev_pwd);
+
+	BUG_ON(prev_root->path.mnt->mnt_ns != current->nsproxy->mnt_ns);
+	prev_root->nsproxy = current->nsproxy;
+	rcu_assign_pointer(current->nsproxy, &krg_ns->root_nsproxy);
+}
+
+void chroot_to_prev_root(const struct prev_root *prev_root)
+{
+	struct fs_struct *fs = current->fs;
+	struct path root, pwd;
+
+	write_lock(&fs->lock);
+	root = fs->root;
+	fs->root = prev_root->path;
+	pwd = fs->pwd;
+	path_get(&fs->root);
+	fs->pwd = fs->root;
+	write_unlock(&fs->lock);
+	path_put(&root);
+	path_put(&pwd);
+
+	rcu_assign_pointer(current->nsproxy, prev_root->nsproxy);
+}
+
+struct file *open_physical_file (char *filename,
+                                 int flags,
+                                 int mode,
+                                 uid_t fsuid,
+                                 gid_t fsgid)
+{
+	const struct cred *old_cred;
+	struct cred *override_cred;
+	struct prev_root prev_root;
+	struct file *file;
+
+	override_cred = prepare_creds();
+	if (!override_cred)
+		return ERR_PTR(-ENOMEM);
+
+	override_cred->fsuid = fsuid;
+	override_cred->fsgid = fsgid;
+	old_cred = override_creds(override_cred);
+
+	chroot_to_physical_root(&prev_root);
+
+	file = filp_open (filename, flags, mode);
+
+	chroot_to_prev_root(&prev_root);
+
+	revert_creds(old_cred);
+	put_cred(override_cred);
+
+	return file;
+}
+
+int close_physical_file (struct file *file)
+{
+	int res;
+
+	res = filp_close (file, current->files);
+
+	return res;
+}
+
+int remove_physical_file (struct file *file)
+{
+	struct dentry *dentry;
+	struct inode *dir;
+	int res = 0;
+
+	dentry = file->f_dentry;
+	dir = dentry->d_parent->d_inode;
+
+	res = vfs_unlink (dir, dentry);
+	dput (dentry);
+	put_filp (file);
+
+	return res;
+}
+
+int remove_physical_dir (struct file *file)
+{
+	struct dentry *dentry;
+	struct inode *dir;
+	int res = 0;
+
+	dentry = file->f_dentry;
+	dir = dentry->d_parent->d_inode;
+
+	res = vfs_rmdir (dir, dentry);
+	dput (dentry);
+	put_filp (file);
+
+	return res;
+}
diff -ruN linux-2.6.29/kerrighed/fs/regular_file_mgr.c android_cluster/linux-2.6.29/kerrighed/fs/regular_file_mgr.c
--- linux-2.6.29/kerrighed/fs/regular_file_mgr.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/fs/regular_file_mgr.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,961 @@
+/** Global management of regular files.
+ *  @file regular_file_mgr.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/mutex.h>
+#include <linux/sched.h>
+#include <linux/file.h>
+#ifdef CONFIG_KRG_IPC
+#include <linux/ipc.h>
+#include <linux/shm.h>
+#include <linux/msg.h>
+#include <linux/ipc_namespace.h>
+#endif
+#include <kddm/kddm.h>
+#include <kerrighed/action.h>
+#include <kerrighed/application.h>
+#include <kerrighed/app_shared.h>
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#include "faf/faf_internal.h"
+#include <kerrighed/faf_file_mgr.h>
+#endif
+#include <kerrighed/file.h>
+#include <kerrighed/file_stat.h>
+#include <kerrighed/ghost_helpers.h>
+#include <kerrighed/regular_file_mgr.h>
+#include <kerrighed/physical_fs.h>
+#include <kerrighed/pid.h>
+#include "mobility.h"
+
+/*****************************************************************************/
+/*                                                                           */
+/*                             REGULAR FILES CREATION                        */
+/*                                                                           */
+/*****************************************************************************/
+
+struct file *reopen_file_entry_from_krg_desc (struct task_struct *task,
+                                              struct regular_file_krg_desc *desc)
+{
+	struct file *file = NULL;
+
+	BUG_ON (!task);
+	BUG_ON (!desc);
+
+	file = open_physical_file (desc->file.filename, desc->file.flags,
+				   desc->file.mode, desc->file.uid,
+				   desc->file.gid);
+
+	if (IS_ERR (file))
+		return file;
+
+	file->f_pos = desc->file.pos;
+
+	return file;
+}
+
+struct file *create_file_entry_from_krg_desc (struct task_struct *task,
+                                              struct regular_file_krg_desc *desc)
+{
+	struct file *file = NULL;
+
+	BUG_ON (!task);
+	BUG_ON (!desc);
+
+	file = open_physical_file(desc->file.filename, desc->file.flags,
+				  desc->file.mode,
+				  task->cred->fsuid, task->cred->fsgid);
+
+	if (IS_ERR (file))
+		return file;
+
+	file->f_pos = desc->file.pos;
+	file->f_dentry->d_inode->i_mode |= desc->file.mode;
+
+	return file;
+}
+
+/** Create a regular file struct from a Kerrighed file descriptor.
+ *  @author Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param task    Task to create the file for.
+ *  @param desc    Kerrighed file descriptor.
+ *
+ *  @return   0 if everything ok.
+ *            Negative value otherwise.
+ */
+static struct file *import_regular_file_from_krg_desc(
+	struct epm_action *action,
+	struct task_struct *task,
+	struct regular_file_krg_desc *desc)
+{
+	struct file *file;
+
+	BUG_ON (!task);
+	BUG_ON (!desc);
+
+	if (desc->type == PIPE)
+		file = reopen_pipe_file_entry_from_krg_desc(task, desc);
+#ifdef CONFIG_KRG_IPC
+	else if (desc->type == SHM)
+		file = reopen_shm_file_entry_from_krg_desc(task, desc);
+#endif
+	else {
+		desc->file.filename = (char *) &desc[1];
+
+		if (desc->file.ctnrid != KDDM_SET_UNUSED)
+			file = create_file_entry_from_krg_desc(task, desc);
+		else
+			file = reopen_file_entry_from_krg_desc(task, desc);
+
+		if (IS_ERR(file))
+			ckpt_err(action, PTR_ERR(file),
+				 "App %ld - Fail to import file %s",
+				 action->restart.app->app_id,
+				 desc->file.filename);
+	}
+
+	return file;
+}
+
+int check_flush_file (struct epm_action *action,
+		      fl_owner_t id,
+		      struct file *file)
+{
+	int err = 0;
+
+	switch (action->type) {
+	case EPM_REMOTE_CLONE:
+	case EPM_MIGRATE:
+	case EPM_CHECKPOINT:
+		  if (file->f_dentry) {
+			  if (file->f_op && file->f_op->flush)
+				  err = file->f_op->flush(file, id);
+		  }
+
+		  break;
+
+	  default:
+		  break;
+	}
+
+	return err;
+}
+
+/** Return a kerrighed descriptor corresponding to the given file.
+ *  @author Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param file       The file to get a Kerrighed descriptor for.
+ *  @param desc       The returned descriptor.
+ *  @param desc_size  Size of the returned descriptor.
+ *
+ *  @return   0 if everything ok.
+ *            Negative value otherwise.
+ */
+int get_regular_file_krg_desc(struct file *file, void **desc,
+			      int *desc_size)
+{
+	char *tmp, *file_name;
+	struct regular_file_krg_desc *data;
+	int size = 0, name_len;
+	int r = -ENOENT;
+
+#ifdef CONFIG_KRG_IPC
+	if (is_shm(file)) {
+		r = get_shm_file_krg_desc(file, desc, desc_size);
+		goto exit;
+	}
+#endif
+	if (is_anonymous_pipe(file)) {
+		r = get_pipe_file_krg_desc(file, desc, desc_size);
+		goto exit;
+	}
+
+	tmp = (char *)__get_free_page(GFP_KERNEL);
+	if (!tmp) {
+		r = -ENOMEM;
+		goto exit;
+	}
+
+	file_name = get_phys_filename(file, tmp, false);
+	if (!file_name)
+		goto exit_free_page;
+
+	name_len = strlen (file_name) + 1;
+	size = sizeof (struct regular_file_krg_desc) + name_len;
+
+	data = kmalloc (size, GFP_KERNEL);
+	if (!data) {
+		r = -ENOMEM;
+		goto exit_free_page;
+	}
+
+	data->type = FILE;
+	data->file.filename = (char *) &data[1];
+
+	strncpy(data->file.filename, file_name, name_len);
+
+	data->file.flags = file->f_flags
+#ifdef CONFIG_KRG_FAF
+		& (~(O_FAF_SRV | O_FAF_CLT));
+#endif
+	data->file.mode = file->f_mode;
+	data->file.pos = file->f_pos;
+	data->file.uid = file->f_cred->uid;
+	data->file.gid = file->f_cred->gid;
+
+	if (
+#ifdef CONFIG_KRG_FAF
+	    !(file->f_flags & (O_FAF_CLT | O_FAF_SRV)) &&
+#endif
+	    file->f_dentry->d_inode->i_mapping->kddm_set
+		)
+		data->file.ctnrid =
+			file->f_dentry->d_inode->i_mapping->kddm_set->id;
+	else
+		data->file.ctnrid = KDDM_SET_UNUSED;
+
+	*desc = data;
+	*desc_size = size;
+	r = 0;
+exit_free_page:
+	free_page ((unsigned long) tmp);
+exit:
+	return r;
+}
+
+/*****************************************************************************/
+
+int ghost_read_file_krg_desc(ghost_t *ghost, void **desc, int *desc_size)
+{
+	int r;
+	r = ghost_read(ghost, desc_size, sizeof (int));
+	if (r)
+		goto error;
+
+	*desc = kmalloc(*desc_size, GFP_KERNEL);
+	if (!(*desc)) {
+		r = -ENOMEM;
+		goto error;
+	}
+
+	r = ghost_read(ghost, *desc, *desc_size);
+	if (r) {
+		kfree(*desc);
+		*desc = NULL;
+	}
+error:
+	return r;
+}
+
+int ghost_write_file_krg_desc(ghost_t *ghost, void *desc, int desc_size)
+{
+	int r;
+
+	r = ghost_write (ghost, &desc_size, sizeof (int));
+	if (r)
+		goto error;
+
+	r = ghost_write (ghost, desc, desc_size);
+error:
+	return r;
+}
+
+static int ghost_write_regular_file_krg_desc(ghost_t *ghost, struct file *file)
+{
+	int r;
+	void *desc;
+	int desc_size;
+
+	r = get_regular_file_krg_desc(file, &desc, &desc_size);
+	if (r)
+		goto error;
+
+	r = ghost_write_file_krg_desc(ghost, desc, desc_size);
+	kfree (desc);
+error:
+	return r;
+}
+
+/*****************************************************************************/
+
+struct file *begin_import_dvfs_file(unsigned long dvfs_objid,
+				    struct dvfs_file_struct **dvfs_file)
+{
+	struct file *file = NULL;
+
+	/* Check if the file struct is already present */
+	*dvfs_file = grab_dvfs_file_struct(dvfs_objid);
+	file = (*dvfs_file)->file;
+	if (file)
+		get_file(file);
+
+	return file;
+}
+
+int end_import_dvfs_file(unsigned long dvfs_objid,
+			 struct dvfs_file_struct *dvfs_file,
+			 struct file *file, int first_import)
+{
+	int r = 0;
+
+	if (IS_ERR(file)) {
+		r = PTR_ERR (file);
+		goto error;
+	}
+
+	if (first_import) {
+		/* This is the first time the file is imported on this node
+		* Setup the DVFS file field and inc the DVFS counter.
+		*/
+		file->f_objid = dvfs_objid;
+		dvfs_file->file = file;
+
+		dvfs_file->count++;
+	}
+
+error:
+	put_dvfs_file_struct(dvfs_objid);
+	return r;
+}
+
+/*****************************************************************************/
+
+enum cr_file_desc_type {
+	CR_FILE_NONE,
+	CR_FILE_POINTER,
+	CR_FILE_REGULAR_DESC,
+	CR_FILE_FAF_DESC
+};
+
+struct cr_file_link {
+	enum cr_file_desc_type desc_type;
+	bool from_substitution;
+	unsigned long dvfs_objid;
+	void *desc;
+};
+
+static int __cr_link_to_file(struct epm_action *action, ghost_t *ghost,
+			     struct task_struct *task,
+			     struct cr_file_link *file_link,
+			     struct file **returned_file)
+{
+	int r = 0;
+
+	if (!file_link) {
+		BUG();
+		r = -E_CR_BADDATA;
+		goto exit;
+	}
+
+	BUG_ON(file_link->desc_type == CR_FILE_NONE);
+
+	if (file_link->desc_type != CR_FILE_POINTER
+	    && file_link->desc_type != CR_FILE_REGULAR_DESC
+	    && file_link->desc_type != CR_FILE_FAF_DESC) {
+		BUG();
+		r = -E_CR_BADDATA;
+		goto exit;
+	}
+
+	if (file_link->desc_type == CR_FILE_POINTER) {
+		*returned_file = file_link->desc;
+		get_file(*returned_file);
+	} else {
+		struct file *file;
+		struct dvfs_file_struct *dvfs_file;
+		int first_import = 0;
+
+		file_link->desc = &file_link[1];
+
+		/* Check if the file struct is already present */
+		file = begin_import_dvfs_file(file_link->dvfs_objid,
+					      &dvfs_file);
+
+		/* the file is not yet opened on this node */
+		if (!file) {
+#ifdef CONFIG_KRG_FAF
+			if (file_link->desc_type == CR_FILE_FAF_DESC)
+				file = create_faf_file_from_krg_desc(
+							task, file_link->desc);
+			else
+#endif
+				file = import_regular_file_from_krg_desc(
+					action, task, file_link->desc);
+			first_import = 1;
+		}
+
+		r = end_import_dvfs_file(file_link->dvfs_objid, dvfs_file, file,
+					 first_import);
+
+		if (r)
+			goto exit;
+
+		BUG_ON(file->f_objid != file_link->dvfs_objid);
+
+		*returned_file = file;
+	}
+exit:
+	return r;
+}
+
+int cr_link_to_file(struct epm_action *action, ghost_t *ghost,
+		    struct task_struct *task, struct file **returned_file)
+{
+	int r;
+	long key;
+	enum shared_obj_type type;
+	struct cr_file_link *file_link;
+
+	BUG_ON(action->type != EPM_CHECKPOINT);
+
+	/* files are linked while loading files_struct or mm_struct */
+	BUG_ON(action->restart.shared != CR_LOAD_NOW);
+
+	r = ghost_read(ghost, &type, sizeof(enum shared_obj_type));
+	if (r)
+		goto error;
+
+	if (type != LOCAL_FILE
+	    && type != DVFS_FILE)
+		goto err_bad_data;
+
+	r = ghost_read(ghost, &key, sizeof(long));
+	if (r)
+		goto error;
+
+	/* look in the table to find the new allocated data
+	 * imported in import_shared_objects */
+
+	file_link = get_imported_shared_object(action->restart.app,
+					       type, key);
+
+	if (file_link->desc_type == CR_FILE_NONE) {
+		*returned_file = NULL;
+		r = 0;
+	} else
+		r = __cr_link_to_file(action, ghost, task, file_link,
+				      returned_file);
+
+error:
+	if (r)
+		ckpt_err(NULL, r,
+			 "Fail to relink process %d of application %ld"
+			 " to file %d:%lu",
+			 task_pid_knr(task), action->restart.app->app_id,
+			 type, key);
+
+	return r;
+
+err_bad_data:
+	r = -E_CR_BADDATA;
+	goto error;
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                          REGULAR FILES IMPORT/EXPORT                      */
+/*                                                                           */
+/*****************************************************************************/
+
+/** Export a regular file descriptor into the given ghost.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost      the ghost to write data to.
+ *  @param file       The file to export.
+ *
+ *  @return   0 if everything ok.
+ *            Negative value otherwise.
+ */
+int regular_file_export (struct epm_action *action,
+			 ghost_t *ghost,
+                         struct task_struct *task,
+                         int index,
+                         struct file *file)
+{
+	int r = 0;
+
+	BUG_ON(action->type == EPM_CHECKPOINT
+	       && action->checkpoint.shared == CR_SAVE_LATER);
+
+	check_flush_file(action, task->files, file);
+
+	r = ghost_write_regular_file_krg_desc(ghost, file);
+
+	return r;
+}
+
+int __regular_file_import_from_desc(struct epm_action *action,
+				    struct regular_file_krg_desc *desc,
+				    struct task_struct *task,
+				    struct file **returned_file)
+{
+	int r = 0;
+	struct file *file;
+
+	file = import_regular_file_from_krg_desc(action, task, desc);
+	if (IS_ERR(file)) {
+		r = PTR_ERR (file);
+		goto exit;
+	}
+
+	check_flush_file(action, task->files, file);
+	*returned_file = file;
+
+exit:
+	return r;
+}
+
+/** Import a regular file descriptor from the given ghost.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost          The ghost to read data from.
+ *  @param task           The task data are imported for.
+ *  @param returned_file  The file struct where data should be imported to.
+ *
+ *  @return   0 if everything ok.
+ *            Negative value otherwise.
+ */
+int regular_file_import(struct epm_action *action,
+			ghost_t *ghost,
+			struct task_struct *task,
+			struct file **returned_file)
+{
+	struct regular_file_krg_desc *desc;
+	int desc_size, r = 0;
+
+	BUG_ON(action->type == EPM_CHECKPOINT);
+
+	r = ghost_read_file_krg_desc(ghost, (void **)(&desc), &desc_size);
+	if (r)
+		goto exit;
+
+	r = __regular_file_import_from_desc(action, desc, task, returned_file);
+
+	kfree (desc);
+exit:
+	return r;
+}
+
+
+
+struct dvfs_mobility_operations dvfs_mobility_regular_ops = {
+	.file_export = regular_file_export,
+	.file_import = regular_file_import,
+};
+
+static int cr_export_now_file(struct epm_action *action, ghost_t *ghost,
+			      struct task_struct *task,
+			      union export_args *args)
+{
+	int r, supported;
+
+	supported = can_checkpoint_file(args->file_args.file);
+
+	r = ghost_write(ghost, &supported, sizeof(supported));
+	if (r)
+		goto error;
+
+	if (supported)
+		r = regular_file_export(action, ghost, task,
+					args->file_args.index,
+					args->file_args.file);
+
+error:
+	if (r) {
+		char *buffer, *filename;
+		filename = alloc_filename(args->file_args.file, &buffer);
+		if (!IS_ERR(filename)) {
+			ckpt_err(action, r,
+				 "Fail to save information needed to reopen "
+				 "file %s as fd %d of process %d (%s)",
+				 filename, args->file_args.index,
+				 task_pid_knr(task), task->comm);
+			free_filename(buffer);
+		} else {
+			ckpt_err(action, r,
+				 "Fail to save information needed to reopen "
+				 "fd %d of process %d (%s)",
+				 args->file_args.index,
+				 task_pid_knr(task), task->comm);
+		}
+	}
+
+	return r;
+}
+
+int cr_export_user_info_file(struct epm_action *action, ghost_t *ghost,
+			     unsigned long key, struct export_obj_info *export)
+{
+	int r, index, keylen, nodelen;
+	char *tmp, *file_name;
+	struct file *file;
+	struct task_struct *task;
+	struct export_obj_info *_export;
+	kerrighed_node_t file_node;
+
+	/* do not export info about mapped file */
+	if (export->args.file_args.index == -1)
+		return 0;
+
+	file = export->args.file_args.file;
+
+	file_name = alloc_filename(file, &tmp);
+	if (IS_ERR(file_name)) {
+		r = PTR_ERR(file_name);
+		goto exit;
+	}
+
+	if (is_socket(file))
+		r = ghost_printf(ghost, "socket  ");
+
+	else if (is_anonymous_pipe(file))
+		r = ghost_printf(ghost, "pipe    ");
+
+	else if (is_named_pipe(file))
+		r = ghost_printf(ghost, "fifo    ");
+
+	else if (is_tty(file))
+		r = ghost_printf(ghost, "tty     ");
+
+	else if (is_char_device(file))
+		r = ghost_printf(ghost, "char    ");
+
+	else if (is_block_device(file))
+		r = ghost_printf(ghost, "block   ");
+
+	else if (is_link(file))
+		r = ghost_printf(ghost, "link    ");
+
+	else if (is_directory(file))
+		r = ghost_printf(ghost, "dir     ");
+
+	else
+		r = ghost_printf(ghost, "file    ");
+
+	if (r)
+		goto err_free_filename;
+
+	if (file->f_objid)
+		/* if the file is shared, there is no host node */
+		file_node = KERRIGHED_NODE_ID_NONE;
+	else
+		file_node = kerrighed_node_id;
+
+	nodelen = sizeof(file_node)*2;
+	keylen = sizeof(key)*2;
+
+	task = export->task;
+	index = export->args.file_args.index;
+
+	r = ghost_printf(ghost, "|%0*hX%0*lX|%s|%d:%d",
+			 nodelen, file_node, keylen, key,
+			 file_name, task_pid_knr(task), index);
+	if (r)
+		goto err_free_filename;
+
+	list_for_each_entry(_export, &export->next, next) {
+		task = _export->task;
+		index = _export->args.file_args.index;
+
+		r = ghost_printf(ghost, ",%d:%d",
+				 task_pid_knr(task), index);
+		if (r)
+			goto err_free_filename;
+	}
+
+	r = ghost_printf(ghost, "\n");
+
+err_free_filename:
+	free_filename(tmp);
+exit:
+	return r;
+}
+
+
+static int prepare_restart_data_unsupported_file(void **returned_data,
+						 size_t *data_size)
+{
+	struct cr_file_link *file_link;
+
+	*data_size = sizeof(struct cr_file_link);
+	file_link = kzalloc(*data_size, GFP_KERNEL);
+	if (!file_link)
+		return -ENOMEM;
+
+	file_link->desc_type = CR_FILE_NONE;
+	file_link->desc = NULL;
+	file_link->from_substitution = false;
+
+	*returned_data = file_link;
+
+	return 0;
+}
+
+static int prepare_restart_data_local_file(struct file *f,
+					   void **returned_data,
+					   size_t *data_size)
+{
+	struct cr_file_link *file_link;
+
+	*data_size = sizeof(struct cr_file_link);
+	file_link = kzalloc(*data_size, GFP_KERNEL);
+	if (!file_link)
+		return -ENOMEM;
+
+	file_link->desc_type = CR_FILE_POINTER;
+	file_link->desc = f;
+	file_link->from_substitution = false;
+
+	*returned_data = file_link;
+
+	return 0;
+}
+
+static int prepare_restart_data_dvfs_file(struct file *f,
+					  void *desc,
+					  int desc_size,
+					  void **returned_data,
+					  size_t *data_size)
+{
+	struct cr_file_link *file_link;
+
+	*data_size = sizeof(struct cr_file_link) + desc_size;
+	file_link = kzalloc(*data_size, GFP_KERNEL);
+	if (!file_link)
+		return -ENOMEM;
+
+	file_link->desc = &file_link[1];
+	file_link->desc_type = CR_FILE_REGULAR_DESC;
+	file_link->dvfs_objid = f->f_objid;
+	file_link->from_substitution = false;
+	memcpy(file_link->desc, desc, desc_size);
+
+	*returned_data = file_link;
+
+	return 0;
+}
+
+#ifdef CONFIG_KRG_FAF
+void fill_faf_file_krg_desc(faf_client_data_t *data, struct file *file);
+
+static int prepare_restart_data_faf_file(struct file *f,
+					 void **returned_data,
+					 size_t *data_size)
+{
+	struct cr_file_link *file_link;
+
+	*data_size = sizeof(struct cr_file_link) + sizeof(faf_client_data_t);
+	file_link = kmalloc(*data_size, GFP_KERNEL);
+	if (!file_link)
+		return -ENOMEM;
+
+	file_link->desc = &file_link[1];
+	file_link->desc_type = CR_FILE_FAF_DESC;
+	file_link->dvfs_objid = f->f_objid;
+	file_link->from_substitution = false;
+
+	if (f->f_flags & O_FAF_SRV)
+		fill_faf_file_krg_desc(file_link->desc, f);
+	else {
+		BUG_ON(!(f->f_flags & O_FAF_CLT));
+		*(faf_client_data_t*)file_link->desc =
+			*(faf_client_data_t*)f->private_data;
+	}
+
+	*returned_data = file_link;
+
+	return 0;
+}
+#endif
+
+int prepare_restart_data_shared_file(struct file *f,
+				     void *fdesc, int fdesc_size,
+				     void **returned_data, size_t *data_size,
+				     bool from_substitution)
+{
+	int r;
+	struct cr_file_link *file_link;
+
+#ifdef CONFIG_KRG_FAF
+	if (f->f_flags & (O_FAF_CLT | O_FAF_SRV))
+		r = prepare_restart_data_faf_file(f, returned_data,
+						  data_size);
+	else
+#endif
+		r = prepare_restart_data_dvfs_file(f, fdesc, fdesc_size,
+						   returned_data,
+						   data_size);
+
+	if (r)
+		goto error;
+
+	file_link = (struct cr_file_link *)(*returned_data);
+	file_link->from_substitution = from_substitution;
+error:
+	return r;
+}
+
+
+static int prepare_restart_data_supported_file(
+	struct file *f, int local_only,
+	void *fdesc, int fdesc_size,
+	void **returned_data, size_t *data_size)
+{
+	int r;
+
+	if (!local_only) {
+
+		if (!f->f_objid) {
+			/* get a new dvfs objid */
+			r = create_kddm_file_object(f);
+			if (r)
+				goto error;
+		}
+
+#ifdef CONFIG_KRG_FAF
+		r = setup_faf_file_if_needed(f);
+		if (r)
+			goto error;
+#endif
+		r = prepare_restart_data_shared_file(f, fdesc, fdesc_size,
+						     returned_data, data_size,
+						     false);
+	} else
+		r = prepare_restart_data_local_file(f, returned_data,
+						    data_size);
+
+error:
+	return r;
+}
+
+/* if *returned_data is not NULL, the file checkpointed must be
+ * replaced. Thus, we just read the ghost.
+ */
+static int cr_import_now_file(struct epm_action *action,
+			      ghost_t *ghost,
+			      struct task_struct *fake,
+			      int local_only,
+			      void **returned_data,
+			      size_t *data_size)
+{
+	int r, desc_size, supported;
+	struct file *f;
+	void *desc;
+
+	r = ghost_read(ghost, &supported, sizeof(supported));
+	if (r)
+		goto error;
+
+	if (!supported) {
+		r = prepare_restart_data_unsupported_file(returned_data,
+							  data_size);
+		goto error;
+	}
+
+	/* We need to read the file description from the ghost
+	 * even if we may not use it
+	 */
+	r = ghost_read_file_krg_desc(ghost, &desc, &desc_size);
+	if (r)
+		goto error;
+
+	/* File has been substituted at restart-time */
+	if (*returned_data)
+		goto error;
+
+	r = __regular_file_import_from_desc(action, desc, fake, &f);
+	if (r)
+		goto err_free_desc;
+
+	r = prepare_restart_data_supported_file(f, local_only, desc, desc_size,
+						returned_data, data_size);
+
+err_free_desc:
+	kfree(desc);
+error:
+	if (r)
+		ckpt_err(action, r,
+			 "App %ld - Fail to restore a file",
+			 action->restart.app->app_id);
+	return r;
+}
+
+static int cr_import_complete_file(struct task_struct *fake, void *_file_link)
+{
+	struct cr_file_link *file_link = _file_link;
+	struct file *file;
+
+	if (file_link->desc_type == CR_FILE_NONE
+	    || file_link->from_substitution)
+		/* the file has not been imported */
+		return 0;
+
+	if (file_link->desc_type == CR_FILE_POINTER)
+		file = file_link->desc;
+	else {
+		struct dvfs_file_struct *dvfs_file;
+
+		BUG_ON(file_link->desc_type != CR_FILE_REGULAR_DESC
+		       && file_link->desc_type != CR_FILE_FAF_DESC);
+
+		dvfs_file = grab_dvfs_file_struct(file_link->dvfs_objid);
+		file = dvfs_file->file;
+	}
+
+	BUG_ON(atomic_read(&file->f_count) <= 1);
+
+	fput(file);
+
+	if (file_link->desc_type != CR_FILE_POINTER)
+		put_dvfs_file_struct(file_link->dvfs_objid);
+
+	return 0;
+}
+
+static int cr_delete_file(struct task_struct *fake, void *_file_link)
+{
+	int r = 0;
+	struct cr_file_link *file_link = _file_link;
+	struct file *file;
+
+	if (file_link->desc_type == CR_FILE_NONE
+	    || file_link->from_substitution)
+		/* the file has not been imported */
+		return 0;
+
+	if (file_link->desc_type == CR_FILE_POINTER)
+		file = file_link->desc;
+	else {
+		struct dvfs_file_struct *dvfs_file;
+
+		BUG_ON(file_link->desc_type != CR_FILE_REGULAR_DESC
+		       && file_link->desc_type != CR_FILE_FAF_DESC);
+
+		dvfs_file = grab_dvfs_file_struct(file_link->dvfs_objid);
+		if (!dvfs_file) {
+			r = -ENOENT;
+			goto error;
+		}
+
+		file = dvfs_file->file;
+	}
+
+	if (file)
+		fput(file);
+
+error:
+	if (file_link->desc_type != CR_FILE_POINTER)
+		put_dvfs_file_struct(file_link->dvfs_objid);
+	return 0;
+}
+
+struct shared_object_operations cr_shared_file_ops = {
+	.export_now        = cr_export_now_file,
+	.export_user_info  = cr_export_user_info_file,
+	.import_now        = cr_import_now_file,
+	.import_complete   = cr_import_complete_file,
+	.delete            = cr_delete_file,
+};
diff -ruN linux-2.6.29/kerrighed/ghost/file_ghost.c android_cluster/linux-2.6.29/kerrighed/ghost/file_ghost.c
--- linux-2.6.29/kerrighed/ghost/file_ghost.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/ghost/file_ghost.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,491 @@
+/** File Ghost interface.
+ *  @file file_ghost.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ */
+#include <linux/cred.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/fdtable.h>
+#include <linux/syscalls.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+#include <linux/sched.h>
+#include <kerrighed/dvfs.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/file_ghost.h>
+#include <kerrighed/physical_fs.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *  Functions to implement ghost interface                                  *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+/** Read data from a file ghost.
+ *  @author Renaud Lottiaux, Geoffroy Vallée
+ *
+ *  @param  ghost   Ghost to read data from.
+ *  @param  buff    Buffer to store data.
+ *  @param  length  Size of data to read.
+ *
+ *  @return        0 if everything ok
+ *                 Negative value otherwise.
+ */
+int file_ghost_read(ghost_t *ghost, void *buff, size_t length)
+{
+	struct file_ghost_data *ghost_data;
+	struct file *file = NULL;
+	loff_t pos;
+	int r = 0;
+
+	BUG_ON(!ghost);
+	BUG_ON(!buff);
+
+	ghost_data = (struct file_ghost_data *)ghost->data;
+
+	file = ghost_data->file;
+	BUG_ON(!file);
+
+	pos = file_pos_read(file);
+	r = vfs_read(file, (char*)buff, length, &pos);
+	file_pos_write(file, pos);
+
+	if (r == length)
+		r = 0;
+	else if (r >= 0)
+		r = -EFAULT;
+	return r ;
+}
+
+/** Write data to a file ghost.
+ *  @author Renaud Lottiaux, Geoffroy Vallée
+ *
+ *  @param  ghost   Ghost to write data to.
+ *  @param  buff    Buffer to write in the ghost.
+ *  @param  length  Size of data to write.
+ *
+ *  @return        0 if everything ok
+ *                 Negative value otherwise.
+ */
+int file_ghost_write(struct ghost *ghost, const void *buff, size_t length)
+{
+	struct file_ghost_data *ghost_data;
+	struct file *file = NULL;
+	loff_t pos;
+	int r = 0;
+
+	BUG_ON(!ghost);
+	BUG_ON(!buff);
+
+	ghost_data = (struct file_ghost_data *)ghost->data;
+	BUG_ON(!ghost_data);
+
+	file = ghost_data->file;
+	BUG_ON(!file);
+
+	pos = file_pos_read(file);
+	r = vfs_write(file, (char*)buff, length, &pos);
+	file_pos_write(file, pos);
+
+	if (r == length)
+		r = 0;
+	else if (r >= 0)
+		r = -EFAULT;
+
+	return r ;
+}
+
+/** Close a ghost file
+ *  @author Matthieu Fertré
+ *
+ *  @param  ghost    Ghost file to close
+ */
+int file_ghost_close(ghost_t *ghost)
+{
+	struct file *file;
+	int r = 0;
+
+	file = ((struct file_ghost_data *)ghost->data)->file;
+
+	if (ghost->access & GHOST_WRITE
+	    && file->f_op->fsync) {
+		r = file->f_op->fsync(file, file->f_dentry, 1);
+		if (r)
+			printk("<0>-- WARNING -- (%s) : "
+			       "Something wrong in the sync : %d\n",
+			       __PRETTY_FUNCTION__, r);
+	}
+
+	if (((struct file_ghost_data *)ghost->data)->from_fd)
+		fput(file);
+	else
+		filp_close(file, current->files);
+
+	free_ghost(ghost);
+	return r;
+}
+
+/** File ghost operations
+ */
+struct ghost_operations ghost_file_ops = {
+	.read  = &file_ghost_read,
+	.write = &file_ghost_write,
+	.close = &file_ghost_close
+};
+
+void __set_ghost_fs(ghost_fs_t *oldfs)
+{
+	oldfs->fs = get_fs();
+	set_fs(KERNEL_DS);
+	oldfs->cred = NULL;
+}
+
+int set_ghost_fs(ghost_fs_t *oldfs, uid_t uid, gid_t gid)
+{
+	struct cred *new_cred;
+	int r = -ENOMEM;
+
+	new_cred = prepare_creds();
+	if (!new_cred)
+		goto err;
+	new_cred->fsuid = uid;
+	new_cred->fsgid = gid;
+
+	__set_ghost_fs(oldfs);
+	oldfs->cred = override_creds(new_cred);
+	put_cred(new_cred);
+	r = 0;
+
+err:
+	return r;
+}
+
+void unset_ghost_fs(const ghost_fs_t *oldfs)
+{
+	set_fs(oldfs->fs);
+	if (oldfs->cred)
+		revert_creds(oldfs->cred);
+}
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ * Macros and functions used to manage file ghost creation                  *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+// Path where file ghost are saved
+#define CHECKPOINT_PATH "/var/chkpt/"
+#define CHECKPOINT_PATH_LENGTH 45
+
+char checkpointRoot[CHECKPOINT_PATH_LENGTH] = CHECKPOINT_PATH;
+
+char *get_chkpt_dir(long app_id,
+		    unsigned int chkpt_sn)
+{
+	char *buff;
+	char *dirname;
+
+	buff = kmalloc(PATH_MAX*sizeof(char), GFP_KERNEL);
+	if (!buff) {
+		dirname = ERR_PTR(-ENOMEM);
+		goto err_buff;
+	}
+
+	dirname = kmalloc(PATH_MAX*sizeof(char), GFP_KERNEL);
+	if (!dirname) {
+		dirname = ERR_PTR(-ENOMEM);
+		goto err_dirname;
+	}
+
+	snprintf(dirname, PATH_MAX, "%s", checkpointRoot);
+
+	if (app_id) {
+		snprintf(buff, PATH_MAX, "%ld/", app_id);
+		strncat(dirname, buff, PATH_MAX);
+	}
+
+	if (chkpt_sn) {
+		snprintf(buff, PATH_MAX, "v%d/", chkpt_sn);
+		strncat(dirname, buff, PATH_MAX);
+	}
+
+err_dirname:
+	kfree(buff);
+err_buff:
+	return dirname;
+}
+
+static char *__get_chkpt_filebase(long app_id,
+				  unsigned int chkpt_sn,
+				  const char *format,
+				  va_list args)
+{
+	char *full_path;
+	char *rel_path;
+
+	full_path = get_chkpt_dir(app_id, chkpt_sn);
+	if (IS_ERR(full_path))
+		goto err;
+
+	rel_path = kvasprintf(GFP_KERNEL, format, args);
+	if (!rel_path) {
+		kfree(full_path);
+		full_path = ERR_PTR(-ENOMEM);
+		goto err;
+	}
+
+	strncat(full_path, rel_path, PATH_MAX);
+
+err:
+	return full_path;
+}
+
+char *get_chkpt_filebase(long app_id,
+			 unsigned int chkpt_sn,
+			 const char *format,
+			 ...)
+{
+	va_list args;
+	char *filename;
+
+	va_start(args, format);
+	filename = __get_chkpt_filebase(app_id, chkpt_sn, format, args);
+	va_end(args);
+
+	return filename;
+}
+
+int mkdir_chkpt_path(long app_id, unsigned int chkpt_sn)
+{
+	char *buff;
+	char *dirname;
+	int r;
+
+	buff = kmalloc(PATH_MAX*sizeof(char), GFP_KERNEL);
+	if (!buff) {
+		r = -ENOMEM;
+		goto err_buff;
+	}
+
+	dirname = kmalloc(PATH_MAX*sizeof(char), GFP_KERNEL);
+	if (!dirname) {
+		r = -ENOMEM;
+		goto err_dirname;
+	}
+
+	snprintf(dirname, PATH_MAX, "%s", checkpointRoot);
+
+	if (app_id) {
+		snprintf(buff, PATH_MAX, "%ld/", app_id);
+		strncat(dirname, buff, PATH_MAX);
+	}
+
+	r = sys_mkdir(dirname, S_IRWXUGO|S_ISVTX);
+	if (r && r != -EEXIST)
+		goto err;
+
+	/* really force the mode without looking at umask */
+	r = sys_chmod(dirname, S_IRWXUGO|S_ISVTX);
+	if (r)
+		goto err;
+
+	if (chkpt_sn) {
+		snprintf(buff, PATH_MAX, "v%d/", chkpt_sn);
+		strncat(dirname, buff, PATH_MAX);
+
+		r = sys_mkdir(dirname, S_IRWXU);
+		if (r && r != -EEXIST)
+			goto err;
+		r = 0;
+	}
+
+err:
+	kfree(dirname);
+err_dirname:
+	kfree(buff);
+err_buff:
+	return r;
+}
+
+static ghost_t *__create_file_ghost(int access, struct file *file, int from_fd)
+{
+	ghost_t *ghost;
+	struct file_ghost_data *ghost_data;
+	int r;
+
+	/* A file ghost can only be used in uni-directional mode */
+	BUG_ON(!(!(access & GHOST_READ) ^ !(access & GHOST_WRITE)));
+
+	ghost = create_ghost(GHOST_FILE, access);
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		goto err_file;
+	}
+
+	ghost_data = kmalloc(sizeof(struct file_ghost_data), GFP_KERNEL);
+	if (!ghost_data) {
+		r = -ENOMEM;
+		goto err_ghost;
+	}
+
+	ghost_data->file = file;
+	ghost_data->from_fd = from_fd;
+
+	ghost->data = ghost_data;
+	ghost->ops = &ghost_file_ops;
+
+	return ghost;
+
+err_ghost:
+	free_ghost(ghost);
+
+err_file:
+	return ERR_PTR(r);
+}
+
+/** Create a new file ghost.
+ *  @author Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param  access   Ghost access (READ/WRITE)
+ *  @param  file     File to read/write data to/from.
+ *
+ *  @return        ghost_t if everything ok
+ *                 ERR_PTR otherwise.
+ */
+ghost_t *create_file_ghost(int access,
+			   long app_id,
+			   unsigned int chkpt_sn,
+			   const char *format,
+			   ...)
+{
+	struct file *file;
+	va_list args;
+	char *filename;
+	struct prev_root prev_root;
+
+	ghost_t *ghost;
+	int r;
+
+	chroot_to_physical_root(&prev_root);
+
+	/* Create directory if not exist */
+	if (access & GHOST_WRITE) {
+		r = mkdir_chkpt_path(app_id, chkpt_sn);
+		if (r)
+			goto err;
+	}
+
+	/* Create a ghost to host the checkpoint */
+	va_start(args, format);
+	filename = __get_chkpt_filebase(app_id, chkpt_sn, format, args);
+	va_end(args);
+
+	if (IS_ERR(filename)) {
+		r = PTR_ERR(filename);
+		goto err;
+	}
+
+	if (access & GHOST_WRITE)/* fail if already exists */
+		file = filp_open(filename, O_CREAT|O_EXCL|O_WRONLY, S_IRUSR|S_IWUSR);
+	else
+		file = filp_open(filename, O_RDONLY, S_IRWXU);
+
+	kfree(filename);
+
+	if (IS_ERR(file)) {
+		r = PTR_ERR(file);
+		goto err;
+	}
+
+	/* Create a ghost to host the checkoint */
+	ghost = __create_file_ghost(access, file, 0);
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		goto err_file;
+	}
+
+out:
+	chroot_to_prev_root(&prev_root);
+
+	return ghost;
+
+err_file:
+	filp_close(file, current->files);
+err:
+	ghost = ERR_PTR(r);
+	goto out;
+}
+
+void unlink_file_ghost(ghost_t *ghost)
+{
+	struct file_ghost_data *ghost_data = ghost->data;
+	struct dentry *dentry = ghost_data->file->f_dentry;
+	struct inode *dir = dentry->d_parent->d_inode;
+
+	BUG_ON(ghost_data->from_fd);
+
+	mutex_lock_nested(&dir->i_mutex, I_MUTEX_PARENT);
+	vfs_unlink(dir, dentry);
+	mutex_unlock(&dir->i_mutex);
+}
+
+/** Create a new file ghost.
+ *  @author Matthieu Fertré
+ *
+ *  @param  access   Ghost access (READ/WRITE)
+ *  @param  file     File descriptor to read/write data to/from.
+ *
+ *  @return        ghost_t if everything ok
+ *                 ERR_PTR otherwise.
+ */
+ghost_t *create_file_ghost_from_fd(int access, unsigned int fd)
+{
+	struct file *file;
+	ghost_t *ghost;
+	int r;
+
+	file = fget(fd);
+	if (!file) {
+		r = -EBADF;
+		goto err;
+	}
+
+	/* check the access right */
+	if ((access & GHOST_WRITE && !(file->f_mode & FMODE_WRITE))
+	    || (access & GHOST_READ && !(file->f_mode & FMODE_READ))) {
+		r = -EACCES;
+		goto err_file;
+	}
+
+	/* Create a ghost to host the checkoint */
+	ghost = __create_file_ghost(access, file, 1);
+	if (IS_ERR(ghost)) {
+		r = PTR_ERR(ghost);
+		goto err_file;
+	}
+
+	return ghost;
+
+err_file:
+	fput(file);
+err:
+	return ERR_PTR(r);
+}
+
+loff_t get_file_ghost_pos(ghost_t *ghost)
+{
+	struct file *file;
+
+	file = ((struct file_ghost_data *)ghost->data)->file;
+
+	return file->f_pos;
+}
+
+void set_file_ghost_pos(ghost_t *ghost, loff_t file_pos)
+{
+	struct file *file;
+
+	file = ((struct file_ghost_data *)ghost->data)->file;
+	file->f_pos = file_pos;
+}
diff -ruN linux-2.6.29/kerrighed/ghost/ghost_api.c android_cluster/linux-2.6.29/kerrighed/ghost/ghost_api.c
--- linux-2.6.29/kerrighed/ghost/ghost_api.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/ghost/ghost_api.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,86 @@
+/** Ghost interface.
+ *  @file ghost_api.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ */
+
+#include <linux/slab.h>
+#include <kerrighed/ghost.h>
+
+static struct kmem_cache *ghost_cachep;
+
+/** Create a new ghost struct. */
+ghost_t *create_ghost(ghost_type_t type, int access)
+{
+	ghost_t *ghost ;
+
+	ghost = kmem_cache_alloc(ghost_cachep, GFP_KERNEL);
+	if (!ghost)
+		goto outofmemory ;
+
+	ghost->type = type ;
+	ghost->size = 0 ;
+	ghost->ops = NULL ;
+	ghost->data = NULL ;
+	ghost->access = access ;
+
+	return ghost ;
+
+outofmemory:
+	return ERR_PTR(-ENOMEM);
+}
+
+/** Free ghost data structures. */
+void free_ghost(ghost_t *ghost)
+{
+	if (ghost->data)
+		kfree (ghost->data);
+
+	kmem_cache_free(ghost_cachep, ghost);
+}
+
+int ghost_printf(ghost_t *ghost, char *format, ...)
+{
+	va_list args;
+	char *buffer;
+	int r, len;
+
+	va_start(args, format);
+	buffer = kvasprintf(GFP_KERNEL, format, args);
+	va_end(args);
+
+	if (!buffer)
+		return -ENOMEM;
+
+	len = strlen(buffer);
+
+	r = ghost_write(ghost, buffer, len);
+
+	kfree(buffer);
+	return r;
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              INITIALIZATION                               */
+/*                                                                           */
+/*****************************************************************************/
+
+int init_ghost(void)
+{
+	unsigned long cache_flags = SLAB_PANIC;
+
+#ifdef CONFIG_DEBUG_SLAB
+	cache_flags |= SLAB_POISON;
+#endif
+	ghost_cachep = kmem_cache_create("ghost",
+					 sizeof(ghost_t),
+					 0, cache_flags,
+					 NULL);
+
+	return 0;
+}
+
+void cleanup_ghost(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/ghost/Makefile android_cluster/linux-2.6.29/kerrighed/ghost/Makefile
--- linux-2.6.29/kerrighed/ghost/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/ghost/Makefile	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,3 @@
+obj-y += ghost_api.o file_ghost.o network_ghost.o
+
+EXTRA_CFLAGS = -Ikerrighed -Wall -Werror
diff -ruN linux-2.6.29/kerrighed/ghost/network_ghost.c android_cluster/linux-2.6.29/kerrighed/ghost/network_ghost.c
--- linux-2.6.29/kerrighed/ghost/network_ghost.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/ghost/network_ghost.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,101 @@
+/** Network Ghost interface.
+ *  @file network_ghost.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2007-2008, Louis Rilling - Kerlabs.
+ */
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/network_ghost.h>
+
+/** Read data from a network ghost.
+ *  @author Renaud Lottiaux, Geoffroy Vallée
+ *
+ *  @param  ghost   Ghost to read data from.
+ *  @param  buff    Buffer to store data.
+ *  @param  length  Size of data to read.
+ *
+ *  @return        0 if everything ok
+ *                 Negative value otherwise.
+ */
+int network_ghost_read(struct ghost *ghost, void *buff, size_t length)
+{
+	struct rpc_desc *desc = ghost->data;
+	int retval;
+
+	retval = rpc_unpack(desc, 0, buff, length);
+	if (retval == RPC_EPIPE)
+		retval = -EPIPE;
+	BUG_ON(retval > 0);
+
+	return retval;
+}
+
+/** Write data to a network ghost.
+ *  @author Renaud Lottiaux, Geoffroy Vallée
+ *
+ *  @param  ghost   Ghost to write data to.
+ *  @param  buff    Buffer to write in the ghost.
+ *  @param  length  Size of data to write.
+ *
+ *  @return        0 if everything ok
+ *                 Negative value otherwise.
+ */
+int network_ghost_write(struct ghost *ghost, const void *buff, size_t length)
+{
+	struct rpc_desc *desc = ghost->data;
+	int retval;
+
+	retval = rpc_pack(desc, 0, buff, length);
+
+	return retval;
+}
+
+/** Close a network ghost.
+ *  @author Matthieu Fertré
+ *
+ *  @param  ghost   Ghost to close.
+ *
+ *  @return        0 if everything ok
+ *                 Negative value otherwise.
+ */
+int network_ghost_close(struct ghost *ghost)
+{
+	ghost->data = NULL;
+	free_ghost(ghost);
+	return 0;
+}
+
+/** Netwotk ghost operations
+ */
+struct ghost_operations ghost_network_ops = {
+	.read  = network_ghost_read,
+	.write = network_ghost_write,
+	.close = network_ghost_close
+};
+
+/** Create a network ghost.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  access Ghost access (READ/WRITE)
+ *  @param  desc   RPC descriptor to send/receive on.
+ *
+ *  @return        0 if everything ok
+ *                 Negative value otherwise.
+ */
+ghost_t * create_network_ghost(int access, struct rpc_desc *desc)
+{
+	struct ghost *ghost;
+
+	/* A network ghost can be used in bi-directional mode */
+	BUG_ON(!access);
+
+	ghost = create_ghost(GHOST_NETWORK, access);
+	if (IS_ERR(ghost))
+		return ghost;
+
+	ghost->data = desc;
+	ghost->ops = &ghost_network_ops;
+
+	return ghost;
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/add.c android_cluster/linux-2.6.29/kerrighed/hotplug/add.c
--- linux-2.6.29/kerrighed/hotplug/add.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/add.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,151 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+
+#define MODULE_NAME "Hotplug"
+
+#include <linux/spinlock.h>
+#include <linux/sched.h>
+#include <linux/nsproxy.h>
+#include <linux/hashtable.h>
+#include <linux/uaccess.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/namespace.h>
+#include <kerrighed/krgnodemask.h>
+
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/krg_syscalls.h>
+#include <kerrighed/krg_services.h>
+
+#include "hotplug_internal.h"
+
+int __nodes_add(struct hotplug_context *ctx)
+{
+	hotplug_add_notify(ctx, HOTPLUG_NOTIFY_ADD);
+	return 0;
+}
+
+static void handle_node_add(struct rpc_desc *rpc_desc, void *data, size_t size)
+{
+	struct hotplug_context *ctx;
+	struct krg_namespace *ns = find_get_krg_ns();
+	char *page;
+	int ret;
+
+	BUG_ON(!ns);
+	ctx = hotplug_ctx_alloc(ns);
+	put_krg_ns(ns);
+	if (!ctx) {
+		printk("kerrighed: [ADD] Failed to add nodes!\n");
+		return;
+	}
+	ctx->node_set = *(struct hotplug_node_set *)data;
+
+	__nodes_add(ctx);
+
+	hotplug_ctx_put(ctx);
+
+	page = (char *)__get_free_page(GFP_KERNEL);
+	if (page) {
+		ret = krgnodelist_scnprintf(page, PAGE_SIZE, krgnode_online_map);
+		BUG_ON(ret >= PAGE_SIZE);
+		printk("Kerrighed is running on %d nodes: %s\n",
+		       num_online_krgnodes(), page);
+		free_page((unsigned long)page);
+	} else {
+		printk("Kerrighed is running on %d nodes\n", num_online_krgnodes());
+	}
+}
+
+static int do_nodes_add(struct hotplug_context *ctx)
+{
+	char *page;
+	kerrighed_node_t node;
+	int ret;
+
+	page = (char *)__get_free_page(GFP_KERNEL);
+	if (!page)
+		return -ENOMEM;
+
+	ret = krgnodelist_scnprintf(page, PAGE_SIZE, ctx->node_set.v);
+	BUG_ON(ret >= PAGE_SIZE);
+	printk("kerrighed: [ADD] Adding nodes %s ...\n", page);
+
+	free_page((unsigned long)page);
+
+	/*
+	 * Send request to all new members
+	 * Current limitation: only not-started nodes can be added to a
+	 * running cluster (ie: a node can't move from a subcluster to another one)
+	 */
+	ret = do_cluster_start(ctx);
+	if (ret) {
+		printk(KERN_ERR "kerrighed: [ADD] Adding nodes failed! err=%d\n",
+		       ret);
+		return ret;
+	}
+
+	/* Send request to all members of the current cluster */
+	for_each_online_krgnode(node)
+		rpc_async(NODE_ADD, node, &ctx->node_set, sizeof(ctx->node_set));
+
+	printk("kerrighed: [ADD] Adding nodes succeeded.\n");
+
+	return ret;
+}
+
+static int nodes_add(void __user *arg)
+{
+	struct __hotplug_node_set __node_set;
+	struct hotplug_context *ctx;
+	int err;
+
+	if (copy_from_user(&__node_set, arg, sizeof(struct __hotplug_node_set)))
+		return -EFAULT;
+
+	ctx = hotplug_ctx_alloc(current->nsproxy->krg_ns);
+	if (!ctx)
+		return -ENOMEM;
+
+	ctx->node_set.subclusterid = __node_set.subclusterid;
+	err = krgnodemask_copy_from_user(&ctx->node_set.v, &__node_set.v);
+	if (err)
+		goto out;
+
+	err = -EPERM;
+	if (ctx->node_set.subclusterid != kerrighed_subsession_id)
+		goto out;
+
+	if (!krgnode_online(kerrighed_node_id))
+		goto out;
+
+	err = -ENONET;
+	if (!krgnodes_subset(ctx->node_set.v, krgnode_present_map))
+		goto out;
+
+	err = -EPERM;
+	if (krgnodes_intersects(ctx->node_set.v, krgnode_online_map))
+		goto out;
+
+	err = do_nodes_add(ctx);
+
+out:
+	hotplug_ctx_put(ctx);
+
+	return err;
+}
+
+int hotplug_add_init(void)
+{
+	rpc_register_void(NODE_ADD, handle_node_add, 0);
+
+	register_proc_service(KSYS_HOTPLUG_ADD, nodes_add);
+	return 0;
+}
+
+void hotplug_add_cleanup(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/cluster.c android_cluster/linux-2.6.29/kerrighed/hotplug/cluster.c
--- linux-2.6.29/kerrighed/hotplug/cluster.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/cluster.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,862 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+
+#define MODULE_NAME "Hotplug"
+
+#include <linux/sched.h>
+#include <linux/rwsem.h>
+#include <linux/mutex.h>
+#include <linux/completion.h>
+#include <linux/string.h>
+#include <linux/capability.h>
+#include <linux/limits.h>
+#include <linux/kthread.h>
+#include <linux/syscalls.h>
+#include <linux/sysfs.h>
+#include <linux/kobject.h>
+#include <linux/ipc.h>
+#include <asm/uaccess.h>
+#include <asm/ioctl.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/krgnodemask.h>
+
+#include <kerrighed/krgflags.h>
+
+#include <kerrighed/krg_syscalls.h>
+#include <kerrighed/krg_services.h>
+#include <kerrighed/workqueue.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/namespace.h>
+#include <net/krgrpc/rpc.h>
+#ifdef CONFIG_KRG_KDDM
+#include <kddm/kddm.h>
+#endif
+#ifdef CONFIG_KRG_PROC
+#include <kerrighed/task.h>
+#include <kerrighed/pid.h>
+#endif
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/signal.h>
+#include <kerrighed/children.h>
+#endif
+#ifdef CONFIG_KRG_SCHED
+#include <kerrighed/scheduler/info.h>
+#endif
+
+#include "hotplug_internal.h"
+
+#define ADVERTISE_PERIOD (2*HZ)
+#define UNIVERSE_PERIOD (60*HZ)
+
+enum {
+	CLUSTER_UNDEF,
+	CLUSTER_DEF,
+};
+
+static char clusters_status[KERRIGHED_MAX_CLUSTERS];
+
+static struct hotplug_context *cluster_start_ctx;
+static struct cluster_start_msg {
+	struct hotplug_node_set node_set;
+	unsigned long seq_id;
+} cluster_start_msg;
+static DEFINE_SPINLOCK(cluster_start_lock);
+static DEFINE_MUTEX(cluster_start_mutex);
+static DECLARE_COMPLETION(cluster_started);
+
+#ifdef CONFIG_KRG_IPC
+#define CLUSTER_INIT_OPT_CLONE_FLAGS_IPC CLONE_NEWIPC
+#else
+#define CLUSTER_INIT_OPT_CLONE_FLAGS_IPC 0
+#endif
+#ifdef CONFIG_KRG_PROC
+#define CLUSTER_INIT_OPT_CLONE_FLAGS_PID CLONE_NEWPID
+#else
+#define CLUSTER_INIT_OPT_CLONE_FLAGS_PID 0
+#endif
+static unsigned long cluster_init_opt_clone_flags =
+	CLUSTER_INIT_OPT_CLONE_FLAGS_IPC|CLUSTER_INIT_OPT_CLONE_FLAGS_PID;
+static DEFINE_SPINLOCK(cluster_init_opt_clone_flags_lock);
+
+static char cluster_init_helper_path[PATH_MAX];
+static char *cluster_init_helper_argv[] = {
+	cluster_init_helper_path,
+	NULL
+};
+static char *cluster_init_helper_envp[] = {
+	"HOME=/",
+	"PATH=/sbin:/bin:/usr/sbin:/usr/bin",
+	NULL
+};
+static struct cred *cluster_init_helper_cred;
+static struct krg_namespace *cluster_init_helper_ns;
+static struct completion cluster_init_helper_ready;
+
+static struct completion krg_container_continue;
+static struct completion krg_container_done;
+
+static ssize_t isolate_uts_show(struct kobject *obj,
+				struct kobj_attribute *attr,
+				char *page)
+{
+	int isolate = !!(cluster_init_opt_clone_flags & CLONE_NEWUTS);
+	return sprintf(page, "%d\n", isolate);
+}
+
+static ssize_t isolate_uts_store(struct kobject *obj,
+				 struct kobj_attribute *attr,
+				 const char *page, size_t count)
+{
+	unsigned long val = simple_strtoul(page, NULL, 0);
+
+	spin_lock(&cluster_init_opt_clone_flags_lock);
+	if (val)
+		cluster_init_opt_clone_flags |= CLONE_NEWUTS;
+	else
+		cluster_init_opt_clone_flags &= ~CLONE_NEWUTS;
+	spin_unlock(&cluster_init_opt_clone_flags_lock);
+
+	return count;
+}
+
+static struct kobj_attribute isolate_uts_attr =
+	__ATTR(isolate_uts, 0644, isolate_uts_show, isolate_uts_store);
+
+static ssize_t isolate_ipc_show(struct kobject *obj,
+				struct kobj_attribute *attr,
+				char *page)
+{
+	int isolate = !!(cluster_init_opt_clone_flags & CLONE_NEWIPC);
+	return sprintf(page, "%d\n", isolate);
+}
+
+#ifdef CONFIG_KRG_IPC
+static struct kobj_attribute isolate_ipc_attr =
+	__ATTR(isolate_ipc, 0444, isolate_ipc_show, NULL);
+#else
+static ssize_t isolate_ipc_store(struct kobject *obj,
+				 struct kobj_attribute *attr,
+				 const char *page, size_t count)
+{
+	unsigned long val = simple_strtoul(page, NULL, 0);
+
+	spin_lock(&cluster_init_opt_clone_flags_lock);
+	if (val)
+		cluster_init_opt_clone_flags |= CLONE_NEWIPC;
+	else
+		cluster_init_opt_clone_flags &= ~CLONE_NEWIPC;
+	spin_unlock(&cluster_init_opt_clone_flags_lock);
+
+	return count;
+}
+
+static struct kobj_attribute isolate_ipc_attr =
+	__ATTR(isolate_ipc, 0644, isolate_ipc_show, isolate_ipc_store);
+#endif /* !CONFIG_KRG_IPC */
+
+static ssize_t isolate_mnt_show(struct kobject *obj,
+				struct kobj_attribute *attr,
+				char *page)
+{
+	int isolate = !!(cluster_init_opt_clone_flags & CLONE_NEWNS);
+	return sprintf(page, "%d\n", isolate);
+}
+
+static ssize_t isolate_mnt_store(struct kobject *obj,
+				 struct kobj_attribute *attr,
+				 const char *page, size_t count)
+{
+	unsigned long val = simple_strtoul(page, NULL, 0);
+
+	spin_lock(&cluster_init_opt_clone_flags_lock);
+	if (val)
+		cluster_init_opt_clone_flags |= CLONE_NEWNS;
+	else
+		cluster_init_opt_clone_flags &= ~CLONE_NEWNS;
+	spin_unlock(&cluster_init_opt_clone_flags_lock);
+
+	return count;
+}
+
+static struct kobj_attribute isolate_mnt_attr =
+	__ATTR(isolate_mnt, 0644, isolate_mnt_show, isolate_mnt_store);
+
+static ssize_t isolate_pid_show(struct kobject *obj,
+				struct kobj_attribute *attr,
+				char *page)
+{
+	int isolate = !!(cluster_init_opt_clone_flags & CLONE_NEWPID);
+	return sprintf(page, "%d\n", isolate);
+}
+
+#ifdef CONFIG_KRG_PROC
+static struct kobj_attribute isolate_pid_attr =
+	__ATTR(isolate_pid, 0444, isolate_pid_show, NULL);
+#else
+static ssize_t isolate_pid_store(struct kobject *obj,
+				 struct kobj_attribute *attr,
+				 const char *page, size_t count)
+{
+	unsigned long val = simple_strtoul(page, NULL, 0);
+
+	spin_lock(&cluster_init_opt_clone_flags_lock);
+	if (val)
+		cluster_init_opt_clone_flags |= CLONE_NEWPID;
+	else
+		cluster_init_opt_clone_flags &= ~CLONE_NEWPID;
+	spin_unlock(&cluster_init_opt_clone_flags_lock);
+
+	return count;
+}
+
+static struct kobj_attribute isolate_pid_attr =
+	__ATTR(isolate_pid, 0644, isolate_pid_show, isolate_pid_store);
+#endif /* !CONFIG_KRG_PROC */
+
+static ssize_t isolate_net_show(struct kobject *obj,
+				struct kobj_attribute *attr,
+				char *page)
+{
+	int isolate = !!(cluster_init_opt_clone_flags & CLONE_NEWNET);
+	return sprintf(page, "%d\n", isolate);
+}
+
+static ssize_t isolate_net_store(struct kobject *obj,
+				 struct kobj_attribute *attr,
+				 const char *page, size_t count)
+{
+	unsigned long val = simple_strtoul(page, NULL, 0);
+
+	spin_lock(&cluster_init_opt_clone_flags_lock);
+	if (val)
+		cluster_init_opt_clone_flags |= CLONE_NEWNET;
+	else
+		cluster_init_opt_clone_flags &= ~CLONE_NEWNET;
+	spin_unlock(&cluster_init_opt_clone_flags_lock);
+
+	return count;
+}
+
+static struct kobj_attribute isolate_net_attr =
+	__ATTR(isolate_net, 0644, isolate_net_show, isolate_net_store);
+
+static ssize_t isolate_user_show(struct kobject *obj,
+				 struct kobj_attribute *attr,
+				 char *page)
+{
+	int isolate = !!(cluster_init_opt_clone_flags & CLONE_NEWUSER);
+	return sprintf(page, "%d\n", isolate);
+}
+
+static ssize_t isolate_user_store(struct kobject *obj,
+				  struct kobj_attribute *attr,
+				  const char *page, size_t count)
+{
+	unsigned long val = simple_strtoul(page, NULL, 0);
+
+	spin_lock(&cluster_init_opt_clone_flags_lock);
+	if (val)
+		cluster_init_opt_clone_flags |= CLONE_NEWUSER;
+	else
+		cluster_init_opt_clone_flags &= ~CLONE_NEWUSER;
+	spin_unlock(&cluster_init_opt_clone_flags_lock);
+
+	return count;
+}
+
+static struct kobj_attribute isolate_user_attr =
+	__ATTR(isolate_user, 0644, isolate_user_show, isolate_user_store);
+
+static ssize_t cluster_init_helper_show(struct kobject *obj,
+					struct kobj_attribute *attr,
+					char *page)
+{
+	return sprintf(page, "%s\n", cluster_init_helper_path);
+}
+
+static ssize_t cluster_init_helper_store(struct kobject *obj,
+					 struct kobj_attribute *attr,
+					 const char *page, size_t count)
+{
+	if (count > sizeof(cluster_init_helper_path)
+	    || (count == sizeof(cluster_init_helper_path)
+		&& page[count - 1] != '\0'))
+		return -ENAMETOOLONG;
+
+	mutex_lock(&cluster_start_mutex);
+	strcpy(cluster_init_helper_path, page);
+	mutex_unlock(&cluster_start_mutex);
+
+	return count;
+}
+
+static struct kobj_attribute cluster_init_helper_attr =
+	__ATTR(cluster_init_helper, 0644,
+	       cluster_init_helper_show, cluster_init_helper_store);
+
+static struct attribute *attrs[] = {
+	&isolate_uts_attr.attr,
+	&isolate_ipc_attr.attr,
+	&isolate_mnt_attr.attr,
+	&isolate_pid_attr.attr,
+	&isolate_net_attr.attr,
+	&isolate_user_attr.attr,
+	&cluster_init_helper_attr.attr,
+	NULL,
+};
+
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+
+static void krg_container_abort(int err)
+{
+	put_krg_ns(cluster_init_helper_ns);
+	cluster_init_helper_ns = ERR_PTR(err);
+	complete(&cluster_init_helper_ready);
+}
+
+void krg_ns_root_exit(struct task_struct *task)
+{
+	if (cluster_init_helper_ns
+	    && task->nsproxy->krg_ns == cluster_init_helper_ns)
+		krg_container_abort(-EAGAIN);
+}
+
+/* ns->root_task must be blocked and alive to get a reliable result */
+static bool krg_container_may_conflict(struct krg_namespace *ns)
+{
+	struct task_struct *root_task = ns->root_task;
+	struct task_struct *g, *t;
+#ifndef CONFIG_KRG_PROC
+	struct nsproxy *nsp;
+#endif
+	bool conflict = false;
+
+	/*
+	 * Check that userspace did not leak tasks in the Kerrighed container
+	 * With !KRG_PROC this does not check zombies, but they won't use any
+	 * conflicting resource.
+	 */
+	rcu_read_lock();
+	read_lock(&tasklist_lock);
+	do_each_thread(g, t) {
+		if (t == root_task)
+			continue;
+
+#ifdef CONFIG_KRG_PROC
+		if (task_active_pid_ns(t)->krg_ns_root == ns->root_nsproxy.pid_ns)
+#else
+		nsp = task_nsproxy(t);
+		if (nsp && nsp->krg_ns == ns)
+#endif
+		{
+			conflict = true;
+			break;
+		}
+	} while_each_thread(g, t);
+	read_unlock(&tasklist_lock);
+	rcu_read_unlock();
+	if (conflict)
+		return conflict;
+
+#ifdef CONFIG_KRG_IPC
+	/*
+	 * Check that userspace did not leak IPCs in the Kerrighed
+	 * container
+	 */
+	if (root_task->nsproxy->ipc_ns != ns->root_nsproxy.ipc_ns
+	    || ipc_used(ns->root_nsproxy.ipc_ns))
+		conflict = true;
+#endif
+
+	return conflict;
+}
+
+static int krg_container_cleanup(struct krg_namespace *ns)
+{
+#ifdef CONFIG_KRG_EPM
+	pidmap_map_cleanup(ns);
+#endif
+#ifdef CONFIG_KRG_IPC
+	cleanup_ipc_objects ();
+#endif
+
+	return 0;
+}
+
+static void krg_container_run(void)
+{
+	complete(&cluster_init_helper_ready);
+
+	wait_for_completion(&krg_container_continue);
+	complete(&krg_container_done);
+}
+
+static int krg_container_init(void *arg)
+{
+	struct krg_namespace *ns;
+	int err;
+
+	/* Unblock all signals */
+	spin_lock_irq(&current->sighand->siglock);
+	flush_signal_handlers(current, 1);
+	sigemptyset(&current->blocked);
+	recalc_sigpending();
+	spin_unlock_irq(&current->sighand->siglock);
+
+	/* Install the credentials */
+	commit_creds(cluster_init_helper_cred);
+	cluster_init_helper_cred = NULL;
+
+	/* We can run anywhere, unlike our parent (a krgrpc) */
+	set_cpus_allowed_ptr(current, cpu_all_mask);
+
+	/*
+	 * Our parent is a krgrpc, which runs with elevated scheduling priority.
+	 * Avoid propagating that into the userspace child.
+	 */
+	set_user_nice(current, 0);
+
+	BUG_ON(cluster_init_helper_ns);
+	ns = current->nsproxy->krg_ns;
+	if (!ns) {
+		cluster_init_helper_ns = ERR_PTR(-EPERM);
+		complete(&cluster_init_helper_ready);
+		return 0;
+	}
+	get_krg_ns(ns);
+	cluster_init_helper_ns = ns;
+
+	err = kernel_execve(cluster_init_helper_path,
+			    cluster_init_helper_argv,
+			    cluster_init_helper_envp);
+	BUG_ON(!err);
+	printk(KERN_ERR
+	       "kerrighed: Could not execute container init '%s': err=%d\n",
+	       cluster_init_helper_path, err);
+
+	krg_container_abort(err);
+
+	return 0;
+}
+
+static int __create_krg_container(void *arg)
+{
+	unsigned long clone_flags;
+	int ret;
+
+	ret = krg_set_cluster_creator((void *)1);
+	if (ret)
+		goto err;
+	clone_flags = cluster_init_opt_clone_flags|SIGCHLD;
+	ret = kernel_thread(krg_container_init, NULL, clone_flags);
+	krg_set_cluster_creator(NULL);
+	if (ret < 0)
+		goto err;
+
+	return 0;
+
+err:
+	put_cred(cluster_init_helper_cred);
+	cluster_init_helper_cred = NULL;
+	cluster_init_helper_ns = ERR_PTR(ret);
+	complete(&cluster_init_helper_ready);
+	return ret;
+}
+
+static
+struct krg_namespace *create_krg_container(struct krg_namespace *ns)
+{
+	struct task_struct *t;
+
+	if (ns) {
+		put_krg_ns(ns);
+		return NULL;
+	}
+
+	BUG_ON(cluster_init_helper_ns);
+	init_completion(&cluster_init_helper_ready);
+
+	BUG_ON(cluster_init_helper_cred);
+	cluster_init_helper_cred = prepare_usermodehelper_creds();
+	if (!cluster_init_helper_cred)
+		return NULL;
+
+	t = kthread_run(__create_krg_container, NULL, "krg_init_helper");
+	if (IS_ERR(t)) {
+		put_cred(cluster_init_helper_cred);
+		cluster_init_helper_cred = NULL;
+		return NULL;
+	}
+
+	wait_for_completion(&cluster_init_helper_ready);
+	if (IS_ERR(cluster_init_helper_ns)) {
+		ns = NULL;
+	} else {
+		ns = cluster_init_helper_ns;
+		BUG_ON(!ns);
+	}
+	cluster_init_helper_ns = NULL;
+
+	return ns;
+}
+
+static void handle_cluster_start(struct rpc_desc *desc, void *data, size_t size)
+{
+	struct cluster_start_msg *msg = data;
+	struct hotplug_context *ctx = NULL;
+	int master = rpc_desc_get_client(desc) == kerrighed_node_id;
+	char *page;
+	int ret = 0;
+	int err;
+
+	mutex_lock(&cluster_start_mutex);
+
+	if (master) {
+		err = -EPIPE;
+		spin_lock(&cluster_start_lock);
+		if (cluster_start_ctx
+		    && msg->seq_id == cluster_start_msg.seq_id) {
+			BUG_ON(!krgnodes_equal(msg->node_set.v,
+					       cluster_start_ctx->node_set.v));
+			hotplug_ctx_get(cluster_start_ctx);
+			ctx = cluster_start_ctx;
+			err = 0;
+		}
+		spin_unlock(&cluster_start_lock);
+		if (err)
+			goto cancel;
+	}
+
+	if (kerrighed_subsession_id != -1){
+		printk("WARNING: Rq to add me in a cluster (%d) when I'm already in one (%d)\n",
+		       msg->node_set.subclusterid, kerrighed_subsession_id);
+		goto cancel;
+	}
+
+	if (!master) {
+		struct krg_namespace *ns;
+
+		init_completion(&krg_container_continue);
+		ns = create_krg_container(find_get_krg_ns());
+		if (!ns)
+			goto cancel;
+
+		ctx = hotplug_ctx_alloc(ns);
+		put_krg_ns(ns);
+		if (!ctx)
+			goto cancel;
+		ctx->node_set = msg->node_set;
+	}
+
+	err = rpc_pack_type(desc, ret);
+	if (err)
+		goto cancel;
+	err = rpc_unpack_type(desc, ret);
+	if (err)
+		goto cancel;
+
+	kerrighed_subsession_id = ctx->node_set.subclusterid;
+	__nodes_add(ctx);
+
+	down_write(&kerrighed_init_sem);
+	hooks_start();
+	up_write(&kerrighed_init_sem);
+
+	rpc_enable_all();
+
+	SET_KERRIGHED_CLUSTER_FLAGS(KRGFLAGS_RUNNING);
+	SET_KERRIGHED_NODE_FLAGS(KRGFLAGS_RUNNING);
+	clusters_status[kerrighed_subsession_id] = CLUSTER_DEF;
+
+	page = (char *)__get_free_page(GFP_KERNEL);
+	if (page) {
+		ret = krgnodelist_scnprintf(page, PAGE_SIZE, ctx->node_set.v);
+		BUG_ON(ret >= PAGE_SIZE);
+		printk("Kerrighed is running on %d nodes: %s\n",
+		       krgnodes_weight(ctx->node_set.v), page);
+		free_page((unsigned long)page);
+	} else {
+		printk("Kerrighed is running on %d nodes\n", num_online_krgnodes());
+	}
+	complete_all(&cluster_started);
+
+	if (!master) {
+		init_completion(&krg_container_done);
+		complete(&krg_container_continue);
+		wait_for_completion(&krg_container_done);
+	}
+
+out:
+	mutex_unlock(&cluster_start_mutex);
+	if (ctx)
+		hotplug_ctx_put(ctx);
+	return;
+
+cancel:
+	rpc_cancel(desc);
+	goto out;
+}
+
+static void cluster_start_worker(struct work_struct *work)
+{
+	struct rpc_desc *desc;
+	char *page;
+	kerrighed_node_t node;
+	int ret;
+	int err = -ENOMEM;
+
+	page = (char *)__get_free_page(GFP_KERNEL);
+	if (!page)
+		goto out;
+
+	ret = krgnodelist_scnprintf(page, PAGE_SIZE,
+				    cluster_start_ctx->node_set.v);
+	BUG_ON(ret >= PAGE_SIZE);
+	printk("kerrighed: [ADD] Setting up new nodes %s ...\n", page);
+
+	free_page((unsigned long)page);
+
+	desc = rpc_begin_m(CLUSTER_START, &cluster_start_ctx->node_set.v);
+	if (!desc)
+		goto out;
+	err = rpc_pack_type(desc, cluster_start_msg);
+	if (err)
+		goto end;
+	for_each_krgnode_mask(node, cluster_start_ctx->node_set.v) {
+		err = rpc_unpack_type_from(desc, node, ret);
+		if (err)
+			goto cancel;
+	}
+	ret = 0;
+	err = rpc_pack_type(desc, ret);
+	if (err)
+		goto cancel;
+	/*
+	 * We might wait for a last ack from the nodes, but there would be no
+	 * gain since local cluster start is currently not allowed to fail and
+	 * transactions will be queued until the nodes are ready.
+	 */
+end:
+	rpc_end(desc, 0);
+out:
+	if (err)
+		printk(KERN_ERR "kerrighed: [ADD] Setting up new nodes failed! err=%d\n",
+		       err);
+	else
+		printk("kerrighed: [ADD] Setting up new nodes succeeded.\n");
+	spin_lock(&cluster_start_lock);
+	hotplug_ctx_put(cluster_start_ctx);
+	cluster_start_ctx = NULL;
+	spin_unlock(&cluster_start_lock);
+	return;
+cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -EPIPE;
+	goto end;
+}
+
+static DECLARE_WORK(cluster_start_work, cluster_start_worker);
+
+int do_cluster_start(struct hotplug_context *ctx)
+{
+	int r = -EALREADY;
+
+	spin_lock(&cluster_start_lock);
+	if (!cluster_start_ctx) {
+		r = -EPERM;
+		if (cluster_start_msg.seq_id == ULONG_MAX) {
+			printk(KERN_WARNING "kerrighed: [ADD] "
+					"Max number of add attempts "
+					"reached! You should reboot host.\n");
+		} else {
+			r = 0;
+			hotplug_ctx_get(ctx);
+			cluster_start_ctx = ctx;
+			cluster_start_msg.seq_id++;
+			krgnodes_or(cluster_start_msg.node_set.v,
+					ctx->node_set.v,
+					krgnode_online_map);
+			queue_work(krg_wq, &cluster_start_work);
+		}
+	}
+	spin_unlock(&cluster_start_lock);
+
+	return r;
+}
+
+static void do_cluster_wait_for_start(void)
+{
+	wait_for_completion(&cluster_started);
+}
+
+static int boot_node_ready(struct krg_namespace *ns)
+{
+	struct hotplug_context *ctx;
+	int r;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	ctx = hotplug_ctx_alloc(ns);
+	if (!ctx)
+		return -ENOMEM;
+	ctx->node_set.subclusterid = 0;
+	ctx->node_set.v = krgnodemask_of_node(kerrighed_node_id);
+
+	r = do_cluster_start(ctx);
+	hotplug_ctx_put(ctx);
+
+	if (!r)
+		do_cluster_wait_for_start();
+
+	return r;
+}
+
+static int other_node_ready(struct krg_namespace *ns)
+{
+	BUG_ON(ns != cluster_init_helper_ns);
+
+	if (krg_container_may_conflict(ns))
+		return -EBUSY;
+	if (krg_container_cleanup(ns))
+		return -EBUSY;
+
+	krg_container_run();
+	return 0;
+}
+
+static int node_ready(void __user *arg)
+{
+	struct krg_namespace *ns = current->nsproxy->krg_ns;
+
+	if (!ns)
+		return -EPERM;
+
+	if (!cluster_init_helper_ns)
+		return boot_node_ready(ns);
+	else
+		return other_node_ready(ns);
+}
+
+static int cluster_restart(void *arg)
+{
+	int unused;
+
+	if (!capable(CAP_SYS_BOOT))
+		return -EPERM;
+
+	rpc_async_m(NODE_FAIL, &krgnode_online_map,
+		    &unused, sizeof(unused));
+	
+	return 0;
+}
+
+static int cluster_stop(void *arg)
+{
+	int unused;
+	
+	if (!capable(CAP_SYS_BOOT))
+		return -EPERM;
+
+	rpc_async_m(NODE_FAIL, &krgnode_online_map,
+		    &unused, sizeof(unused));
+	
+	return 0;
+}
+
+static int cluster_status(void __user *arg)
+{
+	int r = -EFAULT;
+	struct hotplug_clusters __user *uclusters = arg;
+	int bcl;
+
+	if (!access_ok(VERIFY_WRITE, uclusters, sizeof(*uclusters)))
+		goto out;
+
+	for (bcl = 0; bcl < KERRIGHED_MAX_CLUSTERS; bcl++)
+		if (__put_user(clusters_status[bcl], &uclusters->clusters[bcl]))
+			goto out;
+	r = 0;
+
+out:
+	return r;
+}
+
+static int cluster_nodes(void __user *arg)
+{
+	int r = -EFAULT;
+	struct hotplug_nodes __user *nodes_arg = arg;
+	char __user *unodes;
+	char state;
+	int bcl;
+
+	if (get_user(unodes, &nodes_arg->nodes))
+		goto out;
+
+	if (!access_ok(VERIFY_WRITE, unodes, KERRIGHED_MAX_NODES))
+		goto out;
+
+	for (bcl = 0; bcl < KERRIGHED_MAX_NODES; bcl++) {
+		if (krgnode_online(bcl))
+			state = HOTPLUG_NODE_ONLINE;
+		else if (krgnode_present(bcl))
+			state = HOTPLUG_NODE_PRESENT;
+		else if (krgnode_possible(bcl))
+			state = HOTPLUG_NODE_POSSIBLE;
+		else
+			state = HOTPLUG_NODE_INVALID;
+		if (__put_user(state, &unodes[bcl]))
+			goto out;
+	}
+	r = 0;
+
+out:
+	return r;
+}
+
+int krgnodemask_copy_from_user(krgnodemask_t *dstp, __krgnodemask_t *srcp)
+{
+	int r;
+
+	r = find_next_bit(srcp->bits, KERRIGHED_HARD_MAX_NODES,
+			  KERRIGHED_MAX_NODES);
+
+	if (r >= KERRIGHED_MAX_NODES && r < KERRIGHED_HARD_MAX_NODES)
+		return -EINVAL;
+
+	bitmap_copy(dstp->bits, srcp->bits, KERRIGHED_MAX_NODES);
+
+	return 0;
+}
+
+int hotplug_cluster_init(void)
+{
+	int bcl;
+
+	if (sysfs_create_group(krghotplugsys, &attr_group))
+		panic("Couldn't initialize /sys/kerrighed/hotplug!\n");
+
+	for (bcl = 0; bcl < KERRIGHED_MAX_CLUSTERS; bcl++) {
+		clusters_status[bcl] = CLUSTER_UNDEF;
+	}
+
+	rpc_register_void(CLUSTER_START, handle_cluster_start, 0);
+
+	register_proc_service(KSYS_HOTPLUG_READY, node_ready);
+	register_proc_service(KSYS_HOTPLUG_SHUTDOWN, cluster_stop);
+	register_proc_service(KSYS_HOTPLUG_RESTART, cluster_restart);
+	register_proc_service(KSYS_HOTPLUG_STATUS, cluster_status);
+	register_proc_service(KSYS_HOTPLUG_NODES, cluster_nodes);
+
+	return 0;
+}
+
+void hotplug_cluster_cleanup(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/failure.c android_cluster/linux-2.6.29/kerrighed/hotplug/failure.c
--- linux-2.6.29/kerrighed/hotplug/failure.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/failure.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,110 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+
+#define MODULE_NAME "Hotplug"
+
+#include <linux/reboot.h>
+#include <linux/fs.h>
+#include <linux/workqueue.h>
+#include <linux/sched.h>
+#include <linux/irqflags.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+#include <asm/uaccess.h>
+
+#include <kerrighed/krg_services.h>
+#include <kerrighed/krg_syscalls.h>
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+
+#include "hotplug_internal.h"
+
+krgnodemask_t failure_vector;
+struct work_struct fail_work;
+struct work_struct recovery_work;
+struct notifier_block *hotplug_failure_notifier_list;
+
+static void recovery_worker(struct work_struct *data)
+{
+	kerrighed_node_t i;
+
+	for_each_krgnode_mask(i, failure_vector){
+		clear_krgnode_online(i);
+		printk("FAILURE OF %d DECIDED\n", i);
+		printk("should ignore messages from this node\n");
+	}
+
+	//knetdev_failure(&failure_vector);
+	//comm_failure(&failure_vector);
+
+#ifdef CONFIG_KRG_KDDM
+	//kddm_failure(&failure_vector);
+#endif
+}
+
+void krg_failure(krgnodemask_t * vector)
+{
+
+	if(__krgnodes_equal(&failure_vector, vector))
+		return;
+	
+	__krgnodes_copy(&failure_vector, vector);
+
+	queue_work(krg_ha_wq, &recovery_work);
+}
+
+static void handle_node_fail(struct rpc_desc *desc, void *data, size_t size)
+{
+	emergency_sync();
+	emergency_remount();
+
+	set_current_state(TASK_INTERRUPTIBLE);
+	schedule_timeout(5 * HZ);
+
+	local_irq_enable();
+	machine_restart(NULL);
+
+	// should never be reached
+	BUG();
+
+}
+
+static int nodes_fail(void __user *arg)
+{
+	struct __hotplug_node_set __node_set;
+	struct hotplug_node_set node_set;
+	int unused;
+	int err;
+	
+	if (copy_from_user(&node_set, arg, sizeof(__node_set)))
+		return -EFAULT;
+
+	node_set.subclusterid = __node_set.subclusterid;
+
+	err = krgnodemask_copy_from_user(&node_set.v, &__node_set.v);
+	if (err)
+		return err;
+	
+	rpc_async_m(NODE_FAIL, &node_set.v,
+		    &unused, sizeof(unused));
+	
+	return 0;
+}
+
+int hotplug_failure_init(void)
+{
+	INIT_WORK(&recovery_work, recovery_worker);
+
+	rpc_register_void(NODE_FAIL, handle_node_fail, 0);
+	
+	register_proc_service(KSYS_HOTPLUG_FAIL, nodes_fail);
+
+	return 0;
+}
+
+void hotplug_failure_cleanup(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/hooks.c android_cluster/linux-2.6.29/kerrighed/hotplug/hooks.c
--- linux-2.6.29/kerrighed/hotplug/hooks.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/hooks.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,64 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+
+#define MODULE_NAME "Hotplug"
+
+#include <linux/semaphore.h>
+#include <linux/sched.h>
+#include <linux/hashtable.h>
+
+#define HOTPLUG_MAX_HOOKS 256
+
+static struct {
+	void (**hook) (void);
+	void *fct;
+} hooks_table[HOTPLUG_MAX_HOOKS];
+
+static int hooks_index;
+static DECLARE_MUTEX (hooks_lock);
+
+void hook_register(void (**hk) (void), void *f)
+{
+
+	BUG_ON(hooks_index >= HOTPLUG_MAX_HOOKS);
+	BUG_ON(hk == NULL);
+
+	down(&hooks_lock);
+
+	hooks_table[hooks_index].hook = hk;
+	hooks_table[hooks_index].fct = f;
+
+	hooks_index++;
+	up(&hooks_lock);
+}
+
+void hooks_start(void)
+{
+	int i;
+
+	down(&hooks_lock);
+	for (i = 0; i < hooks_index; i++) {
+		*(hooks_table[i].hook) = hooks_table[i].fct;
+	}
+}
+
+void hooks_stop(void)
+{
+	int i;
+	
+	for(i = 0; i < hooks_index; i++){
+		*(hooks_table[i].hook) = NULL;
+	}
+	up(&hooks_lock);
+}
+
+int hotplug_hooks_init(void)
+{
+	hooks_index = 0;
+	return 0;
+}
+
+void hotplug_hooks_cleanup(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/hotplug.c android_cluster/linux-2.6.29/kerrighed/hotplug/hotplug.c
--- linux-2.6.29/kerrighed/hotplug/hotplug.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/hotplug.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,63 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+
+#include <linux/workqueue.h>
+#include <linux/slab.h>
+
+#include <kerrighed/hotplug.h>
+#include <kerrighed/namespace.h>
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+
+#include "hotplug_internal.h"
+
+struct workqueue_struct *krg_ha_wq;
+
+struct hotplug_context *hotplug_ctx_alloc(struct krg_namespace *ns)
+{
+	struct hotplug_context *ctx;
+
+	BUG_ON(!ns);
+	ctx = kmalloc(sizeof(*ctx), GFP_KERNEL);
+	if (!ctx)
+		return NULL;
+
+	get_krg_ns(ns);
+	ctx->ns = ns;
+	kref_init(&ctx->kref);
+
+	return ctx;
+}
+
+void hotplug_ctx_release(struct kref *kref)
+{
+	struct hotplug_context *ctx;
+
+	ctx = container_of(kref, struct hotplug_context, kref);
+	put_krg_ns(ctx->ns);
+	kfree(ctx);
+}
+
+int init_hotplug(void)
+{
+	krg_ha_wq = create_workqueue("krgHA");
+	BUG_ON(krg_ha_wq == NULL);
+
+	hotplug_hooks_init();
+
+	hotplug_add_init();
+#ifdef CONFIG_KRG_HOTPLUG_DEL
+	hotplug_remove_init();
+#endif
+	hotplug_failure_init();
+	hotplug_cluster_init();
+	hotplug_namespace_init();
+	hotplug_membership_init();
+
+	return 0;
+};
+
+void cleanup_hotplug(void)
+{
+};
diff -ruN linux-2.6.29/kerrighed/hotplug/hotplug_internal.h android_cluster/linux-2.6.29/kerrighed/hotplug/hotplug_internal.h
--- linux-2.6.29/kerrighed/hotplug/hotplug_internal.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/hotplug_internal.h	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,36 @@
+#ifndef __HOTPLUG_INTERNAL__
+#define __HOTPLUG_INTERNAL__
+
+extern struct kobject *krghotplugsys;
+
+extern struct workqueue_struct *krg_ha_wq;
+
+extern struct work_struct fail_work;
+
+int hooks_start(void);
+void hooks_stop(void);
+
+struct hotplug_context;
+
+int do_cluster_start(struct hotplug_context *ctx);
+int __nodes_add(struct hotplug_context *ctx);
+
+int repair_monitor(void);
+void update_heartbeat(void);
+
+int krgnodemask_copy_from_user(krgnodemask_t *dst, __krgnodemask_t *from);
+
+int krg_set_cluster_creator(void __user *arg);
+
+int heartbeat_init(void);
+int hotplug_add_init(void);
+int hotplug_remove_init(void);
+int hotplug_failure_init(void);
+int hotplug_hooks_init(void);
+int hotplug_cluster_init(void);
+int hotplug_namespace_init(void);
+
+int hotplug_membership_init(void);
+void hotplug_membership_cleanup(void);
+
+#endif
diff -ruN linux-2.6.29/kerrighed/hotplug/hotplug_notifier.c android_cluster/linux-2.6.29/kerrighed/hotplug/hotplug_notifier.c
--- linux-2.6.29/kerrighed/hotplug/hotplug_notifier.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/hotplug_notifier.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,67 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+
+#include <linux/mutex.h>
+#include <linux/notifier.h>
+#include <linux/slab.h>
+
+#include <kerrighed/hotplug.h>
+#include <kerrighed/krgnodemask.h>
+
+static RAW_NOTIFIER_HEAD(hotplug_chain_add);
+static RAW_NOTIFIER_HEAD(hotplug_chain_remove);
+
+static DEFINE_MUTEX(hotplug_mutex);
+
+int register_hotplug_notifier(int (*notifier_call)(struct notifier_block *, hotplug_event_t, void *),
+			      int priority)
+{
+	int err;
+	struct notifier_block *nb;
+
+	/* Insert into the addition chain */
+	nb = kmalloc(sizeof(*nb), GFP_KERNEL);
+	if (!nb)
+		return -ENOMEM;
+	nb->notifier_call = (int (*)(struct notifier_block *, unsigned long, void *))(notifier_call);
+	nb->priority = priority;
+
+	mutex_lock(&hotplug_mutex);
+	err = raw_notifier_chain_register(&hotplug_chain_add, nb);
+	mutex_unlock(&hotplug_mutex);
+
+	if (err)
+		return err;
+	
+	/* Insert into the removal chain */
+	nb = kmalloc(sizeof(*nb), GFP_KERNEL);
+	if (!nb)
+		return -ENOMEM;
+	nb->notifier_call =  (int (*)(struct notifier_block *, unsigned long, void *))(notifier_call);
+	nb->priority = HOTPLUG_PRIO_MAX-priority;
+	
+	mutex_lock(&hotplug_mutex);
+	err = raw_notifier_chain_register(&hotplug_chain_remove, nb);
+	mutex_unlock(&hotplug_mutex);
+
+	return err;
+}
+
+int hotplug_add_notify(struct hotplug_context *ctx, hotplug_event_t event)
+{
+	return raw_notifier_call_chain(&hotplug_chain_add, event, ctx);
+}
+
+int hotplug_remove_notify(struct hotplug_node_set *nodes_set,
+			  hotplug_event_t event)
+{
+	return raw_notifier_call_chain(&hotplug_chain_remove, event,
+				       nodes_set);
+}
+
+int hotplug_failure_notify(struct hotplug_node_set *nodes_set,
+			   hotplug_event_t event)
+{
+	return 0;
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/krginit.c android_cluster/linux-2.6.29/kerrighed/hotplug/krginit.c
--- linux-2.6.29/kerrighed/hotplug/krginit.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/krginit.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,647 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/workqueue.h>
+#include <linux/sysdev.h>
+#include <linux/if.h>
+
+#include <kerrighed/version.h>
+#include <kerrighed/types.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/krgflags.h>
+#include <linux/cluster_barrier.h>
+#include <linux/unique_id.h>
+#include <net/krgrpc/rpc.h>
+#ifdef CONFIG_KRG_PROC
+#include <kerrighed/pid.h>
+#endif
+#ifdef CONFIG_KRG_HOTPLUG
+#include <kerrighed/hotplug.h>
+#endif
+
+void init_node_discovering(void);
+
+/* Node id */
+kerrighed_node_t kerrighed_node_id = -1;
+EXPORT_SYMBOL(kerrighed_node_id);
+
+/* Number of active nodes in the cluster */
+kerrighed_node_t kerrighed_nb_nodes = -1;
+
+/* Min number of node before to start a cluster */
+kerrighed_node_t kerrighed_nb_nodes_min = -1;
+
+/* Session id in order to mix several krg in the same physical network */
+kerrighed_session_t kerrighed_session_id = 0;
+
+/* ID of subcluster in the main one */
+kerrighed_subsession_t kerrighed_subsession_id = -1;
+
+/* Initialisation flags */
+#ifdef CONFIG_KRG_AUTONODEID_ON
+#define IF_AUTONODEID (1<<KRG_INITFLAGS_AUTONODEID)
+#else
+#define IF_AUTONODEID (0)
+#endif
+int kerrighed_init_flags = IF_AUTONODEID;
+
+/* lock around process transformation and hooks install */
+DECLARE_RWSEM(kerrighed_init_sem);
+EXPORT_SYMBOL(kerrighed_init_sem);
+
+int kerrighed_cluster_flags;
+EXPORT_SYMBOL(kerrighed_cluster_flags);
+
+int kerrighed_node_flags;
+EXPORT_SYMBOL(kerrighed_node_flags);
+
+int __krg_panic__ = 0;
+
+struct workqueue_struct *krg_wq;
+struct workqueue_struct *krg_nb_wq;
+
+struct kobject* krgsys;
+struct kobject* krghotplugsys;
+
+#define deffct(p) extern int init_##p(void); extern void cleanup_##p(void)
+
+deffct(tools);
+#ifdef CONFIG_KRG_HOTPLUG
+deffct(hotplug);
+#endif
+#ifdef CONFIG_KRGRPC
+deffct(rpc);
+#endif
+#ifdef CONFIG_KRG_STREAM
+deffct(stream);
+#endif
+deffct(kddm);
+deffct(kermm);
+#ifdef CONFIG_KRG_DVFS
+deffct(dvfs);
+#endif
+#ifdef CONFIG_KRG_IPC
+deffct(keripc);
+#endif
+#ifdef CONFIG_KRG_CAP
+deffct(krg_cap);
+#endif
+#ifdef CONFIG_KRG_PROCFS
+deffct(procfs);
+#endif
+#ifdef CONFIG_KRG_PROC
+deffct(proc);
+#endif
+#ifdef CONFIG_KRG_EPM
+deffct(ghost);
+deffct(epm);
+#endif
+#ifdef CONFIG_KRG_SCHED
+deffct(scheduler);
+#endif
+
+/*
+ * Handle Kernel parameters
+ */
+
+static int __init  parse_autonodeid(char *str) {
+	int v = 0;
+	get_option(&str, &v);
+	if(v)
+		SET_KRG_INIT_FLAGS(KRG_INITFLAGS_AUTONODEID);
+	else
+		CLR_KRG_INIT_FLAGS(KRG_INITFLAGS_AUTONODEID);
+	return 0;
+}
+__setup("autonodeid=",parse_autonodeid);
+
+static int __init  parse_node_id(char *str) {
+	int v;
+	get_option(&str, &v);
+	kerrighed_node_id = v;
+	SET_KRG_INIT_FLAGS(KRG_INITFLAGS_NODEID);
+	return 0;
+}
+__setup("node_id=",parse_node_id);
+
+static int __init  parse_session_id(char *str){
+	int v;
+	get_option(&str, &v);
+	kerrighed_session_id = v;
+	SET_KRG_INIT_FLAGS(KRG_INITFLAGS_SESSIONID);
+	return 0;
+}
+__setup("session_id=",parse_session_id);
+
+static int __init  parse_nb_nodes_min(char *str){
+	int v;
+	get_option(&str, &v);
+	kerrighed_nb_nodes_min = v;
+	return 0;
+}
+__setup("nb_nodes_min=",parse_nb_nodes_min);
+
+/*****************************************************************************/
+/*                                                                           */
+/*                          KERRIGHED INIT FUNCTION                          */
+/*                                                                           */
+/*****************************************************************************/
+
+static inline void check_node_id (int node_id)
+{
+	if ((node_id >= KERRIGHED_MAX_NODES) || (node_id < 0))
+	{
+		printk ("Invalid kerrighed node id %d (Max id = %d)\n",
+			node_id, KERRIGHED_MAX_NODES);
+		BUG();
+	}
+}
+
+static char *read_from_file(char *_filename, int size)
+{
+	int error;
+	struct file *f;
+	char *b, *filename;
+
+	b = kmalloc(size, GFP_ATOMIC);
+	BUG_ON(b==NULL);
+
+	filename = getname(_filename);
+	if (!IS_ERR(filename)) {
+		f = filp_open(filename, O_RDONLY, 0);
+		if (IS_ERR(f)) {
+			printk("error: %ld\n", PTR_ERR(f));
+			goto err_file;
+		}
+
+		error = kernel_read(f, 0, b, size);
+		//printk("read %d bytes\n", error);
+
+		b[error] = 0;
+		//printk(">>>%s<<<\n", b);
+
+		if (f->f_op && f->f_op->flush) {
+			error = f->f_op->flush(f, NULL);
+			if (error)
+				printk("init_ids: Error while closing file %d\n", error);
+		}
+	}
+	return b;
+
+ err_file:
+	kfree(b);
+	return NULL;
+}
+
+/* Remove then CR (if any) */
+static void strip_hostname(char *h)
+{
+	char *i;
+
+	for (i = h; *i; i++) {
+		if (*i == 10) {
+			*i=0;
+			break;
+		}
+	}
+}
+
+static char *get_next_line(char *k)
+{
+	char *i;
+
+	BUG_ON(*k==0);
+
+	for (i = k; *i; i++) {
+		if (*i == 10)
+			return i+1;
+	}
+
+	return NULL;
+}
+
+static void read_kerrighed_nodes(char *_h, char *k)
+{
+	char *ik, *h;
+	int lh;
+
+	if ((_h==NULL) || (k==NULL))
+		return;
+
+	lh = strlen(_h);
+	h = kmalloc(lh+1, GFP_ATOMIC);
+	strncpy(h, _h, lh);
+	h[lh] = ':';
+	h[lh+1] = 0;
+	lh = strlen(h);
+
+	for (ik=k; ik && *ik;) {
+		if (!ISSET_KRG_INIT_FLAGS(KRG_INITFLAGS_SESSIONID)) {
+			if (strncmp("session=", ik, 8) == 0){
+				ik += 8;
+				kerrighed_session_id = simple_strtoul(ik, NULL, 10);
+				SET_KRG_INIT_FLAGS(KRG_INITFLAGS_SESSIONID);
+
+				ik = get_next_line(ik);
+				continue;
+			}
+		}
+
+		if (strncmp("nbmin=", ik, 6) == 0) {
+			ik += 6;
+			kerrighed_nb_nodes_min = simple_strtoul(ik, NULL, 10);
+			ik = get_next_line(ik);
+			continue;
+		}
+
+		if (!ISSET_KRG_INIT_FLAGS(KRG_INITFLAGS_NODEID)) {
+			if (strncmp(h, ik, lh) == 0) {
+				char *end;
+				ik += lh;
+
+				kerrighed_node_id = simple_strtoul(ik, &end, 10);
+				SET_KRG_INIT_FLAGS(KRG_INITFLAGS_NODEID);
+			}
+		}
+
+		ik = get_next_line(ik);
+	}
+}
+
+static void __init init_ids(void)
+{
+	char *hostname, *kerrighed_nodes;
+
+	if (!ISSET_KRG_INIT_FLAGS(KRG_INITFLAGS_NODEID) ||
+	    !ISSET_KRG_INIT_FLAGS(KRG_INITFLAGS_SESSIONID)) {
+		/* first we read the name of the node */
+		hostname = read_from_file("/etc/hostname", 256);
+		if (!hostname) {
+			printk("Can't read /etc/hostname\n");
+			goto out;
+		}
+		strip_hostname(hostname);
+
+		kerrighed_nodes = read_from_file("/etc/kerrighed_nodes", 4096);
+		if (!kerrighed_nodes) {
+			kfree(hostname);
+			printk("Can't read /etc/kerrighed_nodes\n");
+			goto out;
+		}
+		read_kerrighed_nodes(hostname, kerrighed_nodes);
+
+		kfree(kerrighed_nodes);
+		kfree(hostname);
+	}
+
+ out:
+	if (ISSET_KRG_INIT_FLAGS(KRG_INITFLAGS_NODEID)) {
+		check_node_id(kerrighed_node_id);
+#ifdef CONFIG_KRG_HOTPLUG
+		universe[kerrighed_node_id].state = 1;
+		set_krgnode_present(kerrighed_node_id);
+#endif
+	}
+
+	kerrighed_cluster_flags = 0;
+	kerrighed_node_flags = 0;
+	
+	printk("Kerrighed session ID : %d\n", kerrighed_session_id);
+	printk("Kerrighed node ID    : %d\n", kerrighed_node_id);
+	printk("Kerrighed min nodes  : %d\n", kerrighed_nb_nodes_min);
+
+	if (!ISSET_KRG_INIT_FLAGS(KRG_INITFLAGS_NODEID) ||
+	    !ISSET_KRG_INIT_FLAGS(KRG_INITFLAGS_SESSIONID))
+		panic("kerrighed: incomplete session ID / node ID settings!\n");
+
+	return;
+}
+
+int init_kerrighed_communication_system(void)
+{
+	printk("Init Kerrighed low-level framework...\n");
+
+	if (init_tools())
+		goto err_tools;
+
+	kerrighed_nb_nodes = 0;
+
+#ifdef CONFIG_KRGRPC
+	if (init_rpc())
+		goto err_rpc;
+#endif
+
+#ifdef CONFIG_KRG_HOTPLUG
+	if (init_hotplug())
+		goto err_hotplug;
+#endif
+
+	printk("Init Kerrighed low-level framework (nodeid %d) : done\n", kerrighed_node_id);
+
+	return 0;
+
+#ifdef CONFIG_KRG_HOTPLUG
+err_hotplug:
+	cleanup_hotplug();
+#endif
+#ifdef CONFIG_KRGRPC
+err_rpc:
+#endif
+	cleanup_tools();
+err_tools:
+	return -1;
+}
+
+#ifdef CONFIG_KERRIGHED
+int init_kerrighed_upper_layers(void)
+{
+	printk("Init Kerrighed distributed services...\n");
+
+#ifdef CONFIG_KRG_KDDM
+	if (init_kddm())
+		goto err_kddm;
+#endif
+
+#ifdef CONFIG_KRG_EPM
+	if (init_ghost())
+		goto err_ghost;
+#endif
+
+#ifdef CONFIG_KRG_STREAM
+	if (init_stream())
+		goto err_palantir;
+#endif
+
+#ifdef CONFIG_KRG_MM
+	if (init_kermm())
+		goto err_kermm;
+#endif
+
+#ifdef CONFIG_KRG_DVFS
+	if (init_dvfs())
+		goto err_dvfs;
+#endif
+
+#ifdef CONFIG_KRG_IPC
+	if (init_keripc())
+		goto err_keripc;
+#endif
+
+#ifdef CONFIG_KRG_CAP
+	if (init_krg_cap())
+		goto err_krg_cap;
+#endif
+
+#ifdef CONFIG_KRG_PROC
+	if (init_proc())
+		goto err_proc;
+#endif
+
+#ifdef CONFIG_KRG_PROCFS
+	if (init_procfs())
+		goto err_procfs;
+#endif
+
+#ifdef CONFIG_KRG_EPM
+	if (init_epm())
+		goto err_epm;
+#endif
+
+	printk("Init Kerrighed distributed services: done\n");
+
+#ifdef CONFIG_KRG_SCHED
+	if (init_scheduler())
+		goto err_sched;
+#endif
+
+	return 0;
+
+#ifdef CONFIG_KRG_SCHED
+	cleanup_scheduler();
+      err_sched:
+#endif
+#ifdef CONFIG_KRG_EPM
+	cleanup_epm();
+      err_epm:
+#endif
+#ifdef CONFIG_KRG_IPC
+	cleanup_keripc();
+      err_keripc:
+#endif
+#ifdef CONFIG_KRG_DVFS
+	cleanup_dvfs();
+      err_dvfs:
+#endif
+#ifdef CONFIG_KRG_PROCFS
+	cleanup_procfs();
+      err_procfs:
+#endif
+#ifdef CONFIG_KRG_PROC
+	cleanup_proc();
+      err_proc:
+#endif
+#ifdef CONFIG_KRG_CAP
+	cleanup_krg_cap();
+      err_krg_cap:
+#endif
+#ifdef CONFIG_KRG_MM
+	cleanup_kermm();
+      err_kermm:
+#endif
+#ifdef CONFIG_KRG_KDDM
+	cleanup_kddm();
+      err_kddm:
+#endif
+#ifdef CONFIG_KRG_STREAM
+	cleanup_stream();
+      err_palantir:
+#endif
+#ifdef CONFIG_KRG_EPM
+	cleanup_ghost();
+      err_ghost:
+#endif
+#ifdef CONFIG_KRGRPC
+	cleanup_rpc();
+#endif
+	return -1;
+}
+#endif
+
+#if 0
+static ssize_t kerrighed_operation_show(struct kobject *obj, struct kobj_attribute *attr,
+					char *page) {
+        return sprintf(page, "blabla\n");
+}
+
+static ssize_t kerrighed_operation_store(struct kobject *obj, struct kobj_attribute *attr,
+					const char *buf, size_t count) {
+	printk("requested_operation: %s\n", buf);
+        return count;
+}
+
+static struct kobj_attribute operation =
+		__ATTR(operation, 0644,
+			kerrighed_operation_show,
+			kerrighed_operation_store);
+#endif
+
+static ssize_t node_id_show(struct kobject *obj, struct kobj_attribute *attr,
+			    char *page)
+{
+	return sprintf(page, "%d\n", kerrighed_node_id);
+}
+static struct kobj_attribute kobj_attr_node_id =
+		__ATTR_RO(node_id);
+
+static ssize_t session_id_show(struct kobject *obj, struct kobj_attribute *attr,
+			       char *page)
+{
+	return sprintf(page, "%d\n", kerrighed_session_id);
+}
+static struct kobj_attribute kobj_attr_session_id =
+		__ATTR_RO(session_id);
+
+static ssize_t subsession_id_show(struct kobject *obj, struct kobj_attribute *attr,
+				  char *page)
+{
+	return sprintf(page, "%d\n", kerrighed_subsession_id);
+}
+static struct kobj_attribute kobj_attr_subsession_id =
+		__ATTR_RO(subsession_id);
+
+static ssize_t max_nodes_show(struct kobject *obj, struct kobj_attribute *attr,
+			      char *page)
+{
+	return sprintf(page, "%d\n", KERRIGHED_MAX_NODES);
+}
+static struct kobj_attribute kobj_attr_max_nodes =
+		__ATTR_RO(max_nodes);
+
+static ssize_t max_subclusters_show(struct kobject *obj, struct kobj_attribute *attr,
+				    char *page)
+{
+	return sprintf(page, "%d\n", KERRIGHED_MAX_CLUSTERS);
+}
+static struct kobj_attribute kobj_attr_max_subclusters =
+		__ATTR_RO(max_subclusters);
+
+static ssize_t abi_show(struct kobject *obj, struct kobj_attribute *attr,
+			char *page)
+{
+	return sprintf(page, "%s\n", KERRIGHED_ABI);
+}
+static struct kobj_attribute kobj_attr_abi =
+		__ATTR_RO(abi);
+
+static ssize_t net_devices_store(struct kobject *obj,
+				 struct kobj_attribute *attr,
+				 const char *page, size_t count)
+{
+	char buf[IFNAMSIZ + 2];
+	char *name;
+	int err;
+
+	if (sysfs_streq(page, "+ALL") || sysfs_streq(page, "ALL")) {
+		rpc_enable_alldev();
+		return count;
+	} else if (sysfs_streq(page, "-ALL")) {
+		rpc_disable_alldev();
+		return count;
+	}
+
+	name = strncpy(buf, page, IFNAMSIZ + 2);
+	if (buf[IFNAMSIZ + 1])
+		return -EINVAL;
+	name = strstrip(name);
+
+	switch (name[0]) {
+	case '-':
+		err = rpc_disable_dev(name + 1);
+		break;
+	case '+':
+		name++;
+		/* Fallthrough */
+	default:
+		err = rpc_enable_dev(name);
+		break;
+	}
+	if (err)
+		return err;
+
+	return count;
+}
+
+static struct kobj_attribute kobj_attr_net_devices =
+	__ATTR(net_devices, 0200, NULL, net_devices_store);
+
+static struct attribute *attrs[] = {
+	&kobj_attr_node_id.attr,
+	&kobj_attr_session_id.attr,
+	&kobj_attr_subsession_id.attr,
+	&kobj_attr_max_nodes.attr,
+	&kobj_attr_max_subclusters.attr,
+	&kobj_attr_abi.attr,
+	&kobj_attr_net_devices.attr,
+	NULL
+};
+
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+
+static int init_sysfs(void){
+	int r;
+
+	krgsys = kobject_create_and_add("kerrighed", NULL);
+	if(!krgsys)
+		return -1;
+
+	krghotplugsys = kobject_create_and_add("hotplug", krgsys);
+	if(!krghotplugsys)
+		return -1;
+
+	r = sysfs_create_group(krgsys, &attr_group);
+	if(r)
+		kobject_put(krgsys);
+
+	return 0;
+}
+
+void __init kerrighed_init(void){
+	printk("Kerrighed: stage 0\n");
+	init_ids();
+
+	printk("Kerrighed: stage 1\n");
+
+	init_sysfs();
+	krg_wq = create_workqueue("krg");
+	krg_nb_wq = create_workqueue("krgNB");
+	BUG_ON(krg_wq == NULL);
+	BUG_ON(krg_nb_wq == NULL);
+
+	init_unique_ids();
+	init_node_discovering();
+
+	printk("Kerrighed: stage 2\n");
+
+	if (init_kerrighed_communication_system())
+		return;
+
+	init_cluster_barrier();
+
+#ifdef CONFIG_KERRIGHED
+	if (init_kerrighed_upper_layers())
+		return;
+#endif
+
+	SET_KERRIGHED_CLUSTER_FLAGS(KRGFLAGS_LOADED);
+	SET_KERRIGHED_NODE_FLAGS(KRGFLAGS_LOADED);
+
+	printk("Kerrighed... loaded!\n");
+
+	rpc_enable(CLUSTER_START);
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/krgsyms.c android_cluster/linux-2.6.29/kerrighed/hotplug/krgsyms.c
--- linux-2.6.29/kerrighed/hotplug/krgsyms.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/krgsyms.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,97 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+
+#include <kerrighed/krgsyms.h>
+#include <linux/module.h>
+#include <linux/hashtable.h>
+#include <linux/init.h>
+
+/*****************************************************************************/
+/*                                                                           */
+/*                          KERRIGHED KSYM MANAGEMENT                        */
+/*                                                                           */
+/*****************************************************************************/
+
+
+#define KRGSYMS_HTABLE_SIZE 256
+
+static hashtable_t *krgsyms_htable;
+static void* krgsyms_table[KRGSYMS_TABLE_SIZE];
+
+int krgsyms_register(enum krgsyms_val v, void* p)
+{
+	if( (v < 0) || (v >= KRGSYMS_TABLE_SIZE) ){
+		printk("krgsyms_register: Incorrect krgsym value (%d)\n", v);
+		BUG();
+		return -1;
+	};
+
+	if(krgsyms_table[v])
+		printk("krgsyms_register_symbol(%d, %p): value already set in table\n",
+					 v, p);
+
+	if(hashtable_find(krgsyms_htable, (unsigned long)p) != NULL)
+	{
+		printk("krgsyms_register_symbol(%d, %p): value already set in htable\n",
+					 v, p);
+		BUG();
+	}
+
+	hashtable_add(krgsyms_htable, (unsigned long)p, (void*)v);
+	krgsyms_table[v] = p;
+
+	return 0;
+};
+EXPORT_SYMBOL(krgsyms_register);
+
+int krgsyms_unregister(enum krgsyms_val v)
+{
+	void *p;
+
+	if( (v < 0) || (v >= KRGSYMS_TABLE_SIZE) ){
+		printk("krgsyms_unregister: Incorrect krgsym value (%d)\n", v);
+		BUG();
+		return -1;
+	};
+
+	p = krgsyms_table[v];
+	krgsyms_table[v] = NULL;
+	hashtable_remove(krgsyms_htable, (unsigned long)p);
+
+	return 0;
+};
+EXPORT_SYMBOL(krgsyms_unregister);
+
+enum krgsyms_val krgsyms_export(void* p)
+{
+	return (enum krgsyms_val)hashtable_find(krgsyms_htable, (unsigned long)p);
+};
+
+void* krgsyms_import(enum krgsyms_val v)
+{
+	if( (v < 0) || (v >= KRGSYMS_TABLE_SIZE) ){
+		printk("krgsyms_import: Incorrect krgsym value (%d)\n", v);
+		BUG();
+		return NULL;
+	};
+
+	if ((v!=0) && (krgsyms_table[v] == NULL))
+	{
+		printk ("undefined krgsymbol (%d)\n", v);
+		BUG();
+	}
+
+	return krgsyms_table[v];
+};
+
+static __init int init_krgsyms(void)
+{
+	krgsyms_htable = hashtable_new(KRGSYMS_HTABLE_SIZE);
+	if (!krgsyms_htable)
+		panic("Could not setup krgsyms table!\n");
+
+	return 0;
+};
+
+pure_initcall(init_krgsyms);
diff -ruN linux-2.6.29/kerrighed/hotplug/krg_syscalls.c android_cluster/linux-2.6.29/kerrighed/hotplug/krg_syscalls.c
--- linux-2.6.29/kerrighed/hotplug/krg_syscalls.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/krg_syscalls.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,182 @@
+/** Interface to create / remove Kerrighed syscalls.
+ *  @file krg_syscalls.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2009, Kerlabs
+ */
+#include <linux/proc_fs.h>
+#include <linux/fs.h>
+#include <linux/module.h>
+#include <linux/nsproxy.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/hashtable.h>
+
+#include <kerrighed/debug.h>
+
+#include <kerrighed/krg_syscalls.h>
+#include <kerrighed/procfs.h>
+
+#define PROC_HASH_TABLE_SIZE 256
+
+struct proc_dir_entry *proc_services = NULL;
+
+hashtable_t *proc_service_functions;
+
+static int proc_services_ioctl(struct inode *inode, struct file *filp,
+			       unsigned int cmd, unsigned long arg);
+static ssize_t proc_services_read(struct file *, char *, size_t, loff_t *);
+
+static struct file_operations proc_services_files_ops = {
+	.ioctl = proc_services_ioctl,
+	.read = proc_services_read,
+};
+
+/** IO Control for the file /proc/kerrighed/services.
+ *  @author Renaud Lottiaux
+ */
+static int proc_services_ioctl(struct inode *inode, struct file *filp,
+			       unsigned int cmd, unsigned long arg)
+{
+	struct proc_service_entry *service_entry;
+
+	service_entry =
+	    (struct proc_service_entry *)hashtable_find(proc_service_functions,
+							cmd);
+
+	if (service_entry != NULL) {
+		if (service_entry->restricted && !current->nsproxy->krg_ns)
+			return -EPERM;
+		service_entry->count++;
+		return service_entry->fct((void *)arg);
+	}
+#ifdef CONFIG_KRG_DEBUG
+	PANIC("Kerrighed command %d-%d (0x%08x) unknown\n", cmd & 0xE0,
+	      cmd & 0x1F, cmd);
+#endif
+
+	return -EINVAL;
+}
+
+static ssize_t proc_services_read(struct file *f, char *buff,
+				  size_t count, loff_t * off)
+{
+	int index;
+	ssize_t ret;
+
+	printk("proc_service_read: start - count=%ld\n", (long)count);
+
+	ret = 0;
+
+	for (index = 0;
+	     (index < proc_service_functions->hashtable_size) && (count >= 50);
+	     index++) {
+		struct hash_list *ht;
+		struct proc_service_entry *se;
+
+		se = proc_service_functions->table[index].data;
+		if (se != NULL) {
+			printk("%s: %lu\n", se->label, se->count);
+
+			for (ht = proc_service_functions->table[index].next;
+			     (ht != NULL) && (count >= 50); ht = ht->next) {
+				int l;
+				se = ht->data;
+				printk("%s: %lu\n", se->label, se->count);
+				l = sprintf(buff + ret, "%s: %lu\n", se->label,
+					    se->count);
+				count -= l;
+				ret += l;
+			};
+		};
+	};
+
+	printk("proc_service_read: stop - (count=%ld)\n", (long)ret);
+	return ret;
+};
+
+/** Add a service to the /proc/kerrighed/services
+ *  @author Renaud Lottiaux
+ *
+ *  @param cmd   Identifier of the service.
+ *  @param fun   Service function.
+ */
+int __register_proc_service(unsigned int cmd, proc_service_function_t fun,
+			    bool restricted)
+{
+	struct proc_service_entry *service_entry;
+
+	service_entry =
+	    (struct proc_service_entry *)hashtable_find(proc_service_functions,
+							cmd);
+	if (service_entry != NULL && fun != NULL) {
+		PANIC("Kerrighed command %d-%d (0x%08x) already registered\n",
+		      cmd & 0xE0, cmd & 0x1F, cmd);
+		return -1;
+	}
+
+	service_entry = kmalloc(sizeof(struct proc_service_entry), GFP_KERNEL);
+
+	service_entry->fct = fun;
+	sprintf(service_entry->label, "%d-%d (0x%08x)",
+		cmd & 0xE0, cmd & 0x1F, cmd);
+	service_entry->count = 0;
+	service_entry->restricted = restricted;
+
+	hashtable_add(proc_service_functions, cmd, service_entry);
+	return 0;
+}
+
+int register_proc_service(unsigned int cmd, proc_service_function_t fun)
+{
+	return __register_proc_service(cmd, fun, true);
+}
+EXPORT_SYMBOL(register_proc_service);
+
+/** Remove a service from the /proc/kerrighed/services
+ *  @author Renaud Lottiaux
+ *
+ *  @param cmd   Identifier of the service.
+ */
+int unregister_proc_service(unsigned int cmd)
+{
+	if (hashtable_remove(proc_service_functions, cmd) == NULL)
+		return -EINVAL;
+
+	return 0;
+}
+EXPORT_SYMBOL(unregister_proc_service);
+
+/** Initialisation of the /proc/kerrighed/services file.
+ *  @author Renaud Lottiaux
+ */
+int krg_syscalls_init(void)
+{
+	int err = 0;
+
+	printk("Init kerrighed syscall mechanism\n");
+
+	/* Create the /proc/kerrighed/services */
+
+	proc_services = create_proc_entry("services", 0644, proc_kerrighed);
+	if (proc_services == NULL)
+		err = -EMFILE;
+	else {
+		proc_service_functions = hashtable_new(PROC_HASH_TABLE_SIZE);
+		proc_services->proc_fops = &proc_services_files_ops;
+	}
+
+	return err;
+}
+
+/** Destroy of the /proc/kerrighed/services file.
+ *  @author Renaud Lottiaux
+ */
+int krg_syscalls_finalize(void)
+{
+	remove_proc_entry("services", proc_kerrighed);
+
+	hashtable_free(proc_service_functions);
+
+	return 0;
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/krg_tools.c android_cluster/linux-2.6.29/kerrighed/hotplug/krg_tools.c
--- linux-2.6.29/kerrighed/hotplug/krg_tools.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/krg_tools.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,116 @@
+/** Initilize the tool module.
+ *  @file krg_tools.c
+ *
+ *  Copyright (C) 2006-2009, Pascal Gallard, Kerlabs.
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+#include <asm/uaccess.h>
+
+#include <kerrighed/procfs.h>
+#include <kerrighed/krg_syscalls.h>
+#include <kerrighed/krg_services.h>
+
+static int tools_proc_nb_max_nodes(void* arg)
+{
+	int r, v = KERRIGHED_MAX_NODES;
+
+	r = 0;
+	
+	if(copy_to_user((void*)arg, (void*)&v, sizeof(v)))
+		r = -EFAULT;
+
+	return r;
+}
+
+static int tools_proc_nb_max_clusters(void* arg)
+{
+	int r, v = KERRIGHED_MAX_CLUSTERS;
+
+	r = 0;
+
+	if(copy_to_user((void*)arg, (void*)&v, sizeof(v)))
+		r = -EFAULT;
+
+	return r;
+}
+
+static int tools_proc_node_id(void *arg)
+{
+        int node_id = kerrighed_node_id;
+        int r = 0;
+
+        if (copy_to_user((void *)arg, (void *)&node_id, sizeof(int)))
+                r = -EFAULT;
+
+        return r;
+}
+
+static int tools_proc_nodes_count(void *arg)
+{
+        int nb_nodes = num_online_krgnodes();
+        int r = 0;
+
+        if (copy_to_user((void *)arg, (void *)&nb_nodes, sizeof(int)))
+                r = -EFAULT;
+
+        return r;
+}
+
+int init_tools(void)
+{
+	int error;
+
+	if ((error = kerrighed_proc_init()))
+		goto ErrorProc;
+	if ((error = krg_syscalls_init()))
+		goto ErrorSys;
+
+	error = register_proc_service(KSYS_NB_MAX_NODES, tools_proc_nb_max_nodes);
+	if (error != 0) {
+		error = -EINVAL;
+		goto Error;
+	}
+
+	error = register_proc_service(KSYS_NB_MAX_CLUSTERS, tools_proc_nb_max_clusters);
+	if (error != 0) {
+		error = -EINVAL;
+		goto Error;
+	}
+
+	error = register_proc_service(KSYS_GET_NODE_ID, tools_proc_node_id);
+	if (error != 0) {
+		error = -EINVAL;
+		goto Error;
+	}
+
+	error = register_proc_service(KSYS_GET_NODES_COUNT, tools_proc_nodes_count);
+	if (error != 0) {
+		error = -EINVAL;
+		goto Error;
+	}
+	
+	printk("Kerrighed tools - init module\n");
+ Done:
+	return error;
+
+	krg_syscalls_finalize();
+ ErrorSys:
+	kerrighed_proc_finalize();
+ ErrorProc:
+ Error:
+	goto Done;
+}
+EXPORT_SYMBOL(init_tools);
+
+void cleanup_tools(void)
+{
+	krg_syscalls_finalize();
+#ifdef CONFIG_KERRIGHED
+	kerrighed_proc_finalize();
+#endif
+
+	printk("iluvatar - end module\n");
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/Makefile android_cluster/linux-2.6.29/kerrighed/hotplug/Makefile
--- linux-2.6.29/kerrighed/hotplug/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/Makefile	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,13 @@
+#
+# Makefile for the Kerrighed Remote Procedure Call layer
+#
+
+obj-y := krginit.o krgsyms.o krg_tools.o krg_syscalls.o procfs.o node_discovering.o 
+
+obj-$(CONFIG_KRG_HOTPLUG) += add.o cluster.o failure.o hooks.o hotplug.o hotplug_notifier.o membership.o namespace.o
+obj-$(CONFIG_KRG_HOTPLUG_DEL) += remove.o
+obj-$(CONFIG_KRG_HOTPLUG_XCH) += replace.o
+
+EXTRA_CFLAGS += -I$(M) -Wall -Werror
+
+# end of file
diff -ruN linux-2.6.29/kerrighed/hotplug/membership.c android_cluster/linux-2.6.29/kerrighed/hotplug/membership.c
--- linux-2.6.29/kerrighed/hotplug/membership.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/membership.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,88 @@
+#include <linux/notifier.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+
+static void membership_online_add(krgnodemask_t *vector)
+{
+	kerrighed_node_t i;
+
+	__for_each_krgnode_mask(i, vector){
+		if(krgnode_online(i))
+			continue;
+		set_krgnode_online(i);
+		kerrighed_nb_nodes++;
+	}
+}
+
+static void membership_online_remove(krgnodemask_t *vector)
+{
+	kerrighed_node_t i;
+
+	__for_each_krgnode_mask(i, vector){
+		if(!krgnode_online(i))
+			continue;
+		clear_krgnode_online(i);
+		kerrighed_nb_nodes--;
+	}
+}
+
+static
+int membership_online_notification(struct notifier_block *nb,
+				   hotplug_event_t event,
+				   void *data)
+{
+	
+	switch(event){
+	case HOTPLUG_NOTIFY_ADD:{
+		struct hotplug_context *ctx = data;
+		membership_online_add(&ctx->node_set.v);
+		break;
+	}
+
+	case HOTPLUG_NOTIFY_REMOVE_LOCAL:{
+		kerrighed_node_t node;
+		for_each_online_krgnode(node)
+			if(node != kerrighed_node_id)
+				clear_krgnode_online(node);
+	}
+		
+	case HOTPLUG_NOTIFY_REMOVE_ADVERT:{
+		struct hotplug_node_set *node_set = data;
+		membership_online_remove(&node_set->v);
+		break;
+	}
+
+	default:
+		break;
+
+	} /* switch */
+	
+	return NOTIFY_OK;
+}
+
+static
+int membership_present_notification(struct notifier_block *nb,
+				    hotplug_event_t event, void *data)
+{
+	switch(event){
+	default:
+		break;
+	} /* switch */
+
+	return NOTIFY_OK;
+}
+
+int hotplug_membership_init(void)
+{
+	register_hotplug_notifier(membership_present_notification,
+				  HOTPLUG_PRIO_MEMBERSHIP_PRESENT);
+	register_hotplug_notifier(membership_online_notification,
+				  HOTPLUG_PRIO_MEMBERSHIP_ONLINE);
+	return 0;
+}
+
+void hotplug_membership_cleanup(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/namespace.c android_cluster/linux-2.6.29/kerrighed/hotplug/namespace.c
--- linux-2.6.29/kerrighed/hotplug/namespace.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/namespace.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,156 @@
+/*
+ *  kerrighed/hotplug/namespace.c
+ *
+ *  Copyright (C) 2009 Louis Rilling - Kerlabs
+ */
+
+#include <linux/sched.h>
+#include <linux/cred.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/rcupdate.h>
+#include <linux/nsproxy.h>
+#include <linux/utsname.h>
+#include <linux/ipc_namespace.h>
+#include <linux/mnt_namespace.h>
+#include <linux/pid_namespace.h>
+#include <linux/user_namespace.h>
+#include <net/net_namespace.h>
+#include <kerrighed/namespace.h>
+#include <kerrighed/krg_services.h>
+#include <kerrighed/krg_syscalls.h>
+
+static struct krg_namespace *krg_ns;
+static DEFINE_SPINLOCK(krg_ns_lock);
+
+int copy_krg_ns(struct task_struct *task, struct nsproxy *new)
+{
+	struct krg_namespace *ns = task->nsproxy->krg_ns;
+	struct user_namespace *user_ns = __task_cred(task)->user->user_ns;
+	int retval = 0;
+
+	if (!ns && current->create_krg_ns) {
+		ns = kmalloc(sizeof(*ns), GFP_KERNEL);
+
+		spin_lock_irq(&krg_ns_lock);
+		/* Only one krg_ns can live at once. */
+		if (!krg_ns) {
+			if (ns) {
+				atomic_set(&ns->count, 1);
+
+				atomic_set(&ns->root_nsproxy.count, 1);
+				get_uts_ns(new->uts_ns);
+				ns->root_nsproxy.uts_ns = new->uts_ns;
+				get_ipc_ns(new->ipc_ns);
+				ns->root_nsproxy.ipc_ns = new->ipc_ns;
+				get_mnt_ns(new->mnt_ns);
+				ns->root_nsproxy.mnt_ns = new->mnt_ns;
+				get_pid_ns(new->pid_ns);
+				ns->root_nsproxy.pid_ns = new->pid_ns;
+				get_net(new->net_ns);
+				ns->root_nsproxy.net_ns = new->net_ns;
+				ns->root_nsproxy.krg_ns = ns;
+
+				get_user_ns(user_ns);
+				ns->root_user_ns = user_ns;
+
+				get_task_struct(task);
+				ns->root_task = task;
+
+				BUG_ON(ns->root_nsproxy.pid_ns->krg_ns_root);
+				ns->root_nsproxy.pid_ns->krg_ns_root =
+					ns->root_nsproxy.pid_ns;
+
+				rcu_assign_pointer(krg_ns, ns);
+			} else {
+				retval = -ENOMEM;
+			}
+		} else {
+			kfree(ns);
+			ns = NULL;
+		}
+		spin_unlock_irq(&krg_ns_lock);
+	} else if (ns) {
+		get_krg_ns(ns);
+	}
+
+	new->krg_ns = ns;
+
+	return retval;
+}
+
+static void delayed_free_krg_ns(struct rcu_head *rcu)
+{
+	struct krg_namespace *ns = container_of(rcu, struct krg_namespace, rcu);
+
+	BUG_ON(atomic_read(&ns->root_nsproxy.count) != 1);
+	if (ns->root_nsproxy.uts_ns)
+		put_uts_ns(ns->root_nsproxy.uts_ns);
+	if (ns->root_nsproxy.ipc_ns)
+		put_ipc_ns(ns->root_nsproxy.ipc_ns);
+	if (ns->root_nsproxy.mnt_ns)
+		put_mnt_ns(ns->root_nsproxy.mnt_ns);
+	if (ns->root_nsproxy.pid_ns)
+		put_pid_ns(ns->root_nsproxy.pid_ns);
+	if (ns->root_nsproxy.net_ns)
+		put_net(ns->root_nsproxy.net_ns);
+	if (ns->root_user_ns)
+		put_user_ns(ns->root_user_ns);
+
+	put_task_struct(ns->root_task);
+
+	kfree(ns);
+}
+
+void free_krg_ns(struct krg_namespace *ns)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&krg_ns_lock, flags);
+	BUG_ON(ns != krg_ns);
+	rcu_assign_pointer(krg_ns, NULL);
+	spin_unlock_irqrestore(&krg_ns_lock, flags);
+
+	call_rcu(&ns->rcu, delayed_free_krg_ns);
+}
+
+struct krg_namespace *find_get_krg_ns(void)
+{
+	struct krg_namespace *ns;
+
+	rcu_read_lock();
+	ns = rcu_dereference(krg_ns);
+	if (ns)
+		if (!atomic_add_unless(&ns->count, 1, 0))
+			ns = NULL;
+	rcu_read_unlock();
+
+	return ns;
+}
+
+bool can_create_krg_ns(unsigned long flags)
+{
+	return current->create_krg_ns
+#ifdef CONFIG_KRG_IPC
+		&& (flags & CLONE_NEWIPC)
+#endif
+#ifdef CONFIG_KRG_PROC
+		&& (flags & CLONE_NEWPID)
+#endif
+		;
+}
+
+int krg_set_cluster_creator(void __user *arg)
+{
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	current->create_krg_ns = !!arg;
+	return 0;
+}
+
+int hotplug_namespace_init(void)
+{
+	return __register_proc_service(KSYS_HOTPLUG_SET_CREATOR,
+				       krg_set_cluster_creator, false);
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/node_discovering.c android_cluster/linux-2.6.29/kerrighed/hotplug/node_discovering.c
--- linux-2.6.29/kerrighed/hotplug/node_discovering.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/node_discovering.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,57 @@
+#include <linux/module.h>
+
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/hotplug.h>
+
+krgnodemask_t krgnode_possible_map;
+krgnodemask_t krgnode_present_map;
+krgnodemask_t krgnode_online_map;
+EXPORT_SYMBOL(krgnode_online_map);
+
+struct universe_elem universe[KERRIGHED_MAX_NODES];
+
+void krg_node_reachable(kerrighed_node_t);
+void krg_node_unreachable(kerrighed_node_t);
+
+void krg_node_arrival(kerrighed_node_t nodeid)
+{
+	set_krgnode_present(nodeid);
+	krg_node_reachable(nodeid);
+#ifdef CONFIG_KRG_HOTPLUG
+	universe[nodeid].state = 1;
+#endif
+}
+
+void krg_node_departure(kerrighed_node_t nodeid)
+{
+#ifdef CONFIG_KRG_HOTPLUG
+	universe[nodeid].state = 0;
+#endif
+	clear_krgnode_present(nodeid);
+	krg_node_unreachable(nodeid);
+}
+
+void init_node_discovering(void)
+{
+	int i;
+
+	krgnodes_setall(krgnode_possible_map);
+	krgnodes_clear(krgnode_present_map);
+	krgnodes_clear(krgnode_online_map);
+	
+#ifdef CONFIG_KRG_HOTPLUG
+	for (i = 0; i < KERRIGHED_MAX_NODES; i++) {
+		universe[i].state = 0;
+		universe[i].subid = -1;
+	}
+#endif
+
+	if (ISSET_KRG_INIT_FLAGS(KRG_INITFLAGS_NODEID)) {
+#ifdef CONFIG_KRG_HOTPLUG
+		universe[kerrighed_node_id].state = 1;
+#endif
+		set_krgnode_present(kerrighed_node_id);
+	}
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/procfs.c android_cluster/linux-2.6.29/kerrighed/hotplug/procfs.c
--- linux-2.6.29/kerrighed/hotplug/procfs.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/procfs.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,148 @@
+/** Definition of /proc/kerrighed.
+ *  @file procfs.c
+ *
+ *  Copyright (C) 2006-2008, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/module.h>
+#include <linux/proc_fs.h>
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <kerrighed/procfs.h>
+
+#define KRG_VERSION "$Name:  $ ($Date: 2004/11/29 17:44:08 $):1.0-rc4"
+
+struct proc_dir_entry *proc_kerrighed = NULL;
+EXPORT_SYMBOL(proc_kerrighed);
+
+/** Generic read function for 'unsigned long long' entry.
+ *  @author Pascal Gallard
+ *
+ *  @param buffer           Buffer to write data to.
+ *  @param buffer_location  Alternative buffer to return...
+ *  @param offset           Offset of the first byte to write in the buffer.
+ *  @param buffer_length    Length of given buffer
+ *
+ *  @return  Number of bytes written.
+ */
+
+#define CREATE_PROCFS_READ_TYPE(label, type, format) \
+  int procfs_read_##label(char *buffer, char **start, \
+                          off_t offset, int count, int *eof, \
+                          void *data) { \
+    count = sprintf(buffer, format"\n", *(type*)data); \
+    *eof = 1; \
+    return count; \
+}
+
+CREATE_PROCFS_READ_TYPE(int, int, "%d");
+CREATE_PROCFS_READ_TYPE(unsigned_int, unsigned int, "%u");
+
+CREATE_PROCFS_READ_TYPE(long, long, "%ld");
+CREATE_PROCFS_READ_TYPE(unsigned_long, unsigned long, "%lu");
+
+CREATE_PROCFS_READ_TYPE(long_long, long long, "%lld");
+CREATE_PROCFS_READ_TYPE(unsigned_long_long, unsigned long long, "%llu");
+
+/** Close a dir tree in the /proc/kerrighed
+ *  @author Gael Utard.
+ */
+void procfs_deltree(struct proc_dir_entry *entry)
+{
+	struct proc_dir_entry *subdir, *next;
+
+	subdir = entry->subdir;
+	if (subdir) {
+		for (next = subdir->next; next;
+		     subdir = next, next = subdir->next)
+			procfs_deltree(subdir);
+
+		procfs_deltree(subdir);
+	}
+
+	remove_proc_entry(entry->name, entry->parent);
+}
+EXPORT_SYMBOL(procfs_deltree);
+
+static char *krg_version = "Kerrighed v1.0-RC1 (" KRG_VERSION
+#ifdef CONFIG_SMP
+    " SMP"
+#endif
+#ifdef CONFIG_DEBUG_KERNEL
+    " DEBUG"
+#  ifdef CONFIG_DEBUG_STACKOVERFLOW
+    " +stackoverflow"
+#  endif
+#  ifdef CONFIG_DEBUG_HIGHMEM
+    " +highmem"
+#  endif
+#  ifdef CONFIG_DEBUG_SLAB
+    " +slab"
+#  endif
+#  ifdef CONFIG_DEBUG_IOVIRT
+    " +iovirt"
+#  endif
+#  ifdef CONFIG_MAGIC_SYSRQ
+    " +sysrq"
+#  endif
+#  ifdef CONFIG_DEBUG_SPINLOCK
+    " +spinlock"
+#  endif
+#  ifdef CONFIG_FRAME_POINTER
+    " +frame_pointer"
+#  endif
+#  ifdef CONFIG_KDB
+    " +kdb"
+#  endif
+#  ifdef CONFIG_KALLSYMS
+    " +kallsyms"
+#  endif
+#endif
+    ;
+
+int read_version(char *buffer, char **start, off_t offset,
+		 int count, int *eof, void *data)
+{
+	static char mybuffer[256];
+	static int len;
+
+	if (offset == 0)
+		len = snprintf(mybuffer, 256, "%s\n", krg_version);
+
+	if (offset + count >= len) {
+		count = len - offset;
+		if (count < 0)
+			count = 0;
+		*eof = 1;
+	}
+
+	memcpy(buffer, &mybuffer[offset], count);
+
+	return count;
+}
+
+/** Initialisation of the /proc/kerrighed directory.
+ *  @author Renaud Lottiaux
+ */
+int kerrighed_proc_init()
+{
+	int err = 0;
+
+	/* Create the /proc/kerrighed */
+
+	proc_kerrighed = create_proc_entry("kerrighed", S_IFDIR | 0755, NULL);
+
+	if (proc_kerrighed == NULL)
+		err = -EMFILE;
+
+	return err;
+}
+
+/** Destroy of the /proc/kerrighed directory.
+ *  @author Renaud Lottiaux
+ */
+void kerrighed_proc_finalize()
+{
+	remove_proc_entry("kerrighed", NULL);
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/remove.c android_cluster/linux-2.6.29/kerrighed/hotplug/remove.c
--- linux-2.6.29/kerrighed/hotplug/remove.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/remove.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,261 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+
+#define MODULE_NAME "Hotplug"
+
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/sched.h>
+#include <linux/fs.h>
+#include <linux/reboot.h>
+#include <linux/workqueue.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/hashtable.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/krgflags.h>
+#include <asm/uaccess.h>
+#include <asm/ioctl.h>
+
+#include <tools/workqueue.h>
+#include <tools/krg_syscalls.h>
+#include <tools/krg_services.h>
+#include <rpc/rpc.h>
+
+#include "hotplug_internal.h"
+
+inline
+void do_local_node_remove(struct hotplug_node_set *node_set)
+{
+	kerrighed_node_t node;
+
+	SET_KERRIGHED_NODE_FLAGS(KRGFLAGS_STOPPING);
+	printk("do_local_node_remove\n");
+
+	printk("...notify local\n");
+	hotplug_remove_notify(node_set, HOTPLUG_NOTIFY_REMOVE_LOCAL);
+	printk("...notify_distant\n");
+	hotplug_remove_notify(node_set, HOTPLUG_NOTIFY_REMOVE_DISTANT);
+
+	printk("...confirm\n");
+	rpc_sync_m(NODE_REMOVE_CONFIRM, &krgnode_online_map, node_set, sizeof(*node_set));
+
+	CLEAR_KERRIGHED_NODE_FLAGS(KRGFLAGS_RUNNING);
+
+	for_each_online_krgnode(node)
+		if(node != kerrighed_node_id)
+			clear_krgnode_online(node);
+
+	hooks_stop();
+	SET_KERRIGHED_NODE_FLAGS(KRGFLAGS_STOPPED);
+
+#if 0
+	printk("...sleep\n");
+	set_current_state(TASK_INTERRUPTIBLE);
+	schedule_timeout(10*HZ);
+
+	printk("...try to reboot\n");
+	queue_work(krg_nb_wq, &fail_work);
+#endif
+}
+
+inline
+void do_other_node_remove(struct hotplug_node_set *node_set)
+{
+	printk("do_other_node_remove\n");
+	hotplug_remove_notify(node_set, HOTPLUG_NOTIFY_REMOVE_ADVERT);
+	rpc_async_m(NODE_REMOVE_ACK, &node_set->v, NULL, 0);				
+}
+
+static void handle_node_remove(struct rpc_desc *desc, void *data, size_t size)
+{
+	struct hotplug_node_set *node_set;
+
+	printk("handle_node_remove\n");
+	node_set = data;
+
+	if(!krgnode_isset(kerrighed_node_id, node_set->v)){
+		do_other_node_remove(node_set);
+		return;
+	}
+
+	do_local_node_remove(node_set);
+}
+
+/* we receive the ack from cluster about our remove operation */
+static void handle_node_remove_ack(struct rpc_desc *desc, void *data, size_t size)
+{
+	printk("Need to take care that node %d ack the remove (if needed)\n", desc->client);
+}
+
+/* cluster receive the confirmation about the remove operation */
+static int handle_node_remove_confirm(struct rpc_desc *desc, void *data, size_t size)
+{
+	if(desc->client==kerrighed_node_id)
+		return 0;
+	
+	hotplug_remove_notify((void*)&desc->client, HOTPLUG_NOTIFY_REMOVE_ACK);
+	printk("Kerrighed: node %d removed\n", desc->client);
+	return 0;
+}
+
+inline void __fwd_remove_cb(struct hotplug_node_set *node_set)
+{
+	printk("__fwd_remove_cb: begin (%d / %d)\n", node_set->subclusterid, kerrighed_subsession_id);
+	if (node_set->subclusterid == kerrighed_subsession_id) {
+
+		rpc_async_m(NODE_REMOVE, &krgnode_online_map, node_set, sizeof(*node_set));
+		
+	} else {
+		kerrighed_node_t node;
+
+		printk("__fwd_remove_cb: m1\n");
+		node = 0;
+		while ((universe[node].subid != node_set->subclusterid)
+		       && (node < KERRIGHED_MAX_NODES))
+			node++;
+		printk("__fwd_remove_cb: m2 (%d/%d)\n", node, KERRIGHED_MAX_NODES);
+
+		if (node == KERRIGHED_MAX_NODES) {
+			BUG();
+			printk
+			    ("WARNING: here we have no idea... may be the next one will be more luky!\n");
+			node = kerrighed_node_id + 1;
+		}
+
+		printk("send a NODE_FWD_REMOVE to %d\n", node);
+		rpc_async(NODE_FWD_REMOVE, node, node_set, sizeof(*node_set));
+	}
+}
+
+static void handle_node_fwd_remove(struct rpc_desc *desc, void *data, size_t size)
+{
+	__fwd_remove_cb(data);
+}
+
+static int nodes_remove(void __user *arg)
+{
+	struct __hotplug_node_set __node_set;
+	struct hotplug_node_set node_set;
+	int err;
+
+	if (copy_from_user(&__node_set, arg, sizeof(struct __hotplug_node_set)))
+		return -EFAULT;
+
+	node_set.subclusterid = __node_set.subclusterid;
+	err = krgnodemask_copy_from_user(&node_set.v, &__node_set.v);
+	if (err)
+		return err;
+
+	if (!krgnodes_subset(node_set.v, krgnode_present_map))
+		return -ENONET;
+
+	if (!krgnodes_subset(node_set.v, krgnode_online_map))
+		return -EPERM;
+
+	/* TODO: Really required? */
+	if (krgnode_isset(kerrighed_node_id, node_set.v))
+		return -EPERM;
+
+	__fwd_remove_cb(&node_set);
+	return 0;
+}
+
+static void handle_node_poweroff(struct rpc_desc *desc)
+{
+	emergency_sync();
+	emergency_remount();
+
+	set_current_state(TASK_INTERRUPTIBLE);
+	schedule_timeout(5 * HZ);
+
+	local_irq_enable();
+	kernel_power_off();
+
+	// should never be reached
+	BUG();
+
+}
+
+static int nodes_poweroff(void __user *arg)
+{
+	struct __hotplug_node_set __node_set;
+	struct hotplug_node_set node_set;
+	int unused;
+	int err;
+	
+	if (copy_from_user(&__node_set, arg, sizeof(__node_set)))
+		return -EFAULT;
+
+	node_set.subclusterid = __node_set.subclusterid;
+
+	err = krgnodemask_copy_from_user(&node_set.v, &__node_set.v);
+	if (err)
+		return err;
+
+	rpc_async_m(NODE_POWEROFF, &node_set.v,
+		    &unused, sizeof(unused));
+	
+	return 0;
+}
+
+/* Currently unused... Commented to avoid compilation warning.
+static void handle_node_reboot(struct rpc_desc *desc, void *data, size_t size)
+{
+	emergency_sync();
+	emergency_remount();
+
+	set_current_state(TASK_INTERRUPTIBLE);
+	schedule_timeout(5 * HZ);
+
+	local_irq_enable();
+	machine_restart(NULL);
+
+	// should never be reached
+	BUG();
+
+}
+*/
+
+static int nodes_reboot(void __user *arg)
+{
+	struct __hotplug_node_set __node_set;
+	struct hotplug_node_set node_set;
+	int unused;
+	
+	if (copy_from_user(&__node_set, arg, sizeof(__node_set)))
+		return -EFAULT;
+
+	node_set.subclusterid = __node_set.subclusterid;
+	
+	err = krgnodemask_copy_from_user(&node_set.v, &__node_set.v);
+	if (err)
+		return err;
+
+	rpc_async_m(NODE_REBOOT, &node_set.v,
+		    &unused, sizeof(unused));
+	
+	return 0;
+}
+
+
+int hotplug_remove_init(void)
+{
+	rpc_register(NODE_POWEROFF, handle_node_poweroff, 0);
+	rpc_register_void(NODE_REMOVE, handle_node_remove, 0);
+	rpc_register_void(NODE_REMOVE_ACK, handle_node_remove_ack, 0);
+	rpc_register_void(NODE_FWD_REMOVE, handle_node_fwd_remove, 0);
+	rpc_register_int(NODE_REMOVE_CONFIRM, handle_node_remove_confirm, 0);
+	
+	register_proc_service(KSYS_HOTPLUG_REMOVE, nodes_remove);
+	register_proc_service(KSYS_HOTPLUG_POWEROFF, nodes_poweroff);
+	register_proc_service(KSYS_HOTPLUG_REBOOT, nodes_reboot);
+
+	return 0;
+}
+
+void hotplug_remove_cleanup(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/hotplug/replace.c android_cluster/linux-2.6.29/kerrighed/hotplug/replace.c
--- linux-2.6.29/kerrighed/hotplug/replace.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/hotplug/replace.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,19 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+
+#define MODULE_NAME "Hotplug"
+
+#include <linux/spinlock.h>
+#include <linux/sched.h>
+#include <kerrighed/hashtable.h>
+
+int hotplug_replace_init(void)
+{
+
+	return 0;
+}
+
+void hotplug_replace_cleanup(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/Kconfig.Kerrighed android_cluster/linux-2.6.29/kerrighed/Kconfig.Kerrighed
--- linux-2.6.29/kerrighed/Kconfig.Kerrighed	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/Kconfig.Kerrighed	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,433 @@
+menu "Cluster support"
+
+config KERRIGHED
+	bool "Add the Kerrighed support"
+	depends on PREEMPT_NONE && !KEYS && !COMPAT
+	default y
+	select KRG_TOOLS if !KERRIGHED_DEVEL
+	select KRG_COMMUNICATION_FRAMEWORK if !KERRIGHED_DEVEL
+	select KRG_HOTPLUG if !KERRIGHED_DEVEL
+	select KRG_KDDM if !KERRIGHED_DEVEL
+	select KRG_CAP if !KERRIGHED_DEVEL
+	select KRG_PROCFS if !KERRIGHED_DEVEL
+	select KRG_MM if !KERRIGHED_DEVEL
+	select KRG_DVFS if !KERRIGHED_DEVEL
+	select KRG_FAF if !KERRIGHED_DEVEL
+	select KRG_IPC if !KERRIGHED_DEVEL
+	select KRG_PROC if !KERRIGHED_DEVEL
+	select KRG_EPM if !KERRIGHED_DEVEL
+	select KRG_SCHED if !KERRIGHED_DEVEL
+	help
+	  Say Y if you want to use the Kerrighed features.	
+
+comment "Kerrighed requires kernel preemption to be fully disabled (PREEMPT_NONE)"
+	depends on !PREEMPT_NONE
+
+comment "Kerrighed does not support access key retention (KEYS)"
+	depends on KEYS
+
+comment "Kerrighed does not support 32 bits emulation (COMPAT)"
+	depends on COMPAT
+
+config KERRIGHED_DEVEL
+	bool "Allow to build a subset of Kerrighed (!! Kerrighed development only !!)"
+	depends on KERRIGHED
+	default n
+	help
+	  The only reason to enable this, is that you intend to port Kerrighed
+		on a new architecture.
+
+		Just say N, and forget about it.
+
+config KRG_TOOLS
+	bool "Tools"
+	depends on KERRIGHED
+	help
+	  Common library in Kerrighed (without communication)
+
+config KRG_COMMUNICATION_FRAMEWORK
+       bool "Communication layer"
+       depends on KERRIGHED
+       select TIPC
+       select KRGRPC
+       help
+         All-purpose communication layer
+
+config KRG_AUTONODEID
+	bool "Allow node id to bet set automatically"
+	depends on KRG_COMMUNICATION_FRAMEWORK
+	default y
+	help
+	  Compile support to let the kernel automatically set the node id.
+	  The method to compute the node id can be chosen with kernel boot
+	  parameter "autonodeid", which default value can be changed with
+	  the choice menu below. Available methods are described in that
+	  menu.
+
+	  Enabling this option will use 8 bits instead of 7 bits for node ids,
+	  which doubles the size of various node id masks, especially used in
+	  KDDM metadata.
+
+	  If unsure, say Y.
+
+choice
+	prompt "Default value of boot parameter autonodeid"
+	default KRG_AUTONODEID_ON
+	depends on KRG_AUTONODEID
+
+config KRG_AUTONODEID_ON
+	bool "IP address (autonodeid=1)"
+	help
+	  During the boot the kernel must be able to get an IP address
+	  (dhcp, command line, etc.), and will set the node id to the least
+	  significant byte of the IP address.
+
+	  In order to avoid any headache, be sure that all your nodes are
+	  on the same /24 network.
+
+	  This value can be overridden by kernel boot parameter autonodeid.
+
+	  If unsure, say Y.
+
+config KRG_AUTONODEID_OFF
+	bool "off (autonodeid=0)"
+	help
+	  The kernel will not attempt to automatically set the node id. A valid
+	  node id must be provided with kernel boot parameter "node_id".
+
+	  This value can be overridden by kernel boot parameter "autonodeid".
+
+	  If unsure, say N.
+
+endchoice
+
+config KRG_HOTPLUG
+	bool "KerHotplug"
+	depends on KERRIGHED
+	help
+	  Support for nodes management
+
+config KRG_HOTPLUG_DEL
+	bool "KerHotplug: removal support"
+	depends on KRG_HOTPLUG
+	help
+	  Support node removal
+
+config KRG_HOTPLUG_XCH
+	bool "KerHotplug: replacement support"
+	depends on KRG_HOTPLUG
+	help
+	  Support node replacement
+
+config KRG_DISABLE_HEARTBEAT
+       bool "Disable failure detector"
+       depends on KRG_HOTPLUG
+       help
+         If you don't want automatic reconfiguration on a node failure
+         just select this option.
+
+config KRG_KDDM
+	bool "Kernel Distributed Data Management (KDDM)"
+	depends on KRG_COMMUNICATION_FRAMEWORK
+	help
+	  Cluster wide data management. This is the general purpose
+	  mechanism used in Kerrighed to implement distributed
+	  services.
+
+	  If unsure, say Y.
+
+config KRG_KDDM_DEBUG
+	def_bool KRG_KDDM && KRG_DEBUG
+
+config KRG_CAP
+	bool "KerCapabilities"
+	depends on KERRIGHED
+	help
+	  Kerrighed capabilities mechanism.
+
+	  If unsure, say Y.
+
+config KRG_PROCFS
+	bool "ProcFS"
+	depends on KRG_KDDM && KERRIGHED
+	help
+	  Global /proc mechanism.
+
+	  If unsure, say Y.
+
+#config KRG_SYNC
+#	bool "KerSync"
+#	depends on KERRIGHED && KRG_KDDM
+#	help
+#	  Synchronization library
+
+config KRG_MM
+	bool "KerMM"
+	depends on KERRIGHED && KRG_KDDM
+	help
+	  Kerrighed's global memory management
+
+config KRG_DVFS
+	bool "Distributed VFS"
+#	depends on KRG_MM && KERRIGHED
+	depends on KERRIGHED
+	help
+	  Kerrighed's distributed VFS. This is the Kerrighed basic
+	  bloc for distributed file management.
+
+	  If unsure, say Y.
+
+config KRG_FAF
+	bool "File access forwarding"
+	depends on KERRIGHED && KRG_DVFS
+	select EPOLL
+	help
+	  Enable migration of open files and streams for those which
+	  cannot be more efficiently managed by other Kerrighed mechanisms.
+
+	  If unsure, say Y.
+
+config KRG_IPC
+	bool "KerIPC"
+	depends on KERRIGHED && KRG_KDDM && KRG_MM && IPC_NS
+	help
+	  Kerrighed's global IPC management
+
+config KRG_PROC
+	bool "KerProc"
+	depends on KERRIGHED && KRG_KDDM
+	help
+	  Low-level global process management: global PIDs, syscalls affecting
+	  remote tasks (including signals), global proc PID entries.
+
+	  If unsure, say Y.
+
+config KRG_EPM
+	bool "KerEPM"
+	depends on KERRIGHED && KRG_PROC && KRG_CAP && KRG_MM && (KRG_DVFS || KRG_FAF)
+	help
+	  Enhanced Process Management:
+	    process migration, checkpoint/restart, remote clone.
+
+	  If unsure, say Y.
+
+config KRG_IPC_EPM
+       def_bool KRG_IPC && KRG_EPM
+
+menuconfig KRG_SCHED
+	bool "Kerrighed support for global scheduling"
+	depends on KERRIGHED
+	select CONFIGFS_FS
+	default y
+	help
+	  Compile support for global schedulers.
+
+	  This framework allows, at run-time, to build and configure schedulers
+	  from third parties hot-pluggable components.
+
+	  If you want automagic loading of scheduler modules while configuring
+	  a scheduler, you must select "Automatic module loading" (KMOD) in
+	  section "Loadable module support".
+
+	  If unsure, say Y.
+
+if KRG_SCHED
+
+config KRG_SCHED_COMPAT
+	bool "Compile components needed to emulate Kerrighed 2.3 hard-coded scheduler"
+	depends on KRG_SCHED && KRG_EPM
+	default y
+	help
+	  This option selects the components needed to obtain the same
+	  scheduling policy as in Kerrighed 2.3 hard-coded scheduler.
+	  See the sample script krg_legacy_scheduler located in Kerrighed's
+	  tools to setup this compatible scheduler.
+
+	  If unsure, say Y.
+
+config KRG_SCHED_COMPAT_FORCE
+	def_tristate m if KRG_SCHED_COMPAT
+	select KRG_SCHED_MIGRATION_PROBE
+	select KRG_SCHED_MOSIX_PROBE
+	select KRG_SCHED_THRESHOLD_FILTER
+	select KRG_SCHED_FREQ_LIMIT_FILTER
+	select KRG_SCHED_REMOTE_CACHE_FILTER
+	select KRG_SCHED_MOSIX_LOAD_BALANCER
+	select KRG_SCHED_ROUND_ROBIN_BALANCER
+
+config KRG_SCHED_CPU_PROBE
+	tristate "Sample CPU probe"
+	depends on m
+	default n
+	help
+	  Sample CPU probe exposing the CPU load of a node
+
+	  If unsure, say N.
+
+config KRG_SCHED_MEM_PROBE
+	tristate "Sample memory probe"
+	depends on m
+	default n
+	help
+	  Sample memory probe exposing the total RAM and free RAM available on
+	  a node
+
+	  If unsure, say N.
+
+config KRG_SCHED_MIGRATION_PROBE
+	tristate "Migration probe"
+	depends on KRG_EPM && m
+	default n
+	select MODULE_HOOK
+	help
+	  Probe counting the number of ongoing task migrations out of a node
+	  and registering the last migration event.
+
+	  If unsure, say N.
+
+config KRG_SCHED_MOSIX_PROBE
+	tristate "MOSIX-like CPU probe"
+	depends on m
+	default n
+	select MODULE_HOOK
+	help
+	  MOSIX-like CPU probe exposing the CPU load of a node as well as the
+	  CPU load induced by individual tasks
+
+	  If unsure, say N.
+
+config KRG_SCHED_THRESHOLD_FILTER
+	tristate "Threshold filter"
+	depends on m
+	default n
+	help
+	  Filter propagating events only if the value exceeds the configured
+	  threshold
+
+	  If unsure, say N.
+
+config KRG_SCHED_FREQ_LIMIT_FILTER
+	tristate "Frequency limit filter"
+	depends on m
+	default n
+	help
+	  Filter propagating events only if, for a configured action monitored,
+	  no action is ongoing and the last one occured more than a configured
+	  delay before
+
+	  If unsure, say N.
+
+config KRG_SCHED_REMOTE_CACHE_FILTER
+	tristate "Remote cache filter"
+	depends on m
+	default n
+	help
+	  Filter caching values from the other nodes, at a configured refresh
+	  rate
+
+	  If unsure, say N.
+
+config KRG_SCHED_ECHO_POLICY
+	tristate "Sample echo policy"
+	depends on m
+	default n
+	help
+	  Sample policy that logs the values read on its ports when update
+	  notifications are received
+
+	  If unsure, say N.
+
+config KRG_SCHED_MOSIX_LOAD_BALANCER
+	tristate "MOSIX-like load balancing policy"
+	depends on KRG_EPM && m
+	default n
+	help
+	  Migration-based load balancing policy using simplified algorithms
+	  from the CPU load balancing policy found in MOSIX
+
+	  If unsure, say N.
+
+config KRG_SCHED_ROUND_ROBIN_BALANCER
+	tristate "Round robin balancing policy"
+	depends on m
+	default n
+	help
+	  Policy that selects target nodes in a round robin manner when called
+
+	  If unsure, say N.
+
+endif
+
+config CLUSTER_WIDE_PROC_INFRA
+	bool
+
+config CLUSTER_WIDE_PROC
+	bool "See Cluster wide stats in /proc"
+	depends on KERRIGHED
+	default y
+	select CLUSTER_WIDE_PROC_INFRA
+	help
+	  Say yes if you want to see your cluster as a big SMP through the
+	  /proc files (meminfo, cpuinfo, etc).
+	  
+	  If unsure, say Y.
+
+config CLUSTER_WIDE_PROC_CPUINFO
+	bool
+	depends on KERRIGHED
+	default CLUSTER_WIDE_PROC || KRG_PROC
+	select CLUSTER_WIDE_PROC_INFRA
+
+config CLUSTER_WIDE_PROC_MEMINFO
+	bool
+	depends on KERRIGHED
+	default CLUSTER_WIDE_PROC || KRG_MM
+	select CLUSTER_WIDE_PROC_INFRA
+
+config CLUSTER_WIDE_PROC_LOADAVG
+	bool
+	depends on KERRIGHED
+	default CLUSTER_WIDE_PROC || KRG_PROC
+	select CLUSTER_WIDE_PROC_INFRA
+
+config CLUSTER_WIDE_PROC_STAT
+	bool
+	depends on KERRIGHED
+	default CLUSTER_WIDE_PROC || KRG_PROC
+	select CLUSTER_WIDE_PROC_INFRA
+
+config CLUSTER_WIDE_PROC_UPTIME
+	bool
+	depends on KERRIGHED
+	default CLUSTER_WIDE_PROC || KRG_PROC
+	select CLUSTER_WIDE_PROC_INFRA
+
+config KRG_SYSCALL_EXIT_HOOK
+        bool "Syscall exit hook"
+	depends on KERRIGHED
+	default n
+
+config DEBUG_SEG_FAULT
+	bool "Debug seg faults"
+	depends on KERRIGHED
+	default n
+
+config KRG_DEBUG
+	bool "Debug subsystem"
+	depends on KERRIGHED
+	default n
+	select DEBUG_FS
+	help
+	  Activate Kerrighed's debug subsystem. With this
+          subsystem, most Kerrighed debugs can be activated
+          dynamically at runtime, after having mounted debugfs. To
+          activate debugs for the other subsystems, you will also
+          have to activate them in their respective source
+          directories.
+
+          This option will increase code size and probably slow
+          down the system.
+
+          If unsure, say N.
+
+endmenu
diff -ruN linux-2.6.29/kerrighed/Makefile android_cluster/linux-2.6.29/kerrighed/Makefile
--- linux-2.6.29/kerrighed/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/Makefile	2014-05-27 23:04:10.462027176 -0700
@@ -0,0 +1,9 @@
+
+obj-$(CONFIG_KRG_HOTPLUG) += hotplug/
+obj-$(CONFIG_KRG_CAP) += capability/
+obj-$(CONFIG_KRG_PROCFS) += procfs/
+obj-$(CONFIG_KRG_PROC) += proc/
+obj-$(CONFIG_KRG_DVFS) += fs/
+obj-$(CONFIG_KRG_EPM) += epm/ ghost/
+obj-$(CONFIG_KRG_MM) += mm/
+obj-$(CONFIG_KRG_SCHED) += scheduler/
diff -ruN linux-2.6.29/kerrighed/mm/injection.c android_cluster/linux-2.6.29/kerrighed/mm/injection.c
--- linux-2.6.29/kerrighed/mm/injection.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/injection.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,371 @@
+/** Memory injection code.
+ *  @file injection.c
+ *
+ *  Copyright (C) 2008, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/rmap.h>
+#include <linux/swap.h>
+#include <linux/pagemap.h>
+#include <linux/vmstat.h>
+#include <linux/pagevec.h>
+#include <linux/cpuset.h>
+#include <linux/mm_inline.h>
+#include <asm/tlbflush.h>
+#include <linux/module.h>
+#include <kerrighed/sys/types.h>
+
+#include <net/krgrpc/rpc.h>
+#include <net/krgrpc/rpcid.h>
+#include <kddm/kddm.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/dynamic_node_info_linker.h>
+#include "injection.h"
+#include "mm_struct.h"
+
+kerrighed_node_t last_chosen_node = KERRIGHED_NODE_ID_NONE;
+
+int node_mem_usage[KERRIGHED_MAX_NODES];
+EXPORT_SYMBOL(node_mem_usage);
+static atomic_t mem_usage_notified = ATOMIC_INIT(FREE_MEM);
+
+struct tasklet_struct notify_tasklet;
+
+unsigned long low_mem_limit;
+unsigned long low_mem_limit_delta;
+
+/*********************************** Policies ********************************/
+
+kerrighed_node_t select_injection_node_ff(void)
+{
+       kerrighed_node_t start_node, node;
+       int shrink_caches = 0;
+
+       if (last_chosen_node == KERRIGHED_NODE_ID_NONE)
+               start_node = krgnode_next_online_in_ring (kerrighed_node_id);
+       else
+               start_node = last_chosen_node;
+
+       node = start_node;
+
+retry:
+       if ( (node_mem_usage[node] == FREE_MEM) ||
+	    (shrink_caches && (node_mem_usage[node] == LOW_MEM))) {
+	       last_chosen_node = node;
+	       return node;
+       }
+
+       node = krgnode_next_online_in_ring (node);
+       if (node == kerrighed_node_id)
+	       node = krgnode_next_online_in_ring (node);
+       if (node != start_node)
+               goto retry;
+
+       if (!shrink_caches) {
+               shrink_caches = 1;
+               goto retry;
+       }
+
+       return KERRIGHED_NODE_ID_NONE;
+}
+
+
+kerrighed_node_t select_injection_node_rr(void)
+{
+       kerrighed_node_t start_node, node;
+       int shrink_caches = 0;
+
+       if (last_chosen_node == KERRIGHED_NODE_ID_NONE)
+               start_node = kerrighed_node_id;
+       else
+               start_node = last_chosen_node;
+
+       node = krgnode_next_online_in_ring (start_node);
+       if (node == kerrighed_node_id)
+	       node = krgnode_next_online_in_ring (node);
+retry:
+       if ( (node_mem_usage[node] == FREE_MEM) ||
+	    (shrink_caches && (node_mem_usage[node] == LOW_MEM))) {
+	       last_chosen_node = node;
+	       return node;
+       }
+
+       node = krgnode_next_online_in_ring (node);
+       if (node == kerrighed_node_id)
+	       node = krgnode_next_online_in_ring (node);
+       if (node != start_node)
+               goto retry;
+
+       if (!shrink_caches) {
+               shrink_caches = 1;
+               goto retry;
+       }
+
+       return KERRIGHED_NODE_ID_NONE;
+}
+
+
+/************************** Low mem notify management ************************/
+
+
+void handle_notify_low_mem (struct rpc_desc* desc,
+			    void *msg,
+			    size_t size)
+{
+	kerrighed_node_t nodeid = desc->client;
+	int old_val;
+
+	old_val = node_mem_usage[nodeid];
+	node_mem_usage[nodeid] = *((int*)msg);
+
+	switch (node_mem_usage[nodeid]) {
+	  case FREE_MEM:
+//		  printk ("## MEM NOTIFY - Node[%d] switched to FREE_MEM\n",
+//			  nodeid);
+		  if (old_val == OUT_OF_MEM)
+			  rpc_disable_lowmem_mode(nodeid);
+		  break;
+
+	  case LOW_MEM:
+//		  printk ("## MEM NOTIFY - Node[%d] switched to LOW_MEM\n",
+//			  nodeid);
+		  if (old_val == OUT_OF_MEM)
+			  rpc_disable_lowmem_mode(nodeid);
+		  break;
+
+	  case OUT_OF_MEM:
+//		  printk ("## MEM NOTIFY - Node[%d] switched to OUT_OF_MEM\n",
+//			  nodeid);
+		  rpc_enable_lowmem_mode(nodeid);
+		  break;
+	}
+}
+
+
+
+static void do_notify_mem(unsigned long unused)
+{
+	krgnodemask_t nodes;
+
+	krgnodes_copy(nodes, krgnode_online_map);
+	krgnode_clear(kerrighed_node_id, nodes);
+
+	rpc_async_m(RPC_MM_NOTIFY_LOW_MEM, &nodes, &mem_usage_notified,
+		    sizeof(mem_usage_notified));
+}
+
+
+
+void krg_notify_mem(int mem_usage)
+{
+	long free_pages, cache_pages;
+	int old_val;
+
+	if (mem_usage)
+		goto set_mem_usage;
+
+	free_pages = nr_free_pages();
+
+	if (free_pages < low_mem_limit) {
+		cache_pages = global_page_state(NR_FILE_PAGES)
+			- total_swapcache_pages;
+		/* - buffer_pages */
+
+		if (cache_pages < low_mem_limit)
+			mem_usage = OUT_OF_MEM;
+		else
+			if (atomic_read(&mem_usage_notified) != OUT_OF_MEM)
+				mem_usage = LOW_MEM;
+	}
+
+	if (free_pages > low_mem_limit + low_mem_limit_delta)
+		mem_usage = FREE_MEM;
+
+	if (!mem_usage)
+		return;
+
+set_mem_usage:
+	old_val = atomic_xchg(&mem_usage_notified, mem_usage);
+
+	if (old_val == mem_usage)
+		return;
+
+	switch (mem_usage) {
+	  case FREE_MEM:
+//		  printk ("## MEM NOTIFY - Switch local node to FREE_MEM\n");
+		  if (old_val == OUT_OF_MEM)
+			  rpc_disable_local_lowmem_mode();
+		  break;
+
+	  case LOW_MEM:
+//		  printk ("## MEM NOTIFY - Switch local node to LOW_MEM\n");
+		  if (old_val == OUT_OF_MEM)
+			  rpc_disable_local_lowmem_mode();
+		  break;
+
+	  case OUT_OF_MEM:
+//		  printk ("## MEM NOTIFY - Switch local node to OUT_OF_MEM\n");
+		  rpc_enable_local_lowmem_mode();
+		  break;
+	}
+
+	tasklet_hi_schedule(&notify_tasklet);
+}
+
+
+
+/************************** KDDM shrinker ************************/
+
+static int flush_page(struct page *page,
+		      struct mm_struct *mm,
+		      objid_t objid,
+		      pte_t *pte,
+		      spinlock_t *ptl)
+{
+	struct kddm_set *set = mm->anon_vma_kddm_set;
+	kerrighed_node_t dest_node;
+	int r = SWAP_FAIL;
+
+	pte_unmap_unlock(pte, ptl);
+
+	/* Check if the KDDM has not been destroyed since the page selection */
+	if (mm->anon_vma_kddm_set == NULL)
+		return SWAP_FAIL;
+
+	/* mm_id == 0 means the mm is being freed */
+	if (mm->mm_id == 0)
+		return SWAP_FAIL;
+
+	if (PageMigratable(page))
+		dest_node = select_injection_node_rr();
+	else
+		dest_node = KERRIGHED_NODE_ID_NONE;
+
+	SetPageSwapCache(page);
+	r = _kddm_flush_object(set, objid, dest_node);
+	ClearPageSwapCache(page);
+
+	if (r) {
+		if ((r == -ENOSPC) && (dest_node == KERRIGHED_NODE_ID_NONE))
+			return SWAP_FLUSH_FAIL;
+		else
+			return SWAP_FAIL;
+	}
+	else
+		ClearPageMigratable(page);
+
+
+	return SWAP_SUCCESS;
+}
+
+
+
+static int try_to_flush_one(struct page *page, struct vm_area_struct *vma)
+{
+        struct mm_struct *mm = vma->vm_mm;
+        unsigned long address;
+	objid_t objid;
+        pte_t *pte;
+        spinlock_t *ptl;
+        int ret = SWAP_AGAIN;
+
+	objid = page->index;
+	if (vma->vm_file)
+		objid += (vma->vm_start >> PAGE_SHIFT) - vma->vm_pgoff;
+
+        address = objid * PAGE_SIZE;
+
+        pte = page_check_address(page, mm, address, &ptl, 0);
+        if (!pte)
+		return ret;
+
+        /*
+         * If the page is mlock()d, we cannot swap it out.
+         * If it's recently referenced (perhaps page_referenced
+         * skipped over this mm) then we should reactivate it.
+         */
+        if (((vma->vm_flags & VM_LOCKED) ||
+	     (ptep_clear_flush_young(vma, address, pte)))) {
+		pte_unmap_unlock(pte, ptl);
+		return SWAP_FAIL;
+	}
+
+	return flush_page(page, mm, objid, pte, ptl);
+}
+
+
+
+int try_to_flush_page(struct page *page)
+{
+        struct anon_vma *anon_vma;
+        struct vm_area_struct *vma;
+	int ret = SWAP_AGAIN;
+
+	krg_notify_mem(OUT_OF_MEM);
+
+	anon_vma = page_lock_anon_vma(page);
+        if (!anon_vma)
+                return SWAP_AGAIN;
+
+	list_for_each_entry(vma, &anon_vma->head, anon_vma_node) {
+		if (page_mapcount(page) <= 1)
+			break;
+		ret = try_to_unmap_one(page, vma, 0);
+		if (ret == SWAP_FAIL)
+			goto exit;
+	}
+
+	page_unlock_anon_vma(anon_vma);
+
+	if (page_mapcount(page) == 1)
+		ret = try_to_flush_one(page, vma);
+
+exit:
+	return ret;
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              INITIALIZATION                               */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+static inline void init_low_mem_limit(void)
+{
+	struct zone *zone;
+
+	low_mem_limit = 0;
+
+	for_each_zone(zone) {
+		low_mem_limit += zone->pages_low;
+	}
+
+	low_mem_limit *= 2;
+	low_mem_limit_delta = low_mem_limit;
+}
+
+
+
+void mm_injection_init (void)
+{
+	int i;
+
+	tasklet_init(&notify_tasklet, do_notify_mem, 0);
+
+	init_low_mem_limit();
+
+	rpc_register_void(RPC_MM_NOTIFY_LOW_MEM, handle_notify_low_mem, 0);
+
+	for (i = 0; i < KERRIGHED_MAX_NODES; i++)
+		node_mem_usage[i] = FREE_MEM;
+}
+
+
+
+void mm_injection_finalize (void)
+{
+}
+
diff -ruN linux-2.6.29/kerrighed/mm/injection.h android_cluster/linux-2.6.29/kerrighed/mm/injection.h
--- linux-2.6.29/kerrighed/mm/injection.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/injection.h	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,21 @@
+/** Memory injection.
+ *  @file injection.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __MEMORY_INJECTION__
+#define __MEMORY_INJECTION__
+
+#include <kerrighed/sys/types.h>
+
+#define FREE_MEM 1
+#define LOW_MEM 2
+#define OUT_OF_MEM 3
+
+extern int node_mem_usage[KERRIGHED_MAX_NODES];
+
+void mm_injection_init (void);
+void mm_injection_finalize (void);
+
+#endif // __MEMORY_INJECTION__
diff -ruN linux-2.6.29/kerrighed/mm/Makefile android_cluster/linux-2.6.29/kerrighed/mm/Makefile
--- linux-2.6.29/kerrighed/mm/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/Makefile	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,9 @@
+obj-$(CONFIG_KRG_MM) := krgmm.o
+
+krgmm-y := memory_int_linker.o mm.o vma_struct.o page_table_tree.o \
+		   memory_io_linker.o mm_struct.o mm_struct_io_linker.o \
+		   mm_server.o injection.o
+
+krgmm-$(CONFIG_KRG_EPM) += mobility.o
+
+EXTRA_CFLAGS += -Wall -Werror
diff -ruN linux-2.6.29/kerrighed/mm/memory_int_linker.c android_cluster/linux-2.6.29/kerrighed/mm/memory_int_linker.c
--- linux-2.6.29/kerrighed/mm/memory_int_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/memory_int_linker.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,378 @@
+/** Container memory interface linker.
+ *  @file memory_int_linker.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/mm.h>
+#include <linux/memcontrol.h>
+#include <linux/mm_inline.h>
+#include <asm/pgtable.h>
+#include <asm/system.h>
+#include <asm/string.h>
+#include <linux/slab.h>
+#include <linux/pagemap.h>
+#include <linux/rmap.h>
+#include <linux/acct.h>
+#include <linux/swap.h>
+#include <linux/swapops.h>
+#include <kerrighed/pid.h>
+#include <asm/tlb.h>
+
+#include <kddm/kddm.h>
+#include "memory_int_linker.h"
+#include "memory_io_linker.h"
+#include "mm_struct.h"
+
+
+struct vm_operations_struct null_vm_ops = {};
+extern struct vm_operations_struct shm_vm_ops;
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                       MEMORY INTERFACE LINKER CREATION                    */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+/** Link a VMA to the anon memory kddm set
+ *  @author Renaud Lottiaux
+ *
+ *  @param  vma          vm_area to link with a kddm set.
+ *
+ *  @return   0 If everything OK,
+ *            Negative value otherwise.
+ */
+int check_link_vma_to_anon_memory_kddm_set (struct vm_area_struct *vma)
+{
+	int r = 0;
+
+	/* Easy case */
+	if (vma->anon_vma || vma->vm_flags & VM_KDDM)
+		goto link;
+
+	/* Don't link shared mapping */
+	if (vma->vm_flags & VM_SHARED)
+		return r;
+
+	/* Don't link non shared read-only file */
+	if (vma->vm_file && !(vma->vm_flags & VM_WRITE))
+		return r;
+
+link:
+	/* Do not share the VDSO page as anonymous memory. Anyway it is always
+	 * available on all nodes. */
+	if (arch_vma_name(vma))
+		return r;
+
+	/* Don't link already linked VMAs */
+	if ((vma->vm_ops == &anon_memory_kddm_vmops) ||
+	    (vma->vm_ops == &shm_vm_ops))
+		return r;
+
+	/*** Make the VMA a kddm set VMA ***/
+
+	BUG_ON(vma->initial_vm_ops == &anon_memory_kddm_vmops);
+	if (vma->vm_ops == NULL)
+		vma->initial_vm_ops = &null_vm_ops;
+	else
+		vma->initial_vm_ops = vma->vm_ops;
+	vma->vm_ops = &anon_memory_kddm_vmops;
+	vma->vm_flags |= VM_KDDM;
+
+	return r;
+}
+
+
+
+static inline void memory_kddm_readahead (struct kddm_set * set,
+                                          objid_t start,
+					  int upper_limit)
+{
+	int i, ra_restart_limit;
+	int ra_start, ra_end;
+
+	return ;
+
+	/* Disable prefetching for threads */
+	if (!thread_group_empty(current))
+		return;
+
+	ra_restart_limit = set->last_ra_start + (set->ra_window_size / 2);
+
+	if (start <= set->last_ra_start - set->ra_window_size / 2)
+	{
+		ra_start = start;
+		ra_end = min_t (int, set->last_ra_start - 1,
+				ra_start + set->ra_window_size);
+
+		goto do_prefetch;
+	}
+
+	if (start >= ra_restart_limit)
+	{
+		ra_end = start + set->ra_window_size;
+		ra_start = max_t (int, start, set->last_ra_start +
+				  set->ra_window_size);
+
+		goto do_prefetch;
+	}
+
+	return;
+
+do_prefetch:
+	set->last_ra_start = start;
+	ra_end = min_t (int, ra_end, upper_limit);
+
+	for (i = ra_start; i < ra_end; i++)
+		_async_kddm_grab_object_no_ft (set, i);
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                    MEMORY INTERFACE LINKER OPERATIONS                     */
+/*                                                                           */
+/*****************************************************************************/
+
+static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
+{
+	if (likely(vma->vm_flags & VM_WRITE))
+		pte = pte_mkwrite(pte);
+	return pte;
+}
+
+void map_kddm_page(struct vm_area_struct *vma,
+		   unsigned long address,
+		   struct page *page,
+		   int write)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	spinlock_t *ptl;
+	pte_t *ptep, pte;
+
+	ptep = get_locked_pte(mm, address, &ptl);
+	BUG_ON(!ptep);
+
+	pte = mk_pte(page, vma->vm_page_prot);
+	if (write)
+		pte = maybe_mkwrite(pte_mkdirty(pte), vma);
+	else
+		pte = pte_wrprotect(pte);
+	set_pte_at(mm, address, ptep, pte);
+	update_mmu_cache(vma, address, pte);
+	pte_unmap_unlock(ptep, ptl);
+
+	if (cap_raised(current->krg_caps.effective, CAP_USE_REMOTE_MEMORY))
+		SetPageMigratable(page);
+}
+
+/** Handle a nopage fault on an anonymous VMA.
+ * @author Renaud Lottiaux
+ *
+ *  @param  vma           vm_area of the faulting address area
+ *  @param  address       address of the page fault
+ *  @param  write_access  0 = read fault, 1 = write fault
+ *  @return               Physical address of the page
+ */
+
+int anon_memory_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
+{
+	struct page *page, *new_page;
+	struct kddm_set *set;
+	objid_t objid;
+	unsigned long address;
+	int ret = VM_FAULT_MINOR;
+	int write_access = vmf->flags & FAULT_FLAG_WRITE;
+
+	address = (unsigned long)(vmf->virtual_address) & PAGE_MASK;
+
+	BUG_ON(!vma);
+
+	set = vma->vm_mm->anon_vma_kddm_set;
+
+	BUG_ON(!set);
+
+	objid = address / PAGE_SIZE;
+
+	if (thread_group_empty(current)) {
+		write_access = 1;
+		if (set->def_owner != kerrighed_node_id)
+			memory_kddm_readahead (set, objid,
+					       vma->vm_start / PAGE_SIZE);
+	}
+
+	if (vma->vm_file)
+	{
+		/* Mapped file VMA no page access */
+
+		/* First, try to check if the page already exist in the anon
+		 * kddm set */
+		if (write_access)
+			page = _kddm_grab_object_manual_ft(set, objid);
+		else
+			page = _kddm_get_object_manual_ft (set, objid);
+
+		if (page != NULL)
+			goto map_page;
+
+		/* Ok, the page is not present in the anon kddm set, let's
+		 * load it */
+
+		ret = vma->initial_vm_ops->fault(vma, vmf);
+		if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))
+			goto exit_error;
+
+		/* Copy the cache page into an anonymous page (copy on write
+		 * will be done later on)
+		 *
+		 * TODO : check if this is still usefull
+		 *
+		 */
+		new_page = alloc_page_vma(GFP_HIGHUSER, vma, address);
+		if (!new_page)
+		{
+			ret = VM_FAULT_OOM;
+			goto exit_error;
+		}
+		copy_user_highpage(new_page, vmf->page, address, vma);
+
+		if (ret & VM_FAULT_LOCKED) {
+			unlock_page(vmf->page);
+			ret &= ~VM_FAULT_LOCKED;
+		}
+
+		page_cache_release(vmf->page);
+		page = new_page;
+
+		_kddm_set_object(set, objid, page);
+	}
+	else
+	{
+		/* Memory VMA no page access */
+
+		if (write_access)
+			/* TODO: ensure that all work done by
+			 * alloc_zeroed_user_highpage() is done on
+			 * archs other than x86.
+			 */
+			page = _kddm_grab_object (set, objid);
+		else
+			page = _kddm_get_object (set, objid);
+	}
+
+map_page:
+	if (page->mapping) {
+		if (page_mapcount(page) == 0) {
+			printk ("Null mapping count, non null mapping address "
+				": 0x%p\n", page->mapping);
+			page->mapping = NULL;
+		}
+		else {
+/********************* DEBUG ONLY *********************************/
+			struct anon_vma *anon_vma;
+
+			BUG_ON (!PageAnon(page));
+
+			anon_vma = (void *)page->mapping - PAGE_MAPPING_ANON;
+			if (anon_vma != vma->anon_vma) {
+				printk ("Page mapping : %p - VMA anon : %p\n",
+					anon_vma, vma->anon_vma);
+
+				printk ("Fault af 0x%08lx for "
+	       "process %s (%d - %p). Access : %d in vma [0x%08lx:0x%08lx] "
+	       "(0x%08lx) - file: 0x%p, anon_vma : 0x%p\n", address,
+	       current->comm, current->pid, current, write_access,
+	       vma->vm_start, vma->vm_end, (unsigned long) vma, vma->vm_file,
+	       vma->anon_vma);
+
+				while(1) schedule();
+			}
+
+/********************* END DEBUG ONLY ******************************/
+		}
+	}
+
+	map_kddm_page(vma, objid * PAGE_SIZE, page, write_access);
+
+	vmf->page = page;
+
+exit_error:
+	_kddm_put_object (set, objid);
+
+	return ret;
+}
+
+
+
+/** Handle a wppage fault on a memory kddm set.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  vma       vm_area of the faulting address area
+ *  @param  virtaddr  Virtual address of the page fault
+ *  @return           Physical address of the page
+ */
+struct page *anon_memory_wppage (struct vm_area_struct *vma,
+				 unsigned long address,
+				 struct page *old_page)
+{
+	struct page *page;
+	struct kddm_set *set;
+	objid_t objid;
+
+	BUG_ON (vma == NULL);
+
+	set = vma->vm_mm->anon_vma_kddm_set;
+
+	BUG_ON (set == NULL);
+
+	objid = address / PAGE_SIZE;
+
+	/* If the old page is hosted by a KDDM, the KDDM layer will do the
+	 * copy on write. If the page is not hosted by a KDDM, we must copy the
+	 * page here, after the grab.
+	 */
+	if (old_page && old_page->obj_entry)
+		old_page = NULL;
+
+	if (set->def_owner != kerrighed_node_id)
+		memory_kddm_readahead (set, objid, vma->vm_start / PAGE_SIZE);
+
+	page = _kddm_grab_object_cow (set, objid);
+
+	if (old_page && old_page != page)
+		copy_user_highpage(page, old_page, address, vma);
+
+	map_kddm_page(vma, objid * PAGE_SIZE, page, 1);
+
+	_kddm_put_object (set, objid);
+
+	return page;
+}
+
+
+void anon_memory_close (struct vm_area_struct *vma)
+{
+}
+
+
+/*
+ * Virtual Memory Operation.
+ *  Redefinition of some virtual memory operations. Used to handle page faults
+ *  on a memory kddm set.
+ *  @arg @c nopage is called when a page is touched for the first time
+ * 	 (i.e. the page is not in memory and is not swap).
+ *  @arg @c wppage is called when a page with read access is touch with a write
+ *          access.
+ *  @arg @c map is called when a vma is created or extended by do_mmap().
+ */
+struct vm_operations_struct anon_memory_kddm_vmops = {
+	close:  anon_memory_close,
+	fault: anon_memory_fault,
+	wppage: anon_memory_wppage,
+};
diff -ruN linux-2.6.29/kerrighed/mm/memory_int_linker.h android_cluster/linux-2.6.29/kerrighed/mm/memory_int_linker.h
--- linux-2.6.29/kerrighed/mm/memory_int_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/memory_int_linker.h	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,84 @@
+/** KDDM Memory interface Linker.
+ *  @file memory_int_linker.h
+ *
+ *  Link kddm sets and linux memory system.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __MEMORY_INT_LINKER__
+#define __MEMORY_INT_LINKER__
+
+#include <linux/mm.h>
+
+#include <kddm/kddm.h>
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+extern struct vm_operations_struct anon_memory_kddm_vmops;
+extern struct vm_operations_struct null_vm_ops;
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/** Link a VMA to an anon kddm set.
+ *  @author Renaud Lottiaux
+ *
+ *  @param vma     vma structure to link to the anon KDDM set.
+ *
+ *  The kddm set must have been allocated and initialized. The
+ *  VM_CONTAINER flag is added to the vm_cflags field of the vma. The
+ *  kddm set id is stored in the vm_ctnr field and vm operations are
+ *  set to the operations used by kddm sets, depending on the
+ *  kddm set type.
+ */
+int check_link_vma_to_anon_memory_kddm_set (struct vm_area_struct *vma);
+
+static inline void restore_initial_vm_ops (struct vm_area_struct *vma)
+{
+	if (vma->initial_vm_ops == NULL)
+		return;
+
+	if (vma->initial_vm_ops == &null_vm_ops)
+		vma->vm_ops = NULL;
+	else
+		vma->vm_ops = vma->initial_vm_ops;
+}
+
+
+
+/* Return the page table entry associated to a virtual address */
+static inline pte_t *get_pte_no_lock (struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t * pgd = pgd_offset(mm, addr);
+	pud_t * pud = pud_alloc(mm, pgd, addr);
+	pmd_t * pmd;
+
+	if (!pud)
+		return NULL;
+
+	pmd = pmd_alloc(mm, pud, addr);
+	if (!pmd)
+		return NULL;
+
+	if (unlikely(!pmd_present(*(pmd))) &&
+	    __pte_alloc(mm, pmd, addr))
+		return NULL;
+
+	return pte_offset_map(pmd, addr);
+}
+
+#endif /* __MEMORY_INT_LINKER__ */
diff -ruN linux-2.6.29/kerrighed/mm/memory_io_linker.c android_cluster/linux-2.6.29/kerrighed/mm/memory_io_linker.c
--- linux-2.6.29/kerrighed/mm/memory_io_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/memory_io_linker.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,263 @@
+/** KDDM memory IO linker.
+ *  @file memory_io_linker.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/mm.h>
+#include <asm/pgtable.h>
+#include <asm/system.h>
+#include <asm/string.h>
+#include <linux/slab.h>
+#include <linux/swap.h>
+#include <linux/rmap.h>
+#include <linux/swapops.h>
+#include <linux/pagemap.h>
+#include <linux/mm_inline.h>
+#include <asm/tlbflush.h>
+
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+#include <kerrighed/page_table_tree.h>
+
+#include "memory_io_linker.h"
+#include "memory_int_linker.h"
+
+/*****************************************************************************/
+/*                                                                           */
+/*                       MEMORY KDDM SET IO FUNCTIONS                       */
+/*                                                                           */
+/*****************************************************************************/
+
+/** Allocate an object
+ *  @author Renaud Lottiaux
+ */
+int memory_alloc_object (struct kddm_obj * obj_entry,
+			 struct kddm_set * set,
+			 objid_t objid)
+{
+	struct page *page = alloc_page (GFP_HIGHUSER);
+
+	if (!page)
+		return -ENOMEM;
+
+	obj_entry->object = page;
+
+	return 0;
+}
+
+/** Import an object
+ *  @author Renaud Lottiaux
+ *
+ *  @param  object    The object to import data in.
+ *  @param  buffer    Data to import in the object.
+ */
+int memory_import_object (struct rpc_desc *desc,
+			  struct kddm_set *set,
+			  struct kddm_obj *obj_entry,
+			  objid_t objid,
+			  int flags)
+{
+	struct page *page = obj_entry->object;
+	char *data;
+
+	data = (char *)kmap(page);
+	rpc_unpack(desc, 0, data, PAGE_SIZE);
+	kunmap(page);
+
+//	copy_buff_to_highpage ((struct page *) obj_entry->object, buffer);
+	return 0;
+}
+
+/** Export an object
+ *  @author Renaud Lottiaux
+ *
+ *  @param  buffer    Buffer to export object data in.
+ *  @param  object    The object to export data from.
+ */
+int memory_export_object (struct rpc_desc *desc,
+			  struct kddm_set *set,
+			  struct kddm_obj *obj_entry,
+			  objid_t objid,
+			  int flags)
+{
+	struct page *page = (struct page *)obj_entry->object;
+	char *data;
+
+	data = (char *)kmap_atomic(page, KM_USER0);
+	rpc_pack(desc, 0, data, PAGE_SIZE);
+	kunmap_atomic(data, KM_USER0);
+
+//	copy_highpage_to_buff (buffer, (struct page *) obj_entry->object);
+	return 0;
+}
+
+/** Handle a kddm set memory page first touch
+ *  @author Renaud Lottiaux
+ *
+ *  @param  obj_entry  Kddm Set page descriptor.
+ *  @param  set        Kddm Set descriptor
+ *  @param  objid      Id of the page to create.
+ *
+ *  @return  0 if everything is ok. Negative value otherwise.
+ */
+int memory_first_touch (struct kddm_obj * obj_entry,
+                        struct kddm_set * set,
+                        objid_t objid,
+			int flags)
+{
+	int res = 0;
+	struct page *page;
+
+	if (!obj_entry->object) {
+		page = alloc_page (GFP_HIGHUSER | __GFP_ZERO);
+
+		if (!page)
+			res = -ENOMEM;
+//		else
+//			page->obj_entry = obj_entry;
+
+		obj_entry->object = page;
+	}
+
+	return res;
+}
+
+/** Insert a new kddm set page in the file cache.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  obj_entry  Descriptor of the page to insert.
+ *  @param  set        Kddm Set descriptor
+ *  @param  padeid     Id of the page to insert.
+ */
+int memory_insert_page (struct kddm_obj * obj_entry,
+                        struct kddm_set * set,
+                        objid_t objid)
+{
+	struct page *page;
+
+	page = obj_entry->object;
+
+	return 0;
+}
+
+/** Invalidate a kddm set memory page.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  set      Kddm Set descriptor
+ *  @param  objid    Id of the page to invalidate
+ */
+int memory_invalidate_page (struct kddm_obj * obj_entry,
+                            struct kddm_set * set,
+                            objid_t objid)
+{
+	if (obj_entry->object) {
+		struct page *page = (struct page *) obj_entry->object;
+
+		BUG_ON(swap_pte_page(page));
+
+		/* Invalidate page table entry */
+		kddm_pt_invalidate (set, objid, obj_entry, page);
+
+		ClearPageMigratable(page);
+
+		/* Free the page */
+		page_cache_release(page);
+	}
+
+	return 0;
+}
+
+void memory_change_state (struct kddm_obj * obj_entry,
+			  struct kddm_set * set,
+			  objid_t objid,
+			  kddm_obj_state_t state)
+{
+	struct page *page = obj_entry->object;
+
+	if (!page)
+		return ;
+
+	/* If the page is not mapped, we have nothing to do */
+	if (swap_pte_page(page))
+		return;
+
+	/* Page to be swap are no more mapped. Nothing to do here. */
+	if (PageSwapCache(page))
+		return;
+
+	switch (state) {
+	  case READ_COPY :
+	  case READ_OWNER :
+		  wait_lock_page(page);
+
+		  if (page_mapped(page)) {
+			  BUG_ON ((page->mapping == NULL) &&
+				  (page != ZERO_PAGE(NULL)));
+
+			  SetPageToSetReadOnly(page);
+			  try_to_unmap(page, 0);
+			  ClearPageToSetReadOnly(page);
+		  }
+
+		  unlock_page(page);
+		  break ;
+
+	  default:
+		  break ;
+	}
+}
+
+/** Handle a kddm set memory page remove.
+ *  @author Renaud Lottiaux
+ *
+ *  @param  set      Kddm Set descriptor
+ *  @param  padeid   Id of the page to remove
+ */
+int memory_remove_page (void *object,
+                        struct kddm_set * set,
+                        objid_t objid)
+{
+	struct page *page = (struct page *) object;
+	struct kddm_obj *obj_entry;
+	swp_entry_t entry;
+
+	if (!page)
+		return 0;
+
+	if (swap_pte_page(page)) {
+		entry = get_swap_entry_from_page(page);
+		free_swap_and_cache(entry);
+	}
+	else {
+		obj_entry = page->obj_entry;
+
+		/* Invalidate page table entry */
+		kddm_pt_invalidate (set, objid, obj_entry, page);
+
+		ClearPageMigratable(page);
+
+		/* Free the page */
+		free_page_and_swap_cache(page);
+	}
+
+	return 0;
+}
+
+/****************************************************************************/
+
+/* Init the memory IO linker */
+
+struct iolinker_struct memory_linker = {
+	first_touch:       memory_first_touch,
+	remove_object:     memory_remove_page,
+	invalidate_object: memory_invalidate_page,
+	change_state:      memory_change_state,
+	insert_object:     memory_insert_page,
+	linker_name:       "mem ",
+	linker_id:         MEMORY_LINKER,
+	alloc_object:      memory_alloc_object,
+	export_object:     memory_export_object,
+	import_object:     memory_import_object,
+};
diff -ruN linux-2.6.29/kerrighed/mm/memory_io_linker.h android_cluster/linux-2.6.29/kerrighed/mm/memory_io_linker.h
--- linux-2.6.29/kerrighed/mm/memory_io_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/memory_io_linker.h	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,25 @@
+/** KDDM Memory Linker.
+ *  @file memory_linker.h
+ *
+ *  Link kddm sets and linux memory system.
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __MEMORY_LINKER__
+#define __MEMORY_LINKER__
+
+#include <kddm/kddm.h>
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+extern struct iolinker_struct memory_linker;
+
+
+#endif
diff -ruN linux-2.6.29/kerrighed/mm/mm.c android_cluster/linux-2.6.29/kerrighed/mm/mm.c
--- linux-2.6.29/kerrighed/mm/mm.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/mm.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,92 @@
+/** KerMM module initialization.
+ *  @file mm.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2009, Renaud Lottiaux, Kerlabs.
+ *
+ *  Implementation of functions used to initialize and finalize the
+ *  kermm module.
+ */
+
+#include <linux/mm.h>
+#include <asm/mman.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/swap.h>
+#include <linux/swapops.h>
+#include <asm/pgtable.h>
+#include <asm/uaccess.h>
+#include <kerrighed/krgsyms.h>
+#include <kerrighed/mm.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/page_table_tree.h>
+#include <kddm/kddm.h>
+#include "mm_struct.h"
+#include "memory_int_linker.h"
+#include "memory_io_linker.h"
+#include "mm_struct_io_linker.h"
+#include "mm_server.h"
+#include "injection.h"
+
+
+/** Initialisation of the DSM module.
+ *  @author Renaud Lottiaux
+ *
+ *  Start object server, object manager and kddm set manager threads.
+ *  Register kermm services in the /proc/kerrighed/services.
+ */
+int init_kermm(void)
+{
+	printk("KerMM initialisation : start\n");
+
+	krgsyms_register (KRGSYMS_VM_OPS_NULL, &null_vm_ops);
+	krgsyms_register (KRGSYMS_VM_OPS_FILE_GENERIC, &generic_file_vm_ops);
+	special_mapping_vm_ops_krgsyms_register ();
+	krgsyms_register (KRGSYMS_VM_OPS_MEMORY_KDDM_VMOPS,
+			  &anon_memory_kddm_vmops);
+
+	krgsyms_register (KRGSYMS_ARCH_UNMAP_AREA, arch_unmap_area);
+	krgsyms_register (KRGSYMS_ARCH_UNMAP_AREA_TOPDOWN,
+			  arch_unmap_area_topdown);
+	krgsyms_register (KRGSYMS_ARCH_GET_UNMAP_AREA, arch_get_unmapped_area);
+	krgsyms_register (KRGSYMS_ARCH_GET_UNMAP_AREA_TOPDOWN,
+			  arch_get_unmapped_area_topdown);
+	krgsyms_register (KRGSYMS_KDDM_PT_OPS, &kddm_pt_set_ops);
+
+	register_io_linker (MEMORY_LINKER, &memory_linker);
+	register_io_linker (MM_STRUCT_LINKER, &mm_struct_io_linker);
+
+	mm_struct_init ();
+	mm_server_init();
+	mm_injection_init();
+
+	printk ("KerMM initialisation done\n");
+
+	return 0;
+}
+
+
+
+/** Cleanup of the DSM module.
+ *  @author Renaud Lottiaux
+ *
+ *  Kill object manager, object server and kddm set manager threads.
+ */
+void cleanup_kermm (void)
+{
+	printk ("KerMM termination : start\n");
+
+	mm_injection_finalize();
+	mm_server_finalize();
+	mm_struct_finalize();
+
+	krgsyms_unregister (KRGSYMS_VM_OPS_FILE_GENERIC);
+	special_mapping_vm_ops_krgsyms_unregister ();
+	krgsyms_unregister (KRGSYMS_VM_OPS_MEMORY_KDDM_VMOPS);
+	krgsyms_unregister (KRGSYMS_ARCH_UNMAP_AREA);
+	krgsyms_unregister (KRGSYMS_ARCH_UNMAP_AREA_TOPDOWN);
+	krgsyms_unregister (KRGSYMS_ARCH_GET_UNMAP_AREA);
+	krgsyms_unregister (KRGSYMS_ARCH_GET_UNMAP_AREA_TOPDOWN);
+
+	printk ("KerMM termination done\n");
+}
diff -ruN linux-2.6.29/kerrighed/mm/mm_server.c android_cluster/linux-2.6.29/kerrighed/mm/mm_server.c
--- linux-2.6.29/kerrighed/mm/mm_server.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/mm_server.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,190 @@
+/** Kerrighed MM servers.
+ *  @file mm_server.c
+ *
+ *  Copyright (C) 2008-2010, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/kernel.h>
+#include <linux/mm.h>
+
+#include <net/krgrpc/rpc.h>
+#include "mm_struct.h"
+#include "mm_server.h"
+#include "memory_int_linker.h"
+
+/** Handler for remote mmap.
+ *  @author Renaud Lottiaux
+ */
+int handle_do_mmap_region (struct rpc_desc* desc,
+			   void *msgIn, size_t size)
+{
+	struct mm_mmap_msg *msg = msgIn;
+	struct vm_area_struct *vma;
+	struct mm_struct *mm;
+
+	mm = krg_get_mm(msg->mm_id);
+
+	if (!mm)
+		return 0;
+
+	down_write(&mm->mmap_sem);
+
+	__mmap_region(mm, NULL, msg->start, msg->len, msg->flags,
+		      msg->vm_flags, msg->pgoff, 1);
+
+	vma = find_vma(mm, msg->start);
+	BUG_ON(!vma || vma->vm_start != msg->start);
+
+	check_link_vma_to_anon_memory_kddm_set (vma);
+
+	up_write(&mm->mmap_sem);
+
+	krg_put_mm(msg->mm_id);
+
+	return 0;
+}
+
+/** Handler for remote mremap.
+ *  @author Renaud Lottiaux
+ */
+int handle_do_mremap (struct rpc_desc* desc,
+		      void *msgIn, size_t size)
+{
+	struct mm_mmap_msg *msg = msgIn;
+	struct mm_struct *mm;
+
+	mm = krg_get_mm(msg->mm_id);
+
+	if (!mm)
+		return 0;
+
+	down_write(&mm->mmap_sem);
+
+	__do_mremap(mm, msg->addr, msg->old_len, msg->new_len, msg->flags,
+		    msg->new_addr, &msg->_new_addr, msg->lock_limit);
+
+	up_write(&mm->mmap_sem);
+
+	krg_put_mm(msg->mm_id);
+
+	return 0;
+}
+
+/** Handler for remote munmap.
+ *  @author Renaud Lottiaux
+ */
+int handle_do_munmap (struct rpc_desc* desc,
+		      void *msgIn, size_t size)
+{
+	struct mm_mmap_msg *msg = msgIn;
+	struct mm_struct *mm;
+
+	mm = krg_get_mm(msg->mm_id);
+
+	if (!mm)
+		return 0;
+
+	down_write(&mm->mmap_sem);
+
+	do_munmap(mm, msg->start, msg->len);
+
+	up_write(&mm->mmap_sem);
+
+	krg_put_mm(msg->mm_id);
+
+	return 0;
+}
+
+/** Handler for remote brk.
+ *  @author Renaud Lottiaux
+ */
+int handle_do_brk (struct rpc_desc* desc,
+		   void *msgIn, size_t size)
+{
+	struct mm_mmap_msg *msg = msgIn;
+	struct mm_struct *mm;
+
+	mm = krg_get_mm(msg->mm_id);
+
+	if (!mm)
+		return 0;
+
+	down_write(&mm->mmap_sem);
+
+	__sys_brk(mm, msg->brk, msg->lock_limit, msg->data_limit);
+
+	up_write(&mm->mmap_sem);
+
+	krg_put_mm(msg->mm_id);
+
+	return 0;
+}
+
+/** Handler for remote expand_stack.
+ *  @author Renaud Lottiaux
+ */
+int handle_expand_stack (struct rpc_desc* desc,
+			 void *msgIn, size_t size)
+{
+	struct mm_mmap_msg *msg = msgIn;
+	struct vm_area_struct *vma;
+	struct mm_struct *mm;
+	int r;
+
+	mm = krg_get_mm(msg->mm_id);
+
+	if (!mm)
+		return -EINVAL;
+
+	down_write(&mm->mmap_sem);
+
+	vma = find_vma(mm, msg->start);
+
+	r = __expand_stack(vma, msg->flags);
+
+	up_write(&mm->mmap_sem);
+
+	krg_put_mm(msg->mm_id);
+
+	return r;
+}
+
+/** Handler for remote mprotect.
+ *  @author Renaud Lottiaux
+ */
+int handle_do_mprotect (struct rpc_desc* desc,
+			void *msgIn, size_t size)
+{
+	struct mm_mmap_msg *msg = msgIn;
+	struct mm_struct *mm;
+
+	mm = krg_get_mm(msg->mm_id);
+
+	if (!mm)
+		return 0;
+
+	do_mprotect (mm, msg->start, msg->len, msg->prot, msg->personality);
+
+	krg_put_mm(msg->mm_id);
+
+	return 0;
+}
+
+/* MM handler Initialisation */
+
+void mm_server_init (void)
+{
+	rpc_register_int(RPC_MM_MMAP_REGION, handle_do_mmap_region, 0);
+	rpc_register_int(RPC_MM_MREMAP, handle_do_mremap, 0);
+	rpc_register_int(RPC_MM_MUNMAP, handle_do_munmap, 0);
+	rpc_register_int(RPC_MM_DO_BRK, handle_do_brk, 0);
+	rpc_register_int(RPC_MM_EXPAND_STACK, handle_expand_stack, 0);
+	rpc_register_int(RPC_MM_MPROTECT, handle_do_mprotect, 0);
+}
+
+
+
+/* MM server Finalization */
+
+void mm_server_finalize (void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/mm/mm_server.h android_cluster/linux-2.6.29/kerrighed/mm/mm_server.h
--- linux-2.6.29/kerrighed/mm/mm_server.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/mm_server.h	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,61 @@
+/** Kerrighed MM Server.
+ *  @file mm_server.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __MM_SERVER__
+#define __MM_SERVER__
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+typedef struct mm_mmap_msg {
+	unique_id_t mm_id;
+	union {
+		unsigned long start;
+		unsigned long brk;
+		unsigned long addr;
+	};
+	union {
+		size_t len;
+		unsigned long lock_limit;
+	};
+	union {
+		unsigned long new_len;
+		unsigned int vm_flags;
+		unsigned long prot;
+	};
+	union {
+		unsigned long flags;
+		unsigned long data_limit;
+		int personality;
+	};
+	union {
+		unsigned long old_len;
+		unsigned long pgoff;
+	};
+	unsigned long new_addr;
+	unsigned long _new_addr;
+} mm_mmap_msg_t;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+void mm_server_init (void);
+void mm_server_finalize (void);
+
+
+#endif // __MM_SERVER__
diff -ruN linux-2.6.29/kerrighed/mm/mm_struct.c android_cluster/linux-2.6.29/kerrighed/mm/mm_struct.c
--- linux-2.6.29/kerrighed/mm/mm_struct.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/mm_struct.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,619 @@
+/** Distributed management of the MM structure.
+ *  @file mm_struct.c
+ *
+ *  Copyright (C) 2008-2009, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/mm.h>
+#include <linux/sched.h>
+#include <linux/file.h>
+#include <linux/swap.h>
+#include <linux/swapops.h>
+#include <linux/proc_fs.h>
+#include <asm/mmu_context.h>
+#include <net/krgrpc/rpc.h>
+#include <net/krgrpc/rpcid.h>
+#include <kerrighed/krginit.h>
+#include <asm/uaccess.h>
+#include <kerrighed/krg_services.h>
+#include <kddm/kddm.h>
+#include <kerrighed/page_table_tree.h>
+#include <kerrighed/hotplug.h>
+#include "memory_int_linker.h"
+#include "memory_io_linker.h"
+#include "mm_struct.h"
+#include "vma_struct.h"
+#include "mm_server.h"
+
+void (*kh_mm_get) (struct mm_struct *mm) = NULL;
+void (*kh_mm_release) (struct mm_struct *mm, int notify) = NULL;
+
+struct mm_struct *(*kh_copy_mm)(struct task_struct *tsk,
+				struct mm_struct *oldmm,
+				unsigned long clone_flags) = NULL;
+
+void (*kh_fill_pte)(struct mm_struct *mm, unsigned long addr,
+		    pte_t *pte) = NULL;
+void (*kh_zap_pte)(struct mm_struct *mm, unsigned long addr,
+		   pte_t *pte) = NULL;
+
+int krg_do_execve(struct task_struct *tsk, struct mm_struct *mm)
+{
+	if (can_use_krg_cap(current, CAP_USE_REMOTE_MEMORY))
+		return init_anon_vma_kddm_set(tsk, mm);
+
+	return 0;
+}
+
+int reinit_mm(struct mm_struct *mm)
+{
+	unique_id_t mm_id;
+
+	/* Backup mm_id which is set to 0 in mm_init... */
+	mm_id = mm->mm_id;
+	if (!mm_init(mm, NULL))
+		return -ENOMEM;
+
+	mm->mm_id = mm_id;
+	mm->locked_vm = 0;
+	mm->mmap = NULL;
+	mm->mmap_cache = NULL;
+	mm->map_count = 0;
+	cpus_clear (mm->cpu_vm_mask);
+	mm->mm_rb = RB_ROOT;
+	mm->nr_ptes = 0;
+	mm->token_priority = 0;
+	mm->last_interval = 0;
+	/* Insert the new mm struct in the list of active mm */
+	spin_lock (&mmlist_lock);
+	list_add (&mm->mmlist, &init_mm.mmlist);
+	spin_unlock (&mmlist_lock);
+#ifdef CONFIG_PROC_FS
+	mm->exe_file = NULL;
+	mm->num_exe_file_vmas = 0;
+#endif
+
+	return 0;
+}
+
+
+
+struct mm_struct *alloc_fake_mm(struct mm_struct *src_mm)
+{
+	struct mm_struct *mm;
+	int r;
+
+	mm = allocate_mm();
+	if (!mm)
+		return NULL;
+
+	if (src_mm == NULL) {
+		memset(mm, 0, sizeof(*mm));
+		if (!mm_init(mm, NULL))
+			goto err_put_mm;
+	}
+	else {
+		*mm = *src_mm;
+
+		r = reinit_mm(mm);
+		if (r)
+			goto err_put_mm;
+	}
+
+	atomic_set(&mm->mm_ltasks, 0);
+
+	return mm;
+
+err_put_mm:
+	mmput(mm);
+	return NULL;
+}
+
+void mm_struct_pin(struct mm_struct *mm)
+{
+	down_read(&mm->remove_sem);
+}
+
+void mm_struct_unpin(struct mm_struct *mm)
+{
+	up_read(&mm->remove_sem);
+}
+
+/* Unique mm_struct id generator root */
+unique_id_root_t mm_struct_unique_id_root;
+
+/* mm_struct KDDM set */
+struct kddm_set *mm_struct_kddm_set = NULL;
+
+void kcb_fill_pte(struct mm_struct *mm, unsigned long addr, pte_t pte);
+void kcb_zap_pte(struct mm_struct *mm, unsigned long addr, pte_t pte);
+
+
+
+void break_distributed_cow(struct kddm_set *set, struct mm_struct *mm)
+{
+	struct vm_area_struct *vma;
+	unsigned long addr;
+
+	for (vma = mm->mmap; vma != NULL; vma = vma->vm_next) {
+		if (vma->vm_ops != &anon_memory_kddm_vmops)
+			continue;
+
+		for (addr = vma->vm_start;
+		     addr < vma->vm_end;
+		     addr += PAGE_SIZE)
+			_kddm_grab_object_no_ft(set, addr / PAGE_SIZE);
+	}
+}
+
+
+
+void break_distributed_cow_put(struct kddm_set *set, struct mm_struct *mm)
+{
+	struct vm_area_struct *vma;
+	unsigned long addr;
+
+	for (vma = mm->mmap; vma != NULL; vma = vma->vm_next) {
+		cond_resched();
+		if (vma->vm_ops != &anon_memory_kddm_vmops)
+			continue;
+
+		for (addr = vma->vm_start;
+		     addr < vma->vm_end;
+		     addr += PAGE_SIZE)
+			_kddm_put_object(set, addr / PAGE_SIZE);
+	}
+}
+
+
+
+/* Duplicate a MM struct for a distant fork. The resulting MM will be used
+ * to store pages locally for a remote process through a memory KDDM set.
+ */
+struct mm_struct *krg_dup_mm(struct task_struct *tsk, struct mm_struct *src_mm)
+{
+	struct mm_struct *mm;
+	int err = -ENOMEM;
+
+	if (src_mm->anon_vma_kddm_set)
+		break_distributed_cow(src_mm->anon_vma_kddm_set, src_mm);
+
+	mm = allocate_mm();
+	if (!mm)
+		goto fail_nomem;
+
+	memcpy(mm, src_mm, sizeof(*mm));
+
+	err = reinit_mm(mm);
+	if (err)
+		goto exit_put_mm;
+
+	err = init_new_context(NULL, mm);
+	if (err)
+		goto fail_nocontext;
+
+	/* The duplicated mm does not yet belong to any real process */
+	atomic_set(&mm->mm_ltasks, 0);
+
+        err = __dup_mmap(mm, src_mm, 1);
+        if (err)
+                goto exit_put_mm;
+
+        mm->hiwater_rss = get_mm_rss(mm);
+        mm->hiwater_vm = mm->total_vm;
+
+	err = init_anon_vma_kddm_set(tsk, mm);
+	if (err)
+		goto exit_put_mm;
+
+	if (src_mm->anon_vma_kddm_set)
+		break_distributed_cow_put(src_mm->anon_vma_kddm_set, src_mm);
+
+	dup_mm_exe_file(src_mm, mm);
+#ifdef CONFIG_PROCFS
+	/* reinit_mm() reset it */
+	mm->num_exe_file_vmas = src_mm->num_exe_file_vmas;
+#endif
+
+	/* MM not used locally -> drop the mm_users count
+	 * (was setup to 1 in alloc and inc in
+	 * create_mm_struct_object) */
+	atomic_dec(&mm->mm_users);
+
+        return mm;
+
+exit_put_mm:
+        mmput(mm);
+
+fail_nomem:
+        return ERR_PTR(err);
+
+fail_nocontext:
+        /*
+         * If init_new_context() failed, we cannot use mmput() to free the mm
+         * because it calls destroy_context()
+         */
+	pgd_free(mm, mm->pgd);
+        free_mm(mm);
+        return ERR_PTR(err);
+}
+
+
+
+void create_mm_struct_object(struct mm_struct *mm)
+{
+	struct mm_struct *_mm;
+
+	BUG_ON(atomic_read(&mm->mm_ltasks) > 1);
+
+	atomic_inc(&mm->mm_users); // Get a reference count for the KDDM.
+
+	krgnode_set(kerrighed_node_id, mm->copyset);
+
+	mm->mm_id = get_unique_id(&mm_struct_unique_id_root);
+
+	_mm = _kddm_grab_object_manual_ft(mm_struct_kddm_set, mm->mm_id);
+	BUG_ON(_mm);
+	_kddm_set_object(mm_struct_kddm_set, mm->mm_id, mm);
+
+	krg_put_mm(mm->mm_id);
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                                KERNEL HOOKS                               */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+struct mm_struct *dup_mm(struct task_struct *tsk);
+
+static struct mm_struct *kcb_copy_mm(struct task_struct * tsk,
+				     struct mm_struct *oldmm,
+				     unsigned long clone_flags)
+{
+	struct mm_struct *mm = NULL;
+
+	if (oldmm->anon_vma_kddm_set)
+		break_distributed_cow(oldmm->anon_vma_kddm_set, oldmm);
+
+	mm = dup_mm(tsk);
+	if (!mm)
+		goto done_put;
+
+	mm->mm_id = 0;
+	mm->anon_vma_kddm_set = NULL;
+	mm->anon_vma_kddm_id = 0;
+	krgnodes_clear (mm->copyset);
+
+	if (clone_flags & CLONE_VFORK)
+		goto done_put;
+
+	if (cap_raised(tsk->krg_caps.effective, CAP_USE_REMOTE_MEMORY) ||
+	    oldmm->anon_vma_kddm_set) {
+		if (init_anon_vma_kddm_set(tsk, mm) != 0) {
+			BUG();
+			mmput(mm);
+			mm = NULL;
+			goto done_put;
+		}
+	}
+
+done_put:
+	if (oldmm->anon_vma_kddm_set)
+		break_distributed_cow_put(oldmm->anon_vma_kddm_set, oldmm);
+
+	return mm;
+}
+
+
+int init_anon_vma_kddm_set(struct task_struct *tsk,
+			   struct mm_struct *mm)
+{
+	struct kddm_set *set;
+
+	mm->mm_id = 0;
+	krgnodes_clear (mm->copyset);
+
+	set = __create_new_kddm_set(kddm_def_ns, 0, &kddm_pt_set_ops, mm,
+				    MEMORY_LINKER, kerrighed_node_id,
+				    PAGE_SIZE, NULL, 0, 0);
+
+	if (IS_ERR(set))
+		return PTR_ERR(set);
+
+	create_mm_struct_object(mm);
+
+	return 0;
+}
+
+
+
+void krg_check_vma_link(struct vm_area_struct *vma)
+{
+	BUG_ON (!vma->vm_mm->anon_vma_kddm_set);
+	check_link_vma_to_anon_memory_kddm_set (vma);
+}
+
+
+
+void kcb_mm_get(struct mm_struct *mm)
+{
+	if (!mm)
+		return;
+
+	if (!mm->mm_id) {
+		atomic_inc (&mm->mm_tasks);
+		return;
+	}
+
+	krg_grab_mm(mm->mm_id);
+	atomic_inc (&mm->mm_tasks);
+	krg_put_mm(mm->mm_id);
+}
+
+
+
+void clean_up_mm_struct (struct mm_struct *mm)
+{
+	struct vm_area_struct *vma, *next, *prev;
+
+	/* Take the semaphore to avoid race condition with mm_remove_object */
+
+	down_write(&mm->mmap_sem);
+
+	prev = NULL;
+	vma = mm->mmap;
+
+	while (vma) {
+		next = vma->vm_next;
+
+		if (!anon_vma(vma)) {
+			detach_vmas_to_be_unmapped(mm, vma, prev, vma->vm_end);
+			unmap_region(mm, vma, prev, vma->vm_start,
+				     vma->vm_end);
+			remove_vma_list(mm, vma);
+		}
+		else
+			prev = vma;
+
+		vma = next;
+	}
+	up_write(&mm->mmap_sem);
+}
+
+
+
+static void kcb_mm_release(struct mm_struct *mm, int notify)
+{
+	if (!mm)
+		return;
+
+	BUG_ON(!mm->mm_id);
+
+	if (!notify) {
+		/* Not a real exit: clean up VMAs */
+		BUG_ON (atomic_read(&mm->mm_ltasks) != 0);
+		clean_up_mm_struct(mm);
+		mm_struct_unpin(mm);
+		return;
+	}
+
+	krg_grab_mm(mm->mm_id);
+	atomic_dec (&mm->mm_tasks);
+
+	if (atomic_read(&mm->mm_tasks) == 0) {
+		struct kddm_set *set = mm->anon_vma_kddm_set;
+		unique_id_t mm_id = mm->mm_id;
+
+		mm->mm_id = 0;
+
+		_kddm_remove_frozen_object(mm_struct_kddm_set, mm_id);
+		_destroy_kddm_set(set);
+	}
+	else
+		krg_put_mm(mm->mm_id);
+}
+
+
+void krg_do_mmap_region(struct vm_area_struct *vma,
+			unsigned long flags,
+			unsigned int vm_flags)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	struct mm_mmap_msg msg;
+	krgnodemask_t copyset;
+
+	if (!mm->anon_vma_kddm_set)
+		return;
+
+	BUG_ON (!mm->mm_id);
+
+	check_link_vma_to_anon_memory_kddm_set (vma);
+
+	if (!(vma->vm_flags & VM_KDDM))
+		return;
+
+	if (krgnode_is_unique(kerrighed_node_id, mm->copyset))
+		return;
+
+	msg.mm_id = mm->mm_id;
+	msg.start = vma->vm_start;
+	msg.len = vma->vm_end - vma->vm_start;
+	msg.flags = flags;
+	msg.vm_flags = vm_flags;
+	msg.pgoff = vma->vm_pgoff;
+
+	krgnodes_copy(copyset, mm->copyset);
+	krgnode_clear(kerrighed_node_id, copyset);
+
+	rpc_sync_m(RPC_MM_MMAP_REGION, &copyset, &msg, sizeof(msg));
+}
+
+
+void krg_do_munmap(struct mm_struct *mm,
+		   unsigned long start,
+		   size_t len)
+{
+	struct mm_mmap_msg msg;
+	krgnodemask_t copyset;
+
+	if (!mm->mm_id)
+		return;
+
+	if (krgnode_is_unique(kerrighed_node_id, mm->copyset))
+		return;
+
+	msg.mm_id = mm->mm_id;
+	msg.start = start;
+	msg.len = len;
+
+	krgnodes_copy(copyset, mm->copyset);
+	krgnode_clear(kerrighed_node_id, copyset);
+
+	rpc_sync_m(RPC_MM_MUNMAP, &copyset, &msg, sizeof(msg));
+}
+
+void krg_do_mremap(struct mm_struct *mm, unsigned long addr,
+		   unsigned long old_len, unsigned long new_len,
+		   unsigned long flags, unsigned long new_addr,
+		   unsigned long _new_addr, unsigned long lock_limit)
+{
+	struct mm_mmap_msg msg;
+	krgnodemask_t copyset;
+
+	if (!mm->mm_id)
+		return;
+
+	if (krgnode_is_unique(kerrighed_node_id, mm->copyset))
+		return;
+
+	msg.mm_id = mm->mm_id;
+	msg.addr = addr;
+	msg.old_len = old_len;
+	msg.new_len = new_len;
+	msg.flags = flags;
+	msg.new_addr = new_addr;
+	msg._new_addr = _new_addr;
+	msg.lock_limit = lock_limit;
+
+	krgnodes_copy(copyset, mm->copyset);
+	krgnode_clear(kerrighed_node_id, copyset);
+
+	rpc_sync_m(RPC_MM_MREMAP, &copyset, &msg, sizeof(msg));
+}
+
+void krg_do_brk(struct mm_struct *mm,
+		unsigned long brk,
+		unsigned long lock_limit,
+		unsigned long data_limit)
+{
+	struct mm_mmap_msg msg;
+	krgnodemask_t copyset;
+
+	BUG_ON (!mm->mm_id);
+
+	if (krgnode_is_unique(kerrighed_node_id, mm->copyset))
+		return;
+
+	msg.mm_id = mm->mm_id;
+	msg.brk = brk;
+	msg.lock_limit = lock_limit;
+	msg.data_limit = data_limit;
+
+	krgnodes_copy(copyset, mm->copyset);
+	krgnode_clear(kerrighed_node_id, copyset);
+
+	rpc_sync_m(RPC_MM_DO_BRK, &copyset, &msg, sizeof(msg));
+}
+
+int krg_expand_stack(struct vm_area_struct *vma,
+		     unsigned long address)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	struct mm_mmap_msg msg;
+	krgnodemask_t copyset;
+	int r;
+
+	BUG_ON (!mm->mm_id);
+
+	if (krgnode_is_unique(kerrighed_node_id, mm->copyset))
+		return 0;
+
+	msg.mm_id = mm->mm_id;
+	msg.start = vma->vm_start;
+	msg.flags = address;
+
+	krgnodes_copy(copyset, mm->copyset);
+	krgnode_clear(kerrighed_node_id, copyset);
+
+	r = rpc_sync_m(RPC_MM_EXPAND_STACK, &copyset, &msg, sizeof(msg));
+
+	return r;
+}
+
+void krg_do_mprotect(struct mm_struct *mm,
+		     unsigned long start,
+		     size_t len,
+		     unsigned long prot,
+		     int personality)
+{
+	struct mm_mmap_msg msg;
+	krgnodemask_t copyset;
+
+	if (!mm->mm_id)
+		return;
+
+	if (krgnode_is_unique(kerrighed_node_id, mm->copyset))
+		return;
+
+	msg.mm_id = mm->mm_id;
+	msg.start = start;
+	msg.len = len;
+	msg.prot = prot;
+	msg.personality = personality;
+
+	krgnodes_copy(copyset, mm->copyset);
+	krgnode_clear(kerrighed_node_id, copyset);
+
+	rpc_sync_m(RPC_MM_MPROTECT, &copyset, &msg, sizeof(msg));
+}
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              INITIALIZATION                               */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+void mm_struct_init (void)
+{
+	init_unique_id_root (&mm_struct_unique_id_root);
+
+	mm_struct_kddm_set = create_new_kddm_set(kddm_def_ns,
+						 MM_STRUCT_KDDM_ID,
+						 MM_STRUCT_LINKER,
+						 KDDM_UNIQUE_ID_DEF_OWNER,
+						 sizeof (struct mm_struct),
+						 KDDM_LOCAL_EXCLUSIVE);
+
+	if (IS_ERR(mm_struct_kddm_set))
+		OOM;
+
+	hook_register(&kh_copy_mm, kcb_copy_mm);
+	hook_register(&kh_mm_get, kcb_mm_get);
+	hook_register(&kh_mm_release, kcb_mm_release);
+	hook_register(&kh_fill_pte, kcb_fill_pte);
+	hook_register(&kh_zap_pte, kcb_zap_pte);
+}
+
+
+
+void mm_struct_finalize (void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/mm/mm_struct.h android_cluster/linux-2.6.29/kerrighed/mm/mm_struct.h
--- linux-2.6.29/kerrighed/mm/mm_struct.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/mm_struct.h	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,74 @@
+/** Distributed management of the MM structure.
+ *  @file mm_struct.h
+ *
+ *  @author Renaud Lottiaux.
+ */
+
+
+#ifndef MM_STRUCT_H
+#define MM_STRUCT_H
+
+#include <kddm/kddm_get_object.h>
+#include <kddm/kddm_grab_object.h>
+#include <kddm/kddm_put_object.h>
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+extern struct kddm_set *mm_struct_kddm_set;
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+struct mm_struct *alloc_fake_mm(struct mm_struct *src_mm);
+int reinit_mm(struct mm_struct *mm);
+
+
+int init_anon_vma_kddm_set(struct task_struct *tsk,
+			   struct mm_struct *mm);
+
+struct mm_struct *krg_dup_mm(struct task_struct *tsk,struct mm_struct *src_mm);
+
+static inline struct mm_struct *krg_get_mm(unique_id_t mm_id)
+{
+	if (mm_id)
+		return _kddm_get_object (mm_struct_kddm_set, mm_id);
+	else
+		return NULL;
+}
+
+static inline struct mm_struct *krg_grab_mm(unique_id_t mm_id)
+{
+	if (mm_id)
+		return _kddm_grab_object (mm_struct_kddm_set, mm_id);
+	else
+		return NULL;
+}
+
+void kcb_mm_get(struct mm_struct *mm);
+
+static inline void krg_put_mm(unique_id_t mm_id)
+{
+	if (mm_id)
+		_kddm_put_object (mm_struct_kddm_set, mm_id);
+}
+
+void create_mm_struct_object(struct mm_struct *mm);
+
+void mm_struct_finalize (void);
+void mm_struct_init (void);
+
+#endif // MM_STRUCT_H
diff -ruN linux-2.6.29/kerrighed/mm/mm_struct_io_linker.c android_cluster/linux-2.6.29/kerrighed/mm/mm_struct_io_linker.c
--- linux-2.6.29/kerrighed/mm/mm_struct_io_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/mm_struct_io_linker.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,189 @@
+/** MM Struct Linker.
+ *  @file mm_struct_io_linker.c
+ *
+ *  Copyright (C) 2008-2009, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/rmap.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+
+#include "mm_struct.h"
+#include "vma_struct.h"
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                       MM_STRUCT KDDM SET IO FUNCTIONS                     */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+int mm_alloc_object (struct kddm_obj *obj_entry,
+		     struct kddm_set *set,
+		     objid_t objid)
+{
+	obj_entry->object = NULL;
+	return 0;
+}
+
+
+
+int mm_first_touch (struct kddm_obj *obj_entry,
+		    struct kddm_set *set,
+		    objid_t objid,
+		    int flags)
+{
+	/* Should never be called */
+	BUG();
+
+	return 0;
+}
+
+
+
+int mm_remove_object (void *object,
+		      struct kddm_set *set,
+		      objid_t objid)
+{
+	struct mm_struct *mm = object;
+
+	/* Ensure that no thread uses this signal_struct copy */
+	down_write(&mm->remove_sem);
+	up_write(&mm->remove_sem);
+
+	/* Take the mmap_sem to avoid race condition with clean_up_mm_struct */
+
+	atomic_inc(&mm->mm_count);
+	down_write(&mm->mmap_sem);
+
+	mmput(mm);
+
+	up_write(&mm->mmap_sem);
+
+	mm->mm_id = 0;
+
+	mmdrop(mm);
+
+	return 0;
+}
+
+
+
+/** Export an MM struct
+ *  @author Renaud Lottiaux
+ *
+ *  @param  buffer    Buffer to export object data in.
+ *  @param  obj_entry  Object entry of the object to export.
+ */
+int mm_export_object (struct rpc_desc *desc,
+		      struct kddm_set *set,
+		      struct kddm_obj *obj_entry,
+		      objid_t objid,
+		      int flags)
+{
+	struct mm_struct *mm;
+	krgsyms_val_t unmap_id, get_unmap_id;
+
+	mm = obj_entry->object;
+
+	krgnode_set (desc->client, mm->copyset);
+
+	rpc_pack(desc, 0, &mm->mm_id, sizeof(unique_id_t));
+	rpc_pack(desc, 0, &mm->anon_vma_kddm_id, sizeof(unique_id_t));
+	rpc_pack(desc, 0, &mm->context.vdso, sizeof(void*));
+	rpc_pack(desc, 0, &mm->copyset, sizeof(krgnodemask_t));
+
+	get_unmap_id = krgsyms_export(mm->get_unmapped_area);
+	BUG_ON(mm->get_unmapped_area && get_unmap_id == KRGSYMS_UNDEF);
+	rpc_pack_type(desc, get_unmap_id);
+
+	unmap_id = krgsyms_export(mm->unmap_area);
+	BUG_ON(mm->unmap_area && unmap_id == KRGSYMS_UNDEF);
+	rpc_pack_type(desc, unmap_id);
+
+	return 0;
+}
+
+
+
+/** Import an MM struct
+ *  @author Renaud Lottiaux
+ *
+ *  @param  obj_entry  Object entry of the object to import.
+ *  @param  _buffer   Data to import in the object.
+ */
+int mm_import_object (struct rpc_desc *desc,
+		      struct kddm_set *_set,
+		      struct kddm_obj *obj_entry,
+		      objid_t objid,
+		      int flags)
+{
+	struct mm_struct *mm;
+	krgsyms_val_t unmap_id, get_unmap_id;
+	struct kddm_set *set;
+	unique_id_t mm_id, kddm_id;
+	void *context_vdso;
+	int r;
+
+	mm = obj_entry->object;
+
+	r = rpc_unpack (desc, 0, &mm_id, sizeof(unique_id_t));
+	if (r)
+		return r;
+
+	r = rpc_unpack (desc, 0, &kddm_id, sizeof(unique_id_t));
+	if (r)
+		return r;
+
+	r = rpc_unpack (desc, 0, &context_vdso, sizeof(void*));
+	if (r)
+		return r;
+
+	if (mm == NULL) {
+		/* First import */
+		set = _find_get_kddm_set(kddm_def_ns, kddm_id);
+		BUG_ON (set == NULL);
+
+		mm = set->obj_set;
+		mm->mm_id = mm_id;
+		atomic_inc(&mm->mm_users);
+		obj_entry->object = mm;
+		put_kddm_set(set);
+		mm->context.vdso = context_vdso;
+	}
+
+	r = rpc_unpack(desc, 0, &mm->copyset, sizeof(krgnodemask_t));
+	if (r)
+		return r;
+
+	r = rpc_unpack_type(desc, get_unmap_id);
+	if (r)
+		return r;
+
+	mm->get_unmapped_area = krgsyms_import (get_unmap_id);
+
+	r = rpc_unpack_type(desc, unmap_id);
+	if (r)
+		return r;
+	mm->unmap_area = krgsyms_import (unmap_id);
+
+	return 0;
+}
+
+
+
+/****************************************************************************/
+
+/* Init the mm_struct IO linker */
+
+struct iolinker_struct mm_struct_io_linker = {
+	alloc_object:      mm_alloc_object,
+	first_touch:       mm_first_touch,
+	export_object:     mm_export_object,
+	import_object:     mm_import_object,
+	remove_object:     mm_remove_object,
+	linker_name:       "MM ",
+	linker_id:         MM_STRUCT_LINKER,
+};
diff -ruN linux-2.6.29/kerrighed/mm/mm_struct_io_linker.h android_cluster/linux-2.6.29/kerrighed/mm/mm_struct_io_linker.h
--- linux-2.6.29/kerrighed/mm/mm_struct_io_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/mm_struct_io_linker.h	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,48 @@
+/** MM Struct Linker.
+ *  @file mm_struct_io_linker.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef __MM_STRUCT_LINKER__
+#define __MM_STRUCT_LINKER__
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                 MACROS                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+extern struct iolinker_struct mm_struct_io_linker;
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+
+
+#endif // __MM_STRUCT_LINKER__
diff -ruN linux-2.6.29/kerrighed/mm/mobility.c android_cluster/linux-2.6.29/kerrighed/mm/mobility.c
--- linux-2.6.29/kerrighed/mm/mobility.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/mobility.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,1395 @@
+/** Implementation of process Virtual Memory mobility mechanisms.
+ *  @file vm_mobility.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2009, Renaud Lottiaux, Kerlabs.
+ *
+ *  Implementation of functions used to migrate, duplicate and checkpoint
+ *  process virtual memory.
+ */
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/rmap.h>
+#include <linux/swap.h>
+#include <linux/vmalloc.h>
+#include <linux/init_task.h>
+#include <asm/elf.h>
+#include <linux/file.h>
+#ifndef CONFIG_USERMODE
+#include <asm/ldt.h>
+#else
+#include <asm/arch/ldt.h>
+#endif
+#include <asm/pgalloc.h>
+#include <asm/tlbflush.h>
+#include <asm/pgtable.h>
+#include <kerrighed/krgsyms.h>
+#include <kerrighed/krginit.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+#include <kerrighed/file_stat.h>
+#include <kerrighed/ghost.h>
+#include <kerrighed/ghost_helpers.h>
+#include <kerrighed/action.h>
+#include <kerrighed/application.h>
+#include <kerrighed/app_shared.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/sys/checkpoint.h>
+#include "vma_struct.h"
+
+#include "memory_int_linker.h"
+#include "memory_io_linker.h"
+#include "mm_struct.h"
+
+#define FILE_TABLE_SIZE 16
+
+void unimport_mm_struct(struct task_struct *task);
+
+void __vma_link_file(struct vm_area_struct *vma);
+
+extern struct vm_operations_struct special_mapping_vmops;
+
+/*****************************************************************************/
+/*                                                                           */
+/*                               TOOLS FUNCTIONS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+void free_ghost_mm (struct task_struct *tsk)
+{
+	/* if not NULL, mm_release will try to write in userspace... which
+	 * does not exist anyway since we are in kernel thread context
+	 */
+	tsk->clear_child_tid = NULL;
+	/* Do not notify end of vfork here */
+	tsk->vfork_done = NULL;
+	mmput (tsk->mm);
+
+	/* exit_mm supposes current == tsk, and therefore, leaves one
+	 * reference to tsk->mm because of mm->active_mm which will be dropped
+	 * during schedule.
+	 * The ghost mm will never be scheduled out because no real process is
+	 * associated to it, thereofore, we take care of the active_mm case
+	 * here
+	 */
+	if (!tsk->mm) {
+		mmdrop (tsk->active_mm);
+		tsk->active_mm = NULL;
+	}
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              EXPORT FUNCTIONS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+struct cr_mm_region_excluded {
+	struct cr_mm_region region;
+	struct mm_struct *mm;
+};
+
+static int is_page_contained_in_mm_region(struct cr_mm_region *region,
+					  unsigned long addr)
+{
+	if (region->addr <= addr
+	    && addr + PAGE_SIZE <= region->addr + region->size)
+		return 1;
+
+	return 0;
+}
+
+static int is_page_excluded_from_checkpoint(struct app_struct *app,
+					    struct mm_struct *mm,
+					    unsigned long addr)
+{
+	struct cr_mm_region *mm_region;
+	struct cr_mm_region_excluded *mm_excl_region;
+
+	mm_region = app->checkpoint.first_mm_region;
+
+	while (mm_region) {
+
+		mm_excl_region = container_of(mm_region,
+					      struct cr_mm_region_excluded,
+					      region);
+
+		if (mm == mm_excl_region->mm
+		    && is_page_contained_in_mm_region(
+			    &mm_excl_region->region, addr))
+			return 1;
+
+		mm_region = mm_region->next;
+	}
+
+	return 0;
+}
+
+static int __cr_exclude_mm_region(struct app_struct *app, struct mm_struct *mm,
+				  unsigned long addr, size_t size)
+{
+	struct cr_mm_region_excluded *mm_region;
+
+	mm_region = kmalloc(sizeof(struct cr_mm_region_excluded), GFP_KERNEL);
+	if (!mm_region)
+		return -ENOMEM;
+
+	mm_region->mm = mm;
+	mm_region->region.addr = addr;
+	mm_region->region.size = size;
+
+	/* we don't care about order */
+	if (app->checkpoint.first_mm_region)
+		mm_region->region.next = app->checkpoint.first_mm_region;
+	else
+		mm_region->region.next = NULL;
+
+	app->checkpoint.first_mm_region = &mm_region->region;
+
+	return 0;
+}
+
+int cr_exclude_mm_region(struct app_struct *app, pid_t pid,
+			 unsigned long addr, size_t size)
+{
+	task_state_t *t;
+
+	list_for_each_entry(t, &app->tasks, next_task) {
+		if (task_pid_knr(t->task) == pid)
+			return __cr_exclude_mm_region(app, t->task->mm,
+						      addr, size);
+	}
+
+	return 0; /* process is not on this node, simply ignore */
+}
+
+void cr_free_mm_exclusions(struct app_struct *app)
+{
+	struct cr_mm_region_excluded *mm_region;
+	struct cr_mm_region *element;
+
+	element = app->checkpoint.first_mm_region;
+
+	while (element) {
+		mm_region = container_of(element,
+					 struct cr_mm_region_excluded,
+					 region);
+
+		element = mm_region->region.next;
+
+		kfree(mm_region);
+	}
+
+	app->checkpoint.first_mm_region = NULL;
+}
+
+/** Export one physical page of a process.
+ *  @author Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param app      Application hosting task(s) related to the vma.
+ *  @param ghost    Ghost where data should be stored.
+ *  @param vma      The memory area hosting the page.
+ *  @param addr     Virtual address of the page.
+ *
+ *  @return  1 if a page has been exported.
+ *           0 if no page has been exported.
+ *           Negative value otherwise.
+ */
+static int cr_export_one_page(struct app_struct *app, ghost_t *ghost,
+			      struct vm_area_struct *vma, unsigned long addr)
+{
+	struct kddm_set *set = NULL;
+	unsigned long pfn;
+	spinlock_t *ptl;
+	struct page *page = NULL;
+	char *page_addr;
+	objid_t objid = 0;
+	pgprot_t prot;
+	pte_t *pte;
+	int put_page = 0;
+	int nr_exported = 0;
+	int page_excluded = 0;
+	int r;
+
+	pte = get_locked_pte(vma->vm_mm, addr, &ptl);
+	if (pte && pte_present(*pte)) {
+		pfn = pte_pfn(*pte);
+		page = pfn_to_page(pfn);
+		prot = pte_pgprot(*pte);
+		pte_unmap_unlock(pte, ptl);
+		if (!page || !PageAnon(page))
+			goto exit;
+	} else {
+		if (pte)
+			pte_unmap_unlock(pte, ptl);
+
+		set = vma->vm_mm->anon_vma_kddm_set;
+		if (set) {
+			objid = addr / PAGE_SIZE;
+			page = kddm_get_object_no_ft(kddm_def_ns, set->id,
+						     objid);
+			prot = vma->vm_page_prot;
+			put_page = 1;
+		}
+		if (!page)
+			goto exit;
+	}
+
+	page_addr = (char *)kmap(page);
+
+	/* Export the virtual address of the page */
+	r = ghost_write(ghost, &addr, sizeof (unsigned long));
+	if (r)
+		goto unmap;
+
+	/* Export the page protection */
+	r = ghost_write(ghost, &prot, sizeof(pgprot_t));
+	if (r)
+		goto unmap;
+
+	/* Export the physical page content unless it has been
+	 * excluded from the chekpoint by the programmer */
+	page_excluded = is_page_excluded_from_checkpoint(app, vma->vm_mm, addr);
+	r = ghost_write_type(ghost, page_excluded);
+	if (r)
+		goto unmap;
+
+	if (!page_excluded) {
+		r = ghost_write(ghost, (void*)page_addr, PAGE_SIZE);
+		if (r)
+			goto unmap;
+	}
+
+unmap:
+	kunmap(page);
+	nr_exported = r ? r : 1;
+
+exit:
+	if (put_page)
+		kddm_put_object(kddm_def_ns, set->id, objid);
+
+	return nr_exported;
+}
+
+/** Export the physical pages hosted by a VMA.
+ *  @author Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param app      Application hosting task(s) related to the vma.
+ *  @param ghost    Ghost where data should be stored.
+ *  @param tsk      Task to export memory pages from.
+ *  @param vma      The VMA to export pages from.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+static int cr_export_vma_pages(struct app_struct *app, ghost_t *ghost,
+			       struct vm_area_struct *vma)
+{
+	unsigned long addr;
+	int nr_pages_sent = 0;
+	int r;
+
+	if (!anon_vma(vma))
+		goto done;
+
+	for (addr = vma->vm_start; addr < vma->vm_end; addr += PAGE_SIZE) {
+		r = cr_export_one_page(app, ghost, vma, addr);
+		if (r < 0)
+			goto out;
+		nr_pages_sent += r;
+	}
+
+done:
+	/* Mark the end of the page exported */
+	addr = 0;
+	r = ghost_write(ghost, &addr, sizeof (unsigned long));
+	if (r)
+		goto out;
+
+	r = ghost_write(ghost, &nr_pages_sent, sizeof (int)) ;
+
+out:
+	return r;
+}
+
+/** This function exports the physical memory pages of a process
+ *  @author Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param app         Application hosting task(s) related to the mm_struct.
+ *  @param ghost       Ghost where pages should be stored.
+ *  @param mm          mm_struct to export memory pages to.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int cr_export_process_pages(struct app_struct *app,
+			    ghost_t * ghost,
+			    struct mm_struct *mm)
+{
+	struct vm_area_struct *vma;
+	int r = 0;
+
+	BUG_ON(!app);
+	BUG_ON(!mm);
+
+	/* Export process VMAs */
+	vma = mm->mmap;
+	BUG_ON(!vma);
+
+	while (vma) {
+		if (vma->vm_ops != &special_mapping_vmops) {
+			r = cr_export_vma_pages(app, ghost, vma);
+			if (r)
+				goto out;
+		}
+		vma = vma->vm_next;
+	}
+
+	{
+		int magic = 962134;
+		r = ghost_write(ghost, &magic, sizeof(int));
+	}
+
+out:
+	return r;
+}
+
+/** Export one VMA into the ghost.
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost    Ghost where data should be stored.
+ *  @param tsk      The task to export the VMA from.
+ *  @param vma      The VMA to export.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+static int export_one_vma (struct epm_action *action,
+			   ghost_t *ghost,
+                           struct task_struct *tsk,
+                           struct vm_area_struct *vma,
+			   hashtable_t *file_table)
+{
+	krgsyms_val_t vm_ops_type, initial_vm_ops_type;
+	int r;
+
+	/* First, check if we need to link the VMA to the anon kddm_set */
+
+	if (tsk->mm->anon_vma_kddm_set)
+		check_link_vma_to_anon_memory_kddm_set (vma);
+
+	/* Export the vm_area_struct */
+	r = ghost_write (ghost, vma, sizeof (struct vm_area_struct));
+	if (r)
+		goto out;
+
+#ifdef CONFIG_KRG_DVFS
+	/* Export the associated file */
+	r = export_vma_file (action, ghost, tsk, vma, file_table);
+	if (r)
+		goto out;
+#endif
+	/* Define and export the vm_ops type of the vma */
+
+	r = -EPERM;
+	vm_ops_type = krgsyms_export (vma->vm_ops);
+	if (vma->vm_ops && vm_ops_type == KRGSYMS_UNDEF)
+		goto out;
+
+	/* shmem_vm_ops (posix shm) is supported only for checkpoint/restart */
+	if (action->type != EPM_CHECKPOINT
+	    && vma->vm_ops && vm_ops_type == KRGSYMS_VM_OPS_SHMEM)
+		goto out;
+
+	initial_vm_ops_type = krgsyms_export (vma->initial_vm_ops);
+	if (vma->initial_vm_ops && initial_vm_ops_type == KRGSYMS_UNDEF)
+		goto out;
+
+	BUG_ON(vma->vm_private_data && vm_ops_type != KRGSYMS_VM_OPS_SPECIAL_MAPPING);
+
+	r = ghost_write (ghost, &vm_ops_type, sizeof (krgsyms_val_t));
+	if (r)
+		goto out;
+
+	r = ghost_write (ghost, &initial_vm_ops_type, sizeof (krgsyms_val_t));
+
+out:
+	return r;
+}
+
+
+
+/** This function export the list of VMA to the ghost
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where file data should be stored.
+ *  @param tsk    Task to export vma data from.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int export_vmas (struct epm_action *action,
+		 ghost_t *ghost,
+                 struct task_struct *tsk)
+{
+	struct vm_area_struct *vma;
+	hashtable_t *file_table;
+
+	int r;
+
+	BUG_ON (tsk == NULL);
+	BUG_ON (tsk->mm == NULL);
+
+	file_table = hashtable_new (FILE_TABLE_SIZE);
+	if (!file_table)
+		return -ENOMEM;
+
+	/* Export process VMAs */
+
+	r = ghost_write(ghost, &tsk->mm->map_count, sizeof(int));
+	if (r)
+		goto out;
+
+	vma = tsk->mm->mmap;
+
+	while (vma != NULL) {
+		r = export_one_vma (action, ghost, tsk, vma, file_table);
+		if (r)
+			goto out;
+		vma = vma->vm_next;
+	}
+
+	{
+		int magic = 650874;
+
+		r = ghost_write(ghost, &magic, sizeof(int));
+	}
+
+out:
+	hashtable_free(file_table);
+
+	return r;
+}
+
+
+
+/** This function exports the context structure of a process
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where data should be stored.
+ *  @param mm     MM hosting context to export.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int export_context_struct (ghost_t * ghost,
+                           struct mm_struct *mm)
+{
+	int r = 0;
+
+#ifndef CONFIG_USERMODE
+	if (mm->context.ldt) {
+		r = ghost_write(ghost,
+				mm->context.ldt,
+				mm->context.size * LDT_ENTRY_SIZE);
+		if (r)
+			goto err;
+	}
+#endif
+err:
+	return r;
+}
+
+static int export_mm_counters(struct epm_action *action,
+			      ghost_t *ghost,
+			      struct mm_struct* mm,
+			      struct mm_struct *exported_mm)
+{
+	int r;
+
+	r = ghost_write(ghost, mm, sizeof(struct mm_struct));
+	r = ghost_write(ghost, &exported_mm->mm_tasks, sizeof(atomic_t));
+	return r;
+}
+
+static int cr_add_vmas_files_to_shared_table(struct task_struct *task)
+{
+	int r = 0;
+	struct vm_area_struct *vma;
+	vma = task->mm->mmap;
+
+	while (vma != NULL) {
+
+		if (vma->vm_file
+		    && !is_anon_shared_mmap(vma->vm_file)) {
+			r = cr_add_file_to_shared_table(task, -1,
+							vma->vm_file, 0);
+			if (r == -ENOKEY) /* the file was already in the list */
+				r = 0;
+
+			if (r)
+				goto error;
+		}
+		vma = vma->vm_next;
+	}
+
+error:
+	return r;
+}
+
+static int cr_add_exe_file_to_shared_table(struct task_struct *task)
+{
+	int r = 0;
+
+#ifdef CONFIG_PROC_FS
+	r = cr_add_file_to_shared_table(task, -1, task->mm->exe_file, 0);
+	if (r == -ENOKEY) /* the file was already in the list */
+		r = 0;
+#endif
+
+	return r;
+}
+
+static int cr_export_later_mm_struct(struct epm_action *action,
+				     ghost_t *ghost,
+				     struct task_struct *task)
+{
+	int r;
+	long key;
+
+	BUG_ON(action->type != EPM_CHECKPOINT);
+	BUG_ON(action->checkpoint.shared != CR_SAVE_LATER);
+
+	key = (long)(task->mm);
+
+	r = ghost_write(ghost, &key, sizeof(long));
+	if (r)
+		goto exit;
+
+	r = add_to_shared_objects_list(task->application,
+				       MM_STRUCT, key, LOCAL_ONLY, task,
+				       NULL, 0);
+
+	if (r == -ENOKEY) { /* the mm_struct was already in the list */
+		r = 0;
+		goto exit;
+	}
+
+	r = cr_add_exe_file_to_shared_table(task);
+	if (r)
+		goto exit;
+
+	r = cr_add_vmas_files_to_shared_table(task);
+
+exit:
+	return r;
+}
+
+
+
+static inline int do_export_mm_struct(struct epm_action *action,
+				      ghost_t *ghost,
+				      struct mm_struct *mm)
+{
+	int r;
+
+	switch (action->type) {
+	  case EPM_CHECKPOINT:
+		  krg_get_mm(mm->mm_id);
+		  r = ghost_write(ghost, mm, sizeof(struct mm_struct));
+		  krg_put_mm(mm->mm_id);
+		  break;
+
+	  default:
+		  r = ghost_write(ghost, &mm->mm_id, sizeof(unique_id_t));
+	}
+
+	return r;
+}
+
+
+
+/** This function exports the virtual memory of a process
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where VM data should be stored.
+ *  @param tsk    Task to export memory data from.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int export_mm_struct(struct epm_action *action,
+		     ghost_t *ghost,
+		     struct task_struct *tsk)
+{
+	struct mm_struct *mm, *exported_mm;
+	int r = 0;
+
+	mm = tsk->mm;
+	exported_mm = mm;
+
+	switch (action->type) {
+	  case EPM_CHECKPOINT:
+		  if (action->checkpoint.shared == CR_SAVE_LATER) {
+			  r = cr_export_later_mm_struct(action, ghost, tsk);
+			  return r;
+		  }
+		  break;
+
+	  case EPM_REMOTE_CLONE:
+		  if (!(action->remote_clone.clone_flags & CLONE_VM)) {
+
+			  exported_mm = krg_dup_mm(tsk, mm);
+			  if (IS_ERR(exported_mm))
+				  return PTR_ERR(exported_mm);
+
+			  break;
+		  }
+		  /* else fall through */
+
+	  case EPM_MIGRATE:
+		  if (mm->anon_vma_kddm_set == NULL) {
+			  r = init_anon_vma_kddm_set(tsk, mm);
+			  if (r)
+				  goto exit_put_mm;
+		  }
+
+		  break;
+
+	  default:
+		  BUG();
+        }
+
+	/* Check some currently unsupported cases */
+	BUG_ON(mm->core_state);
+	BUG_ON(!hlist_empty(&mm->ioctx_list));
+
+	r = do_export_mm_struct (action, ghost, exported_mm);
+	if (r)
+		goto up_mmap_sem;
+
+	down_read(&mm->mmap_sem);
+	r = export_context_struct(ghost, exported_mm);
+	if (r)
+		goto up_mmap_sem;
+
+#ifdef CONFIG_KRG_DVFS
+	r = export_mm_exe_file(action, ghost, tsk);
+	if (r)
+		goto up_mmap_sem;
+#endif
+
+	r = export_vmas(action, ghost, tsk);
+	if (r)
+		goto up_mmap_sem;
+
+	r = export_mm_counters(action, ghost, mm, exported_mm);
+
+up_mmap_sem:
+	up_read(&mm->mmap_sem);
+	if (r)
+		goto out;
+
+	if (action->type == EPM_CHECKPOINT) {
+		r = cr_export_process_pages(tsk->application, ghost, mm);
+		if (r)
+			goto out;
+	}
+
+out:
+	return r;
+
+exit_put_mm:
+	if (exported_mm != mm)
+		mmput(exported_mm);
+	return r;
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              IMPORT FUNCTIONS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+int cr_import_vma_pages(ghost_t *ghost,
+			struct mm_struct *mm,
+			struct vm_area_struct *vma)
+{
+	void *page_addr;
+	unsigned long address = 0;
+	int nr_pages_received = 0;
+	int nr_pages_sent;
+	int page_excluded;
+	pgd_t *pgd;
+	pgprot_t prot;
+	int r;
+
+	BUG_ON(!vma);
+
+	while (1) {
+		struct page *new_page = NULL;
+		pud_t *pud;
+		pmd_t *pmd;
+		pte_t *pte;
+
+		r = ghost_read(ghost, &address, sizeof(unsigned long));
+		if (r)
+			goto err_read;
+
+		if (address == 0)   /* We have reach the last VMA Page. */
+			break;
+
+		r = ghost_read(ghost, &prot, sizeof(pgprot_t));
+		if (r)
+			goto err_read;
+
+		new_page = alloc_page(GFP_HIGHUSER);
+
+		BUG_ON(!new_page);
+
+		pgd = pgd_offset(mm, address);
+		pud = pud_alloc(mm, pgd, address);
+		pmd = pmd_alloc(mm, pud, address);
+		BUG_ON(!pmd);
+
+		pte = pte_alloc_map(mm, pmd, address);
+		BUG_ON(!pte);
+		set_pte (pte, mk_pte(new_page, prot));
+
+		BUG_ON(unlikely(anon_vma_prepare(vma)));
+
+		page_add_new_anon_rmap(new_page, vma, address);
+
+		page_addr = kmap(new_page);
+
+		r = ghost_read_type(ghost, page_excluded);
+		if (r)
+			goto err_read;
+
+		if (!page_excluded) {
+			r = ghost_read (ghost, page_addr, PAGE_SIZE);
+			if (r)
+				goto err_read;
+		}
+
+		nr_pages_received++;
+
+		kunmap(new_page);
+	}
+
+	r = ghost_read(ghost, &nr_pages_sent, sizeof (int));
+
+	BUG_ON(nr_pages_sent != nr_pages_received);
+
+err_read:
+	return r;
+}
+
+/** This function imports the physical memory pages of a process
+ *  @author Renaud Lottiaux, Matthieu Fertré
+ *
+ *  @param ghost       Ghost where pages should be read from.
+ *  @param mm          mm_struct to import memory pages in.
+ *  @param incremental Tell whether or not the checkpoint is an incremental one
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int cr_import_process_pages(struct epm_action *action,
+			    ghost_t *ghost,
+			    struct mm_struct *mm)
+{
+	struct vm_area_struct *vma;
+	int r = 0;
+
+	BUG_ON(!mm);
+
+	vma = mm->mmap;
+	BUG_ON(!vma);
+
+	while (vma) {
+
+		if (vma->vm_ops != &special_mapping_vmops) {
+			r = cr_import_vma_pages(ghost, mm, vma);
+			if (r)
+				goto exit;
+		}
+		vma = vma->vm_next;
+	}
+
+	{
+		int magic;
+
+		r = ghost_read(ghost, &magic, sizeof(int));
+		BUG_ON(!r && magic != 962134);
+	}
+exit:
+	return r;
+}
+
+
+
+static inline void unmap_hole (struct mm_struct *mm,
+			       unsigned long start,
+			       unsigned long end)
+{
+	unsigned long total_vm, locked_vm;
+
+	total_vm = mm->total_vm;
+	locked_vm = mm->locked_vm;
+	do_munmap (mm, start, end - start);
+	mm->total_vm = total_vm;
+	mm->locked_vm = locked_vm;
+}
+
+
+
+int reconcile_vmas(struct mm_struct *mm, struct vm_area_struct *vma,
+		   unsigned long *last_end)
+{
+	struct vm_area_struct *old;
+	int had_anon_vma = 0, r = 0;
+
+	/* If the is a hole between the last imported VMA and the current one,
+	 * unmap every in between.
+	 */
+	if (vma->vm_start != *last_end) {
+		/// TODO: remove this deprecated code
+		unmap_hole (mm, *last_end, vma->vm_start);
+	}
+
+	if (vma->anon_vma) {
+		had_anon_vma = 1;
+		vma->anon_vma = NULL;
+	}
+
+	old = find_vma(mm, vma->vm_start);
+
+	/* Easy case: no conflict with existing VMA, just map the new VMA */
+	if (!old || (old->vm_start >= vma->vm_end)) {
+		r = insert_vm_struct (mm, vma);
+		if (had_anon_vma)
+			anon_vma_prepare(vma);
+		goto done;
+	}
+
+#ifdef CONFIG_KRG_DEBUG
+	/* Paranoia checks */
+	BUG_ON ((old->vm_start != vma->vm_start) ||
+		(old->vm_end != vma->vm_end));
+	BUG_ON (old->vm_flags != vma->vm_flags);
+	BUG_ON (old->vm_ops != vma->vm_ops);
+	BUG_ON (old->vm_file && !vma->vm_file);
+	BUG_ON (old->vm_file && vma->vm_file &&
+		(old->vm_file->f_dentry != vma->vm_file->f_dentry));
+	BUG_ON ((old->vm_pgoff != vma->vm_pgoff) && vma->vm_file);
+#endif
+	if (vma->vm_file && !old->vm_file) {
+		struct file *file = vma->vm_file;
+
+		get_file(file);
+		old->vm_file = file;
+		r = file->f_op->mmap(file, vma);
+                if (r)
+			goto err;
+                if (vma->vm_flags & VM_EXECUTABLE)
+                        added_exe_file_vma(mm);
+		old->initial_vm_ops = vma->initial_vm_ops;
+		anon_vma_lock(old);
+		__vma_link_file(old);
+		anon_vma_unlock(old);
+	}
+
+	remove_vma(vma);
+
+	vma = old;
+done:
+	*last_end = vma->vm_end;
+err:
+	return r;
+}
+
+
+
+/** Import one VMA from the ghost.
+ *  @author  Geoffroy Vallee, Renaud Lottiaux
+ *
+ *  @param ghost    Ghost where data are be stored.
+ *  @param tsk      The task to import the VMA to.
+ *  @param vma      The VMA to import.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+static int import_one_vma (struct epm_action *action,
+			   ghost_t *ghost,
+                           struct task_struct *tsk,
+			   unsigned long *last_end,
+			   hashtable_t *file_table)
+{
+	struct vm_area_struct *vma;
+	krgsyms_val_t vm_ops_type, initial_vm_ops_type;
+	int r;
+
+	vma = kmem_cache_alloc (vm_area_cachep, GFP_KERNEL);
+	if (!vma)
+		return -ENOMEM;
+
+	/* Import the vm_area_struct */
+	r = ghost_read (ghost, vma, sizeof (struct vm_area_struct));
+	if (r)
+		goto err_vma;
+
+	partial_init_vma(tsk->mm, vma);
+
+#ifdef CONFIG_KRG_DVFS
+	/* Import the associated file */
+	r = import_vma_file (action, ghost, tsk, vma, file_table);
+	if (r)
+		goto err_vma;
+#endif
+
+	/* Import the vm_ops type of the vma */
+	r = ghost_read (ghost, &vm_ops_type, sizeof (krgsyms_val_t));
+	if (r)
+		goto err_vm_ops;
+	r = ghost_read (ghost, &initial_vm_ops_type, sizeof (krgsyms_val_t));
+	if (r)
+		goto err_vm_ops;
+
+	vma->vm_ops = krgsyms_import (vm_ops_type);
+	vma->initial_vm_ops = krgsyms_import (initial_vm_ops_type);
+
+	BUG_ON (vma->vm_ops == &generic_file_vm_ops && vma->vm_file == NULL);
+
+	if (action->type == EPM_REMOTE_CLONE
+	    && !(action->remote_clone.clone_flags & CLONE_VM)) {
+		check_link_vma_to_anon_memory_kddm_set (vma);
+		vma->vm_flags &= ~VM_LOCKED;
+	}
+
+	if (action->type == EPM_CHECKPOINT)
+		restore_initial_vm_ops(vma);
+
+	if (vm_ops_type == KRGSYMS_VM_OPS_SPECIAL_MAPPING)
+		import_vdso_context(vma);
+
+	if (vma->vm_flags & VM_EXECUTABLE)
+		added_exe_file_vma(vma->vm_mm);
+	r = reconcile_vmas(tsk->mm, vma, last_end);
+	if (r)
+		goto err_reconcile;
+
+exit:
+	return r;
+
+err_reconcile:
+	if (vma->vm_flags & VM_EXECUTABLE)
+		removed_exe_file_vma(vma->vm_mm);
+err_vm_ops:
+#ifdef CONFIG_KRG_DVFS
+	if (vma->vm_file)
+		fput(vma->vm_file);
+#endif
+err_vma:
+	kmem_cache_free(vm_area_cachep, vma);
+	goto exit;
+}
+
+
+static void file_table_fput(void *_file, void *data)
+{
+	struct file *file = _file;
+
+	fput(file);
+}
+
+
+/** This function imports the list of VMA from the ghost
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where file data should be stored.
+ *  @param tsk    Task to import vma data to.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+static int import_vmas (struct epm_action *action,
+			ghost_t *ghost,
+			struct task_struct *tsk)
+{
+	unsigned long last_end = 0;
+	hashtable_t *file_table;
+	struct mm_struct *mm;
+	int nr_vma = -1;
+	int i, r;
+
+	BUG_ON (tsk == NULL);
+
+	file_table = hashtable_new (FILE_TABLE_SIZE);
+	if (!file_table)
+		return -ENOMEM;
+
+	mm = tsk->mm;
+
+	r = ghost_read(ghost, &nr_vma, sizeof(int));
+	if (r)
+		goto exit;
+
+	for (i = 0; i < nr_vma; i++) {
+		r = import_one_vma (action, ghost, tsk, &last_end, file_table);
+		if (r)
+			/* import_mm_struct will cleanup */
+			goto exit;
+	}
+
+	if (last_end != TASK_SIZE)
+		unmap_hole (mm, last_end, TASK_SIZE);
+
+	flush_tlb_all ();
+
+	{
+		int magic = 0;
+
+		r = ghost_read(ghost, &magic, sizeof(int));
+		BUG_ON (!r && magic != 650874);
+	}
+
+exit:
+
+	__hashtable_foreach_data(file_table, file_table_fput, NULL);
+
+	hashtable_free(file_table);
+
+	return r;
+}
+
+
+
+/** This function imports the context structure of a process
+ *  @author Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where data are stored.
+ *  @param mm     MM context to import data in.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+static int import_context_struct(ghost_t * ghost, struct mm_struct *mm)
+{
+	int r = 0;
+
+#ifndef CONFIG_USERMODE
+
+	if (mm->context.ldt) {
+		int orig_size = mm->context.size;
+
+		mm->context.ldt = NULL;
+		mm->context.size = 0;
+
+		r = alloc_ldt (&mm->context, orig_size, 0);
+		if (r < 0)
+			return r;
+
+		r = ghost_read(ghost, mm->context.ldt,
+			       mm->context.size * LDT_ENTRY_SIZE);
+		if (r)
+			goto exit;
+	}
+
+	mutex_init(&mm->context.lock);
+#endif
+exit:
+	return r;
+}
+
+static int import_mm_counters(struct epm_action *action,
+			      ghost_t *ghost,
+			      struct mm_struct* mm)
+{
+	struct mm_struct *src_mm;
+	int r;
+
+	r = -ENOMEM;
+	src_mm = allocate_mm();
+	if (!src_mm)
+		goto err;
+
+	r = ghost_read(ghost, src_mm, sizeof(struct mm_struct));
+	if (r)
+		goto out_free_mm;
+
+	mm->mmap_base = src_mm->mmap_base;
+	mm->task_size = src_mm->task_size;
+	mm->def_flags = src_mm->def_flags;
+	mm->start_code = src_mm->start_code;
+	mm->end_code = src_mm->end_code;
+	mm->start_data = src_mm->start_data;
+	mm->end_data = src_mm->end_data;
+	mm->start_brk = src_mm->start_brk;
+	mm->start_stack = src_mm->start_stack;
+	mm->arg_start = src_mm->arg_start;
+	mm->arg_end = src_mm->arg_end;
+	mm->env_start = src_mm->env_start;
+	mm->env_end = src_mm->env_end;
+	mm->cached_hole_size = src_mm->cached_hole_size;
+	mm->free_area_cache = src_mm->free_area_cache;
+	mm->hiwater_rss = src_mm->hiwater_rss;
+	mm->hiwater_vm = src_mm->hiwater_vm;
+	mm->total_vm = src_mm->total_vm;
+	mm->locked_vm = src_mm->locked_vm;
+	mm->shared_vm = src_mm->shared_vm;
+	mm->exec_vm = src_mm->exec_vm;
+	mm->stack_vm = src_mm->stack_vm;
+	mm->reserved_vm = src_mm->reserved_vm;
+	mm->brk = src_mm->brk;
+	mm->flags = src_mm->flags;
+
+	r = ghost_read(ghost, &mm->mm_tasks, sizeof(atomic_t));
+
+out_free_mm:
+	free_mm(src_mm);
+err:
+	return r;
+}
+
+static int cr_link_to_mm_struct(struct epm_action *action,
+				ghost_t *ghost,
+				struct task_struct *tsk)
+{
+	int r;
+	long key;
+	struct mm_struct *mm;
+
+	r = ghost_read(ghost, &key, sizeof(long));
+	if (r)
+		goto err;
+
+	mm = get_imported_shared_object(action->restart.app,
+					MM_STRUCT, key);
+
+	if (!mm) {
+		r = -E_CR_BADDATA;
+		goto err;
+	}
+
+        /* the task is not yet hashed, no need to lock */
+	atomic_inc(&mm->mm_users);
+
+	tsk->mm = mm;
+	tsk->active_mm = mm;
+
+	r = import_mm_struct_end(mm, tsk);
+err:
+	return r;
+}
+
+
+
+static inline int do_import_mm_struct(struct epm_action *action,
+				      ghost_t *ghost,
+				      struct mm_struct **returned_mm)
+{
+	struct mm_struct *mm;
+	unique_id_t mm_id;
+	int r = 0;
+
+	switch(action->type) {
+	  case EPM_CHECKPOINT:
+		  mm = allocate_mm();
+		  if (!mm)
+			  goto done;
+
+		  r = ghost_read (ghost, mm, sizeof (struct mm_struct));
+		  if (r)
+			  goto exit_free_mm;
+
+		  r = reinit_mm(mm);
+		  if (r)
+			  goto exit_free_mm;
+
+		  atomic_set(&mm->mm_ltasks, 0);
+		  mm->mm_id = 0;
+		  mm->anon_vma_kddm_set = NULL;
+		  mm->anon_vma_kddm_id = KDDM_SET_UNUSED;
+		  break;
+
+	  default:
+		  r = ghost_read (ghost, &mm_id, sizeof (unique_id_t));
+		  if (r)
+			  return r;
+		  mm = krg_get_mm(mm_id);
+		  if (mm)
+			  /* Reflect the belonging to the ghost task struct */
+			  atomic_inc(&mm->mm_users);
+	}
+
+done:
+	if (!mm)
+		return -ENOMEM;
+
+	*returned_mm = mm;
+
+	return r;
+
+exit_free_mm:
+	free_mm(mm);
+	return r;
+}
+
+
+
+/** This function imports the mm_struct of a process
+ *  @author  Geoffroy Vallee, Renaud Lottiaux
+ *
+ *  @param ghost  Ghost where file data should be loaded from.
+ *  @param tsk    Task to import file data in.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int import_mm_struct (struct epm_action *action,
+		      ghost_t *ghost,
+                      struct task_struct *tsk)
+{
+	struct mm_struct *mm = NULL;
+	struct kddm_set *set;
+	int r;
+
+	if (action->type == EPM_CHECKPOINT
+	    && action->restart.shared == CR_LINK_ONLY) {
+		r = cr_link_to_mm_struct(action, ghost, tsk);
+		return r;
+	}
+
+	r = do_import_mm_struct (action, ghost, &mm);
+	if (r)
+		return r;
+
+	tsk->mm = mm;
+	tsk->active_mm = mm;
+
+	/* Import context */
+	r = import_context_struct(ghost, mm);
+	if (unlikely (r < 0))
+		goto err;
+
+	/* Just paranoia check */
+	BUG_ON(mm->core_state);
+
+#ifdef CONFIG_KRG_DVFS
+	r = import_mm_exe_file(action, ghost, tsk);
+	if (r)
+		goto err;
+#endif
+
+	r = import_vmas (action, ghost, tsk);
+	if (r < 0)
+		goto err;
+
+	r = import_mm_counters(action, ghost, mm);
+	if (r)
+		goto err;
+
+	mm->hiwater_rss = get_mm_rss(mm);
+	mm->hiwater_vm = mm->total_vm;
+
+	if (action->type == EPM_REMOTE_CLONE
+	    && !(action->remote_clone.clone_flags & CLONE_VM))
+		mm->locked_vm = 0;
+
+	if (action->type == EPM_CHECKPOINT)
+		r = cr_import_process_pages(action, ghost, mm);
+	else
+		r = import_mm_struct_end(mm, tsk);
+
+	if (r)
+		goto err;
+
+	set = mm->anon_vma_kddm_set;
+
+	krg_put_mm (mm->mm_id);
+
+	return 0;
+
+err:
+	krg_put_mm (mm->mm_id);
+	unimport_mm_struct(tsk);
+	return r;
+}
+
+
+
+void unimport_mm_struct(struct task_struct *task)
+{
+	free_ghost_mm(task);
+}
+
+
+
+static int cr_export_now_mm_struct(struct epm_action *action, ghost_t *ghost,
+				   struct task_struct *task,
+				   union export_args *args)
+{
+	int r;
+	r = export_mm_struct(action, ghost, task);
+	if (r)
+		ckpt_err(action, r,
+			 "Fail to save struct mm_struct of process %d (%s)",
+			 task_pid_knr(task), task->comm);
+	return r;
+}
+
+
+static int cr_import_now_mm_struct(struct epm_action *action, ghost_t *ghost,
+				   struct task_struct *fake, int local_only,
+				   void **returned_data, size_t *data_size)
+{
+	int r;
+	BUG_ON(*returned_data != NULL);
+
+	r = import_mm_struct(action, ghost, fake);
+	if (r) {
+		ckpt_err(action, r,
+			 "Fail to restore a struct mm_struct",
+			 action->restart.app->app_id);
+		goto err;
+	}
+
+	*returned_data = fake->mm;
+err:
+	return r;
+}
+
+static int cr_import_complete_mm_struct(struct task_struct *fake, void *_mm)
+{
+	struct mm_struct *mm = _mm;
+	mmput(mm);
+
+	return 0;
+}
+
+static int cr_delete_mm_struct(struct task_struct *fake, void *_mm)
+{
+	struct mm_struct *mm = _mm;
+	mmput(mm);
+
+	return 0;
+}
+
+struct shared_object_operations cr_shared_mm_struct_ops = {
+        .export_now         = cr_export_now_mm_struct,
+	.export_user_info   = NULL,
+	.import_now         = cr_import_now_mm_struct,
+	.import_complete    = cr_import_complete_mm_struct,
+	.delete             = cr_delete_mm_struct,
+};
diff -ruN linux-2.6.29/kerrighed/mm/page_table_tree.c android_cluster/linux-2.6.29/kerrighed/mm/page_table_tree.c
--- linux-2.6.29/kerrighed/mm/page_table_tree.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/page_table_tree.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,888 @@
+/** KDDM page table tree management.
+ *  @file page_table_tree.c
+ *
+ *  Copyright (C) 2008, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/mm.h>
+#include <linux/hugetlb.h>
+#include <linux/rmap.h>
+#include <linux/pagemap.h>
+#include <linux/delayacct.h>
+#include <asm/pgtable.h>
+#include <linux/swap.h>
+#include <linux/swapops.h>
+
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+#include <kerrighed/page_table_tree.h>
+
+#include "memory_int_linker.h"
+#include "mm_struct.h"
+#include "vma_struct.h"
+
+/*****************************************************************************/
+/*                                                                           */
+/*                             HELPER FUNCTIONS                              */
+/*                                                                           */
+/*****************************************************************************/
+
+
+
+static inline void unmap_page(struct mm_struct *mm,
+			      unsigned long addr,
+			      struct page *page,
+			      pte_t *ptep)
+{
+	pte_clear(mm, addr, ptep);
+
+	update_hiwater_rss(mm);
+
+	if (PageAnon(page))
+		dec_mm_counter(mm, anon_rss);
+	else
+		dec_mm_counter(mm, file_rss);
+
+	page_remove_rmap(page);
+}
+
+
+
+/* The ZERO_PAGE is considered as a file page but not linked to any file.
+ * Moreover, this page is not linked to any mapping.
+ * Managing this page would introduce too much particular cases.
+ */
+static inline struct page *replace_zero_page(struct mm_struct *mm,
+					     struct vm_area_struct *vma,
+					     struct page *page,
+					     pte_t *ptep,
+					     unsigned long addr)
+{
+	struct page *new_page;
+
+	new_page = alloc_page_vma(GFP_HIGHUSER | __GFP_ZERO, vma, addr);
+	if (!new_page)
+		return NULL;
+
+	BUG_ON (TestSetPageLockedKDDM(new_page));
+
+	unmap_page (mm, addr, page, ptep);
+
+	set_pte (ptep, mk_pte (new_page, vma->vm_page_prot));
+	page_add_anon_rmap(new_page, vma, addr);
+	inc_mm_counter(mm, anon_rss);
+
+	return new_page;
+}
+
+
+static inline struct kddm_obj *init_pte_alloc_obj_entry(struct kddm_set *set,
+						objid_t objid,
+						struct kddm_obj **_obj_entry)
+{
+	struct kddm_obj *obj_entry;
+
+	if (!*_obj_entry) {
+		obj_entry = alloc_kddm_obj_entry(set, objid);
+		if (!obj_entry)
+			BUG();
+	}
+	else {
+		obj_entry = *_obj_entry;
+		*_obj_entry = NULL;
+		change_prob_owner(obj_entry,
+				  kddm_io_default_owner(set, objid));
+	}
+
+	return obj_entry;
+}
+
+
+static inline struct kddm_obj *init_swap_pte(struct mm_struct *mm,
+					     pte_t *ptep,
+					     struct kddm_set *set,
+					     objid_t objid,
+					     struct kddm_obj *_obj_entry)
+{
+	struct kddm_obj *obj_entry;
+	swp_entry_t entry;
+
+	if (pte_none(*ptep))
+		return _obj_entry;
+
+	if (pte_obj_entry(ptep)) {
+		obj_entry = get_obj_entry_from_pte(mm, objid * PAGE_SIZE,
+						   ptep, NULL);
+		atomic_inc(&obj_entry->count);
+		BUG_ON(obj_entry_count(obj_entry) == 1);
+		return _obj_entry;
+	}
+
+	/* pte_file not yet supported */
+	BUG_ON (pte_file(*ptep));
+
+	/* OK, we have a swap entry. */
+	entry = pte_to_swp_entry(*ptep);
+
+	/* Migration entries not yet supported */
+	BUG_ON(is_migration_entry(entry));
+
+	obj_entry = init_pte_alloc_obj_entry(set, objid, &_obj_entry);
+
+	/* Set the first bit in order to distinguish pages from swap ptes */
+	obj_entry->object = (void *) mk_swap_pte_page(ptep);
+	kddm_change_obj_state(set, obj_entry, objid, WRITE_OWNER);
+
+	set_swap_pte_obj_entry(ptep, obj_entry);
+
+	return _obj_entry;
+}
+
+static inline struct kddm_obj *init_pte(struct mm_struct *mm,
+					pte_t *ptep,
+					struct kddm_set *set,
+					objid_t objid,
+					struct vm_area_struct *vma,
+					struct kddm_obj *_obj_entry)
+{
+	struct page *page = NULL, *new_page;
+	unsigned long addr = objid * PAGE_SIZE;
+	struct kddm_obj *obj_entry;
+
+	if (!pte_present(*ptep))
+		return init_swap_pte(mm, ptep, set, objid, _obj_entry);
+
+	page = pfn_to_page(pte_pfn(*ptep));
+
+	wait_lock_kddm_page(page);
+
+	if (!PageAnon(page)) {
+		if (!(page == ZERO_PAGE(NULL)))
+			goto done;
+		new_page = replace_zero_page(mm, vma, page, ptep, addr);
+		/* new_page is returned locked */
+		unlock_kddm_page(page);
+		page = new_page;
+	}
+
+	atomic_inc (&page->_kddm_count);
+	if (page->obj_entry != NULL) {
+		struct kddm_obj *obj_entry = page->obj_entry;
+		atomic_inc(&obj_entry->count);
+		BUG_ON(obj_entry_count(obj_entry) == 1);
+		goto done;
+	}
+
+	obj_entry = init_pte_alloc_obj_entry(set, objid, &_obj_entry);
+
+	BUG_ON (kddm_io_default_owner(set, objid) != kerrighed_node_id);
+	obj_entry->object = page;
+	ADD_TO_SET(COPYSET(obj_entry), kerrighed_node_id);
+	ADD_TO_SET(RMSET(obj_entry), kerrighed_node_id);
+	kddm_change_obj_state(set, obj_entry, objid, WRITE_OWNER);
+
+	BUG_ON (page->obj_entry != NULL);
+
+	page->obj_entry = obj_entry;
+done:
+	unlock_kddm_page(page);
+
+	return _obj_entry;
+}
+
+
+
+struct kddm_obj *get_obj_entry_from_pte(struct mm_struct *mm,
+					unsigned long addr,
+					pte_t *ptep,
+					struct kddm_obj *new_obj)
+{
+	struct kddm_obj *obj_entry = NULL;
+	struct page *page;
+
+        if (pte_present(*ptep)) {
+		page = pfn_to_page(pte_pfn(*ptep));
+		BUG_ON(!page);
+
+		if (!PageAnon(page)) {
+			if (new_obj) {
+				unmap_page (mm, addr, page, ptep);
+				set_pte_obj_entry(ptep, new_obj);
+			}
+			return new_obj;
+		}
+
+		wait_lock_kddm_page(page);
+
+		if (new_obj) {
+			if (page->obj_entry != NULL)
+				printk ("WARN: entry %p in page %p\n",
+					page->obj_entry, page);
+			if (page->obj_entry == NULL) {
+				atomic_inc(&page->_kddm_count);
+				page->obj_entry = new_obj;
+			}
+		}
+		obj_entry = page->obj_entry;
+		unlock_kddm_page(page);
+	}
+	else {
+		if ((pte_val(*ptep) == 0) && new_obj)
+			set_pte_obj_entry(ptep, new_obj);
+
+		if (pte_obj_entry(ptep))
+			obj_entry = get_pte_obj_entry(ptep);
+	}
+
+	return obj_entry;
+}
+
+
+
+static inline pte_t *kddm_pt_lookup_pte (struct mm_struct *mm,
+					 unsigned long objid,
+					 spinlock_t **ptl)
+{
+	unsigned long address = objid * PAGE_SIZE;
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+
+	pgd = pgd_offset(mm, address);
+	pud = pud_offset(pgd, address);
+	if (pud_none(*pud) || unlikely(pud_bad(*pud)))
+		return NULL;
+
+	pmd = pmd_offset(pud, address);
+	if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))
+		return NULL;
+
+	pte = pte_offset_map_lock(mm, pmd, address, ptl);
+	if (!pte)
+		pte_unmap_unlock(ptep, *ptl);
+
+	return pte;
+}
+
+static inline void __pt_for_each_pte(struct kddm_set *set,
+				     struct mm_struct *mm, pmd_t *pmd,
+				     unsigned long start, unsigned long end,
+				     int(*f)(unsigned long, void*, void*),
+				     void *priv)
+{
+	struct kddm_obj *obj_entry, *new_obj = NULL;
+	unsigned long addr;
+	spinlock_t *ptl;
+	pte_t *ptep;
+
+	/* Pre-allocate obj_entry to avoid allocation when holding
+	 * mm->page_table_lock (gotten by pte_offset_map_lock).
+	 * This lock being taken during page swap, we can face a recursive
+	 * lock if the kernel have to free memory during obj_entry allocaton.
+	 */
+	if (!f)
+		new_obj = alloc_kddm_obj_entry(set, 0);
+
+	ptep = pte_offset_map_lock(mm, pmd, start, &ptl);
+
+	for (addr = start; addr != end; addr += PAGE_SIZE) {
+		if (f) {
+retry:
+			obj_entry = get_obj_entry_from_pte(mm, addr, ptep,
+							   NULL);
+			if (obj_entry &&
+			    TEST_AND_SET_OBJECT_LOCKED (obj_entry)) {
+				while (TEST_OBJECT_LOCKED (obj_entry))
+					cpu_relax();
+				goto retry;
+			}
+			if (obj_entry) {
+				f(addr / PAGE_SIZE, obj_entry, priv);
+				CLEAR_OBJECT_LOCKED (obj_entry);
+			}
+		}
+		else {
+			new_obj = init_pte(mm, ptep, set, addr / PAGE_SIZE,
+			priv,new_obj);
+
+			/* The object has been used, allocate a new one */
+			if (!new_obj) {
+				pte_unmap_unlock(ptep, ptl);
+				new_obj = alloc_kddm_obj_entry(set, 0);
+				ptep = pte_offset_map_lock(mm, pmd, addr,&ptl);
+			}
+		}
+
+		ptep++;
+	}
+	pte_unmap_unlock(ptep - 1, ptl);
+
+	if (new_obj)
+		put_obj_entry_count(set, new_obj, 0);
+}
+
+static inline void __pt_for_each_pmd(struct kddm_set *set,
+				     struct mm_struct *mm, pud_t *pud,
+				     unsigned long start, unsigned long end,
+				     int(*f)(unsigned long, void*, void*),
+				     void *priv)
+{
+	unsigned long addr, next;
+	pmd_t *pmd;
+
+	pmd = pmd_offset(pud, start);
+
+	for (addr = start; addr != end; addr = next) {
+		next = pmd_addr_end(addr, end);
+		if (pmd_present(*pmd))
+			__pt_for_each_pte(set, mm, pmd, addr, next, f, priv);
+		pmd++;
+	}
+}
+
+static inline void __pt_for_each_pud(struct kddm_set *set,
+				     struct mm_struct *mm, pgd_t *pgd,
+				     unsigned long start, unsigned long end,
+				     int(*f)(unsigned long, void*, void*),
+				     void *priv)
+{
+	unsigned long addr, next;
+	pud_t *pud;
+
+	pud = pud_offset(pgd, start);
+
+	for (addr = start; addr != end; addr = next) {
+		next = pud_addr_end(addr, end);
+		if (pud_present(*pud))
+			__pt_for_each_pmd(set, mm, pud, addr, next, f, priv);
+		pud++;
+	}
+}
+
+static void kddm_pt_for_each(struct kddm_set *set, struct mm_struct *mm,
+			     unsigned long start, unsigned long end,
+			     int(*f)(unsigned long, void*, void*),
+			     void *priv)
+{
+	unsigned long addr, next;
+	pgd_t *pgd;
+
+	pgd = pgd_offset(mm, start);
+
+	for (addr = start; addr != end; addr = next) {
+		next = pgd_addr_end(addr, end);
+		if (pgd_present(*pgd))
+			__pt_for_each_pud(set, mm, pgd, addr, next, f, priv);
+		pgd++;
+	}
+}
+
+
+
+int kddm_pt_invalidate (struct kddm_set *set,
+			objid_t objid,
+			struct kddm_obj *obj_entry,
+			struct page *page)
+{
+	struct mm_struct *mm = set->obj_set;
+	unsigned long addr = objid * PAGE_SIZE;
+	spinlock_t *ptl;
+	pte_t *ptep;
+
+	ptep = get_locked_pte(mm, addr, &ptl);
+	if (!ptep)
+		return -ENOMEM;
+
+	if (!pte_present(*ptep))
+		goto done;
+
+	BUG_ON((pfn_to_page(pte_pfn(*ptep)) != NULL) &&
+	       (pfn_to_page(pte_pfn(*ptep)) != page));
+
+	wait_lock_kddm_page(page);
+
+	if (atomic_dec_and_test(&page->_kddm_count))
+		page->obj_entry = NULL;
+
+	unlock_kddm_page(page);
+
+	unmap_page(mm, addr, page, ptep);
+
+	set_pte_obj_entry(ptep, obj_entry);
+
+done:
+	pte_unmap_unlock(ptep, ptl);
+
+	return 0;
+}
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                             KDDM SET OPERATIONS                           */
+/*                                                                           */
+/*****************************************************************************/
+
+
+int kddm_pt_swap_in (struct mm_struct *mm,
+		     unsigned long addr,
+		     pte_t *orig_pte)
+{
+	struct vm_area_struct *vma;
+        pgd_t *pgd;
+        pud_t *pud;
+        pmd_t *pmd;
+        pte_t *pte;
+
+        pgd = pgd_offset(mm, addr);
+        pud = pud_alloc(mm, pgd, addr);
+        pmd = pmd_alloc(mm, pud, addr);
+        pte = pte_alloc_map(mm, pmd, addr);
+
+	vma = find_vma(mm, addr);
+
+	if (!orig_pte)
+		orig_pte = pte;
+
+	return do_swap_page(mm, vma, addr, pte, pmd, 0, *orig_pte);
+}
+
+static inline struct kddm_obj *generic_lookup_obj_entry(struct kddm_set *set,
+							objid_t objid,
+							struct kddm_obj *n_obj,
+							spinlock_t *ptl,
+							pte_t *ptep)
+{
+	struct mm_struct *mm = set->obj_set;
+	unsigned long addr = objid * PAGE_SIZE;
+	struct kddm_obj *obj_entry;
+
+retry:
+	obj_entry = get_obj_entry_from_pte(mm, addr, ptep, n_obj);
+
+	pte_unmap_unlock(ptep, ptl);
+
+	if (!obj_entry)
+		return NULL;
+
+	if (swap_pte_obj_entry(ptep) ||
+	    swap_pte_page((struct page *)obj_entry->object)) {
+		kddm_obj_path_unlock (set, objid);
+		kddm_pt_swap_in(mm, addr, ptep);
+		kddm_obj_path_lock (set, objid);
+
+		ptep = get_locked_pte(mm, addr, &ptl);
+		if (!ptep)
+			return ERR_PTR(-ENOMEM);
+		goto retry;
+	}
+
+	return obj_entry;
+}
+
+static struct kddm_obj *kddm_pt_lookup_obj_entry(struct kddm_set *set,
+						 objid_t objid)
+{
+	struct mm_struct *mm = set->obj_set;
+	spinlock_t *ptl;
+	pte_t *ptep;
+
+	ptep = kddm_pt_lookup_pte (mm, objid, &ptl);
+	if (!ptep)
+		return NULL;
+
+	return generic_lookup_obj_entry(set, objid, NULL, ptl, ptep);
+}
+
+static struct kddm_obj *kddm_pt_get_obj_entry (struct kddm_set *set,
+					       objid_t objid,
+					       struct kddm_obj *new_obj)
+{
+	struct mm_struct *mm = set->obj_set;
+	spinlock_t *ptl;
+	pte_t *ptep;
+
+	ptep = get_locked_pte(mm, objid * PAGE_SIZE, &ptl);
+	if (!ptep)
+		return ERR_PTR(-ENOMEM);
+
+	return generic_lookup_obj_entry(set, objid, new_obj, ptl, ptep);
+}
+
+
+
+static inline void __kddm_pt_insert_object(struct mm_struct *mm,
+					   struct page *page,
+					   unsigned long addr,
+					   pte_t *ptep,
+					   struct kddm_obj *obj_entry)
+{
+	pte_t entry;
+
+	if (page) {
+		entry = mk_pte(page, vm_get_page_prot(VM_READ));
+		set_pte_at(mm, addr, ptep, entry);
+
+		wait_lock_kddm_page(page);
+		BUG_ON (page->obj_entry);
+		page->obj_entry = obj_entry;
+		atomic_inc(&page->_kddm_count);
+		unlock_kddm_page(page);
+
+		inc_mm_counter(mm, anon_rss);
+		__SetPageUptodate(page);
+	}
+	else
+		set_pte_obj_entry(ptep, obj_entry);
+}
+
+
+
+static inline void add_page_anon_rmap (struct mm_struct *mm,
+				       struct page *page,
+				       unsigned long addr)
+{
+	struct vm_area_struct *vma;
+
+	vma = find_vma(mm, addr);
+	BUG_ON(!vma);
+	if ((vma->anon_vma == NULL) && unlikely(anon_vma_prepare(vma)))
+		BUG();
+
+	page_add_new_anon_rmap(page, vma, addr);
+}
+
+
+
+static void kddm_pt_insert_object(struct kddm_set * set,
+				  objid_t objid,
+				  struct kddm_obj *obj_entry)
+{
+	struct mm_struct *mm = set->obj_set;
+	unsigned long addr = objid * PAGE_SIZE;
+	spinlock_t *ptl;
+	pte_t *ptep;
+	struct page *page = obj_entry->object;
+
+	BUG_ON(!page);
+	BUG_ON(page->obj_entry && page->obj_entry != obj_entry);
+
+	/* Insert the object in the page table */
+	ptep = get_locked_pte(mm, addr, &ptl);
+	if (!ptep)
+		BUG();
+
+	__kddm_pt_insert_object (mm, page, addr, ptep, obj_entry);
+
+	pte_unmap_unlock(ptep, ptl);
+
+	add_page_anon_rmap (mm, page, addr);
+}
+
+struct kddm_obj *kddm_pt_break_cow_object(struct kddm_set *set,
+				    struct kddm_obj *obj_entry, objid_t objid,
+				    int break_type)
+{
+	struct page *new_page = NULL, *old_page = obj_entry->object;
+	struct mm_struct *mm = set->obj_set;
+	struct kddm_obj *new_obj;
+	unsigned long addr = objid * PAGE_SIZE;
+	spinlock_t *ptl;
+	pte_t *ptep;
+	int count, swap_count = 0;
+
+	if (!old_page)
+		return obj_entry;
+
+	BUG_ON(swap_pte_page(old_page));
+
+	wait_lock_kddm_page(old_page);
+	if (page_kddm_count(old_page) == 0) {
+		unlock_kddm_page(old_page);
+		return obj_entry;
+	}
+
+	BUG_ON(obj_entry_count(obj_entry) == 0);
+	BUG_ON(!TEST_OBJECT_LOCKED(obj_entry));
+
+	if (page_kddm_count(old_page) == 1) {
+		count = page_mapcount(old_page);
+		BUG_ON(count == 0);
+		if (PageSwapCache(old_page))
+			swap_count = page_swapcount(old_page);
+		count += swap_count;
+		if (count == 1) {
+			/* Page not shared, nothing to do */
+			unlock_kddm_page(old_page);
+			return obj_entry;
+		}
+		else {
+			/* Page shared */
+			atomic_dec(&old_page->_kddm_count);
+			old_page->obj_entry = NULL;
+			if (obj_entry_count(obj_entry) != 1) {
+				/* Page shared with another KDDM through the
+				 * swap cache. COW the obj entry. */
+				atomic_dec(&obj_entry->count);
+				new_obj = dup_kddm_obj_entry(obj_entry);
+				CLEAR_OBJECT_LOCKED(obj_entry);
+			}
+			else {
+				/* Page shared with a regular MM, no KDDM COW
+				 * but a regular page COW is needed.
+				 * Reuse the obj entry. */
+				new_obj = obj_entry;
+			}
+			unlock_kddm_page(old_page);
+		}
+	}
+	else {
+		/* Page shared with another KDDM. COW the obj entry */
+		BUG_ON(atomic_dec_and_test(&old_page->_kddm_count));
+		BUG_ON(atomic_dec_and_test(&obj_entry->count));
+		new_obj = dup_kddm_obj_entry(obj_entry);
+		CLEAR_OBJECT_LOCKED(obj_entry);
+		unlock_kddm_page(old_page);
+	}
+
+	if (break_type == KDDM_BREAK_COW_COPY) {
+		new_page = alloc_page (GFP_ATOMIC);
+		if (new_page == NULL)
+			return ERR_PTR(-ENOMEM);
+
+		copy_user_highpage(new_page, old_page, addr, NULL);
+	}
+
+	new_obj->object = new_page;
+
+	SET_OBJECT_LOCKED(new_obj);
+
+	ptep = get_locked_pte(mm, addr, &ptl);
+	BUG_ON (!ptep);
+
+	if (pte_present(*ptep))
+		unmap_page (mm, addr, old_page, ptep);
+	else
+		/* The page has been unmapped while we was doing the copy... */
+		old_page = NULL;
+
+	/* Map the new page in the set mm */
+
+	__kddm_pt_insert_object (mm, new_page, addr, ptep, new_obj);
+
+	pte_unmap_unlock(ptep, ptl);
+
+	if (new_page)
+		add_page_anon_rmap (mm, new_page, addr);
+
+	if (old_page)
+		page_cache_release (old_page);
+
+	return new_obj;
+}
+
+
+
+static void kddm_pt_remove_obj_entry (struct kddm_set *set,
+				      objid_t objid)
+{
+	struct mm_struct *mm = set->obj_set;
+	unsigned long addr = objid * PAGE_SIZE;
+	struct kddm_obj *obj_entry;
+	spinlock_t *ptl = NULL;
+	struct page *page;
+	pte_t *ptep;
+
+	ptep = kddm_pt_lookup_pte (mm, objid, &ptl);
+	if (!ptep)
+		return;
+
+	if (!pte_present(*ptep)) {
+		pte_clear(mm, addr, ptep);
+		goto done;
+	}
+
+	obj_entry = get_obj_entry_from_pte(mm, addr, ptep, NULL);
+	page = obj_entry->object;
+
+	wait_lock_kddm_page(page);
+	if (atomic_dec_and_test(&page->_kddm_count))
+		page->obj_entry = NULL;
+	unlock_kddm_page(page);
+
+	unmap_page(mm, addr, page, ptep);
+done:
+	pte_unmap_unlock(ptep, ptl);
+}
+
+
+
+static void kddm_pt_for_each_obj_entry(struct kddm_set *set,
+				       int(*f)(unsigned long, void *, void*),
+				       void *data)
+{
+	struct mm_struct *mm = set->obj_set;
+
+	BUG_ON(!f);
+
+	spin_lock(&mm->page_table_lock);
+	kddm_pt_for_each(set, mm, 0, PAGE_OFFSET, f, data);
+	spin_unlock(&mm->page_table_lock);
+}
+
+
+
+static void kddm_pt_export (struct rpc_desc* desc, struct kddm_set *set)
+{
+	struct mm_struct *mm = set->obj_set;
+
+	krgnode_set (desc->client, mm->copyset);
+
+	rpc_pack_type(desc, mm->mm_id);
+}
+
+
+
+static void *kddm_pt_import (struct rpc_desc* desc, int *free_data)
+{
+	struct mm_struct *mm = NULL;
+	unique_id_t mm_id;
+
+	rpc_unpack_type (desc, mm_id);
+	*free_data = 0;
+
+	if (mm_id)
+		mm = _kddm_find_object_raw (mm_struct_kddm_set, mm_id);
+
+	return mm;
+}
+
+static inline void init_kddm_pt(struct kddm_set *set,
+				struct mm_struct *mm)
+{
+	struct vm_area_struct *vma;
+
+	if (mm == NULL)
+		return;
+
+	for (vma = mm->mmap; vma != NULL; vma = vma->vm_next) {
+		if (anon_vma(vma))
+			kddm_pt_for_each(set, mm, vma->vm_start, vma->vm_end,
+					 NULL, vma);
+	}
+}
+
+static void *kddm_pt_alloc (struct kddm_set *set, void *_data)
+{
+	struct mm_struct *mm = _data;
+	struct vm_area_struct *vma;
+
+	if (mm == NULL) {
+		mm = alloc_fake_mm(NULL);
+
+		if (!mm)
+			return NULL;
+	}
+	else
+		atomic_inc(&mm->mm_users);
+
+	down_write(&mm->mmap_sem);
+
+	mm->anon_vma_kddm_id = set->id;
+
+	init_kddm_pt(set, mm);
+
+	for (vma = mm->mmap; vma != NULL; vma = vma->vm_next)
+		check_link_vma_to_anon_memory_kddm_set (vma);
+
+	mm->anon_vma_kddm_set = set;
+
+	up_write(&mm->mmap_sem);
+
+	return mm;
+}
+
+
+
+static void kddm_pt_free (void *tree,
+			  int (*f)(unsigned long, void *data, void *priv),
+			  void *priv)
+{
+	struct mm_struct *mm = tree;
+
+	mmput(mm);
+}
+
+
+
+/* Call-back called when mapping a page coming from swap */
+void kcb_fill_pte(struct mm_struct *mm, unsigned long addr, pte_t *ptep)
+{
+	struct vm_area_struct *vma;
+
+	vma = find_vma (mm, addr);
+	BUG_ON ((vma == NULL) || (addr < vma->vm_start));
+
+	init_pte(mm, ptep, mm->anon_vma_kddm_set, addr / PAGE_SIZE, vma, NULL);
+}
+
+/* Call-back called during page table destruction for each valid pte */
+void kcb_zap_pte(struct mm_struct *mm, unsigned long addr, pte_t *ptep)
+{
+	struct kddm_set *set = mm->anon_vma_kddm_set;
+	struct kddm_obj *obj_entry;
+	struct page *page = NULL;
+	objid_t objid = addr / PAGE_SIZE;
+
+	BUG_ON(!set);
+
+	obj_entry = get_obj_entry_from_pte(mm, addr, ptep, NULL);
+
+	if (!obj_entry)
+		return;
+
+	if (pte_obj_entry(ptep)) {
+		if (swap_pte_obj_entry(ptep))
+			page = obj_entry->object;
+		pte_clear(mm, addr, ptep);
+		if (page) {
+			swp_entry_t swp_entry;
+			if (swap_pte_page(page))
+				swp_entry = get_swap_entry_from_page(page);
+			else
+				swp_entry.val = page_private(page);
+			free_swap_and_cache(swp_entry);
+		}
+	}
+	else {
+		page = pfn_to_page(pte_pfn(*ptep));
+		BUG_ON(!page);
+
+		wait_lock_kddm_page(page);
+		if (atomic_dec_and_test(&page->_kddm_count))
+			page->obj_entry = NULL;
+		unlock_kddm_page(page);
+	}
+
+	if (atomic_dec_and_test(&obj_entry->count)) {
+		obj_entry->object = NULL;
+		free_kddm_obj_entry(set, obj_entry, objid);
+	}
+}
+
+
+
+struct kddm_set_ops kddm_pt_set_ops = {
+	obj_set_alloc:       kddm_pt_alloc,
+	obj_set_free:        kddm_pt_free,
+	lookup_obj_entry:    kddm_pt_lookup_obj_entry,
+	get_obj_entry:       kddm_pt_get_obj_entry,
+	insert_object:       kddm_pt_insert_object,
+	break_cow:           kddm_pt_break_cow_object,
+	remove_obj_entry:    kddm_pt_remove_obj_entry,
+	for_each_obj_entry:  kddm_pt_for_each_obj_entry,
+	export:              kddm_pt_export,
+	import:              kddm_pt_import,
+};
diff -ruN linux-2.6.29/kerrighed/mm/vma_struct.c android_cluster/linux-2.6.29/kerrighed/mm/vma_struct.c
--- linux-2.6.29/kerrighed/mm/vma_struct.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/vma_struct.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,50 @@
+/** Distributed management of the VMA structure.
+ *  @file vma_struct.c
+ *
+ *  Copyright (C) 2008-2009, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/mm.h>
+#include <linux/rmap.h>
+
+void partial_init_vma(struct mm_struct *mm, struct vm_area_struct *vma)
+{
+	vma->vm_mm = mm;
+	vma->vm_next = NULL;
+	INIT_LIST_HEAD (&vma->anon_vma_node);
+	vma->vm_truncate_count = 0;
+	memset (&vma->shared, 0, sizeof (vma->shared));
+	memset (&vma->vm_rb, 0, sizeof (vma->vm_rb));
+	vma->vm_private_data = NULL;
+}
+
+
+
+int alloc_fake_vma(struct mm_struct *mm,
+		   unsigned long start,
+		   unsigned long end)
+{
+	struct vm_area_struct *vma;
+	int r = 0;
+
+	vma = kmem_cache_zalloc(vm_area_cachep, GFP_ATOMIC);
+	if (!vma)
+		return -ENOMEM;
+
+	partial_init_vma (mm, vma);
+	vma->vm_start = start;
+	vma->vm_end = end;
+	vma->vm_flags = VM_READ | VM_WRITE | VM_MAYREAD | VM_MAYWRITE |
+		VM_MAYEXEC;
+
+	r = insert_vm_struct (mm, vma);
+	if (unlikely(r))
+		goto err;
+
+	vma->anon_vma = NULL;
+
+	return 0;
+err:
+	kmem_cache_free(vm_area_cachep, vma);
+	return r;
+}
diff -ruN linux-2.6.29/kerrighed/mm/vma_struct.h android_cluster/linux-2.6.29/kerrighed/mm/vma_struct.h
--- linux-2.6.29/kerrighed/mm/vma_struct.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/mm/vma_struct.h	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,18 @@
+/** Distributed management of the VMA structure.
+ *  @file vma_struct.h
+ *
+ *  @author Renaud Lottiaux.
+ */
+
+
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+void partial_init_vma(struct mm_struct *mm, struct vm_area_struct *vma);
+
+int alloc_fake_vma(struct mm_struct *mm, unsigned long start,
+		   unsigned long end);
diff -ruN linux-2.6.29/kerrighed/proc/krg_exit.c android_cluster/linux-2.6.29/kerrighed/proc/krg_exit.c
--- linux-2.6.29/kerrighed/proc/krg_exit.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/proc/krg_exit.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,750 @@
+/*
+ *  kerrighed/proc/krg_exit.c
+ *
+ *  Copyright (C) 2006-2007 Pascal Gallard - Kerlabs, Louis Rilling - Kerlabs
+ */
+
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/personality.h>
+#include <linux/sched.h>
+#include <linux/workqueue.h>
+#include <linux/rcupdate.h>
+#include <kerrighed/task.h>
+#ifdef CONFIG_KRG_EPM
+#include <linux/uaccess.h>
+#include <linux/tracehook.h>
+#include <linux/task_io_accounting_ops.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/children.h>
+#include <kerrighed/signal.h>
+#include <kerrighed/application.h>
+#include <kerrighed/krgnodemask.h>
+#include <asm/cputime.h>
+#endif
+#ifdef CONFIG_KRG_SCHED
+#include <kerrighed/scheduler/info.h>
+#endif
+
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/workqueue.h>
+#endif
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/task.h>
+#include <kerrighed/krg_exit.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/action.h>
+#include <kerrighed/migration.h>
+#endif
+
+#ifdef CONFIG_KRG_EPM
+
+static void delay_release_task_worker(struct work_struct *work);
+static DECLARE_WORK(delay_release_task_work, delay_release_task_worker);
+static LIST_HEAD(tasks_to_release);
+static DEFINE_SPINLOCK(tasks_to_release_lock);
+
+struct notify_parent_request {
+	pid_t parent_pid;
+	unsigned int ptrace;
+	struct siginfo info;
+};
+
+static void handle_do_notify_parent(struct rpc_desc *desc,
+				    void *msg, size_t size)
+{
+	struct notify_parent_request *req = msg;
+	struct task_struct *parent;
+	struct sighand_struct *psig;
+	int sig = req->info.si_signo;
+	int err, ret;
+
+	ret = sig;
+
+	read_lock(&tasklist_lock);
+	parent = find_task_by_kpid(req->parent_pid);
+	BUG_ON(!parent);
+
+	/* Adapted from do_notify_parent() for a remote child */
+
+	psig = parent->sighand;
+	spin_lock_irq(&psig->siglock);
+	if (!req->ptrace && sig == SIGCHLD &&
+	    (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN ||
+	     (psig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT))) {
+		/*
+		 * We are exiting and our parent doesn't care.  POSIX.1
+		 * defines special semantics for setting SIGCHLD to SIG_IGN
+		 * or setting the SA_NOCLDWAIT flag: we should be reaped
+		 * automatically and not left for our parent's wait4 call.
+		 * Rather than having the parent do it as a magic kind of
+		 * signal handler, we just set this to tell do_exit that we
+		 * can be cleaned up without becoming a zombie.  Note that
+		 * we still call __wake_up_parent in this case, because a
+		 * blocked sys_wait4 might now return -ECHILD.
+		 *
+		 * Whether we send SIGCHLD or not for SA_NOCLDWAIT
+		 * is implementation-defined: we do (if you don't want
+		 * it, just use SIG_IGN instead).
+		 */
+		ret = -1;
+		if (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN)
+			sig = -1;
+	}
+	if (valid_signal(sig) && sig > 0)
+		__krg_group_send_sig_info(sig, &req->info, parent);
+	wake_up_interruptible_sync(&parent->signal->wait_chldexit);
+	spin_unlock_irq(&psig->siglock);
+
+	read_unlock(&tasklist_lock);
+
+	err = rpc_pack_type(desc, ret);
+	if (err)
+		rpc_cancel(desc);
+}
+
+/*
+ * Expects task->task_obj locked and up to date regarding parent and
+ * parent_node
+ */
+int krg_do_notify_parent(struct task_struct *task, struct siginfo *info)
+{
+	struct notify_parent_request req;
+	int ret;
+	kerrighed_node_t parent_node = task->task_obj->parent_node;
+	struct rpc_desc *desc;
+	int err = -ENOMEM;
+
+	BUG_ON(task->parent != baby_sitter);
+	BUG_ON(parent_node == KERRIGHED_NODE_ID_NONE);
+	BUG_ON(parent_node == kerrighed_node_id);
+
+	req.parent_pid = task->task_obj->parent;
+	req.ptrace = task->ptrace;
+	req.info = *info;
+
+	desc = rpc_begin(PROC_DO_NOTIFY_PARENT, parent_node);
+	if (!desc)
+		goto err;
+	err = rpc_pack_type(desc, req);
+	if (err)
+		goto err_cancel;
+	err = rpc_unpack_type(desc, ret);
+	if (err)
+		goto err_cancel;
+	rpc_end(desc, 0);
+
+out:
+	if (!err)
+		return ret;
+	return 0;
+
+err_cancel:
+	rpc_cancel(desc);
+	rpc_end(desc, 0);
+err:
+	printk(KERN_ERR "error: child %d cannot notify remote parent %d\n",
+	       task_pid_knr(task), req.parent_pid);
+	goto out;
+}
+
+/*
+ * If return value is not NULL, all variables are set, and the children kddm
+ * object will have to be unlocked with krg_children_unlock(@return),
+ * and parent pid location will have to be unlocked with
+ * krg_unlock_pid_location(*parent_pid_p)
+ *
+ * If return value is NULL, parent has no children kddm object. It is up to the
+ * caller to know whether original parent died or is still alive and never had a
+ * children kddm object.
+ */
+static
+struct children_kddm_object *
+parent_children_writelock_pid_location_lock(struct task_struct *task,
+					    pid_t *real_parent_tgid_p,
+					    pid_t *real_parent_pid_p,
+					    pid_t *parent_pid_p,
+					    kerrighed_node_t *parent_node_p)
+{
+	struct children_kddm_object *children_obj;
+	pid_t real_parent_tgid;
+	pid_t real_parent_pid;
+	pid_t parent_pid;
+	struct task_kddm_object *obj;
+	kerrighed_node_t parent_node = KERRIGHED_NODE_ID_NONE;
+	struct timespec backoff_time = {
+		.tv_sec = 1,
+		.tv_nsec = 0
+	};	/* 1 second */
+
+	/*
+	 * Similar to krg_lock_pid_location but we need to acquire
+	 * parent_children_writelock at the same time without deadlocking with
+	 * migration
+	 */
+	for (;;) {
+		children_obj = krg_parent_children_writelock(task,
+							     &real_parent_tgid);
+		if (!children_obj)
+			break;
+		krg_get_parent(children_obj, task,
+			       &parent_pid, &real_parent_pid);
+		obj = krg_task_readlock(parent_pid);
+		BUG_ON(!obj);
+		parent_node = obj->node;
+		if (parent_node != KERRIGHED_NODE_ID_NONE)
+			break;
+		krg_task_unlock(parent_pid);
+		krg_children_unlock(children_obj);
+
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		schedule_timeout(timespec_to_jiffies(&backoff_time) + 1);
+	}
+	BUG_ON(children_obj && parent_node == KERRIGHED_NODE_ID_NONE);
+
+	/*
+	 * If children_obj is not NULL, then children_obj is write-locked and
+	 * obj is read-locked,
+	 * otherwise none is locked.
+	 */
+	if (children_obj) {
+		*real_parent_tgid_p = real_parent_tgid;
+		*real_parent_pid_p = real_parent_pid;
+		*parent_pid_p = parent_pid;
+		*parent_node_p = parent_node;
+	}
+	return children_obj;
+}
+
+int krg_delayed_notify_parent(struct task_struct *leader)
+{
+	struct children_kddm_object *parent_children_obj;
+	pid_t real_parent_tgid;
+	pid_t parent_pid, real_parent_pid;
+	kerrighed_node_t parent_node;
+	int zap_leader;
+
+	parent_children_obj = parent_children_writelock_pid_location_lock(
+				leader,
+				&real_parent_tgid,
+				&real_parent_pid,
+				&parent_pid,
+				&parent_node);
+	__krg_task_writelock_nested(leader);
+
+	write_lock_irq(&tasklist_lock);
+	BUG_ON(task_detached(leader));
+	/*
+	 * Needed to check whether we were reparented to init, and to
+	 * know which task to notify in case parent is still remote
+	 */
+	if (parent_children_obj) {
+		/* Make sure that task_obj is up to date */
+		krg_update_parents(leader, parent_pid, real_parent_pid);
+		leader->task_obj->parent_node = parent_node;
+	} else if (leader->real_parent == baby_sitter
+		   || leader->parent == baby_sitter) {
+		/* Real parent died and let us reparent leader to local init. */
+		krg_reparent_to_local_child_reaper(leader);
+	}
+
+	do_notify_parent(leader, leader->exit_signal);
+
+	zap_leader = task_detached(leader);
+	if (zap_leader)
+		leader->exit_state = EXIT_DEAD;
+	leader->flags &= ~PF_DELAY_NOTIFY;
+	write_unlock_irq(&tasklist_lock);
+
+	__krg_task_unlock(leader);
+	if (parent_children_obj) {
+		krg_unlock_pid_location(parent_pid);
+		if (zap_leader)
+			/*
+			 * Parent was not interested by notification,
+			 * but may have been woken up in do_wait and
+			 * should not see leader as a child
+			 * anymore. Remove leader from its children kddm
+			 * object before parent can access it again.
+			 */
+			krg_remove_child(parent_children_obj, leader);
+		krg_children_unlock(parent_children_obj);
+	}
+
+	return zap_leader;
+}
+
+struct wait_task_request {
+	pid_t pid;
+	pid_t real_parent_tgid;
+	int options;
+};
+
+struct wait_task_result {
+	struct siginfo info;
+	int status;
+	struct rusage ru;
+	cputime_t cutime, cstime, cgtime;
+	unsigned long cmin_flt, cmaj_flt;
+	unsigned long cnvcsw, cnivcsw;
+	unsigned long cinblock, coublock;
+	struct task_io_accounting ioac;
+};
+
+static void handle_wait_task_zombie(struct rpc_desc *desc,
+				    void *_msg, size_t size)
+{
+	struct wait_task_request *req = _msg;
+	struct task_struct *p;
+	struct signal_struct *sig;
+	struct task_cputime cputime;
+	struct wait_task_result res;
+	int retval;
+	int err = -ENOMEM;
+
+	read_lock(&tasklist_lock);
+	p = find_task_by_kpid(req->pid);
+	/*
+	 * Child could be reaped by a another (parent's or child's) thread,
+	 * and its pid could be even already reused.
+	 * Also do not try to reap now if group leader having not-fully-released
+	 * sub-threads.
+	 */
+	if (!p
+	    || !p->task_obj
+	    || p->task_obj->real_parent_tgid != req->real_parent_tgid
+	    || delay_group_leader(p)) {
+		read_unlock(&tasklist_lock);
+		retval = 0;
+		goto out_send_res;
+	}
+
+	/*
+	 * Sample resource counters now since wait_task_zombie() may release p.
+	 */
+	if (!(req->options & WNOWAIT)) {
+		sig = p->signal;
+
+		thread_group_cputime(p, &cputime);
+		res.cutime = cputime_add(cputime.utime, sig->cutime);
+		res.cstime = cputime_add(cputime.stime, sig->cstime);
+		res.cgtime = cputime_add(p->gtime,
+					 cputime_add(sig->gtime, sig->cgtime));
+		res.cmin_flt = p->min_flt + sig->min_flt + sig->cmin_flt;
+		res.cmaj_flt = p->maj_flt + sig->maj_flt + sig->cmaj_flt;
+		res.cnvcsw = p->nvcsw + sig->nvcsw + sig->cnvcsw;
+		res.cnivcsw = p->nivcsw + sig->nivcsw + sig->cnivcsw;
+		res.cinblock = task_io_get_inblock(p) +
+				sig->inblock + sig->cinblock;
+		res.coublock = task_io_get_oublock(p) +
+				sig->oublock + sig->coublock;
+		res.ioac = p->ioac;
+		task_io_accounting_add(&res.ioac, &sig->ioac);
+	}
+	retval = wait_task_zombie(p, req->options,
+				  &res.info,
+				  &res.status, &res.ru);
+	if (!retval)
+		read_unlock(&tasklist_lock);
+
+out_send_res:
+	err = rpc_pack_type(desc, retval);
+	if (err)
+		goto err_cancel;
+	if (retval) {
+		BUG_ON(retval < 0);
+		err = rpc_pack_type(desc, res);
+		if (err)
+			goto err_cancel;
+	}
+
+	return;
+
+err_cancel:
+	rpc_cancel(desc);
+}
+
+int krg_wait_task_zombie(pid_t pid, kerrighed_node_t zombie_location,
+			 int options,
+			 struct siginfo __user *infop,
+			 int __user *stat_addr, struct rusage __user *ru)
+{
+	struct wait_task_request req;
+	int retval;
+	struct wait_task_result res;
+	struct rpc_desc *desc;
+	bool noreap = options & WNOWAIT;
+	int err;
+
+	/*
+	 * Zombie's location does not need to remain locked since it won't
+	 * change afterwards, but this will be needed to support hot removal of
+	 * nodes with zombie migration.
+	 */
+	BUG_ON(!krgnode_online(zombie_location));
+
+	desc = rpc_begin(PROC_WAIT_TASK_ZOMBIE, zombie_location);
+	if (!desc)
+		return -ENOMEM;
+
+	req.pid = pid;
+	/* True as long as no remote ptrace is allowed */
+	req.real_parent_tgid = task_tgid_knr(current);
+	req.options = options;
+	err = rpc_pack_type(desc, req);
+	if (err)
+		goto err_cancel;
+
+	err = rpc_unpack_type(desc, retval);
+	if (err)
+		goto err_cancel;
+	if (retval) {
+		BUG_ON(retval < 0);
+		err = rpc_unpack_type(desc, res);
+		if (err)
+			goto err_cancel;
+
+		if (likely(!noreap)) {
+			struct signal_struct *psig;
+
+			spin_lock_irq(&current->sighand->siglock);
+			psig = current->signal;
+			psig->cutime = cputime_add(psig->cutime,
+						   res.cutime);
+			psig->cstime = cputime_add(psig->cstime,
+						   res.cstime);
+			psig->cgtime = cputime_add(psig->cgtime,
+						   res.cgtime);
+			psig->cmin_flt += res.cmin_flt;
+			psig->cmaj_flt += res.cmaj_flt;
+			psig->cnvcsw += res.cnvcsw;
+			psig->cnivcsw += res.cnivcsw;
+			psig->cinblock += res.cinblock;
+			psig->coublock += res.coublock;
+			task_io_accounting_add(&psig->ioac, &res.ioac);
+			spin_unlock_irq(&current->sighand->siglock);
+		}
+
+		retval = 0;
+		if (ru)
+			retval = copy_to_user(ru, &res.ru, sizeof(res.ru)) ?
+				-EFAULT : 0;
+		if (!retval && stat_addr && likely(!noreap))
+			retval = put_user(res.status, stat_addr);
+		if (!retval && infop) {
+			retval = put_user(res.info.si_signo, &infop->si_signo);
+			if (!retval)
+				retval = put_user(res.info.si_errno,
+						  &infop->si_errno);
+			if (!retval)
+				retval = put_user(res.info.si_code,
+						  &infop->si_code);
+			if (!retval)
+				retval = put_user(res.info.si_status,
+						  &infop->si_status);
+			if (!retval)
+				retval = put_user(res.info.si_pid,
+						  &infop->si_pid);
+			if (!retval)
+				retval = put_user(res.info.si_uid,
+						  &infop->si_uid);
+		}
+		if (!retval)
+			retval = pid;
+	}
+out:
+	rpc_end(desc, 0);
+
+	return retval;
+
+err_cancel:
+	rpc_cancel(desc);
+	if (err > 0)
+		err = -EPIPE;
+	retval = err;
+	goto out;
+}
+
+struct children_kddm_object *
+krg_prepare_exit_ptrace_task(struct task_struct *tracer,
+			     struct task_struct *task)
+{
+	struct children_kddm_object *obj;
+	pid_t real_parent_tgid, real_parent_pid, parent_pid;
+	kerrighed_node_t parent_node;
+
+	/* Prepare a call to do_notify_parent() in __ptrace_detach() */
+
+	/*
+	 * Note: real parent should be locked, not parent. However the children
+	 * object only records real parent, so it's ok.
+	 */
+	obj = rcu_dereference(task->parent_children_obj);
+	if (obj)
+		obj = parent_children_writelock_pid_location_lock(
+			task,
+			&real_parent_tgid,
+			&real_parent_pid,
+			&parent_pid,
+			&parent_node);
+	if (obj)
+		__krg_task_writelock_nested(task);
+	else
+		__krg_task_writelock(task);
+
+	krg_set_child_ptraced(obj, task, 0);
+
+	write_lock_irq(&tasklist_lock);
+	BUG_ON(!task->ptrace);
+
+	if (obj && task->task_obj) {
+		krg_update_parents(task, parent_pid, real_parent_pid);
+		task->task_obj->parent_node = parent_node;
+	} else if (!obj && task->real_parent == baby_sitter) {
+		krg_reparent_to_local_child_reaper(task);
+	}
+
+	return obj;
+}
+
+void krg_finish_exit_ptrace_task(struct task_struct *task,
+				 struct children_kddm_object *obj,
+				 bool dead)
+{
+	pid_t parent_pid;
+
+	if (task->real_parent == baby_sitter)
+		parent_pid = task->task_obj->parent;
+	else
+		parent_pid = task_pid_knr(task->real_parent);
+
+	write_unlock_irq(&tasklist_lock);
+
+	if (obj) {
+		krg_unlock_pid_location(parent_pid);
+		if (dead)
+			krg_remove_child(obj, task);
+		krg_children_unlock(obj);
+	}
+	__krg_task_unlock(task);
+}
+
+#endif /* CONFIG_KRG_EPM */
+
+void *krg_prepare_exit_notify(struct task_struct *task)
+{
+	void *cookie = NULL;
+#ifdef CONFIG_KRG_EPM
+	pid_t real_parent_tgid = 0;
+	pid_t real_parent_pid = 0;
+	pid_t parent_pid = 0;
+	kerrighed_node_t parent_node = KERRIGHED_NODE_ID_NONE;
+#endif
+
+#ifdef CONFIG_KRG_EPM
+	if (rcu_dereference(task->parent_children_obj))
+		cookie = parent_children_writelock_pid_location_lock(
+				task,
+				&real_parent_tgid,
+				&real_parent_pid,
+				&parent_pid,
+				&parent_node);
+#endif /* CONFIG_KRG_EPM */
+
+	if (task->task_obj) {
+		if (cookie)
+			__krg_task_writelock_nested(task);
+		else
+			__krg_task_writelock(task);
+
+#ifdef CONFIG_KRG_EPM
+		write_lock_irq(&tasklist_lock);
+		if (cookie) {
+			/* Make sure that task_obj is up to date */
+			krg_update_parents(task, parent_pid, real_parent_pid);
+			task->task_obj->parent_node = parent_node;
+		} else if (task->real_parent == baby_sitter
+			   || task->parent == baby_sitter) {
+			/* Real parent died and let us reparent to local init. */
+			krg_reparent_to_local_child_reaper(task);
+		}
+		write_unlock_irq(&tasklist_lock);
+#endif /* CONFIG_KRG_EPM */
+	}
+
+	return cookie;
+}
+
+void krg_finish_exit_notify(struct task_struct *task, int signal, void *cookie)
+{
+#ifdef CONFIG_KRG_EPM
+	if (cookie) {
+		struct children_kddm_object *parent_children_obj = cookie;
+		pid_t parent_pid;
+
+		if (task->task_obj)
+			parent_pid = task->task_obj->parent;
+		else
+			parent_pid = task_pid_knr(task->parent);
+		krg_unlock_pid_location(parent_pid);
+
+		if (signal == DEATH_REAP) {
+			/*
+			 * Parent was not interested by notification, but may
+			 * have been woken up in do_wait and should not see tsk
+			 * as a child anymore. Remove tsk from its children kddm
+			 * object before parent can access it again.
+			 */
+			krg_remove_child(parent_children_obj, task);
+		} else {
+			krg_set_child_exit_signal(parent_children_obj, task);
+			krg_set_child_exit_state(parent_children_obj, task);
+			krg_set_child_location(parent_children_obj, task);
+		}
+		krg_children_unlock(parent_children_obj);
+	}
+#endif /* CONFIG_KRG_EPM */
+
+	if (task->task_obj)
+		__krg_task_unlock(task);
+}
+
+void krg_release_task(struct task_struct *p)
+{
+#ifdef CONFIG_KRG_EPM
+	krg_exit_application(p);
+	krg_unhash_process(p);
+	if (p->exit_state != EXIT_MIGRATION) {
+#endif /* CONFIG_KRG_EPM */
+		krg_task_free(p);
+#ifdef CONFIG_KRG_EPM
+		if (krg_action_pending(p, EPM_MIGRATE))
+			/* Migration aborted because p died before */
+			migration_aborted(p);
+	}
+#endif /* CONFIG_KRG_EPM */
+}
+
+#ifdef CONFIG_KRG_EPM
+
+/*
+ * To chain the tasks to release in the worker, we overload the children field
+ * of the task_struct, which is no more used once a task is ready to release.
+ */
+static void delay_release_task_worker(struct work_struct *work)
+{
+	struct task_struct *task;
+
+	for (;;) {
+		task = NULL;
+		spin_lock(&tasks_to_release_lock);
+		if (!list_empty(&tasks_to_release)) {
+			task = list_entry(tasks_to_release.next,
+					  struct task_struct, children);
+			list_del_init(&task->children);
+		}
+		spin_unlock(&tasks_to_release_lock);
+		if (!task)
+			break;
+		release_task(task);
+	}
+}
+
+int krg_delay_release_task(struct task_struct *task)
+{
+	int delayed;
+
+	BUG_ON(!list_empty(&task->children));
+
+	/*
+	 * No need to lock tasklist since if task is current
+	 * thread_group_leader() is safe
+	 */
+	delayed = !thread_group_leader(task) && task == current;
+	if (delayed) {
+		spin_lock(&tasks_to_release_lock);
+		list_add_tail(&task->children, &tasks_to_release);
+		spin_unlock(&tasks_to_release_lock);
+
+		queue_work(krg_wq, &delay_release_task_work);
+	}
+
+	return delayed;
+}
+
+struct notify_remote_child_reaper_msg {
+	pid_t zombie_pid;
+};
+
+static void handle_notify_remote_child_reaper(struct rpc_desc *desc,
+					      void *_msg,
+					      size_t size)
+{
+	struct notify_remote_child_reaper_msg *msg = _msg;
+	struct task_struct *zombie;
+	bool release = false;
+
+	krg_task_writelock(msg->zombie_pid);
+	write_lock_irq(&tasklist_lock);
+
+	zombie = find_task_by_kpid(msg->zombie_pid);
+	BUG_ON(!zombie);
+
+	/* Real parent died and let us reparent zombie to local init. */
+	krg_reparent_to_local_child_reaper(zombie);
+
+	BUG_ON(zombie->exit_state != EXIT_ZOMBIE);
+	BUG_ON(zombie->exit_signal == -1);
+	if (!zombie->ptrace && thread_group_empty(zombie)) {
+		do_notify_parent(zombie, zombie->exit_signal);
+		if (task_detached(zombie)) {
+			zombie->exit_state = EXIT_DEAD;
+			release = true;
+		}
+	}
+
+	write_unlock_irq(&tasklist_lock);
+	krg_task_unlock(msg->zombie_pid);
+
+	if (release)
+		release_task(zombie);
+}
+
+void notify_remote_child_reaper(pid_t zombie_pid,
+				kerrighed_node_t zombie_location)
+{
+	struct notify_remote_child_reaper_msg msg = {
+		.zombie_pid = zombie_pid
+	};
+
+	BUG_ON(zombie_location == KERRIGHED_NODE_ID_NONE);
+	BUG_ON(zombie_location == kerrighed_node_id);
+
+	rpc_async(PROC_NOTIFY_REMOTE_CHILD_REAPER, zombie_location,
+		  &msg, sizeof(msg));
+}
+
+#endif /* CONFIG_KRG_EPM */
+
+/**
+ * @author Pascal Gallard, Louis Rilling
+ */
+void proc_krg_exit_start(void)
+{
+#ifdef CONFIG_KRG_EPM
+	rpc_register_void(PROC_DO_NOTIFY_PARENT, handle_do_notify_parent, 0);
+	rpc_register_void(PROC_NOTIFY_REMOTE_CHILD_REAPER,
+			  handle_notify_remote_child_reaper, 0);
+	rpc_register_void(PROC_WAIT_TASK_ZOMBIE, handle_wait_task_zombie, 0);
+#endif
+}
+
+/**
+ * @author Pascal Gallard, Louis Rilling
+ */
+void proc_krg_exit_exit(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/proc/libproc.c android_cluster/linux-2.6.29/kerrighed/proc/libproc.c
--- linux-2.6.29/kerrighed/proc/libproc.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/proc/libproc.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,28 @@
+/*
+ *  kerrighed/proc/libproc.c
+ *
+ *  Copyright (C) 2007 Louis Rilling - Kerlabs
+ */
+
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/pid.h>
+#include <kddm/io_linker.h>
+
+/* Generic function to assign a default owner to a pid-named kddm object */
+kerrighed_node_t global_pid_default_owner(struct kddm_set *set, objid_t objid,
+					  const krgnodemask_t *nodes,
+					  int nr_nodes)
+{
+	kerrighed_node_t node;
+
+	BUG_ON(!(objid & GLOBAL_PID_MASK));
+	node = ORIG_NODE(objid);
+	if (node < 0 || node >= KERRIGHED_MAX_NODES)
+		/* Invalid ID */
+		node = kerrighed_node_id;
+	if (node != kerrighed_node_id
+	    && unlikely(!__krgnode_isset(node, nodes)))
+		node = __next_krgnode_in_ring(node, nodes);
+	return node;
+}
diff -ruN linux-2.6.29/kerrighed/proc/Makefile android_cluster/linux-2.6.29/kerrighed/proc/Makefile
--- linux-2.6.29/kerrighed/proc/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/proc/Makefile	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,11 @@
+#
+# Kerrighed's low-level Process monitoring and control
+#
+
+obj-$(CONFIG_KRG_PROC) := krg_proc.o
+
+krg_proc-y := proc.o libproc.o \
+	task.o krg_exit.o \
+	remote_cred.o remote_syscall.o
+
+EXTRA_CFLAGS += -Wall -Werror
diff -ruN linux-2.6.29/kerrighed/proc/proc.c android_cluster/linux-2.6.29/kerrighed/proc/proc.c
--- linux-2.6.29/kerrighed/proc/proc.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/proc/proc.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,32 @@
+/*
+ *  kerrighed/proc/proc.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2006-2007 Pascal Gallard - Kerlabs, Louis Rilling - Kerlabs
+ */
+
+#include <linux/kernel.h>
+
+#include "proc_internal.h"
+
+/** Initial function of the module
+ *  @author Geoffroy Vallee, Pascal Gallard
+ */
+int init_proc(void)
+{
+	printk("Proc initialisation: start\n");
+
+	proc_task_start();
+	proc_krg_exit_start();
+
+	proc_remote_syscalls_start();
+	register_remote_syscalls_hooks();
+
+	printk("Proc initialisation: done\n");
+
+	return 0;
+}
+
+void cleanup_proc(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/proc/proc_internal.h android_cluster/linux-2.6.29/kerrighed/proc/proc_internal.h
--- linux-2.6.29/kerrighed/proc/proc_internal.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/proc/proc_internal.h	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,17 @@
+#ifndef __PROC_INTERNAL_H__
+#define __PROC_INTERNAL_H__
+
+#ifdef CONFIG_KRG_PROC
+
+void proc_task_start(void);
+void proc_task_exit(void);
+
+void proc_krg_exit_start(void);
+void proc_krg_exit_exit(void);
+
+void proc_remote_syscalls_start(void);
+void register_remote_syscalls_hooks(void);
+
+#endif /* CONFIG_KRG_PROC */
+
+#endif /* __PROC_INTERNAL_H__ */
diff -ruN linux-2.6.29/kerrighed/proc/remote_cred.c android_cluster/linux-2.6.29/kerrighed/proc/remote_cred.c
--- linux-2.6.29/kerrighed/proc/remote_cred.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/proc/remote_cred.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,232 @@
+/*
+ *  kerrighed/proc/remote_cred.c
+ *
+ *  Copyright (C) 2009 Louis Rilling - Kerlabs
+ */
+#include <linux/cred.h>
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/namespace.h>
+#ifdef CONFIG_KRG_EPM
+#include <linux/user_namespace.h>
+#include <linux/security.h>
+#include <kerrighed/ghost.h>
+
+struct epm_action;
+#endif
+
+int pack_creds(struct rpc_desc *desc, const struct cred *cred)
+{
+	return rpc_pack_type(desc, *cred);
+}
+
+int unpack_creds(struct rpc_desc *desc, struct cred *cred)
+{
+	struct cred tmp;
+	int err;
+
+	err = rpc_unpack_type(desc, tmp);
+	if (err)
+		goto out;
+
+	cred->uid = tmp.uid;
+	cred->gid = tmp.gid;
+	cred->suid = tmp.suid;
+	cred->sgid = tmp.sgid;
+	cred->euid = tmp.euid;
+	cred->egid = tmp.egid;
+	cred->fsuid = tmp.fsuid;
+	cred->fsgid = tmp.fsgid;
+	cred->securebits = tmp.securebits;
+	cred->cap_inheritable = tmp.cap_inheritable;
+	cred->cap_permitted = tmp.cap_permitted;
+	cred->cap_effective = tmp.cap_effective;
+	cred->cap_bset = tmp.cap_bset;
+#ifdef CONFIG_KEYS
+	/* No key sharing accross nodes yet */
+#endif
+#ifdef CONFIG_SECURITY
+	/* No LSM support accross nodes */
+#endif
+	/* No user struct transfer needed? */
+	/* No groups transfer needed? */
+
+out:
+	if (err > 0)
+		err = -EPIPE;
+	return err;
+}
+
+const struct cred *unpack_override_creds(struct rpc_desc *desc)
+{
+	const struct cred *old_cred;
+	struct cred *cred;
+	int err;
+
+	cred = prepare_creds();
+	if (!cred)
+		return ERR_PTR(-ENOMEM);
+	err = unpack_creds(desc, cred);
+	if (err) {
+		put_cred(cred);
+		return ERR_PTR(err);
+	}
+
+	old_cred = override_creds(cred);
+	put_cred(cred);
+
+	return old_cred;
+}
+
+#ifdef CONFIG_KRG_EPM
+
+int export_cred(struct epm_action *action,
+		ghost_t *ghost, struct task_struct *task)
+{
+	const struct cred *cred = __task_cred(task);
+	const struct group_info *groups = cred->group_info;
+	int i, err;
+
+#ifdef CONFIG_KEYS
+	return -EBUSY;
+#endif
+#ifdef CONFIG_SECURITY
+	if (cred->security)
+		return -EBUSY;
+#endif
+	if (cred->user->user_ns != task->nsproxy->krg_ns->root_user_ns)
+		return -EPERM;
+
+	err = ghost_write(ghost, cred, sizeof(*cred));
+	if (err)
+		goto out;
+
+	err = ghost_write(ghost, &groups->ngroups, sizeof(groups->ngroups));
+	if (err)
+		goto out;
+	if (groups->ngroups <= NGROUPS_SMALL) {
+		err = ghost_write(ghost,
+				  &groups->small_block,
+				  sizeof(groups->small_block));
+		goto out;
+	}
+	for (i = 0; i < groups->nblocks; i++) {
+		err = ghost_write(ghost,
+				  groups->blocks[i],
+				  sizeof(*groups->blocks[i] * NGROUPS_PER_BLOCK));
+		if (err)
+			goto out;
+	}
+
+out:
+	return err;
+}
+
+int import_cred(struct epm_action *action,
+		ghost_t *ghost, struct task_struct *task)
+{
+	struct cred tmp_cred;
+	struct cred *cred;
+	struct user_struct *user;
+	struct group_info *groups;
+	int ngroups, i, err;
+
+	err = ghost_read(ghost, &tmp_cred, sizeof(tmp_cred));
+	if (err)
+		goto out;
+
+	cred = prepare_creds();
+
+	cred->uid = tmp_cred.uid;
+	cred->gid = tmp_cred.gid;
+	cred->suid = tmp_cred.suid;
+	cred->sgid = tmp_cred.sgid;
+	cred->euid = tmp_cred.euid;
+	cred->egid = tmp_cred.egid;
+	cred->fsuid = tmp_cred.fsuid;
+	cred->fsgid = tmp_cred.fsgid;
+	cred->securebits = tmp_cred.securebits;
+	cred->cap_inheritable = tmp_cred.cap_inheritable;
+	cred->cap_permitted = tmp_cred.cap_permitted;
+	cred->cap_effective = tmp_cred.cap_effective;
+	cred->cap_bset = tmp_cred.cap_bset;
+
+#ifdef CONFIG_KEYS
+	BUG();
+	key_put(cred->thread_keyring);
+	cred->thread_keyring = NULL;
+	key_put(cred->request_key_auth);
+	cred->request_key_auth = NULL;
+	release_tgcred(cred->tgcred);
+	cred->tgcred = NULL;
+#endif
+
+#ifdef CONFIG_SECURITY
+	BUG_ON(tmp_cred.security);
+	security_cred_free(cred);
+	cred->security = NULL;
+#endif
+
+	user = alloc_uid(task->nsproxy->krg_ns->root_user_ns, cred->uid);
+	if (!user) {
+		err = -ENOMEM;
+		goto out_err;
+	}
+	free_uid(cred->user);
+	cred->user = user;
+
+	err = ghost_read(ghost, &ngroups, sizeof(ngroups));
+	if (err)
+		goto out_err;
+	groups = groups_alloc(ngroups);
+	if (!groups) {
+		err = -ENOMEM;
+		goto out_err;
+	}
+	if (ngroups <= NGROUPS_SMALL) {
+		err = ghost_read(ghost,
+				 &groups->small_block,
+				 sizeof(groups->small_block));
+		if (err)
+			goto err_groups;
+		else
+			goto groups_ok;
+	}
+	for (i = 0; i < groups->nblocks; i++) {
+		err = ghost_read(ghost,
+				 groups->blocks[i],
+				 sizeof(*groups->blocks[i] * NGROUPS_PER_BLOCK));
+		if (err)
+			goto err_groups;
+	}
+groups_ok:
+	put_group_info(cred->group_info);
+	cred->group_info = groups;
+
+	rcu_assign_pointer(task->real_cred, cred);
+	get_cred(cred);
+	rcu_assign_pointer(task->cred, cred);
+	err = 0;
+
+out:
+	return err;
+
+err_groups:
+	groups_free(groups);
+out_err:
+	put_cred(cred);
+	goto out;
+}
+
+void unimport_cred(struct task_struct *task)
+{
+	put_cred(task->cred);
+	put_cred(task->real_cred);
+}
+
+void free_ghost_cred(struct task_struct *ghost)
+{
+	put_cred(ghost->cred);
+	put_cred(ghost->real_cred);
+}
+
+#endif /* CONFIG_KRG_EPM */
diff -ruN linux-2.6.29/kerrighed/proc/remote_syscall.c android_cluster/linux-2.6.29/kerrighed/proc/remote_syscall.c
--- linux-2.6.29/kerrighed/proc/remote_syscall.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/proc/remote_syscall.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,157 @@
+/*
+ *  kerrighed/proc/remote_syscall.c
+ *
+ *  Copyright (C) 2009 Louis Rilling - Kerlabs
+ */
+#include <net/krgrpc/rpc.h>
+#include <linux/cred.h>
+#include <kerrighed/remote_cred.h>
+#include <linux/nsproxy.h>
+#include <linux/pid_namespace.h>
+#include <linux/pid.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/hotplug.h>
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <asm/current.h>
+
+#include <kerrighed/remote_syscall.h>
+
+static void *cluster_started;
+
+struct remote_syscall_header {
+	pid_t pid;
+	size_t payload;
+};
+
+struct rpc_desc *krg_remote_syscall_begin(int req, pid_t pid,
+					  const void *msg, size_t size)
+{
+	struct remote_syscall_header hdr;
+	struct rpc_desc *desc;
+	kerrighed_node_t node;
+	int err = -ESRCH;
+
+	if (!cluster_started)
+		goto err;
+
+	if (!current->nsproxy->krg_ns)
+		goto err;
+
+	if (!is_krg_pid_ns_root(task_active_pid_ns(current)))
+		goto err;
+
+	if (pid < 0 || !(pid & GLOBAL_PID_MASK))
+		goto err;
+
+	node = krg_lock_pid_location(pid);
+	if (node == KERRIGHED_NODE_ID_NONE)
+		goto err;
+
+	err = -ENOMEM;
+	desc = rpc_begin(req, node);
+	if (!desc)
+		goto err_unlock;
+
+	hdr.pid = pid;
+	hdr.payload = size;
+	err = rpc_pack_type(desc, hdr);
+	if (err)
+		goto err_cancel;
+	if (size) {
+		err = rpc_pack(desc, 0, msg, size);
+		if (err)
+			goto err_cancel;
+	}
+	err = pack_creds(desc, current_cred());
+	if (err)
+		goto err_cancel;
+
+	return desc;
+
+err_cancel:
+	rpc_cancel(desc);
+	rpc_end(desc, 0);
+err_unlock:
+	krg_unlock_pid_location(pid);
+err:
+	return ERR_PTR(err);
+}
+
+void krg_remote_syscall_end(struct rpc_desc *desc, pid_t pid)
+{
+	rpc_end(desc, 0);
+	krg_unlock_pid_location(pid);
+}
+
+int krg_remote_syscall_simple(int req, pid_t pid, const void *msg, size_t size)
+{
+	struct rpc_desc *desc;
+	int ret, err;
+
+	desc = krg_remote_syscall_begin(req, pid, msg, size);
+	if (IS_ERR(desc)) {
+		ret = PTR_ERR(desc);
+		goto out;
+	}
+	err = rpc_unpack_type(desc, ret);
+	if (err)
+		ret = err;
+	krg_remote_syscall_end(desc, pid);
+
+out:
+	return ret;
+}
+
+struct pid *krg_handle_remote_syscall_begin(struct rpc_desc *desc,
+					    const void *_msg, size_t size,
+					    void *msg,
+					    const struct cred **old_cred)
+{
+	const struct remote_syscall_header *hdr = _msg;
+	struct pid *pid;
+	int err;
+
+	if (hdr->payload) {
+		err = rpc_unpack(desc, 0, msg, hdr->payload);
+		if (err)
+			goto err_cancel;
+	}
+
+	*old_cred = unpack_override_creds(desc);
+	if (IS_ERR(*old_cred)) {
+		err = PTR_ERR(*old_cred);
+		goto err_cancel;
+	}
+
+	rcu_read_lock();
+	pid = get_pid(find_kpid(hdr->pid));
+	rcu_read_unlock();
+	BUG_ON(!pid);
+
+	return pid;
+
+err_cancel:
+	if (err > 0)
+		err = -EPIPE;
+	rpc_cancel(desc);
+	return ERR_PTR(err);
+}
+
+void krg_handle_remote_syscall_end(struct pid *pid, const struct cred *old_cred)
+{
+	revert_creds(old_cred);
+	put_pid(pid);
+}
+
+void register_remote_syscalls_hooks(void)
+{
+	hook_register(&cluster_started, (void *)true);
+}
+
+void proc_remote_syscalls_start(void)
+{
+	remote_signals_init();
+	remote_sched_init();
+	remote_sys_init();
+}
diff -ruN linux-2.6.29/kerrighed/proc/task.c android_cluster/linux-2.6.29/kerrighed/proc/task.c
--- linux-2.6.29/kerrighed/proc/task.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/proc/task.c	2014-05-27 23:04:10.470027010 -0700
@@ -0,0 +1,647 @@
+/*
+ *  kerrighed/proc/task.c
+ *
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2006-2007 Pascal Gallard - Kerlabs, Louis Rilling - Kerlabs
+ */
+
+/** On each node the system manage a table to know the
+ *  location of migrated process.
+ *  It is interesting to globally manage signal : e.g. when a signal
+ *  arrive from a remote node, the system can find the old local
+ *  process pid and so the process'father.
+ */
+
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/personality.h>
+#include <linux/sched.h>
+#include <linux/pid.h>
+#include <linux/cred.h>
+#include <linux/rwsem.h>
+#include <linux/lockdep.h>
+#include <linux/rcupdate.h>
+#include <linux/kref.h>
+#include <linux/slab.h>
+#include <kerrighed/task.h>
+#include <kerrighed/pid.h>
+
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/libproc.h>
+#include <kddm/kddm.h>
+
+static struct kmem_cache *task_kddm_obj_cachep;
+
+/* kddm set of pid location and task struct */
+static struct kddm_set *task_kddm_set;
+
+void krg_task_get(struct task_kddm_object *obj)
+{
+	if (obj)
+		kref_get(&obj->kref);
+}
+
+static void task_free(struct kref *kref)
+{
+	struct task_kddm_object *obj;
+
+	obj = container_of(kref, struct task_kddm_object, kref);
+	BUG_ON(!obj);
+
+	kmem_cache_free(task_kddm_obj_cachep, obj);
+}
+
+void krg_task_put(struct task_kddm_object *obj)
+{
+	if (obj)
+		kref_put(&obj->kref, task_free);
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int task_alloc_object(struct kddm_obj *obj_entry,
+			     struct kddm_set *set, objid_t objid)
+{
+	struct task_kddm_object *p;
+
+	p = kmem_cache_alloc(task_kddm_obj_cachep, GFP_KERNEL);
+	if (!p)
+		return -ENOMEM;
+
+	p->node = KERRIGHED_NODE_ID_NONE;
+	p->task = NULL;
+	p->pid = objid;
+	p->parent_node = KERRIGHED_NODE_ID_NONE;
+	/*
+	 * If the group leader is another thread, this
+	 * will be fixed later. Before that this is
+	 * only needed to check local/global pids.
+	 */
+	p->group_leader = objid;
+#ifdef CONFIG_KRG_EPM
+	p->pid_obj = NULL;
+#endif
+	init_rwsem(&p->sem);
+	p->write_locked = 0;
+
+	p->alive = 1;
+	kref_init(&p->kref);
+	obj_entry->object = p;
+
+	return 0;
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int task_first_touch(struct kddm_obj *obj_entry,
+			    struct kddm_set *set, objid_t objid, int flags)
+{
+	return task_alloc_object(obj_entry, set, objid);
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int task_import_object(struct rpc_desc *desc,
+			      struct kddm_set *set,
+			      struct kddm_obj *obj_entry,
+			      objid_t objid,
+			      int flags)
+{
+	struct task_kddm_object *dest = obj_entry->object;
+	struct task_kddm_object src;
+	int retval;
+
+	retval = rpc_unpack_type(desc, src);
+	if (retval)
+		return retval;
+
+	write_lock_irq(&tasklist_lock);
+
+	dest->state = src.state;
+	dest->flags = src.flags;
+	dest->ptrace = src.ptrace;
+	dest->exit_state = src.exit_state;
+	dest->exit_code = src.exit_code;
+	dest->exit_signal = src.exit_signal;
+
+	dest->node = src.node;
+	dest->self_exec_id = src.self_exec_id;
+	dest->thread_group_empty = src.thread_group_empty;
+
+	dest->parent = src.parent;
+	dest->parent_node = src.parent_node;
+	dest->real_parent = src.real_parent;
+	dest->real_parent_tgid = src.real_parent_tgid;
+	dest->group_leader = src.group_leader;
+
+	dest->uid = src.uid;
+	dest->euid = src.euid;
+	dest->egid = src.egid;
+
+	dest->utime = src.utime;
+	dest->stime = src.stime;
+
+	dest->dumpable = src.dumpable;
+
+	write_unlock_irq(&tasklist_lock);
+
+	return 0;
+}
+
+/*
+ * Assumes either tasklist_lock read locked with appropriate task_lock held, or
+ * tasklist_lock write locked.
+ */
+static void task_update_object(struct task_kddm_object *obj)
+{
+	struct task_struct *tsk = obj->task;
+	const struct cred *cred;
+
+	if (tsk) {
+		BUG_ON(tsk->task_obj != obj);
+
+		obj->state = tsk->state;
+		obj->flags = tsk->flags;
+		obj->ptrace = tsk->ptrace;
+		obj->exit_state = tsk->exit_state;
+		obj->exit_code = tsk->exit_code;
+		obj->exit_signal = tsk->exit_signal;
+
+		obj->self_exec_id = tsk->self_exec_id;
+
+		BUG_ON(obj->node != kerrighed_node_id &&
+		       obj->node != KERRIGHED_NODE_ID_NONE);
+
+		rcu_read_lock();
+		cred = __task_cred(tsk);
+		obj->uid = cred->uid;
+		obj->euid = cred->euid;
+		obj->egid = cred->egid;
+		rcu_read_unlock();
+
+		obj->utime = task_utime(tsk);
+		obj->stime = task_stime(tsk);
+
+		obj->dumpable = (tsk->mm && get_dumpable(tsk->mm) == 1);
+
+		obj->thread_group_empty = thread_group_empty(tsk);
+	}
+}
+
+/*
+ * @author Pascal Gallard
+ */
+static int task_export_object(struct rpc_desc *desc,
+			      struct kddm_set *set,
+			      struct kddm_obj *obj_entry,
+			      objid_t objid,
+			      int flags)
+{
+	struct task_kddm_object *src = obj_entry->object;
+	struct task_struct *tsk;
+
+	read_lock(&tasklist_lock);
+	tsk = src->task;
+	if (likely(tsk)) {
+		task_lock(tsk);
+		task_update_object(src);
+		task_unlock(tsk);
+	}
+	read_unlock(&tasklist_lock);
+
+	return rpc_pack_type(desc, *src);
+}
+
+static void delayed_task_put(struct rcu_head *rhp)
+{
+	struct task_kddm_object *obj =
+		container_of(rhp, struct task_kddm_object, rcu);
+
+	krg_task_put(obj);
+}
+
+/**
+ *  @author Louis Rilling
+ */
+static int task_remove_object(void *object,
+			      struct kddm_set *set, objid_t objid)
+{
+	struct task_kddm_object *obj = object;
+
+	krg_task_unlink(obj, 0);
+
+#ifdef CONFIG_KRG_EPM
+	rcu_read_lock();
+	krg_pid_unlink_task(rcu_dereference(obj->pid_obj));
+	rcu_read_unlock();
+	BUG_ON(obj->pid_obj);
+#endif
+
+	obj->alive = 0;
+	call_rcu(&obj->rcu, delayed_task_put);
+
+	return 0;
+}
+
+static struct iolinker_struct task_io_linker = {
+	.first_touch   = task_first_touch,
+	.linker_name   = "task ",
+	.linker_id     = TASK_LINKER,
+	.alloc_object  = task_alloc_object,
+	.export_object = task_export_object,
+	.import_object = task_import_object,
+	.remove_object = task_remove_object,
+	.default_owner = global_pid_default_owner,
+};
+
+int krg_task_alloc(struct task_struct *task, struct pid *pid)
+{
+	struct task_kddm_object *obj;
+	int nr = pid_knr(pid);
+
+	task->task_obj = NULL;
+	if (!task->nsproxy->krg_ns)
+		return 0;
+#ifdef CONFIG_KRG_EPM
+	if (krg_current)
+		return 0;
+#endif
+	/* Exclude kernel threads and local pids from using task kddm objects. */
+	/*
+	 * At this stage, current->mm points the mm of the task being duplicated
+	 * instead of the mm of task for which this struct is being allocated,
+	 * but we only need to know whether it is NULL or not, which will be the
+	 * same after copy_mm.
+	 */
+	if (!(nr & GLOBAL_PID_MASK) || !current->mm)
+		return 0;
+
+	obj = krg_task_create_writelock(nr);
+	if (!obj)
+		return -ENOMEM;
+
+	/* Set the link between task kddm object and tsk */
+	obj->task = task;
+	task->task_obj = obj;
+
+	return 0;
+}
+
+void krg_task_fill(struct task_struct *task, unsigned long clone_flags)
+{
+	struct task_kddm_object *obj = task->task_obj;
+
+	BUG_ON((task_tgid_knr(task) & GLOBAL_PID_MASK)
+	       != (task_pid_knr(task) & GLOBAL_PID_MASK));
+
+#ifdef CONFIG_KRG_EPM
+	if (krg_current)
+		return;
+#endif
+	if (!obj)
+		return;
+
+	obj->node = kerrighed_node_id;
+#ifdef CONFIG_KRG_EPM
+	if (task->real_parent == baby_sitter) {
+		BUG_ON(!current->task_obj);
+		if (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {
+			struct task_kddm_object *cur_obj = current->task_obj;
+			obj->real_parent = cur_obj->real_parent;
+			obj->real_parent_tgid = cur_obj->real_parent_tgid;
+		} else {
+			obj->real_parent = task_pid_knr(current);
+			obj->real_parent_tgid = task_tgid_knr(current);
+		}
+	} else
+#endif
+	{
+		obj->real_parent = task_pid_knr(task->real_parent);
+		obj->real_parent_tgid = task_tgid_knr(task->real_parent);
+	}
+	/* Keep parent same as real_parent until ptrace is better supported */
+	obj->parent = obj->real_parent;
+#ifdef CONFIG_KRG_EPM
+	/* Distributed threads are not supported yet. */
+	BUG_ON(task->group_leader == baby_sitter);
+#endif
+	obj->group_leader = task_tgid_knr(task);
+}
+
+void krg_task_commit(struct task_struct *task)
+{
+	if (task->task_obj)
+		__krg_task_unlock(task);
+}
+
+void krg_task_abort(struct task_struct *task)
+{
+	struct task_kddm_object *obj = task->task_obj;
+
+#ifdef CONFIG_KRG_EPM
+	if (krg_current)
+		return;
+#endif
+
+	if (!obj)
+		return;
+
+	obj->write_locked = 2;
+	up_write(&obj->sem);
+
+	_kddm_remove_frozen_object(task_kddm_set, obj->pid);
+}
+
+void __krg_task_free(struct task_struct *task)
+{
+	_kddm_remove_object(task_kddm_set, task_pid_knr(task));
+}
+
+void krg_task_free(struct task_struct *task)
+{
+	/* If the pointer is NULL and the object exists, this is a BUG! */
+	if (!task->task_obj)
+		return;
+
+	__krg_task_free(task);
+}
+
+/* Expects tasklist write locked */
+void __krg_task_unlink(struct task_kddm_object *obj, int need_update)
+{
+	BUG_ON(!obj);
+
+	if (obj->task) {
+		if (need_update)
+			task_update_object(obj);
+		rcu_assign_pointer(obj->task->task_obj, NULL);
+		rcu_assign_pointer(obj->task, NULL);
+	}
+}
+
+void krg_task_unlink(struct task_kddm_object *obj, int need_update)
+{
+	write_lock_irq(&tasklist_lock);
+	__krg_task_unlink(obj, need_update);
+	write_unlock_irq(&tasklist_lock);
+}
+
+int krg_task_alive(struct task_kddm_object *obj)
+{
+	return obj && obj->alive;
+}
+
+/**
+ * @author Pascal Gallard
+ */
+struct task_kddm_object *krg_task_readlock(pid_t pid)
+{
+	struct task_kddm_object *obj;
+
+	/* Filter well known cases of no task kddm object. */
+	if (!(pid & GLOBAL_PID_MASK))
+		return NULL;
+
+	obj = _kddm_get_object_no_ft(task_kddm_set, pid);
+	if (likely(obj)) {
+		down_read(&obj->sem);
+		if (obj->write_locked == 2) {
+			/* Dying object */
+			up_read(&obj->sem);
+			_kddm_put_object(task_kddm_set, pid);
+			return NULL;
+		}
+		/* Marker for unlock. Dirty but temporary. */
+		obj->write_locked = 0;
+	}
+
+	return obj;
+}
+
+struct task_kddm_object *__krg_task_readlock(struct task_struct *task)
+{
+	return krg_task_readlock(task_pid_knr(task));
+}
+
+/**
+ * @author Pascal Gallard
+ */
+static struct task_kddm_object *task_writelock(pid_t pid, int nested)
+{
+	struct task_kddm_object *obj;
+
+	/* Filter well known cases of no task kddm object. */
+	if (!(pid & GLOBAL_PID_MASK))
+		return NULL;
+
+	obj = _kddm_grab_object_no_ft(task_kddm_set, pid);
+	if (likely(obj)) {
+		if (!nested)
+			down_write(&obj->sem);
+		else
+			down_write_nested(&obj->sem, SINGLE_DEPTH_NESTING);
+		if (obj->write_locked == 2) {
+			/* Dying object */
+			up_write(&obj->sem);
+			_kddm_put_object(task_kddm_set, pid);
+			return NULL;
+		}
+		/* Marker for unlock. Dirty but temporary. */
+		obj->write_locked = 1;
+	}
+
+	return obj;
+}
+
+struct task_kddm_object *krg_task_writelock(pid_t pid)
+{
+	return task_writelock(pid, 0);
+}
+
+struct task_kddm_object *__krg_task_writelock(struct task_struct *task)
+{
+	return task_writelock(task_pid_knr(task), 0);
+}
+
+struct task_kddm_object *krg_task_writelock_nested(pid_t pid)
+{
+	return task_writelock(pid, 1);
+}
+
+struct task_kddm_object *__krg_task_writelock_nested(struct task_struct *task)
+{
+	return task_writelock(task_pid_knr(task), 1);
+}
+
+/**
+ * @author Louis Rilling
+ */
+struct task_kddm_object *krg_task_create_writelock(pid_t pid)
+{
+	struct task_kddm_object *obj;
+
+	/* Filter well known cases of no task kddm object. */
+	/* The exact filter is expected to be implemented by the caller. */
+	BUG_ON(!(pid & GLOBAL_PID_MASK));
+
+	obj = _kddm_grab_object(task_kddm_set, pid);
+	if (likely(obj && !IS_ERR(obj))) {
+		down_write(&obj->sem);
+		/* No dying object race or this is really smelly */
+		BUG_ON(obj->write_locked == 2);
+		/* Marker for unlock. Dirty but temporary. */
+		obj->write_locked = 1;
+	} else {
+		_kddm_put_object(task_kddm_set, pid);
+	}
+
+	return obj;
+}
+
+/**
+ * @author Pascal Gallard
+ */
+void krg_task_unlock(pid_t pid)
+{
+	/* Filter well known cases of no task kddm object. */
+	if (!(pid & GLOBAL_PID_MASK))
+		return;
+
+	{
+		/*
+		 * Dirty tricks here. Hopefully it should be temporary waiting
+		 * for kddm to implement locking on a task basis.
+		 */
+		struct task_kddm_object *obj;
+
+		obj = _kddm_find_object(task_kddm_set, pid);
+		if (likely(obj)) {
+			_kddm_put_object(task_kddm_set, pid);
+			if (obj->write_locked)
+				up_write(&obj->sem);
+			else
+				up_read(&obj->sem);
+		}
+	}
+	_kddm_put_object(task_kddm_set, pid);
+}
+
+void __krg_task_unlock(struct task_struct *task)
+{
+	krg_task_unlock(task_pid_knr(task));
+}
+
+#ifdef CONFIG_KRG_EPM
+/**
+ * @author Pascal Gallard
+ * Set (or update) the location of pid
+ */
+int krg_set_pid_location(struct task_struct *task)
+{
+	struct task_kddm_object *p;
+
+	p = __krg_task_writelock(task);
+	if (likely(p))
+		p->node = kerrighed_node_id;
+	__krg_task_unlock(task);
+
+	return 0;
+}
+
+int krg_unset_pid_location(struct task_struct *task)
+{
+	struct task_kddm_object *p;
+
+	BUG_ON(!(task_pid_knr(task) & GLOBAL_PID_MASK));
+
+	p = __krg_task_writelock(task);
+	BUG_ON(p == NULL);
+	p->node = KERRIGHED_NODE_ID_NONE;
+	__krg_task_unlock(task);
+
+	return 0;
+}
+#endif /* CONFIG_KRG_EPM */
+
+kerrighed_node_t krg_lock_pid_location(pid_t pid)
+{
+	kerrighed_node_t node = KERRIGHED_NODE_ID_NONE;
+	struct task_kddm_object *obj;
+#ifdef CONFIG_KRG_EPM
+	struct timespec back_off_time = {
+		.tv_sec = 0,
+		.tv_nsec = 1000000 /* 1 ms */
+	};
+#endif
+
+	if (!(pid & GLOBAL_PID_MASK))
+		goto out;
+
+	for (;;) {
+		obj = krg_task_readlock(pid);
+		if (likely(obj)) {
+			node = obj->node;
+		} else {
+			krg_task_unlock(pid);
+			break;
+		}
+#ifdef CONFIG_KRG_EPM
+		if (likely(node != KERRIGHED_NODE_ID_NONE))
+			break;
+		/*
+		 * Task is migrating.
+		 * Back off and hope that it will stop migrating.
+		 */
+		krg_task_unlock(pid);
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		schedule_timeout(timespec_to_jiffies(&back_off_time) + 1);
+#else
+		break;
+#endif
+	}
+
+out:
+	return node;
+}
+
+void krg_unlock_pid_location(pid_t pid)
+{
+	krg_task_unlock(pid);
+}
+
+/**
+ * @author David Margery
+ * @author Pascal Gallard (update to kddm architecture)
+ * @author Louis Rilling (split files)
+ */
+void proc_task_start(void)
+{
+	unsigned long cache_flags = SLAB_PANIC;
+
+#ifdef CONFIG_DEBUG_SLAB
+	cache_flags |= SLAB_POISON;
+#endif
+	task_kddm_obj_cachep = KMEM_CACHE(task_kddm_object, cache_flags);
+
+	register_io_linker(TASK_LINKER, &task_io_linker);
+
+	task_kddm_set = create_new_kddm_set(kddm_def_ns, TASK_KDDM_ID,
+					    TASK_LINKER,
+					    KDDM_CUSTOM_DEF_OWNER,
+					    0, 0);
+	if (IS_ERR(task_kddm_set))
+		OOM;
+
+}
+
+/**
+ * @author David Margery
+ * @author Pascal Gallard (update to kddm architecture)
+ * @author Louis Rilling (split files)
+ */
+void proc_task_exit(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/procfs/dynamic_cpu_info_linker.c android_cluster/linux-2.6.29/kerrighed/procfs/dynamic_cpu_info_linker.c
--- linux-2.6.29/kerrighed/procfs/dynamic_cpu_info_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/dynamic_cpu_info_linker.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,103 @@
+/** Dynamic CPU information management.
+ *  @file dynamic_cpu_info_linker.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/swap.h>
+#include <linux/kernel_stat.h>
+#include <linux/hardirq.h>
+
+#include <kerrighed/cpu_id.h>
+#include <kerrighed/workqueue.h>
+#include <kddm/kddm.h>
+
+#include <asm/cputime.h>
+
+#include "dynamic_cpu_info_linker.h"
+#include "static_cpu_info_linker.h"
+
+#include <kerrighed/debug.h>
+
+struct kddm_set *dynamic_cpu_info_kddm_set;
+
+/*****************************************************************************/
+/*                                                                           */
+/*                   DYNAMIC CPU INFO KDDM IO FUNCTIONS                      */
+/*                                                                           */
+/*****************************************************************************/
+
+/****************************************************************************/
+
+/* Init the dynamic cpu info IO linker */
+
+static struct iolinker_struct dynamic_cpu_info_io_linker = {
+	.default_owner = cpu_info_default_owner,
+	.linker_name = "dyn_cpu_nfo",
+	.linker_id = DYNAMIC_CPU_INFO_LINKER,
+};
+
+static void update_dynamic_cpu_info_worker(struct work_struct *data);
+static DECLARE_DELAYED_WORK(update_dynamic_cpu_info_work,
+			    update_dynamic_cpu_info_worker);
+
+/** Update dynamic CPU informations for all local CPU.
+ *  @author Renaud Lottiaux
+ */
+static void update_dynamic_cpu_info_worker(struct work_struct *data)
+{
+	krg_dynamic_cpu_info_t *dynamic_cpu_info;
+	int i, j, cpu_id;
+
+	for_each_online_cpu(i) {
+		cpu_id = krg_cpu_id(i);
+		dynamic_cpu_info =
+			_kddm_grab_object(dynamic_cpu_info_kddm_set, cpu_id);
+
+		/* Compute data for stat proc file */
+
+		dynamic_cpu_info->stat = kstat_cpu(i);
+#ifdef arch_idle_time
+		dynamic_cpu_info->stat.cpustat.idle =
+			cputime64_add(dynamic_cpu_info->stat.cpustat.idle,
+				      arch_idle_time(i));
+#endif
+		dynamic_cpu_info->total_intr = 0;
+		for (j = 0; j < NR_IRQS; j++) {
+			unsigned int *irqs =
+				krg_dynamic_cpu_info_irqs(dynamic_cpu_info);
+#ifdef CONFIG_GENERIC_HARDIRQS
+			irqs[j] = kstat_irqs_cpu(j, i);
+#endif
+			dynamic_cpu_info->total_intr += irqs[j];
+		}
+#ifdef arch_irq_stat_cpu
+		dynamic_cpu_info->total_intr += arch_irq_stat_cpu(i);
+#endif
+
+		_kddm_put_object(dynamic_cpu_info_kddm_set, cpu_id);
+	}
+
+	queue_delayed_work(krg_wq, &update_dynamic_cpu_info_work, HZ);
+}
+
+int dynamic_cpu_info_init(void)
+{
+	register_io_linker(DYNAMIC_CPU_INFO_LINKER,
+			   &dynamic_cpu_info_io_linker);
+
+	/* Create the CPU info container */
+
+	dynamic_cpu_info_kddm_set =
+		create_new_kddm_set(kddm_def_ns,
+				    DYNAMIC_CPU_INFO_KDDM_ID,
+				    DYNAMIC_CPU_INFO_LINKER,
+				    KDDM_CUSTOM_DEF_OWNER,
+				    sizeof(krg_dynamic_cpu_info_t),
+				    0);
+	if (IS_ERR(dynamic_cpu_info_kddm_set))
+		OOM;
+
+	queue_delayed_work(krg_wq, &update_dynamic_cpu_info_work, 0);
+	return 0;
+}
diff -ruN linux-2.6.29/kerrighed/procfs/dynamic_cpu_info_linker.h android_cluster/linux-2.6.29/kerrighed/procfs/dynamic_cpu_info_linker.h
--- linux-2.6.29/kerrighed/procfs/dynamic_cpu_info_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/dynamic_cpu_info_linker.h	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,74 @@
+/** Dynamic per CPU informations management.
+ *  @file dynamic_cpu_info_linker.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef DYNAMIC_CPU_INFO_LINKER_H
+#define DYNAMIC_CPU_INFO_LINKER_H
+
+#include <linux/irqnr.h>
+#include <linux/kernel_stat.h>
+#include <kerrighed/cpu_id.h>
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+/* Dynamic CPU informations */
+
+typedef struct {
+	struct kernel_stat stat;
+#ifdef CONFIG_GENERIC_HARDIRQS
+	unsigned int irqs[NR_IRQS];
+#endif
+	u64 total_intr;
+} krg_dynamic_cpu_info_t;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+extern struct kddm_set *dynamic_cpu_info_kddm_set;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+int dynamic_cpu_info_init(void);
+
+/** Helper function to get dynamic CPU info
+ *  @author Renaud Lottiaux
+ *
+ *  @param node_id   Id of the node hosting the CPU we want informations on.
+ *  @param cpu_id    Id of the CPU we want informations on.
+ *
+ *  @return  Structure containing information on the requested CPU.
+ */
+static inline krg_dynamic_cpu_info_t *get_dynamic_cpu_info(int node_id,
+							   int cpu_id)
+{
+	return _fkddm_get_object(dynamic_cpu_info_kddm_set,
+				 __krg_cpu_id(node_id, cpu_id),
+				 KDDM_NO_FREEZE|KDDM_NO_FT_REQ);
+}
+
+static inline
+unsigned int *krg_dynamic_cpu_info_irqs(krg_dynamic_cpu_info_t *info)
+{
+#ifdef CONFIG_GENERIC_HARDIRQS
+	return &info->irqs[0];
+#else
+	return &info->stat.irqs[0];
+#endif
+}
+
+#endif /* DYNAMIC_CPU_INFO LINKER_H */
diff -ruN linux-2.6.29/kerrighed/procfs/dynamic_node_info_linker.c android_cluster/linux-2.6.29/kerrighed/procfs/dynamic_node_info_linker.c
--- linux-2.6.29/kerrighed/procfs/dynamic_node_info_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/dynamic_node_info_linker.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,193 @@
+/** Dynamic node information management.
+ *  @file dynamic_node_info_linker.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/swap.h>
+#include <linux/kernel_stat.h>
+#include <linux/pagemap.h>
+#include <linux/hugetlb.h>
+#include <linux/mman.h>
+#include <linux/vmstat.h>
+#include <linux/quicklist.h>
+#include <linux/sched.h>
+#include <linux/nsproxy.h>
+#include <linux/pid_namespace.h>
+#include <linux/workqueue.h>
+
+#include <kerrighed/workqueue.h>
+#include <kddm/kddm.h>
+
+#include <kerrighed/dynamic_node_info_linker.h>
+#include "static_node_info_linker.h"
+
+#include <kerrighed/debug.h>
+
+/* Kddm set of node information locations */
+struct kddm_set *dynamic_node_info_kddm_set;
+
+/*****************************************************************************/
+/*                                                                           */
+/*                    DYNAMIC NODE INFO KDDM IO FUNCTIONS                    */
+/*                                                                           */
+/*****************************************************************************/
+
+/****************************************************************************/
+
+/* Init the dynamic node info IO linker */
+
+static struct iolinker_struct dynamic_node_info_io_linker = {
+	.default_owner = node_info_default_owner,
+	.linker_name = "dyn_node_nfo",
+	.linker_id = DYNAMIC_NODE_INFO_LINKER,
+};
+
+static void update_dynamic_node_info_worker(struct work_struct *work);
+static DECLARE_DELAYED_WORK(update_dynamic_node_info_work,
+			    update_dynamic_node_info_worker);
+
+void
+__attribute__((weak))
+krg_arch_fill_dynamic_node_info(krg_dynamic_node_info_t *info)
+{
+}
+
+/** Update the dynamic informations for the local node.
+ *  @author Renaud Lottiaux
+ */
+static void update_dynamic_node_info_worker(struct work_struct *work)
+{
+	krg_dynamic_node_info_t *dynamic_node_info;
+	cputime_t idletime = cputime_add(init_task.utime, init_task.stime);
+	struct sysinfo sysinfo;
+	struct timespec boottime;
+	unsigned long jif, seq;
+	int i;
+
+	dynamic_node_info = _kddm_grab_object(dynamic_node_info_kddm_set,
+					      kerrighed_node_id);
+
+	/* Compute data for uptime proc file */
+
+	cputime_to_timespec(idletime, &dynamic_node_info->idletime);
+	do_posix_clock_monotonic_gettime(&dynamic_node_info->uptime);
+	monotonic_to_bootbased(&dynamic_node_info->uptime);
+
+	/* Compute data for loadavg proc file */
+
+	do {
+		seq = read_seqbegin(&xtime_lock);
+		dynamic_node_info->avenrun[0] = avenrun[0];
+		dynamic_node_info->avenrun[1] = avenrun[1];
+		dynamic_node_info->avenrun[2] = avenrun[2];
+	} while (read_seqretry(&xtime_lock, seq));
+	dynamic_node_info->last_pid = task_active_pid_ns(current)->last_pid;
+	dynamic_node_info->nr_threads = nr_threads;
+	dynamic_node_info->nr_running = nr_running();
+
+	getboottime(&boottime);
+	jif = boottime.tv_sec;
+
+	dynamic_node_info->jif = (unsigned long)jif;
+	dynamic_node_info->total_forks = total_forks;
+	dynamic_node_info->nr_iowait = nr_iowait();
+	dynamic_node_info->nr_context_switches = nr_context_switches();
+
+#ifdef arch_irq_stat
+	dynamic_node_info->arch_irq = arch_irq_stat();
+#else
+	dynamic_node_info->arch_irq = 0;
+#endif
+
+	/* Compute data for meminfo proc file */
+
+	si_meminfo(&sysinfo);
+	si_swapinfo(&sysinfo);
+
+	dynamic_node_info->totalram = sysinfo.totalram;
+	dynamic_node_info->freeram = sysinfo.freeram;
+	dynamic_node_info->bufferram = sysinfo.bufferram;
+	dynamic_node_info->totalhigh = sysinfo.totalhigh;
+	dynamic_node_info->freehigh = sysinfo.freehigh;
+	dynamic_node_info->totalswap = sysinfo.totalswap;
+	dynamic_node_info->freeswap = sysinfo.freeswap;
+	dynamic_node_info->totalram = sysinfo.totalram;
+	dynamic_node_info->swapcache_pages = total_swapcache_pages;
+
+	for_each_lru(i)
+		dynamic_node_info->nr_pages[i - LRU_BASE] = global_page_state(i);
+#ifdef CONFIG_UNEVICTABLE_LRU
+	dynamic_node_info->nr_mlock = global_page_state(NR_MLOCK);
+#else
+	dynamic_node_info->nr_mlock = 0;
+#endif
+	dynamic_node_info->nr_file_pages = global_page_state(NR_FILE_PAGES);
+	dynamic_node_info->nr_file_dirty = global_page_state(NR_FILE_DIRTY);
+	dynamic_node_info->nr_writeback = global_page_state(NR_WRITEBACK);
+	dynamic_node_info->nr_anon_pages = global_page_state(NR_ANON_PAGES);
+	dynamic_node_info->nr_file_mapped = global_page_state(NR_FILE_MAPPED);
+	dynamic_node_info->nr_bounce = global_page_state(NR_BOUNCE);
+	dynamic_node_info->nr_page_table_pages =
+		global_page_state(NR_PAGETABLE);
+	dynamic_node_info->nr_slab_reclaimable =
+		global_page_state(NR_SLAB_RECLAIMABLE);
+	dynamic_node_info->nr_slab_unreclaimable =
+		global_page_state(NR_SLAB_UNRECLAIMABLE);
+	dynamic_node_info->nr_unstable_nfs =
+		global_page_state(NR_UNSTABLE_NFS);
+	dynamic_node_info->nr_writeback_temp =
+		global_page_state(NR_WRITEBACK_TEMP);
+
+	dynamic_node_info->quicklists = quicklist_total_size();
+
+	dynamic_node_info->allowed = ((totalram_pages - hugetlb_total_pages())
+				      * sysctl_overcommit_ratio / 100) +
+		                     total_swap_pages;
+
+//	dynamic_node_info->commited = percpu_counter_read_positive(&vm_committed_as);
+	dynamic_node_info->commited = atomic_long_read(&vm_committed_space);
+
+	get_vmalloc_info(&dynamic_node_info->vmi);
+	dynamic_node_info->vmalloc_total = VMALLOC_TOTAL;
+
+#ifdef CONFIG_HUGETLB_PAGE
+	dynamic_node_info->nr_huge_pages = default_hstate.nr_huge_pages;
+	dynamic_node_info->free_huge_pages = default_hstate.free_huge_pages;
+	dynamic_node_info->resv_huge_pages = default_hstate.resv_huge_pages;
+	dynamic_node_info->surplus_huge_pages =
+		default_hstate.surplus_huge_pages;
+#else
+	dynamic_node_info->nr_huge_pages = 0;
+	dynamic_node_info->free_huge_pages = 0;
+	dynamic_node_info->resv_huge_pages = 0;
+	dynamic_node_info->surplus_huge_pages = 0;
+#endif
+
+	krg_arch_fill_dynamic_node_info(dynamic_node_info);
+
+	_kddm_put_object(dynamic_node_info_kddm_set, kerrighed_node_id);
+
+	queue_delayed_work(krg_wq, &update_dynamic_node_info_work, HZ);
+}
+
+int dynamic_node_info_init(void)
+{
+	register_io_linker(DYNAMIC_NODE_INFO_LINKER,
+			   &dynamic_node_info_io_linker);
+
+	/* Create the node info kddm set */
+
+	dynamic_node_info_kddm_set =
+		create_new_kddm_set(kddm_def_ns, DYNAMIC_NODE_INFO_KDDM_ID,
+				    DYNAMIC_NODE_INFO_LINKER,
+				    KDDM_CUSTOM_DEF_OWNER,
+				    sizeof(krg_dynamic_node_info_t), 0);
+	if (IS_ERR(dynamic_node_info_kddm_set))
+		OOM;
+
+	/* Start periodic updates */
+	queue_delayed_work(krg_wq, &update_dynamic_node_info_work, 0);
+
+	return 0;
+}
diff -ruN linux-2.6.29/kerrighed/procfs/Makefile android_cluster/linux-2.6.29/kerrighed/procfs/Makefile
--- linux-2.6.29/kerrighed/procfs/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/Makefile	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,16 @@
+#
+# Makefile for Kerrighed's global ProcFs
+#
+
+obj-$(CONFIG_KRG_PROCFS) := krg_procfs.o
+
+krg_procfs-y := procfs.o proc.o \
+        static_node_info_linker.o static_cpu_info_linker.o \
+        dynamic_node_info_linker.o dynamic_cpu_info_linker.o \
+	procfs_hotplug.o
+
+krg_procfs-$(CONFIG_KRG_PROC) += proc_pid.o \
+	proc_pid_file.o proc_pid_link.o \
+	proc_pid_fd.o
+
+EXTRA_CFLAGS += -Wall -Werror
diff -ruN linux-2.6.29/kerrighed/procfs/proc.c android_cluster/linux-2.6.29/kerrighed/procfs/proc.c
--- linux-2.6.29/kerrighed/procfs/proc.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/proc.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,1120 @@
+/** /proc/kerrighed/ manager
+ *  @file proc.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <asm/processor.h>
+#include <linux/kernel_stat.h>
+#include <linux/seq_file.h>
+#include <linux/if.h>
+#include <asm/div64.h>
+#include <linux/threads.h>
+#include <linux/vmalloc.h>
+#include <linux/hugetlb.h>
+#include <linux/sched.h>
+#include <linux/nsproxy.h>
+#include <linux/proc_fs.h>
+#include <kerrighed/krgflags.h>
+#include <kerrighed/procfs.h>
+
+#include <kerrighed/hotplug.h>
+#include "proc.h"
+#include "static_node_info_linker.h"
+#include "static_cpu_info_linker.h"
+#include <kerrighed/dynamic_node_info_linker.h>
+#include "dynamic_cpu_info_linker.h"
+
+#define PROC_STAT_DEPEND_ON_CAPABILITY (KERRIGHED_MAX_NODES + 1)
+
+/* /proc/kerrighed entries */
+
+static struct proc_dir_entry *procfs_nodes;	/* /proc/kerrighed/nodes   */
+static struct proc_dir_entry *procfs_nrnodes;	/* /proc/kerrighed/nodes/nrnodes */
+
+static void krg_create_seq_entry(char *name,
+				 mode_t mode,
+				 struct file_operations *f,
+				 struct proc_dir_entry *parent, void *data)
+{
+	struct proc_dir_entry *entry;
+	entry = create_proc_entry(name, mode, parent);
+	if (entry) {
+		entry->proc_fops = f;
+		entry->data = data;
+	}
+}
+
+/** Check if a < b */
+static inline int timespec_lt(struct timespec *a, struct timespec *b)
+{
+	return ((a->tv_sec < b->tv_sec) ||
+		((a->tv_sec == b->tv_sec) && (a->tv_nsec < b->tv_nsec)));
+}
+
+static inline kerrighed_node_t get_req_node(kerrighed_node_t nodeid)
+{
+	if (!current->nsproxy->krg_ns) {
+		if (nodeid == PROC_STAT_DEPEND_ON_CAPABILITY)
+			return kerrighed_node_id;
+		else
+			return nodeid;
+	}
+
+#ifdef CONFIG_KRG_CAP
+	if (nodeid == PROC_STAT_DEPEND_ON_CAPABILITY) {
+		if (cap_raised
+		    (current->krg_caps.effective, CAP_SEE_LOCAL_PROC_STAT))
+			return kerrighed_node_id;
+		else
+			return KERRIGHED_MAX_NODES;
+	}
+#endif
+	return nodeid;
+}
+
+
+static inline krgnodemask_t get_proc_nodes_vector(kerrighed_node_t nodeid)
+{
+	krgnodemask_t nodes;
+	nodeid = get_req_node(nodeid);
+	krgnodes_clear(nodes);
+
+	if (nodeid == KERRIGHED_MAX_NODES) {
+		if (IS_KERRIGHED_NODE(KRGFLAGS_RUNNING))
+			krgnodes_copy(nodes, krgnode_online_map);
+		else
+			krgnode_set(kerrighed_node_id, nodes);
+	} else
+		krgnode_set(nodeid, nodes);
+
+	return nodes;
+}
+
+static void free_stat_buf(int size, void *buf)
+{
+	if (size > PAGE_SIZE)
+		vfree(buf);
+	else
+		kfree(buf);
+}
+
+static int krg_proc_stat_open(struct inode *inode,
+			      struct file *file,
+			      int (*show) (struct seq_file *, void *), int size)
+{
+	char *buf;
+	struct seq_file *m;
+	int res;
+
+	if (size > PAGE_SIZE)
+		buf = vmalloc(size);
+	else
+		buf = kmalloc(size, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	res = single_open(file, show, PROC_I(inode)->krg_procfs_private);
+	if (!res) {
+		m = file->private_data;
+		m->buf = buf;
+		m->size = size;
+	} else
+		free_stat_buf(size, buf);
+
+	return res;
+}
+
+static int krg_proc_stat_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *m = file->private_data;
+
+	free_stat_buf(m->size, m->buf);
+	m->buf = NULL;
+
+	return single_release (inode, file);
+}
+
+/****************************************************************************/
+/*                                                                          */
+/*                   /proc/kerrighed/cpuinfo  Management                    */
+/*                                                                          */
+/****************************************************************************/
+
+struct cpu_info_seq_struct {
+	int cur_node;
+	int last_node;
+	int cpu_id;
+	int req_node;
+	int last_pos;
+};
+
+static void init_cpu_info_seq_struct(struct cpu_info_seq_struct *seq_data)
+{
+	kerrighed_node_t req_node ;
+
+	req_node = get_req_node(seq_data->req_node);
+
+	if (krgnode_online(kerrighed_node_id)) {
+		// Init values to parse CPU.
+		if (req_node == KERRIGHED_MAX_NODES) {
+			// Cluster wide CPU info
+			seq_data->cur_node = nth_online_krgnode(0);
+			seq_data->last_node = KERRIGHED_MAX_NODES - 1;
+		} else {
+			// Node wide CPU info
+			seq_data->cur_node = req_node;
+			seq_data->last_node = req_node;
+		}
+	} else {
+		seq_data->cur_node = kerrighed_node_id;
+		seq_data->last_node = kerrighed_node_id;
+	}
+	seq_data->cpu_id = 0;
+	seq_data->last_pos = 0;
+}
+
+static void go_to_selected_cpu(struct cpu_info_seq_struct *seq_data,
+			       loff_t pos)
+{
+	krg_static_node_info_t *static_node_info;
+	int i;
+
+	for (i = seq_data->last_pos; i < pos; i++) {
+		seq_data->cpu_id++;
+		static_node_info = get_static_node_info(seq_data->cur_node);
+		if (seq_data->cpu_id >= static_node_info->nr_cpu) {
+			seq_data->cur_node =
+				krgnode_next_online(seq_data->cur_node);
+			seq_data->cpu_id = 0;
+		}
+	}
+	seq_data->last_pos = pos;
+}
+
+static void *c_start(struct seq_file *m, loff_t *pos)
+{
+	struct cpu_info_seq_struct *seq_data = m->private;
+	krg_static_cpu_info_t *cpu_info;
+
+	if (!current->nsproxy->krg_ns)
+		return cpuinfo_op.start(m, pos);
+
+	if (*pos == 0)
+		init_cpu_info_seq_struct (seq_data);
+	else {
+		// Switch to the requested CPU.
+		if (unlikely(*pos < seq_data->last_pos))
+			init_cpu_info_seq_struct (seq_data);
+
+		go_to_selected_cpu(seq_data, *pos);
+	}
+
+	if (seq_data->cur_node <= seq_data->last_node) {
+		cpu_info =
+		    get_static_cpu_info(seq_data->cur_node, seq_data->cpu_id);
+		return &cpu_info->info;
+	} else
+		return NULL;
+}
+
+static void *c_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	if (!current->nsproxy->krg_ns)
+		return cpuinfo_op.next(m, v, pos);
+
+	++*pos;
+
+	return c_start(m, pos);
+}
+
+static void c_stop(struct seq_file *m, void *v)
+{
+	if (!current->nsproxy->krg_ns)
+		cpuinfo_op.stop(m, v);
+}
+
+static int show_cpuinfo(struct seq_file *m, void *v)
+{
+	return cpuinfo_op.show(m, v);
+}
+
+struct seq_operations krg_cpuinfo_op = {
+	.start = c_start,
+	.next = c_next,
+	.stop = c_stop,
+	.show = show_cpuinfo,
+};
+
+extern struct seq_operations krg_cpuinfo_op;
+
+static int krg_cpuinfo_open(struct inode *inode, struct file *file)
+{
+	struct cpu_info_seq_struct *seq_data;
+	struct seq_file *m;
+	int ret;
+
+	ret = seq_open(file, &krg_cpuinfo_op);
+	if (ret < 0)
+		return ret;
+
+	seq_data = kmalloc(sizeof(struct cpu_info_seq_struct), GFP_KERNEL);
+	if (seq_data == NULL) {
+		seq_release(inode, file);
+		return -ENOMEM;
+	}
+
+	seq_data->req_node = (long)PROC_I(inode)->krg_procfs_private;
+	m = file->private_data;
+	m->private = seq_data;
+
+	return 0;
+}
+
+static int krg_cpuinfo_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *m = file->private_data;
+	kfree(m->private);
+
+	return seq_release(inode, file);
+}
+
+static struct file_operations proc_krg_cpuinfo_operations = {
+	.open = krg_cpuinfo_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = krg_cpuinfo_release,
+};
+
+/****************************************************************************/
+/*                                                                          */
+/*                   /proc/kerrighed/meminfo  Management                    */
+/*                                                                          */
+/****************************************************************************/
+
+void
+__attribute__((weak))
+krg_arch_accumulate_meminfo(const krg_dynamic_node_info_t *local_info,
+			    krg_dynamic_node_info_t *global_info)
+{
+}
+
+void
+__attribute__((weak))
+krg_arch_report_meminfo(struct seq_file *m, const krg_dynamic_node_info_t *info)
+{
+}
+
+/** Read function for /proc/kerrighed/meminfo entry.
+ *  @author Renaud Lottiaux
+ */
+static int show_meminfo(struct seq_file *p, void *v)
+{
+	kerrighed_node_t nodeid = (long)p->private;
+	krgnodemask_t nodes;
+	krg_dynamic_node_info_t global_dyn_info;
+	krg_dynamic_node_info_t *dyn_info;
+	kerrighed_node_t node;
+	long cached;
+	int i;
+
+	if (!current->nsproxy->krg_ns)
+		return meminfo_proc_show(p, v);
+
+	nodes = get_proc_nodes_vector(nodeid);
+
+	memset(&global_dyn_info, 0, sizeof(krg_dynamic_node_info_t));
+
+	for_each_krgnode_mask(node, nodes) {
+		dyn_info = get_dynamic_node_info(node);
+
+		global_dyn_info.totalram += dyn_info->totalram;
+		global_dyn_info.freeram += dyn_info->freeram;
+		global_dyn_info.bufferram += dyn_info->bufferram;
+		global_dyn_info.totalhigh += dyn_info->totalhigh;
+		global_dyn_info.freehigh += dyn_info->freehigh;
+		global_dyn_info.totalswap += dyn_info->totalswap;
+		global_dyn_info.freeswap += dyn_info->freeswap;
+		global_dyn_info.swapcache_pages += dyn_info->swapcache_pages;
+
+		for_each_lru(i)
+			global_dyn_info.nr_pages[i - LRU_BASE] +=
+				dyn_info->nr_pages[i - LRU_BASE];
+		global_dyn_info.nr_mlock += dyn_info->nr_mlock;
+		global_dyn_info.nr_file_pages += dyn_info->nr_file_pages;
+		global_dyn_info.nr_file_dirty += dyn_info->nr_file_dirty;
+		global_dyn_info.nr_writeback += dyn_info->nr_writeback;
+		global_dyn_info.nr_anon_pages += dyn_info->nr_anon_pages;
+		global_dyn_info.nr_file_mapped += dyn_info->nr_file_mapped;
+		global_dyn_info.nr_page_table_pages +=
+		    dyn_info->nr_page_table_pages;
+		global_dyn_info.nr_slab_reclaimable +=
+			dyn_info->nr_slab_reclaimable;
+		global_dyn_info.nr_slab_unreclaimable +=
+			dyn_info->nr_slab_unreclaimable;
+		global_dyn_info.nr_unstable_nfs += dyn_info->nr_unstable_nfs;
+		global_dyn_info.nr_bounce += dyn_info->nr_bounce;
+		global_dyn_info.nr_writeback_temp +=
+			dyn_info->nr_writeback_temp;
+
+		global_dyn_info.quicklists += dyn_info->quicklists;
+
+		global_dyn_info.vmi.used += dyn_info->vmi.used;
+		if (dyn_info->vmi.largest_chunk >
+		    global_dyn_info.vmi.largest_chunk)
+			global_dyn_info.vmi.largest_chunk =
+			    dyn_info->vmi.largest_chunk;
+		global_dyn_info.vmalloc_total += dyn_info->vmalloc_total;
+
+		global_dyn_info.allowed += dyn_info->allowed;
+		global_dyn_info.commited += dyn_info->commited;
+
+		global_dyn_info.nr_huge_pages += dyn_info->nr_huge_pages;
+		global_dyn_info.free_huge_pages += dyn_info->free_huge_pages;
+		global_dyn_info.resv_huge_pages += dyn_info->resv_huge_pages;
+		global_dyn_info.surplus_huge_pages +=
+			dyn_info->surplus_huge_pages;
+
+		krg_arch_accumulate_meminfo(dyn_info, &global_dyn_info);
+	}
+
+#define K(x) ((x) << (PAGE_SHIFT - 10))
+
+        cached = global_dyn_info.nr_file_pages -
+		global_dyn_info.swapcache_pages - global_dyn_info.bufferram;
+	if (cached < 0)
+		cached = 0;
+
+	seq_printf(p,
+		   "MemTotal:       %8lu kB\n"
+		   "MemFree:        %8lu kB\n"
+		   "Buffers:        %8lu kB\n"
+		   "Cached:         %8lu kB\n"
+		   "SwapCached:     %8lu kB\n"
+		   "Active:         %8lu kB\n"
+		   "Inactive:       %8lu kB\n"
+		   "Active(anon):   %8lu kB\n"
+		   "Inactive(anon): %8lu kB\n"
+		   "Active(file):   %8lu kB\n"
+		   "Inactive(file): %8lu kB\n"
+#ifdef CONFIG_UNEVICTABLE_LRU
+		   "Unevictable:    %8lu kB\n"
+		   "Mlocked:        %8lu kB\n"
+#endif
+#ifdef CONFIG_HIGHMEM
+		   "HighTotal:      %8lu kB\n"
+		   "HighFree:       %8lu kB\n"
+		   "LowTotal:       %8lu kB\n"
+		   "LowFree:        %8lu kB\n"
+#endif
+#ifndef CONFIG_MMU
+#error Is it possible to run Kerrighed without an MMU?
+#endif
+		   "SwapTotal:      %8lu kB\n"
+		   "SwapFree:       %8lu kB\n"
+		   "Dirty:          %8lu kB\n"
+		   "Writeback:      %8lu kB\n"
+		   "AnonPages:      %8lu kB\n"
+		   "Mapped:         %8lu kB\n"
+		   "Slab:           %8lu kB\n"
+		   "SReclaimable:   %8lu kB\n"
+		   "SUnreclaim:     %8lu kB\n"
+		   "PageTables:     %8lu kB\n"
+#ifdef CONFIG_QUICKLIST
+		   "Quicklists:     %8lu kB\n"
+#endif
+		   "NFS_Unstable:   %8lu kB\n"
+		   "Bounce:         %8lu kB\n"
+		   "WritebackTmp:   %8lu kB\n"
+		   "CommitLimit:    %8lu kB\n"
+		   "Committed_AS:   %8lu kB\n"
+		   "VmallocTotal:   %8lu kB\n"
+		   "VmallocUsed:    %8lu kB\n"
+		   "VmallocChunk:   %8lu kB\n",
+		   K(global_dyn_info.totalram),
+		   K(global_dyn_info.freeram),
+		   K(global_dyn_info.bufferram),
+		   K(cached),
+		   K(global_dyn_info.swapcache_pages),
+		   K(global_dyn_info.nr_pages[LRU_ACTIVE_ANON - LRU_BASE] +
+		     global_dyn_info.nr_pages[LRU_ACTIVE_FILE - LRU_BASE]),
+		   K(global_dyn_info.nr_pages[LRU_INACTIVE_ANON - LRU_BASE] +
+		     global_dyn_info.nr_pages[LRU_INACTIVE_FILE - LRU_BASE]),
+		   K(global_dyn_info.nr_pages[LRU_ACTIVE_ANON - LRU_BASE]),
+		   K(global_dyn_info.nr_pages[LRU_INACTIVE_ANON - LRU_BASE]),
+		   K(global_dyn_info.nr_pages[LRU_ACTIVE_FILE - LRU_BASE]),
+		   K(global_dyn_info.nr_pages[LRU_INACTIVE_FILE - LRU_BASE]),
+#ifdef CONFIG_UNEVICTABLE_LRU
+		   K(global_dyn_info.nr_pages[LRU_UNEVICTABLE - LRU_BASE]),
+		   K(global_dyn_info.nr_mlock),
+#endif
+#ifdef CONFIG_HIGHMEM
+		   K(global_dyn_info.totalhigh),
+		   K(global_dyn_info.freehigh),
+		   K(global_dyn_info.totalram - global_dyn_info.totalhigh),
+		   K(global_dyn_info.freeram - global_dyn_info.freehigh),
+#endif
+		   K(global_dyn_info.totalswap),
+		   K(global_dyn_info.freeswap),
+		   K(global_dyn_info.nr_file_dirty),
+		   K(global_dyn_info.nr_writeback),
+		   K(global_dyn_info.nr_anon_pages),
+		   K(global_dyn_info.nr_file_mapped),
+		   K(global_dyn_info.nr_slab_reclaimable +
+		     global_dyn_info.nr_slab_unreclaimable),
+		   K(global_dyn_info.nr_slab_reclaimable),
+		   K(global_dyn_info.nr_slab_unreclaimable),
+		   K(global_dyn_info.nr_page_table_pages),
+#ifdef CONFIG_QUICKLIST
+		   K(global_dyn_info.quicklists),
+#endif
+		   K(global_dyn_info.nr_unstable_nfs),
+		   K(global_dyn_info.nr_bounce),
+		   K(global_dyn_info.nr_writeback_temp),
+		   K(global_dyn_info.allowed),
+		   K(global_dyn_info.commited),
+		   global_dyn_info.vmalloc_total >> 10,
+		   global_dyn_info.vmi.used >> 10,
+		   global_dyn_info.vmi.largest_chunk >> 10);
+
+#ifdef CONFIG_HUGETLB_PAGE
+	seq_printf(p,
+		   "HugePages_Total:   %5lu\n"
+		   "HugePages_Free:    %5lu\n"
+		   "HugePages_Rsvd:    %5lu\n"
+		   "HugePages_Surp:    %5lu\n"
+		   "Hugepagesize:   %8lu kB\n",
+		   global_dyn_info.nr_huge_pages,
+		   global_dyn_info.free_huge_pages,
+		   global_dyn_info.resv_huge_pages,
+		   global_dyn_info.surplus_huge_pages,
+		   1UL << (huge_page_order(&default_hstate) + PAGE_SHIFT - 10));
+#endif
+
+	krg_arch_report_meminfo(p, &global_dyn_info);
+
+	return 0;
+#undef K
+}
+
+static int meminfo_open(struct inode *inode, struct file *file)
+{
+	return krg_proc_stat_open(inode, file, show_meminfo, 1500);
+}
+
+static struct file_operations proc_krg_meminfo_operations = {
+	.open = meminfo_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+/****************************************************************************/
+/*                                                                          */
+/*                     /proc/kerrighed/stat  Management                     */
+/*                                                                          */
+/****************************************************************************/
+
+/** Read function for /proc/kerrighed/stat entry.
+ *  @author Renaud Lottiaux
+ *
+ */
+static int krg_show_stat(struct seq_file *p, void *v)
+{
+	kerrighed_node_t req_nodeid = (long)p->private;
+	krg_dynamic_cpu_info_t *dynamic_cpu_info;
+	struct cpu_usage_stat *stat;
+	krg_dynamic_node_info_t *dynamic_node_info;
+	krg_static_node_info_t *static_node_info;
+	int i, j;
+	krgnodemask_t nodes;
+	cputime64_t user, nice, system, idle, iowait, irq, softirq, steal;
+	cputime64_t guest;
+	unsigned long long nr_context_switches = 0;
+	unsigned long jif = 0, total_forks = 0, nr_running = 0, nr_iowait = 0;
+	kerrighed_node_t node_id;
+	unsigned int *irqs, *cpu_irqs;
+	u64 total_intr = 0;
+#define HEAD_BLANK_LEN 81
+	static const char head_blank[HEAD_BLANK_LEN + 1] = {
+		[ 0 ... HEAD_BLANK_LEN - 2 ] = ' ',
+		[HEAD_BLANK_LEN - 1] = '\n',
+		[HEAD_BLANK_LEN] = '\0'
+	};
+	int head_len;
+
+	if (!current->nsproxy->krg_ns)
+		return show_stat(p, v);
+
+	irqs = kmalloc(sizeof(*irqs) * NR_IRQS, GFP_KERNEL);
+	if (!irqs)
+		return -ENOMEM;
+	for (j = 0; j < NR_IRQS; j++)
+		irqs[j] = 0;
+
+	nodes = get_proc_nodes_vector(req_nodeid);
+
+	user = nice = system = idle = iowait = irq = softirq = steal =
+	    cputime64_zero;
+	guest = cputime64_zero;
+
+	/*
+	 * Keep space to overwrite "cpu" line later in order to get this line
+	 * first without parsing data twice... Yes... Dirty...
+	 */
+	seq_printf(p, "%s", head_blank);
+	for_each_krgnode_mask(node_id, nodes) {
+		static_node_info = get_static_node_info(node_id);
+		dynamic_node_info = get_dynamic_node_info(node_id);
+
+		/* Compute node level stat informations */
+
+		nr_context_switches += dynamic_node_info->nr_context_switches;
+		if (dynamic_node_info->jif > jif)
+			jif = dynamic_node_info->jif;
+		total_forks += dynamic_node_info->total_forks;
+		nr_running += dynamic_node_info->nr_running;
+		nr_iowait += dynamic_node_info->nr_iowait;
+
+		for (i = 0; i < static_node_info->nr_cpu; i++) {
+			dynamic_cpu_info = get_dynamic_cpu_info(node_id, i);
+
+			stat = &dynamic_cpu_info->stat.cpustat;
+			seq_printf(p,
+				   "cpu%d  %llu %llu %llu %llu %llu %llu %llu %llu %llu\n",
+				   __krg_cpu_id(node_id, i),
+				   (unsigned long long)
+				   cputime64_to_clock_t(stat->user),
+				   (unsigned long long)
+				   cputime64_to_clock_t(stat->nice),
+				   (unsigned long long)
+				   cputime64_to_clock_t(stat->system),
+				   (unsigned long long)
+				   cputime64_to_clock_t(stat->idle),
+				   (unsigned long long)
+				   cputime64_to_clock_t(stat->iowait),
+				   (unsigned long long)
+				   cputime64_to_clock_t(stat->irq),
+				   (unsigned long long)
+				   cputime64_to_clock_t(stat->softirq),
+				   (unsigned long long)
+				   cputime64_to_clock_t(stat->steal),
+				   (unsigned long long)
+				   cputime64_to_clock_t(stat->guest));
+
+			/* Compute CPU level stat informations */
+
+			user = cputime64_add(user, stat->user);
+			nice = cputime64_add(nice, stat->nice);
+			system = cputime64_add(system, stat->system);
+			idle = cputime64_add(idle, stat->idle);
+			iowait = cputime64_add(iowait, stat->iowait);
+			irq = cputime64_add(irq, stat->irq);
+			softirq = cputime64_add(softirq, stat->softirq);
+			steal = cputime64_add(steal, stat->steal);
+			guest = cputime64_add(guest, stat->guest);
+
+			cpu_irqs = krg_dynamic_cpu_info_irqs(dynamic_cpu_info);
+			for (j = 0; j < NR_IRQS; j++)
+				irqs[j] += cpu_irqs[j];
+			total_intr += dynamic_cpu_info->total_intr;
+		}
+		total_intr += dynamic_node_info->arch_irq;
+	}
+
+	/* Dirty trick to print "cpu" line at the beginning without parsing data
+	 * twice
+	 */
+	head_len = snprintf(p->buf, HEAD_BLANK_LEN,
+			    "cpu   %llu %llu %llu %llu %llu %llu %llu %llu %llu",
+			    (unsigned long long)cputime64_to_clock_t(user),
+			    (unsigned long long)cputime64_to_clock_t(nice),
+			    (unsigned long long)cputime64_to_clock_t(system),
+			    (unsigned long long)cputime64_to_clock_t(idle),
+			    (unsigned long long)cputime64_to_clock_t(iowait),
+			    (unsigned long long)cputime64_to_clock_t(irq),
+			    (unsigned long long)cputime64_to_clock_t(softirq),
+			    (unsigned long long)cputime64_to_clock_t(steal),
+			    (unsigned long long)cputime64_to_clock_t(guest));
+	/*
+	 * The NUL byte inserted overwrote a blank...
+	 * in the middle of the file!
+	 */
+	if (head_len >= HEAD_BLANK_LEN - 1)
+		/* NUL byte overwrote \n */
+		p->buf[HEAD_BLANK_LEN - 1] = '\n';
+	else
+		p->buf[head_len] = ' ';
+#undef HEAD_BLANK_LEN
+
+	seq_printf(p, "intr %llu", (unsigned long long)total_intr);
+
+	for (j = 0; j < NR_IRQS; j++)
+		seq_printf(p, " %u", irqs[j]);
+	kfree(irqs);
+
+	seq_printf(p,
+		   "\nctxt %llu\n"
+		   "btime %lu\n"
+		   "processes %lu\n"
+		   "procs_running %lu\n"
+		   "procs_blocked %lu\n",
+		   nr_context_switches,
+		   jif,
+		   total_forks,
+		   nr_running,
+		   nr_iowait);
+
+	return 0;
+}
+
+static int stat_open(struct inode *inode, struct file *file)
+{
+	unsigned size;
+
+	size = 256 + NR_IRQS * 8 + NR_CPUS * kerrighed_nb_nodes * 80;
+
+	return krg_proc_stat_open(inode, file, krg_show_stat, size);
+}
+
+static struct file_operations proc_krg_stat_operations = {
+	.open = stat_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = krg_proc_stat_release,
+};
+
+/****************************************************************************/
+/*                                                                          */
+/*                    /proc/kerrighed/loadavg  Management                   */
+/*                                                                          */
+/****************************************************************************/
+
+/* Copied from fs/proc/loadavg.c */
+#define LOAD_INT(x) ((x) >> FSHIFT)
+#define LOAD_FRAC(x) LOAD_INT(((x) & (FIXED_1-1)) * 100)
+
+/** Read function for /proc/kerrighed/loadavg entry.
+ *  @author Renaud Lottiaux
+ *
+ */
+static int show_loadavg(struct seq_file *p, void *v)
+{
+	kerrighed_node_t nodeid = (long)p->private;
+	krg_dynamic_node_info_t *dynamic_node_info;
+	krgnodemask_t nodes;
+	kerrighed_node_t i;
+	int a, b, c, nr_threads, last_pid;
+	long nr_running;
+
+	if (!current->nsproxy->krg_ns)
+		return loadavg_proc_show(p, v);
+
+	a = b = c = nr_running = nr_threads = last_pid = 0;
+
+	nodes = get_proc_nodes_vector(nodeid);
+
+	for_each_krgnode_mask(i, nodes) {
+		dynamic_node_info = get_dynamic_node_info(i);
+		a += dynamic_node_info->avenrun[0];
+		b += dynamic_node_info->avenrun[1];
+		c += dynamic_node_info->avenrun[2];
+
+		nr_running += dynamic_node_info->nr_running;
+		nr_threads += dynamic_node_info->nr_threads;
+		last_pid = dynamic_node_info->last_pid;
+	}
+	a += (FIXED_1 / 200);
+	b += (FIXED_1 / 200);
+	c += (FIXED_1 / 200);
+
+	seq_printf(p, "%d.%02d %d.%02d %d.%02d %ld/%d %d\n",
+		   LOAD_INT(a), LOAD_FRAC(a),
+		   LOAD_INT(b), LOAD_FRAC(b),
+		   LOAD_INT(c), LOAD_FRAC(c),
+		   nr_running, nr_threads,
+		   last_pid);
+
+	return 0;
+}
+
+static int loadavg_open(struct inode *inode, struct file *file)
+{
+	return krg_proc_stat_open(inode, file, show_loadavg, 100);
+}
+
+static struct file_operations proc_krg_loadavg_operations = {
+	.open = loadavg_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = krg_proc_stat_release,
+};
+
+/****************************************************************************/
+/*                                                                          */
+/*                    /proc/kerrighed/nodeid  Management                    */
+/*                                                                          */
+/****************************************************************************/
+
+static int show_nodeid(struct seq_file *p, void *v)
+{
+	kerrighed_node_t nodeid = (long)p->private;
+
+	if (nodeid >= KERRIGHED_MAX_NODES)
+		seq_printf(p, "-\n");
+	else
+		seq_printf(p, "%d\n", nodeid);
+
+	return 0;
+}
+
+static int nodeid_open(struct inode *inode, struct file *file)
+{
+	return krg_proc_stat_open(inode, file, show_nodeid, 10);
+}
+
+static struct file_operations proc_krg_nodeid_operations = {
+	.open = nodeid_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = krg_proc_stat_release,
+};
+
+/****************************************************************************/
+/*                                                                          */
+/*                 /proc/nodes/nrnodes  Management                          */
+/*                                                                          */
+/****************************************************************************/
+
+/** Read function for /proc/nodes/nrnodes entry.
+ *  @author Renaud Lottiaux
+ *
+ *  @param buffer           Buffer to write data to.
+ *  @param buffer_location  Alternative buffer to return...
+ *  @param offset           Offset of the first byte to write in the buffer.
+ *  @param buffer_length    Length of given buffer
+ *
+ *  @return  Number of bytes written.
+ */
+static int krg_nrnodes_read_proc(char *buffer, char **start, off_t offset,
+				 int count, int *eof, void *data)
+{
+	static char mybuffer[64];
+	static int len;
+
+	if (offset == 0)
+		len = snprintf(mybuffer, 40,
+			       "ONLINE:%d\n"
+			       "PRESENT:%d\n"
+			       "POSSIBLE:%d\n",
+			       num_online_krgnodes(),
+			       num_present_krgnodes(),
+			       num_possible_krgnodes());
+
+	if (offset + count >= len) {
+		count = len - offset;
+		if (count < 0)
+			count = 0;
+		*eof = 1;
+	}
+
+	memcpy(buffer, &mybuffer[offset], count);
+
+	return count;
+}
+
+/****************************************************************************/
+/*                                                                          */
+/*                    /proc/kerrighed/session  Management                   */
+/*                                                                          */
+/****************************************************************************/
+
+static int show_session(struct seq_file *p, void *v)
+{
+	seq_printf(p, "%d\n", kerrighed_session_id);
+
+	return 0;
+}
+
+static int session_open(struct inode *inode, struct file *file)
+{
+	return krg_proc_stat_open(inode, file, show_session, 10);
+}
+
+static struct file_operations proc_krg_session_operations = {
+	.open = session_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = krg_proc_stat_release,
+};
+
+/****************************************************************************/
+/*                                                                          */
+/*                    /proc/kerrighed/uptime  Management                    */
+/*                                                                          */
+/****************************************************************************/
+
+/** Read function for /proc/kerrighed/uptime entry.
+ *  @author Renaud Lottiaux
+ */
+static int show_uptime(struct seq_file *p, void *v)
+{
+	kerrighed_node_t nodeid = (long)p->private;
+	krg_dynamic_node_info_t *dynamic_node_info;
+	kerrighed_node_t i, nr_nodes = 0;
+	krgnodemask_t nodes;
+	struct timespec uptime;
+	unsigned long long idle = 0;
+	unsigned long idle_mod;
+
+//	if (!current->nsproxy->krg_ns)
+//		return uptime_proc_show(p, v);
+
+	nodes = get_proc_nodes_vector(nodeid);
+
+	uptime.tv_sec = uptime.tv_nsec = 0;
+
+	for_each_krgnode_mask(i, nodes) {
+		dynamic_node_info = get_dynamic_node_info(i);
+		nr_nodes++;
+
+		if (timespec_lt(&uptime, &dynamic_node_info->uptime))
+			uptime = dynamic_node_info->uptime;
+
+		idle += (unsigned long long)dynamic_node_info->idletime.tv_sec *
+		    NSEC_PER_SEC + dynamic_node_info->idletime.tv_nsec;
+	}
+
+	do_div(idle, nr_nodes);
+	idle_mod = do_div(idle, NSEC_PER_SEC);
+
+	seq_printf(p, "%lu.%02lu %lu.%02lu\n",
+		   (unsigned long)uptime.tv_sec,
+		   (uptime.tv_nsec / (NSEC_PER_SEC / 100)),
+		   (unsigned long)idle, (idle_mod / (NSEC_PER_SEC / 100)));
+
+	return 0;
+}
+
+static int uptime_open(struct inode *inode, struct file *file)
+{
+	return krg_proc_stat_open(inode, file, show_uptime, 100);
+}
+
+static struct file_operations proc_krg_uptime_operations = {
+	.open = uptime_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = krg_proc_stat_release,
+};
+
+/****************************************************************************/
+/*                                                                          */
+/*                   /proc/kerrighed/net/dev  Management                    */
+/*                                                                          */
+/****************************************************************************/
+
+/** Read function for /proc/kerrighed/nodeid entry.
+ *  @author Renaud Lottiaux
+ *
+ *  @param buffer           Buffer to write data to.
+ *  @param buffer_location  Alternative buffer to return...
+ *  @param offset           Offset of the first byte to write in the buffer.
+ *  @param buffer_length    Length of given buffer
+ *
+ *  @return  Number of bytes written.
+ */
+int krg_netdev_read_proc(char *buffer,
+			 char **start,
+			 off_t offset, int count, int *eof, void *data)
+{
+	return count;
+}
+
+/** Create a /proc/nodes/node<x> directory and sub-files.
+ *  @author Renaud Lottiaux
+ *
+ *  @param nodeid   Id of the node to create a proc entry for.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int create_proc_node_info(kerrighed_node_t nodeid)
+{
+	struct proc_dir_entry *node_procfs;
+	char dir_name[80];
+
+	/* Create the /proc/nodes/node<x> entry */
+
+	if (nodeid == KERRIGHED_MAX_NODES)
+		snprintf(dir_name, 80, "cluster");
+	else
+		snprintf(dir_name, 80, "node%d", nodeid);
+
+	node_procfs = create_proc_entry(dir_name, S_IFDIR | S_IRUGO | S_IWUGO |
+					S_IXUGO, procfs_nodes);
+
+	if (node_procfs == NULL)
+		return -ENOMEM;
+
+	/* Create entries in /proc/nodes/node<x> */
+
+	if (nodeid != KERRIGHED_MAX_NODES)
+		krg_create_seq_entry("nodeid", 0, &proc_krg_nodeid_operations,
+				     node_procfs, (void *)((long)nodeid));
+	krg_create_seq_entry("session", 0, &proc_krg_session_operations,
+			     node_procfs, (void *)((long)nodeid));
+#ifdef CONFIG_CLUSTER_WIDE_PROC_CPUINFO
+#define CW_CPUINFO 1
+#else
+#define CW_CPUINFO 0
+#endif
+	if (CW_CPUINFO || nodeid != KERRIGHED_MAX_NODES)
+		krg_create_seq_entry("cpuinfo", 0, &proc_krg_cpuinfo_operations,
+				     node_procfs, (void *)((long)nodeid));
+#ifdef CONFIG_CLUSTER_WIDE_PROC_MEMINFO
+#define CW_MEMINFO 1
+#else
+#define CW_MEMINFO 0
+#endif
+	if (CW_MEMINFO || nodeid != KERRIGHED_MAX_NODES)
+		krg_create_seq_entry("meminfo", 0, &proc_krg_meminfo_operations,
+				     node_procfs, (void *)((long)nodeid));
+#ifdef CONFIG_CLUSTER_WIDE_PROC_LOADAVG
+#define CW_LOADAVG 1
+#else
+#define CW_LOADAVG 0
+#endif
+	if (CW_LOADAVG || nodeid != KERRIGHED_MAX_NODES)
+		krg_create_seq_entry("loadavg", 0, &proc_krg_loadavg_operations,
+				     node_procfs, (void *)((long)nodeid));
+#ifdef CONFIG_CLUSTER_WIDE_PROC_UPTIME
+#define CW_UPTIME 1
+#else
+#define CW_UPTIME 0
+#endif
+	if (CW_UPTIME || nodeid != KERRIGHED_MAX_NODES)
+		krg_create_seq_entry("uptime", 0, &proc_krg_uptime_operations,
+				     node_procfs, (void *)((long)nodeid));
+#ifdef CONFIG_CLUSTER_WIDE_PROC_STAT
+#define CW_STAT 1
+#else
+#define CW_STAT 0
+#endif
+	if (CW_STAT || nodeid != KERRIGHED_MAX_NODES)
+		krg_create_seq_entry("stat", 0, &proc_krg_stat_operations,
+				     node_procfs, (void *)((long)nodeid));
+
+	if (nodeid == kerrighed_node_id)
+		proc_symlink("self", procfs_nodes, dir_name);
+
+	return 0;
+}
+
+/** Remove a /proc/nodes/node<x> directory and sub-files.
+ *  @author Matthieu Fertré
+ *
+ *  @param nodeid   Id of the node to remove a proc entry for.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+int remove_proc_node_info(kerrighed_node_t nodeid)
+{
+	struct proc_dir_entry *subdir, *next;
+	char dir_name[80];
+
+	snprintf(dir_name, 80, "node%d", nodeid);
+
+	subdir = procfs_nodes->subdir;
+	if (subdir) {
+		for (next = subdir->next; next;
+                     subdir = next, next = subdir->next)
+			if (!strcmp(subdir->name, dir_name)){
+				procfs_deltree(subdir);
+				return 0;
+			}
+	}
+
+	return -ENOENT;
+}
+
+/** Init Kerrighed proc stuffs.
+ *  @author Renaud Lottiaux
+ */
+int krg_procfs_init(void)
+{
+	/* Create the /proc/kerrighed/nodes entry */
+
+#ifdef CONFIG_CLUSTER_WIDE_PROC
+	procfs_nodes = create_proc_entry("nodes", S_IFDIR | S_IRUGO | S_IWUGO |
+					 S_IXUGO, NULL);
+#else
+	procfs_nodes = create_proc_entry("nodes", S_IFDIR | S_IRUGO | S_IWUGO |
+					 S_IXUGO, proc_kerrighed);
+#endif
+
+	if (procfs_nodes == NULL) {
+		WARNING("Cannot create /proc/kerrighed/nodes\n");
+		return -ENOMEM;
+	}
+
+	/* Create the /proc/kerrighed/nodes/nrnodes entry */
+
+	procfs_nrnodes =
+	    create_proc_read_entry("nrnodes", S_IRUGO, procfs_nodes,
+				   krg_nrnodes_read_proc, NULL);
+
+	if (procfs_nrnodes == NULL) {
+		WARNING("Cannot create /proc/kerrighed/nodes/nrnodes\n");
+		return -ENOMEM;
+	}
+
+	/* Create cluster-wide entries in /proc/ */
+
+#ifdef CONFIG_CLUSTER_WIDE_PROC_CPUINFO
+	remove_proc_entry("cpuinfo", NULL);
+	krg_create_seq_entry("cpuinfo", 0, &proc_krg_cpuinfo_operations,
+			     NULL,
+			     (void *)((int)PROC_STAT_DEPEND_ON_CAPABILITY));
+#endif
+#ifdef CONFIG_CLUSTER_WIDE_PROC_MEMINFO
+	remove_proc_entry("meminfo", NULL);
+	krg_create_seq_entry("meminfo", 0, &proc_krg_meminfo_operations, NULL,
+			     (void *)((int)PROC_STAT_DEPEND_ON_CAPABILITY));
+#endif
+#ifdef CONFIG_CLUSTER_WIDE_PROC_LOADAVG
+	remove_proc_entry("loadavg", NULL);
+	krg_create_seq_entry("loadavg", 0, &proc_krg_loadavg_operations, NULL,
+			     (void *)((int)PROC_STAT_DEPEND_ON_CAPABILITY));
+#endif
+#ifdef CONFIG_CLUSTER_WIDE_PROC_STAT
+	remove_proc_entry("stat", NULL);
+	krg_create_seq_entry("stat", 0, &proc_krg_stat_operations, NULL,
+			     (void *)((int)PROC_STAT_DEPEND_ON_CAPABILITY));
+#endif
+#ifdef CONFIG_CLUSTER_WIDE_PROC_UPTIME
+	remove_proc_entry("uptime", NULL);
+	krg_create_seq_entry("uptime", 0, &proc_krg_uptime_operations, NULL,
+			     (void *)((int)PROC_STAT_DEPEND_ON_CAPABILITY));
+#endif
+	/*  proc_net_remove("dev"); */
+
+#ifdef CONFIG_CLUSTER_WIDE_PROC_INFRA
+	/* Create the /proc/kerrighed/nodes/cluster entry */
+
+	create_proc_node_info(KERRIGHED_MAX_NODES);
+#endif
+
+	return 0;
+}
+
+/** Finalize Kerrighed proc stuffs.
+ *  @author Renaud Lottiaux
+ */
+int krg_procfs_finalize(void)
+{
+	procfs_deltree(procfs_nodes);
+
+	return 0;
+}
diff -ruN linux-2.6.29/kerrighed/procfs/procfs.c android_cluster/linux-2.6.29/kerrighed/procfs/procfs.c
--- linux-2.6.29/kerrighed/procfs/procfs.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/procfs.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,45 @@
+/** Initialization of procfs stuffs for ProcFS module.
+ *  @file procfs.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include "proc.h"
+#ifdef CONFIG_KRG_PROC
+#include "proc_pid.h"
+#endif
+#include "static_node_info_linker.h"
+#include "static_cpu_info_linker.h"
+#include <kerrighed/dynamic_node_info_linker.h>
+#include "dynamic_cpu_info_linker.h"
+
+int procfs_hotplug_init(void);
+void procfs_hotplug_cleanup(void);
+
+int init_procfs(void)
+{
+	static_node_info_init();
+	static_cpu_info_init();
+	dynamic_node_info_init();
+	dynamic_cpu_info_init();
+
+#ifdef CONFIG_KRG_PROC
+	proc_pid_init();
+#endif
+
+	krg_procfs_init();
+
+	procfs_hotplug_init();
+
+	return 0;
+}
+
+void cleanup_procfs(void)
+{
+	procfs_hotplug_cleanup();
+	krg_procfs_finalize();
+
+#ifdef CONFIG_KRG_PROC
+	proc_pid_finalize();
+#endif
+}
diff -ruN linux-2.6.29/kerrighed/procfs/procfs_hotplug.c android_cluster/linux-2.6.29/kerrighed/procfs/procfs_hotplug.c
--- linux-2.6.29/kerrighed/procfs/procfs_hotplug.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/procfs_hotplug.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,71 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+#include <linux/notifier.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/hotplug.h>
+
+#include "proc.h"
+
+struct notifier_block;
+
+inline
+void procfs_add(krgnodemask_t * v){
+	kerrighed_node_t i;
+
+	__for_each_krgnode_mask(i, v){
+		create_proc_node_info(i);
+	};
+
+};
+
+inline
+void procfs_remove(krgnodemask_t * v){
+	kerrighed_node_t i;
+
+	__for_each_krgnode_mask(i, v){
+		remove_proc_node_info(i);
+	};
+
+};
+
+
+/**
+ *
+ * Notifier related part
+ *
+ */
+
+static int procfs_notification(struct notifier_block *nb, hotplug_event_t event,
+			    void *data){
+	struct hotplug_context *ctx;
+	struct hotplug_node_set *node_set;
+
+	switch(event){
+	case HOTPLUG_NOTIFY_ADD:
+		ctx = data;
+		procfs_add(&ctx->node_set.v);
+		break;
+
+	case HOTPLUG_NOTIFY_REMOVE_ADVERT:
+		node_set = data;
+		procfs_remove(&node_set->v);
+		break;
+	default:
+		break;
+	}
+
+	return NOTIFY_OK;
+};
+
+int procfs_hotplug_init(void){
+	register_hotplug_notifier(procfs_notification, HOTPLUG_PRIO_MEMBERSHIP_ONLINE);
+	return 0;
+};
+
+void procfs_hotplug_cleanup(void){
+};
diff -ruN linux-2.6.29/kerrighed/procfs/proc.h android_cluster/linux-2.6.29/kerrighed/procfs/proc.h
--- linux-2.6.29/kerrighed/procfs/proc.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/proc.h	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,18 @@
+#ifndef KRG_PROCFS_H
+#define KRG_PROCFS_H
+
+#include <kerrighed/sys/types.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+int krg_procfs_init(void);
+int krg_procfs_finalize(void);
+
+int create_proc_node_info(kerrighed_node_t node);
+int remove_proc_node_info(kerrighed_node_t node);
+
+#endif /* KRG_PROCFS_H */
diff -ruN linux-2.6.29/kerrighed/procfs/proc_pid.c android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid.c
--- linux-2.6.29/kerrighed/procfs/proc_pid.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,962 @@
+/** Global cluster information management.
+ *  @file proc_pid_info.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2007, Louis Rilling - Kerlabs.
+ */
+
+#include <linux/kernel.h>
+#include <linux/proc_fs.h>
+#include <linux/procfs_internal.h>
+#include <linux/stat.h>
+#include <linux/sched.h>
+#include <linux/elf.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/task.h>
+#include <kerrighed/krginit.h>
+
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/task.h>
+#include <kerrighed/pid.h>
+#include "proc_pid.h"
+#include "proc_pid_fd.h"
+#include "proc_pid_link.h"
+#include "proc_pid_file.h"
+
+/*
+ * Custom types for remote pid entries, which differ from local pid entries in
+ * arguments of proc_read/proc_show because we cannot provide task structs
+ * as arguments.
+ */
+
+struct krg_pid_entry {
+	char *name;
+	int len;
+	mode_t mode;
+	const struct inode_operations *iop;
+	const struct file_operations *fop;
+	union proc_distant_op op;
+};
+
+#define NOD(NAME, MODE, IOP, FOP, OP) {			\
+	.name = (NAME),					\
+	.len  = sizeof(NAME) - 1,			\
+	.mode = MODE,					\
+	.iop  = IOP,					\
+	.fop  = FOP,					\
+	.op   = OP,					\
+}
+
+#define DIR(NAME, MODE, OTYPE)							\
+	NOD(NAME, (S_IFDIR|(MODE)),						\
+		&krg_proc_##OTYPE##_inode_operations,				\
+		&krg_proc_##OTYPE##_operations,					\
+		{} )
+#define LNK(NAME, OTYPE)					\
+	NOD(NAME, (S_IFLNK|S_IRWXUGO),				\
+		&krg_proc_pid_link_inode_operations, NULL,	\
+		{ .proc_get_link = &krg_proc_##OTYPE##_link } )
+#define REG(NAME, MODE, OTYPE)				\
+	NOD(NAME, (S_IFREG|(MODE)), NULL,		\
+		&krg_proc_##OTYPE##_operations, {})
+#define INF(NAME, MODE, OTYPE)				\
+	NOD(NAME, (S_IFREG|(MODE)),			\
+		NULL, &krg_proc_info_file_operations,	\
+		{ .proc_read = &krg_proc_##OTYPE } )
+#define ONE(NAME, MODE, OTYPE)				\
+	NOD(NAME, (S_IFREG|(MODE)),			\
+		NULL, &krg_proc_single_file_operations,	\
+		{ .proc_show = &krg_proc_##OTYPE } )
+
+static
+unsigned int krg_pid_entry_count_dirs(const struct krg_pid_entry *entries,
+				      unsigned int n)
+{
+	unsigned int i;
+	unsigned int count;
+
+	count = 0;
+	for (i = 0; i < n; ++i) {
+		if (S_ISDIR(entries[i].mode))
+			++count;
+	}
+
+	return count;
+}
+
+static struct inode *krg_proc_pid_make_inode(struct super_block *sb,
+					     struct proc_distant_pid_info *task)
+{
+	struct inode *inode;
+	struct proc_distant_pid_info *ei;
+
+	/* We need a new inode */
+
+	inode = new_inode(sb);
+	if (!inode)
+		goto out;
+
+	/* Common stuff */
+	inode->i_mtime = inode->i_atime = inode->i_ctime = CURRENT_TIME;
+	inode->i_op = &proc_def_inode_operations;
+
+	ei = get_krg_proc_task(inode);
+	krg_task_get(task->task_obj);
+	*ei = *task;
+
+	inode->i_uid = 0;
+	inode->i_gid = 0;
+	if (task->dumpable) {
+		inode->i_uid = task->euid;
+		inode->i_gid = task->egid;
+	}
+
+out:
+	return inode;
+}
+
+static int krg_pid_getattr(struct vfsmount *mnt, struct dentry *dentry,
+			   struct kstat *stat)
+{
+	struct inode *inode = dentry->d_inode;
+	struct proc_distant_pid_info *task;
+
+	generic_fillattr(inode, stat);
+
+	stat->uid = 0;
+	stat->gid = 0;
+	task = get_krg_proc_task(inode);
+	if (krg_task_alive(task->task_obj)) {
+		if ((inode->i_mode == (S_IFDIR|S_IRUGO|S_IXUGO)) ||
+		    task->dumpable) {
+			stat->uid = task->euid;
+			stat->gid = task->egid;
+		}
+	}
+
+	return 0;
+}
+
+static int krg_pid_revalidate(struct dentry *dentry, struct nameidata *nd)
+{
+	/* We need to check that the pid still exists in the system */
+	struct inode *inode = dentry->d_inode;
+	struct proc_distant_pid_info *ei = get_krg_proc_task(inode);
+	struct task_kddm_object *obj;
+	long state = EXIT_DEAD;
+
+	/*
+	 * Optimization: avoid doing krg_task_readlock() when it is obviously
+	 * useless.
+	 */
+	if (!krg_task_alive(ei->task_obj))
+		goto drop;
+	/* If pid is reused in between, the former task_obj field is dead. */
+	obj = krg_task_readlock(ei->pid);
+	if (!krg_task_alive(ei->task_obj) || !obj)
+		goto unlock;
+
+	BUG_ON(obj != ei->task_obj);
+	state = obj->exit_state;
+	if (obj->node == kerrighed_node_id)
+		/*
+		 * The task is probably not dead, but we want the dentry
+		 * to be regenerated with vanilla procfs operations.
+		 */
+		state = EXIT_DEAD;
+	if (state != EXIT_DEAD) {
+		ei->dumpable = obj->dumpable;
+		if ((inode->i_mode == (S_IFDIR|S_IRUGO|S_IXUGO)) ||
+		    obj->dumpable) {
+			inode->i_uid = obj->euid;
+			inode->i_gid = obj->egid;
+		} else {
+			inode->i_uid = 0;
+			inode->i_gid = 0;
+		}
+		inode->i_mode &= ~(S_ISUID | S_ISGID);
+	}
+
+unlock:
+	krg_task_unlock(ei->pid);
+
+	if (state != EXIT_DEAD)
+		return 1;
+drop:
+	d_drop(dentry);
+	return 0;
+}
+
+static int krg_pid_delete_dentry(struct dentry *dentry)
+{
+	struct proc_distant_pid_info *ei = get_krg_proc_task(dentry->d_inode);
+
+	/*
+	 * If the task is local, we want the dentry to be regenerated with
+	 * vanilla procfs operations.
+	 */
+	if (!krg_task_alive(ei->task_obj)
+	    || ei->task_obj->node == kerrighed_node_id)
+		return 1;
+
+	return 0;
+}
+
+static struct dentry_operations krg_pid_dentry_operations = {
+	.d_revalidate = krg_pid_revalidate,
+	.d_delete = krg_pid_delete_dentry,
+};
+
+typedef struct dentry *instantiate_t(struct inode *, struct dentry *,
+				     struct proc_distant_pid_info *,
+				     const void *);
+
+static int krg_proc_fill_cache(struct file *filp,
+			       void *dirent, filldir_t filldir,
+			       char *name, int len,
+			       instantiate_t instantiate,
+			       struct proc_distant_pid_info *task,
+			       const void *ptr)
+{
+	struct dentry *child, *dir = filp->f_path.dentry;
+	struct inode *inode;
+	struct qstr qname;
+	ino_t ino = 0;
+	unsigned type = DT_UNKNOWN;
+
+	qname.name = name;
+	qname.len  = len;
+	qname.hash = full_name_hash(name, len);
+
+	child = d_lookup(dir, &qname);
+	if (!child) {
+		struct dentry *new;
+		new = d_alloc(dir, &qname);
+		if (new) {
+			child = instantiate(dir->d_inode, new, task, ptr);
+			if (child)
+				dput(new);
+			else
+				child = new;
+		}
+	}
+	if (!child || IS_ERR(child) || !child->d_inode)
+		goto end_instantiate;
+	inode = child->d_inode;
+	if (inode) {
+		ino = inode->i_ino;
+		type = inode->i_mode >> 12;
+	}
+	dput(child);
+end_instantiate:
+	if (!ino)
+		ino = find_inode_number(dir, &qname);
+	if (!ino)
+		ino = 1;
+	return filldir(dirent, name, len, filp->f_pos, ino, type);
+}
+
+static struct dentry *krg_proc_pident_instantiate(struct inode *dir,
+						  struct dentry *dentry,
+						  struct proc_distant_pid_info *task,
+						  const void *ptr)
+{
+	const struct krg_pid_entry *p = ptr;
+	struct inode *inode;
+	struct proc_distant_pid_info *new_info;
+	struct dentry *error = ERR_PTR(-ENOENT);
+
+	inode = krg_proc_pid_make_inode(dir->i_sb, task);
+	if (!inode)
+		goto out;
+
+	new_info = get_krg_proc_task(inode);
+	inode->i_mode = p->mode;
+	if (S_ISDIR(inode->i_mode))
+		inode->i_nlink = 2;	/* Use getattr to fix if necessary */
+	if (p->iop)
+		inode->i_op = p->iop;
+	if (p->fop)
+		inode->i_fop = p->fop;
+	new_info->op = p->op;
+	dentry->d_op = &krg_pid_dentry_operations;
+	d_add(dentry, inode);
+	/* Close the race of the process dying before we return the dentry */
+	if (krg_pid_revalidate(dentry, NULL))
+		error = NULL;
+out:
+	return error;
+}
+
+struct dentry *krg_proc_pident_lookup(struct inode *dir,
+				      struct dentry *dentry,
+				      const struct krg_pid_entry *ents,
+				      unsigned int nents)
+{
+	struct dentry *error;
+	struct proc_distant_pid_info *task = get_krg_proc_task(dir);
+	const struct krg_pid_entry *p, *last;
+
+	error = ERR_PTR(-ENOENT);
+
+	if (!krg_task_alive(task->task_obj))
+		goto out;
+
+	/*
+	 * Yes, it does not scale. And it should not. Don't add
+	 * new entries into /proc/<tgid>/ without very good reasons.
+	 */
+	last = &ents[nents - 1];
+	for (p = ents; p <= last; p++) {
+		if (p->len != dentry->d_name.len)
+			continue;
+		if (!memcmp(dentry->d_name.name, p->name, p->len))
+			break;
+	}
+	if (p > last)
+		goto out;
+
+	error = krg_proc_pident_instantiate(dir, dentry, task, p);
+out:
+	return error;
+}
+
+static int krg_proc_pident_fill_cache(struct file *filp,
+				      void *dirent, filldir_t filldir,
+				      struct proc_distant_pid_info *task,
+				      const struct krg_pid_entry *p)
+{
+	return krg_proc_fill_cache(filp, dirent, filldir, p->name, p->len,
+				   krg_proc_pident_instantiate, task, p);
+}
+
+static int krg_proc_pident_readdir(struct file *filp,
+				   void *dirent, filldir_t filldir,
+				   const struct krg_pid_entry *ents,
+				   unsigned int nents)
+{
+	int i;
+	struct dentry *dentry = filp->f_path.dentry;
+	struct inode *inode = dentry->d_inode;
+	struct proc_distant_pid_info *task = get_krg_proc_task(inode);
+	const struct krg_pid_entry *p, *last;
+	ino_t ino;
+	int ret;
+
+	ret = -ENOENT;
+	if (!krg_task_alive(task->task_obj))
+		goto out;
+
+	ret = 0;
+	i = filp->f_pos;
+	switch (i) {
+	case 0:
+		ino = inode->i_ino;
+		if (filldir(dirent, ".", 1, i, ino, DT_DIR) < 0)
+			goto out;
+		i++;
+		filp->f_pos++;
+		/* fall through */
+	case 1:
+		ino = parent_ino(dentry);
+		if (filldir(dirent, "..", 2, i, ino, DT_DIR) < 0)
+			goto out;
+		i++;
+		filp->f_pos++;
+		/* fall through */
+	default:
+		i -= 2;
+		if (i >= nents) {
+			ret = 1;
+			goto out;
+		}
+		p = ents + i;
+		last = &ents[nents - 1];
+		while (p <= last) {
+			if (krg_proc_pident_fill_cache(filp, dirent, filldir, task, p) < 0)
+				goto out;
+			filp->f_pos++;
+			p++;
+		}
+	}
+
+	ret = 1;
+out:
+	return ret;
+}
+
+/* Unsupported entries are commented out */
+static struct krg_pid_entry krg_tgid_base_stuff[] = {
+/* 	DIR("task",       S_IRUGO|S_IXUGO, task), */
+	DIR("fd",         S_IRUSR|S_IXUSR, fd),
+/*      DIR("fdinfo",     S_IRUSR|S_IXUSR, proc_fdinfo_inode_operations, proc_fdinfo_operations), */
+/* #ifdef CONFIG_NET */
+/*      DIR("net",        S_IRUGO|S_IXUGO, proc_net_inode_operations, proc_net_operations), */
+/* #endif */
+	REG("environ",    S_IRUSR, pid_environ),
+	INF("auxv",       S_IRUSR, pid_auxv),
+	ONE("status",     S_IRUGO, pid_status),
+	ONE("personality", S_IRUSR, pid_personality),
+	INF("limits",	  S_IRUSR, pid_limits),
+/* #ifdef CONFIG_SCHED_DEBUG */
+/*      REG("sched",      S_IRUGO|S_IWUSR, proc_pid_sched_operations), */
+/* #endif */
+#ifdef CONFIG_HAVE_ARCH_TRACEHOOK
+	INF("syscall",    S_IRUSR, pid_syscall),
+#endif
+	INF("cmdline",    S_IRUGO, pid_cmdline),
+	ONE("stat",       S_IRUGO, tgid_stat),
+	ONE("statm",      S_IRUGO, pid_statm),
+/* 	REG("maps",       S_IRUGO, maps), */
+/* #ifdef CONFIG_NUMA */
+/* 	REG("numa_maps",  S_IRUGO, numa_maps), */
+/* #endif */
+/* 	REG("mem",        S_IRUSR|S_IWUSR, mem), */
+/* 	LNK("cwd",        cwd), */
+/* 	LNK("root",       root), */
+/* 	LNK("exe",        exe), */
+/* 	REG("mounts",     S_IRUGO, mounts), */
+/*      REG("mountinfo",  S_IRUGO, proc_mountinfo_operations), */
+/* 	REG("mountstats", S_IRUSR, mountstats), */
+/* #ifdef CONFIG_PROC_PAGE_MONITOR */
+/*      REG("clear_refs", S_IWUSR, proc_clear_refs_operations), */
+/*      REG("smaps",      S_IRUGO, proc_smaps_operations), */
+/*      REG("pagemap",    S_IRUSR, proc_pagemap_operations), */
+/* #endif */
+/* #ifdef CONFIG_SECURITY */
+/* 	DIR("attr",       S_IRUGO|S_IXUGO, attr_dir), */
+/* #endif */
+#ifdef CONFIG_KALLSYMS
+	INF("wchan",      S_IRUGO, pid_wchan),
+#endif
+#ifdef CONFIG_STACKTRACE
+	ONE("stack",      S_IRUSR, pid_stack),
+#endif
+#ifdef CONFIG_SCHEDSTATS
+	INF("schedstat",  S_IRUGO, pid_schedstat),
+#endif
+/* #ifdef CONFIG_LATENCYTOP */
+/*      REG("latency",  S_IRUGO, proc_lstats_operations), */
+/* #endif */
+/* #ifdef CONFIG_PROC_PID_CPUSET */
+/*      REG("cpuset",     S_IRUGO, proc_cpuset_operations), */
+/* #endif */
+/* #ifdef CONFIG_CGROUPS */
+/*      REG("cgroup",  S_IRUGO, proc_cgroup_operations), */
+/* #endif */
+	INF("oom_score",  S_IRUGO, pid_oom_score),
+/* 	REG("oom_adj",    S_IRUGO|S_IWUSR, oom_adjust), */
+/* #ifdef CONFIG_AUDITSYSCALL */
+/* 	REG("loginuid",   S_IWUSR|S_IRUGO, loginuid), */
+/*      REG("sessionid",  S_IRUGO, proc_sessionid_operations), */
+/* #endif */
+/* #ifdef CONFIG_FAULT_INJECTION */
+/* 	REG("make-it-fail", S_IRUGO|S_IWUSR, fault_inject), */
+/* #endif */
+/* #if defined(USE_ELF_CORE_DUMP) && defined(CONFIG_ELF_CORE) */
+/*      REG("coredump_filter", S_IRUGO|S_IWUSR, proc_coredump_filter_operations), */
+/* #endif */
+#ifdef CONFIG_TASK_IO_ACCOUNTING
+	INF("io",	S_IRUGO, tgid_io_accounting),
+#endif
+};
+
+static int krg_proc_tgid_base_readdir(struct file *filp,
+				      void *dirent, filldir_t filldir)
+{
+	return krg_proc_pident_readdir(filp, dirent, filldir,
+				       krg_tgid_base_stuff,
+				       ARRAY_SIZE(krg_tgid_base_stuff));
+}
+
+static struct file_operations krg_proc_tgid_base_operations = {
+	.read		= generic_read_dir,
+	.readdir	= krg_proc_tgid_base_readdir,
+};
+
+static struct dentry *krg_proc_tgid_base_lookup(struct inode *dir,
+						struct dentry *dentry,
+						struct nameidata *nd)
+{
+	return krg_proc_pident_lookup(dir, dentry,
+				      krg_tgid_base_stuff,
+				      ARRAY_SIZE(krg_tgid_base_stuff));
+}
+
+static struct inode_operations krg_proc_tgid_base_inode_operations = {
+	.lookup = krg_proc_tgid_base_lookup,
+	.getattr = krg_pid_getattr,
+	.setattr = proc_setattr,
+};
+
+static
+struct dentry *krg_proc_pid_instantiate(struct inode *dir,
+					struct dentry *dentry,
+					struct proc_distant_pid_info *task,
+					const void *ptr)
+{
+	struct dentry *error = ERR_PTR(-ENOENT);
+	struct inode *inode;
+
+	inode =	krg_proc_pid_make_inode(dir->i_sb, task);
+	if (!inode)
+		goto out;
+
+	inode->i_mode = S_IFDIR|S_IRUGO|S_IXUGO;
+	inode->i_op = &krg_proc_tgid_base_inode_operations;
+	inode->i_fop = &krg_proc_tgid_base_operations;
+	inode->i_flags |= S_IMMUTABLE;
+
+	inode->i_nlink = 2 + krg_pid_entry_count_dirs(krg_tgid_base_stuff,
+						      ARRAY_SIZE(krg_tgid_base_stuff));
+
+	dentry->d_op = &krg_pid_dentry_operations;
+
+	d_add(dentry, inode);
+	/*
+	 * There is no race of the process dying before we return the dentry
+	 * because either krg_proc_pid_lookup() or krg_proc_pid_fill_cache()
+	 * has locked the task kddm object.
+	 */
+	error = NULL;
+out:
+	return error;
+}
+
+struct dentry *krg_proc_pid_lookup(struct inode *dir,
+				   struct dentry *dentry, pid_t tgid)
+{
+	/* try and locate pid in the cluster */
+	struct dentry *result = ERR_PTR(-ENOENT);
+	struct proc_distant_pid_info task;
+	struct task_kddm_object *obj;
+
+#ifdef CONFIG_KRG_CAP
+	if (can_use_krg_cap(current, CAP_SEE_LOCAL_PROC_STAT))
+		goto out_no_task;
+#endif
+
+	obj = krg_task_readlock(tgid);
+	if (!obj)
+		goto out;
+
+	task.pid = tgid;
+	task.task_obj = obj;
+	task.dumpable = obj->dumpable;
+	task.euid = obj->euid;
+	task.egid = obj->egid;
+	task.prob_node = obj->node;
+
+	result = krg_proc_pid_instantiate(dir, dentry, &task, NULL);
+
+out:
+	krg_task_unlock(tgid);
+out_no_task:
+	return result;
+}
+
+#define PROC_MAXPIDS 100
+
+struct pid_list_msg {
+	kerrighed_node_t node;
+	pid_t next_tgid;
+};
+
+static int krg_proc_pid_fill_cache(struct file *filp,
+				   void *dirent, filldir_t filldir,
+				   struct tgid_iter iter)
+{
+	char name[PROC_NUMBUF];
+	int len = snprintf(name, sizeof(name), "%d", iter.tgid);
+	struct proc_distant_pid_info proc_task;
+	struct task_kddm_object *obj;
+	int retval = 0;
+
+	obj = krg_task_readlock(iter.tgid);
+	if (iter.task
+#ifdef CONFIG_KRG_EPM
+	    && ((!obj && iter.task->real_parent != baby_sitter)
+		|| (obj && obj->task == iter.task))
+#endif
+	    ) {
+		/* Task is local and not a remaining part of a migrated task. */
+		retval = proc_pid_fill_cache(filp, dirent, filldir, iter);
+		krg_task_unlock(iter.tgid);
+		return retval;
+	}
+#if defined(CONFIG_KRG_EPM) && defined(CONFIG_KRG_CAP)
+	if (can_use_krg_cap(current, CAP_SEE_LOCAL_PROC_STAT))
+		return retval;
+#endif
+
+	if (obj) {
+		proc_task.task_obj = obj;
+		proc_task.pid = iter.tgid;
+		if (obj->node == KERRIGHED_NODE_ID_NONE)
+			proc_task.prob_node = kerrighed_node_id;
+		else
+			proc_task.prob_node = obj->node;
+		proc_task.dumpable = obj->dumpable;
+		proc_task.euid = obj->euid;
+		proc_task.egid = obj->egid;
+
+		retval = krg_proc_fill_cache(filp, dirent, filldir, name, len,
+					     krg_proc_pid_instantiate,
+					     &proc_task, NULL);
+	}
+	krg_task_unlock(iter.tgid);
+
+	return retval;
+}
+
+/* Must be called under rcu_read_lock() */
+static struct task_kddm_object *next_tgid(pid_t tgid,
+					  struct pid_namespace *pid_ns,
+					  struct pid_namespace *pidmap_ns)
+{
+	struct pid *pid;
+	struct task_struct *task;
+	struct task_kddm_object *task_obj;
+
+retry:
+	task_obj = NULL;
+	pid = krg_find_ge_pid(tgid, pid_ns, pidmap_ns);
+	if (pid) {
+		tgid = pid_nr_ns(pid, pid_ns) + 1;
+		task = pid_task(pid, PIDTYPE_PID);
+		if (task && !has_group_leader_pid(task))
+			goto retry;
+		if (task) {
+
+			/*
+			 * If task_obj is not NULL, it won't be freed until
+			 * rcu_read_unlock()
+			 */
+			task_obj = rcu_dereference(task->task_obj);
+#ifdef CONFIG_KRG_EPM
+			if (!task_obj)
+				/* Try again in case task is migrating */
+				task_obj = krg_pid_task(pid);
+		} else {
+			task_obj = krg_pid_task(pid);
+#endif
+		}
+		if (!task_obj || task_obj->group_leader != task_obj->pid)
+			goto retry;
+	}
+
+	return task_obj;
+}
+
+static void handle_req_available_tgids(struct rpc_desc *desc,
+				       void *_msg, size_t size)
+{
+	struct pid_list_msg *msg = _msg;
+	struct pid_namespace *pid_ns = find_get_krg_pid_ns();
+	struct pid_namespace *pidmap_ns;
+	pid_t pid_array[PROC_MAXPIDS];
+	pid_t tgid;
+	struct task_kddm_object *task;
+	int nr_tgids = 0;
+	int retval;
+
+	if (msg->node == kerrighed_node_id)
+		pidmap_ns = pid_ns;
+	else
+		pidmap_ns = node_pidmap(msg->node);
+	BUG_ON(!pidmap_ns);
+	tgid = msg->next_tgid;
+	BUG_ON(tgid < GLOBAL_PID_MASK);
+	rcu_read_lock();
+	for (task = next_tgid(tgid, pid_ns, pidmap_ns);
+	     task;
+	     task = next_tgid(tgid + 1, pid_ns, pidmap_ns)) {
+		tgid = task->pid;
+		pid_array[nr_tgids++] = tgid;
+		if (nr_tgids >= PROC_MAXPIDS)
+			break;
+	}
+	rcu_read_unlock();
+
+	put_pid_ns(pid_ns);
+
+	retval = rpc_pack_type(desc, nr_tgids);
+	if (retval)
+		goto out_err_cancel;
+	retval = rpc_pack_type(desc, pid_array);
+	if (retval)
+		goto out_err_cancel;
+
+out:
+	return;
+
+out_err_cancel:
+	rpc_cancel(desc);
+	goto out;
+}
+
+static int fill_next_remote_tgids(kerrighed_node_t node,
+				  struct file *filp,
+				  void *dirent, filldir_t filldir,
+				  loff_t offset)
+{
+	struct pid_namespace *ns = filp->f_dentry->d_sb->s_fs_info;
+	kerrighed_node_t host_node;
+	struct tgid_iter iter;
+	struct pid_list_msg msg;
+	struct rpc_desc *desc;
+	pid_t pid_array[PROC_MAXPIDS];
+	int nr_pids;
+#ifdef CONFIG_KRG_EPM
+	struct pid *pid = NULL;
+#endif
+	int i;
+	int retval;
+
+	BUG_ON(!is_krg_pid_ns_root(ns));
+
+	host_node = pidmap_node(node);
+	if (host_node == KERRIGHED_NODE_ID_NONE)
+		return 0;
+
+	msg.node = node;
+	iter.tgid = filp->f_pos - offset;
+	if (iter.tgid < GLOBAL_PID_MASK)
+		iter.tgid = GLOBAL_PID_MASK;
+	msg.next_tgid = iter.tgid;
+
+	desc = rpc_begin(REQ_AVAILABLE_TGIDS, host_node);
+	if (!desc)
+		goto out_unlock;
+
+	retval = rpc_pack_type(desc, msg);
+	if (retval)
+		goto err_cancel;
+
+	retval = rpc_unpack_type(desc, nr_pids);
+	if (retval)
+		goto err_cancel;
+	retval = rpc_unpack_type(desc, pid_array);
+	if (retval)
+		goto err_cancel;
+
+	retval = rpc_end(desc, 0);
+	if (retval)
+		goto out_unlock;
+
+	for (i = 0; i < nr_pids; i++) {
+		iter.tgid = pid_array[i];
+		filp->f_pos = iter.tgid + offset;
+		iter.task = NULL;
+#ifdef CONFIG_KRG_EPM
+		rcu_read_lock();
+		pid = find_pid_ns(iter.tgid, ns);
+		if (pid) {
+			iter.task = pid_task(pid, PIDTYPE_PID);
+			if (iter.task)
+				get_task_struct(iter.task);
+		}
+		rcu_read_unlock();
+#ifdef CONFIG_KRG_CAP
+		if (!iter.task
+		    && can_use_krg_cap(current, CAP_SEE_LOCAL_PROC_STAT))
+			continue;
+#endif
+#endif /* CONFIG_KRG_EPM */
+		retval = krg_proc_pid_fill_cache(filp, dirent, filldir, iter);
+#ifdef CONFIG_KRG_EPM
+		if (iter.task)
+			put_task_struct(iter.task);
+#endif
+		if (retval < 0) {
+			retval = -EAGAIN;
+			goto out;
+		}
+	}
+	retval = nr_pids < ARRAY_SIZE(pid_array) ? 0 : nr_pids;
+
+out:
+	return retval;
+
+out_unlock:
+	retval = 0; /* Tell caller to proceed with next node */
+	goto out;
+
+err_cancel:
+	rpc_cancel(desc);
+	rpc_end(desc, 0);
+	goto out_unlock;
+}
+
+static int fill_next_local_tgids(struct file *filp,
+				 void *dirent, filldir_t filldir,
+				 loff_t offset)
+{
+	struct pid_namespace *ns = filp->f_dentry->d_sb->s_fs_info;
+	struct tgid_iter iter;
+	pid_t tgid = filp->f_pos - offset;
+	struct pid *pid;
+#ifdef CONFIG_KRG_EPM
+	struct task_kddm_object *task_obj;
+#endif
+	int global_mode = tgid & GLOBAL_PID_MASK;
+	int nr;
+	int retval;
+
+	BUG_ON(!is_krg_pid_ns_root(ns));
+
+	rcu_read_lock();
+	for (;;) {
+		pid = find_ge_pid(tgid, ns);
+		if (!pid)
+			break;
+		nr = pid_nr_ns(pid, ns);
+		if (!global_mode && (nr & GLOBAL_PID_MASK))
+			break;
+
+		tgid = nr + 1;
+		iter.tgid = nr;
+		iter.task = pid_task(pid, PIDTYPE_PID);
+		if (!iter.task) {
+#ifdef CONFIG_KRG_EPM
+#ifdef CONFIG_KRG_CAP
+			if (can_use_krg_cap(current, CAP_SEE_LOCAL_PROC_STAT))
+				continue;
+#endif
+			/* Maybe a migrated thread group leader */
+			task_obj = krg_pid_task(pid);
+			if (!task_obj
+			    || task_obj->pid != task_obj->group_leader)
+#endif
+				continue;
+		} else if (has_group_leader_pid(iter.task)) {
+			get_task_struct(iter.task);
+		} else {
+			continue;
+		}
+
+		rcu_read_unlock();
+
+		filp->f_pos = iter.tgid + offset;
+		retval = krg_proc_pid_fill_cache(filp, dirent, filldir, iter);
+		if (iter.task)
+			put_task_struct(iter.task);
+		if (retval < 0)
+			return retval; /* EF: was -EAGAIN */
+
+		rcu_read_lock();
+	}
+	rcu_read_unlock();
+
+	return 0;
+}
+
+static int __fill_next_tgids(kerrighed_node_t node,
+			     struct file *filp,
+			     void *dirent, filldir_t filldir,
+			     loff_t offset)
+{
+	if (node == kerrighed_node_id)
+		return fill_next_local_tgids(filp, dirent, filldir, offset);
+	else
+		return fill_next_remote_tgids(node,
+					      filp, dirent, filldir, offset);
+}
+
+static int fill_next_tgids(kerrighed_node_t node,
+			   struct file *filp,
+			   void *dirent, filldir_t filldir,
+			   loff_t offset)
+{
+	pid_t tgid;
+	int retval;
+
+	do {
+		retval = __fill_next_tgids(node,
+					   filp, dirent, filldir, offset);
+		if (retval > 0) {
+			tgid = filp->f_pos - offset;
+			if ((tgid & INTERNAL_PID_MASK) >= PID_MAX_LIMIT - 1) {
+				retval = 0;
+				break;
+			}
+			/* Start from first tgid *not filled* for next chunk */
+			filp->f_pos++;
+		}
+	} while (retval > 0);
+
+	return retval;
+}
+
+int krg_proc_pid_readdir(struct file *filp,
+			 void *dirent, filldir_t filldir,
+			 loff_t offset)
+{
+	pid_t tgid;
+	kerrighed_node_t node;
+	int retval = 0;
+
+	if ((unsigned long) filp->f_pos >=
+	    (unsigned long)(KERRIGHED_PID_MAX_LIMIT + offset))
+		goto out;
+
+	/* First local PIDs */
+	tgid = filp->f_pos - offset;
+	if (!(tgid & GLOBAL_PID_MASK)) {
+		retval = fill_next_tgids(kerrighed_node_id,
+					 filp, dirent, filldir, offset);
+		if (retval)
+			goto out;
+	}
+
+	/* Second global PIDs */
+	tgid = filp->f_pos - offset;
+	if (!(tgid & GLOBAL_PID_MASK)) {
+		tgid = GLOBAL_PID_NODE(0, 0);
+		filp->f_pos = tgid + offset;
+	}
+	retval = pidmap_map_read_lock();
+	if (retval)
+		goto out;
+	node = ORIG_NODE(tgid);
+	for (; node < KERRIGHED_MAX_NODES;
+	     node++,
+	     filp->f_pos = GLOBAL_PID_NODE(0, node) + offset) {
+#if defined(CONFIG_KRG_CAP) && !defined(CONFIG_KRG_EPM)
+		if (node != kerrighed_node_id
+		    && can_use_krg_cap(current, CAP_SEE_LOCAL_PROC_STAT))
+			continue;
+#endif
+
+		retval = fill_next_tgids(node, filp, dirent, filldir, offset);
+		if (retval)
+			break;
+	}
+	pidmap_map_read_unlock();
+
+out:
+	return retval;
+}
+
+/** Init cluster info stuffs.
+ *  @author Renaud Lottiaux
+ */
+int proc_pid_init(void)
+{
+	rpc_register_void(REQ_AVAILABLE_TGIDS, handle_req_available_tgids, 0);
+
+	proc_pid_file_init();
+
+	return 0;
+}
+
+/** Init cluster info stuffs.
+ *  @author Renaud Lottiaux
+ */
+int proc_pid_finalize(void)
+{
+	proc_pid_file_finalize();
+
+	return 0;
+}
diff -ruN linux-2.6.29/kerrighed/procfs/proc_pid_fd.c android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid_fd.c
--- linux-2.6.29/kerrighed/procfs/proc_pid_fd.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid_fd.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,37 @@
+/** Global /proc/<pid>/fd management
+ *  @file proc_pid_fd.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2007, Louis Rilling - Kerlabs.
+ */
+
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/procfs_internal.h>
+
+#include "proc_pid.h"
+
+static int krg_proc_readfd(struct file *filp, void *dirent, filldir_t filldir)
+{
+	return 0;
+}
+
+struct file_operations krg_proc_fd_operations = {
+	.read = generic_read_dir,
+	.readdir = krg_proc_readfd,
+};
+
+static struct dentry *krg_proc_lookupfd(struct inode *dir,
+					struct dentry *dentry,
+					struct nameidata *nd)
+{
+	return ERR_PTR(-ENOENT);
+}
+
+/*
+ * proc directories can do almost nothing..
+ */
+struct inode_operations krg_proc_fd_inode_operations = {
+	.lookup = krg_proc_lookupfd,
+	.setattr = proc_setattr,
+};
diff -ruN linux-2.6.29/kerrighed/procfs/proc_pid_fd.h android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid_fd.h
--- linux-2.6.29/kerrighed/procfs/proc_pid_fd.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid_fd.h	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,15 @@
+/**  /proc/<pid>/fd information management.
+ *  @file proc_pid_fd.h
+ *
+ *  @author David Margery
+ */
+
+#ifndef __PROC_PID_FD_H__
+#define __PROC_PID_FD_H__
+
+#include <linux/fs.h>
+
+extern struct file_operations krg_proc_fd_operations;
+extern struct inode_operations krg_proc_fd_inode_operations;
+
+#endif /* __PROC_PID_FD_H__ */
diff -ruN linux-2.6.29/kerrighed/procfs/proc_pid_file.c android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid_file.c
--- linux-2.6.29/kerrighed/procfs/proc_pid_file.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid_file.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,934 @@
+/** Global /proc/<pid>/<file> management
+ *  @file proc_pid_file.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2007, Louis Rilling - Kerlabs.
+ */
+
+#include <linux/proc_fs.h>
+#include <linux/procfs_internal.h>
+#include <linux/fs.h>
+#include <linux/file.h>
+#include <linux/namei.h>
+#include <linux/mount.h>
+#include <linux/seq_file.h>
+#include <linux/anon_inodes.h>
+#include <linux/syscalls.h>
+#include <linux/pid_namespace.h>
+#include <linux/uaccess.h>
+#include <linux/cred.h>
+#include <linux/gfp.h>
+
+#include <kerrighed/sys/types.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/remote_cred.h>
+
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/task.h>
+#include "proc_pid.h"
+
+/* REG() entries */
+
+struct environ_read_msg {
+	pid_t pid;
+	size_t count;
+	loff_t pos;
+};
+
+static int do_handle_environ_read(struct task_struct *task,
+				  char *buf, size_t count, loff_t *ppos)
+{
+	struct pid_namespace *ns = find_get_krg_pid_ns();
+	struct vfsmount *mnt = ns->proc_mnt;
+	struct file *file;
+	struct nameidata nd;
+	char str_buf[PROC_NUMBUF + sizeof("/environ")];
+	int ret;
+
+	sprintf(str_buf, "%d/environ", task_pid_nr_ns(task, ns));
+	ret = vfs_path_lookup(mnt->mnt_root, mnt, str_buf, 0, &nd);
+	if (ret)
+		goto out;
+
+	file = dentry_open(nd.path.dentry,
+			   nd.path.mnt,
+			   O_RDONLY,
+			   current_cred());
+	if (IS_ERR(file)) {
+		ret = PTR_ERR(file);
+		/* dentry_open() dropped nd.path ref counts */
+		goto out;
+	}
+
+	ret = vfs_read(file, buf, count, ppos);
+
+	/* Drops nd.path == file->f_path ref counts */
+	fput(file);
+
+out:
+	put_pid_ns(ns);
+	return ret;
+}
+
+static void handle_read_proc_pid_environ(struct rpc_desc *desc,
+					 void *_msg, size_t size)
+{
+	struct environ_read_msg *msg = _msg;
+	struct task_struct *tsk;
+	const struct cred *old_cred;
+	unsigned long page = 0;
+	int res;
+	int err;
+
+	rcu_read_lock();
+	tsk = find_task_by_kpid(msg->pid);
+	BUG_ON(!tsk);
+	get_task_struct(tsk);
+	rcu_read_unlock();
+
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred)) {
+		err = PTR_ERR(old_cred);
+		goto out_err_cancel;
+	}
+
+	page = __get_free_page(GFP_TEMPORARY);
+	if (!page)
+		res = -ENOMEM;
+	else
+		res = do_handle_environ_read(tsk,
+					     (char *)page, msg->count,
+					     &msg->pos);
+
+	revert_creds(old_cred);
+
+	err = rpc_pack_type(desc, res);
+	if (err)
+		goto out_err_cancel;
+	if (res > 0) {
+		err = rpc_pack(desc, 0, (char *)page, res);
+		if (err)
+			goto out_err_cancel;
+	}
+	err = rpc_pack_type(desc, msg->pos);
+	if (err)
+		goto out_err_cancel;
+
+out:
+	put_task_struct(tsk);
+	if (page)
+		free_page(page);
+	if (err)
+		res = err;
+	return;
+
+out_err_cancel:
+	rpc_cancel(desc);
+	goto out;
+}
+
+static int do_environ_read(struct file *file, struct proc_distant_pid_info *task,
+			   char *buf, size_t count, loff_t *ppos)
+{
+	struct environ_read_msg msg;
+	struct rpc_desc *desc;
+	int bytes_read;
+	loff_t new_pos;
+	int err;
+
+	BUG_ON(task->prob_node == KERRIGHED_NODE_ID_NONE);
+
+	msg.pid = task->pid;
+	msg.count = count;
+	msg.pos = *ppos;
+
+	err = -ENOMEM;
+	desc = rpc_begin(REQ_PROC_PID_ENVIRON, task->prob_node);
+	if (!desc)
+		goto out_err;
+
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto out_err_cancel;
+	err = pack_creds(desc, current_cred());
+	if (err)
+		goto out_err_cancel;
+
+	err = rpc_unpack_type(desc, bytes_read);
+	if (err)
+		goto out_err_cancel;
+	if (bytes_read > 0) {
+		BUG_ON(bytes_read > count);
+		err = rpc_unpack(desc, 0, buf, bytes_read);
+	}
+	if (err)
+		goto out_err_cancel;
+	err = rpc_unpack_type(desc, new_pos);
+	if (err)
+		goto out_err_cancel;
+	*ppos = new_pos;
+
+	rpc_end(desc, 0);
+
+out:
+	return bytes_read;
+
+out_err_cancel:
+	if (err > 0)
+		err = -EPIPE;
+	rpc_cancel(desc);
+	rpc_end(desc, 0);
+out_err:
+	bytes_read = err;
+	goto out;
+}
+
+static ssize_t krg_proc_pid_environ_read(struct file *file, char __user *buf,
+					 size_t count, loff_t *ppos)
+{
+	struct proc_distant_pid_info *task =
+		get_krg_proc_task(file->f_dentry->d_inode);
+	unsigned long page;
+	size_t c = count;
+	loff_t pos = *ppos;
+	int ret = -ESRCH;
+
+	if (!current->nsproxy->krg_ns)
+		goto out_no_task;
+
+	/* TODO: if pid is reused in between, we may think the entry is still
+	 * valid! */
+	task->prob_node = krg_lock_pid_location(task->pid);
+	if (task->prob_node == KERRIGHED_NODE_ID_NONE)
+		/* Task is dead. */
+		goto out_no_task;
+
+	ret = -ENOMEM;
+	if (!(page = __get_free_page(GFP_TEMPORARY)))
+		goto out;
+
+	while (c) {
+		ret = do_environ_read(file, task,
+				      (char *)page, min(c, (size_t)PAGE_SIZE),
+				      &pos);
+		if (ret > 0)
+			if (copy_to_user(buf, (void *)page, ret))
+				ret = -EFAULT;
+		if (ret < 0)
+			goto out_free;
+		*ppos = pos;
+		if (!ret)
+			break;
+		c -= ret;
+	}
+	ret = count - c;
+
+out_free:
+	free_page(page);
+out:
+	krg_unlock_pid_location(task->pid);
+out_no_task:
+	return ret;
+}
+
+const struct file_operations krg_proc_pid_environ_operations = {
+	.read		= krg_proc_pid_environ_read,
+};
+
+/* INF() entries */
+
+/* Common part */
+
+#define PROC_BLOCK_SIZE (3*1024)	/* 4K page size but our output routines use some slack for overruns */
+
+static ssize_t krg_proc_info_read(struct file *file, char *buf,
+				  size_t count, loff_t *ppos)
+{
+	struct inode *inode = file->f_dentry->d_inode;
+	unsigned long page;
+	ssize_t length;
+	struct proc_distant_pid_info *task = get_krg_proc_task(inode);
+
+	length = -ESRCH;
+	if (!current->nsproxy->krg_ns)
+		goto out_no_task;
+
+	/*
+	 * TODO: if pid is reused in between, we may think the entry is still
+	 * valid!
+	 */
+	task->prob_node = krg_lock_pid_location(task->pid);
+	if (task->prob_node == KERRIGHED_NODE_ID_NONE)
+		/* Task is dead. */
+		goto out_no_task;
+
+	if (count > PROC_BLOCK_SIZE)
+		count = PROC_BLOCK_SIZE;
+
+	length = -ENOMEM;
+	if (!(page = __get_free_page(GFP_TEMPORARY)))
+		goto out;
+
+	length = task->op.proc_read(task, (char *)page);
+
+	if (length >= 0)
+		length = simple_read_from_buffer(buf, count, ppos, (char *)page, length);
+
+	free_page(page);
+out:
+	krg_unlock_pid_location(task->pid);
+out_no_task:
+	return length;
+}
+
+const struct file_operations krg_proc_info_file_operations = {
+	.read = krg_proc_info_read,
+};
+
+/* Helpers */
+
+struct generic_proc_read_msg {
+	pid_t pid;
+};
+
+typedef int proc_read_t(struct task_struct *task, char *buffer);
+
+static void handle_generic_proc_read(struct rpc_desc *desc, void *_msg,
+				     proc_read_t *proc_read,
+				     enum rpcid REQ)
+{
+	struct generic_proc_read_msg *msg = _msg;
+	struct task_struct *tsk;
+	const struct cred *old_cred = NULL;
+	unsigned long page = 0;
+	int res;
+	int err;
+
+	rcu_read_lock();
+	tsk = find_task_by_kpid(msg->pid);
+	BUG_ON(!tsk);
+	get_task_struct(tsk);
+	rcu_read_unlock();
+
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred)) {
+		err = res = PTR_ERR(old_cred);
+		old_cred = NULL;
+		if (res == -ENOMEM)
+			goto out_res;
+		goto out_err_cancel;
+	}
+
+	page = __get_free_page(GFP_TEMPORARY);
+	if (!page)
+		res = -ENOMEM;
+	else
+		res = proc_read(tsk, (char *)page);
+
+out_res:
+	err = rpc_pack_type(desc, res);
+	if (err)
+		goto out_err_cancel;
+	if (res > 0) {
+		err = rpc_pack(desc, 0, (char *)page, res);
+		if (err)
+			goto out_err_cancel;
+	}
+
+out:
+	put_task_struct(tsk);
+	if (page)
+		free_page(page);
+	if (old_cred)
+		revert_creds(old_cred);
+	if (err)
+		res = err;
+	return;
+
+out_err_cancel:
+	rpc_cancel(desc);
+	goto out;
+}
+
+static int generic_proc_read(struct proc_distant_pid_info *task,
+			     char *buffer, enum rpcid req)
+{
+	struct generic_proc_read_msg msg;
+	struct rpc_desc *desc;
+	int bytes_read;
+	int err;
+
+	BUG_ON(task->prob_node == KERRIGHED_NODE_ID_NONE);
+
+	msg.pid = task->pid;
+
+	err = -ENOMEM;
+	desc = rpc_begin(req, task->prob_node);
+	if (!desc)
+		goto out_err;
+
+	err = rpc_pack_type(desc, msg);
+	if (err)
+		goto out_err_cancel;
+	err = pack_creds(desc, current_cred());
+	if (err)
+		goto out_err_cancel;
+
+	err = rpc_unpack_type(desc, bytes_read);
+	if (err)
+		goto out_err_cancel;
+	if (bytes_read > 0)
+		err = rpc_unpack(desc, 0, buffer, bytes_read);
+	if (err)
+		goto out_err_cancel;
+
+	rpc_end(desc, 0);
+
+out:
+	return bytes_read;
+
+out_err_cancel:
+	if (err > 0)
+		err = -EPIPE;
+	rpc_cancel(desc);
+	rpc_end(desc, 0);
+out_err:
+	bytes_read = err;
+	goto out;
+}
+
+/* Entries */
+
+static void handle_read_proc_pid_cmdline(struct rpc_desc *desc,
+					 void *_msg, size_t size)
+{
+	handle_generic_proc_read(desc, _msg, proc_pid_cmdline,
+				 REQ_PROC_PID_CMDLINE);
+}
+
+int krg_proc_pid_cmdline(struct proc_distant_pid_info *task, char *buffer)
+{
+	return generic_proc_read(task, buffer, REQ_PROC_PID_CMDLINE);
+}
+
+static void handle_read_proc_pid_auxv(struct rpc_desc *desc,
+				      void *_msg, size_t size)
+{
+	handle_generic_proc_read(desc, _msg, proc_pid_auxv,
+				 REQ_PROC_PID_CMDLINE);
+}
+
+int krg_proc_pid_auxv(struct proc_distant_pid_info *task, char *buffer)
+{
+	return generic_proc_read(task, buffer, REQ_PROC_PID_AUXV);
+}
+
+static void handle_read_proc_pid_limits(struct rpc_desc *desc,
+					void *_msg, size_t size)
+{
+	handle_generic_proc_read(desc, _msg, proc_pid_limits,
+				 REQ_PROC_PID_LIMITS);
+}
+
+int krg_proc_pid_limits(struct proc_distant_pid_info *task, char *buffer)
+{
+	return generic_proc_read(task, buffer, REQ_PROC_PID_LIMITS);
+}
+
+#ifdef CONFIG_HAVE_ARCH_TRACEHOOK
+static void handle_read_proc_pid_syscall(struct rpc_desc *desc,
+					 void *_msg, size_t size)
+{
+	handle_generic_proc_read(desc, _msg, proc_pid_syscall,
+				 REQ_PROC_PID_SYSCALL);
+}
+
+int krg_proc_pid_syscall(struct proc_distant_pid_info *task, char *buffer)
+{
+	return generic_proc_read(task, buffer, REQ_PROC_PID_SYSCALL);
+}
+#endif
+
+#ifdef CONFIG_KALLSYMS
+static void handle_read_proc_pid_wchan(struct rpc_desc *desc,
+				       void *_msg, size_t size)
+{
+	handle_generic_proc_read(desc, _msg, proc_pid_wchan,
+				 REQ_PROC_PID_WCHAN);
+}
+
+int krg_proc_pid_wchan(struct proc_distant_pid_info *task, char *buffer)
+{
+	return generic_proc_read(task, buffer, REQ_PROC_PID_WCHAN);
+}
+#endif
+
+#ifdef CONFIG_SCHEDSTATS
+static void handle_read_proc_pid_schedstat(struct rpc_desc *desc,
+					   void *_msg, size_t size)
+{
+	handle_generic_proc_read(desc, _msg, proc_pid_schedstat,
+				 REQ_PROC_PID_SCHEDSTAT);
+}
+
+int krg_proc_pid_schedstat(struct proc_distant_pid_info *task, char *buffer)
+{
+	return generic_proc_read(task, buffer, REQ_PROC_PID_SCHEDSTAT);
+}
+#endif
+
+static void handle_read_proc_pid_oom_score(struct rpc_desc *desc,
+					   void *_msg, size_t size)
+{
+	handle_generic_proc_read(desc, _msg, proc_oom_score,
+				 REQ_PROC_PID_OOM_SCORE);
+}
+
+int krg_proc_pid_oom_score(struct proc_distant_pid_info *task, char *buffer)
+{
+	return generic_proc_read(task, buffer, REQ_PROC_PID_OOM_SCORE);
+}
+
+#ifdef CONFIG_TASK_IO_ACCOUNTING
+static void handle_read_proc_tgid_io_accounting(struct rpc_desc *desc,
+						void *_msg, size_t size)
+{
+	handle_generic_proc_read(desc, _msg, proc_tgid_io_accounting,
+				 REQ_PROC_TGID_IO_ACCOUNTING);
+}
+
+int krg_proc_tgid_io_accounting(struct proc_distant_pid_info *task, char *buffer)
+{
+	return generic_proc_read(task, buffer, REQ_PROC_TGID_IO_ACCOUNTING);
+}
+#endif
+
+/* ONE() entries */
+
+/* Common part */
+static ssize_t krg_proc_single_read(struct file *file, char __user *buf,
+				    size_t count, loff_t *ppos)
+{
+	struct inode *inode = file->f_dentry->d_inode;
+	struct pid_namespace *ns;
+	struct proc_distant_pid_info *task = get_krg_proc_task(inode);
+	unsigned long page;
+	size_t c = count;
+	ssize_t length;
+
+	ns = inode->i_sb->s_fs_info;
+	BUG_ON(!is_krg_pid_ns_root(ns));
+
+	length = -ESRCH;
+	if (!current->nsproxy->krg_ns)
+		goto out_no_task;
+
+	/*
+	 * TODO: if pid is reused in between, we may think the entry is still
+	 * valid!
+	 */
+	task->prob_node = krg_lock_pid_location(task->pid);
+	if (task->prob_node == KERRIGHED_NODE_ID_NONE)
+		/* Task is dead. */
+		goto out_no_task;
+
+	length = -ENOMEM;
+	if (!(page = __get_free_page(GFP_TEMPORARY)))
+		goto out;
+
+	while (c) {
+		length = task->op.proc_show(file, task,
+					    (char *)page,
+					    min(c, (size_t)PAGE_SIZE));
+		if (length > 0)
+			length = simple_read_from_buffer(buf, count, ppos, (char *)page, length);
+		if (length < 0)
+			goto out_free;
+		if (!length)
+			break;
+		c -= length;
+	}
+	length = count - c;
+
+out_free:
+	free_page(page);
+
+out:
+	krg_unlock_pid_location(task->pid);
+out_no_task:
+	return length;
+}
+
+struct krg_proc_single_private {
+	void (*release)(struct inode *inode, struct file *file);
+	void *data;
+};
+
+int krg_proc_single_release(struct inode *inode, struct file *file)
+{
+	struct krg_proc_single_private *private = file->private_data;
+
+	if (private)
+		private->release(inode, file);
+	return 0;
+}
+
+const struct file_operations krg_proc_single_file_operations = {
+	.read = krg_proc_single_read,
+	.release = krg_proc_single_release,
+};
+
+/* Helpers */
+struct generic_proc_show_msg {
+	pid_t pid;
+};
+
+typedef int proc_show_t(struct seq_file *,
+			struct pid_namespace *, struct pid *,
+			struct task_struct *);
+
+struct anonymous_proc_single_data {
+	struct task_struct *task;
+	struct pid_namespace *ns;
+	proc_show_t *proc_show;
+};
+
+static int krg_proc_handler_single_show(struct seq_file *m, void *v)
+{
+	struct anonymous_proc_single_data *data = m->private;
+	struct task_struct *task = data->task;
+
+	return data->proc_show(m, data->ns, task_pid(task), task);
+}
+
+static
+int krg_proc_handler_single_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *m = file->private_data;
+	struct anonymous_proc_single_data *data = m->private;
+
+	put_pid_ns(data->ns);
+	put_task_struct(data->task);
+	kfree(data);
+	return single_release(inode, file);
+}
+
+static const struct file_operations krg_proc_handler_single_file_operations = {
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = krg_proc_handler_single_release,
+};
+
+static int krg_proc_handler_single_getfd(struct task_struct *task,
+					 struct pid_namespace *ns,
+					 proc_show_t *proc_show)
+{
+	struct anonymous_proc_single_data *data;
+	struct file *file;
+	int fd, err;
+
+	fd = -ENOMEM;
+	data = kmalloc(sizeof(*data), GFP_KERNEL);
+	if (!data)
+		goto out;
+	get_task_struct(task);
+	data->task = task;
+	get_pid_ns(ns);
+	data->ns = ns;
+	data->proc_show = proc_show;
+
+	fd = anon_inode_getfd("krg-proc-handler-single",
+			      &krg_proc_handler_single_file_operations,
+			      NULL,
+			      0);
+	if (fd < 0)
+		goto err_free_data;
+
+	file = fget(fd);
+	BUG_ON(!file);
+	err = single_open(file, krg_proc_handler_single_show, data);
+	fput(file);
+	if (err) {
+		sys_close(fd);
+		fd = err;
+		goto err_free_data;
+	}
+
+out:
+	return fd;
+
+err_free_data:
+	put_pid_ns(ns);
+	put_task_struct(data->task);
+	kfree(data);
+	goto out;
+}
+
+static void handle_generic_proc_show(struct rpc_desc *desc, void *_msg,
+				     proc_show_t *proc_show,
+				     enum rpcid REQ)
+{
+	struct generic_proc_show_msg *msg = _msg;
+	struct pid_namespace *ns = find_get_krg_pid_ns();
+	struct task_struct *tsk;
+	const struct cred *old_cred = NULL;
+	unsigned long page = 0;
+	int fd = -1;
+	size_t count;
+	int res;
+	int err;
+
+	rcu_read_lock();
+	tsk = find_task_by_pid_ns(msg->pid, ns);
+	BUG_ON(!tsk);
+	get_task_struct(tsk);
+	rcu_read_unlock();
+
+	old_cred = unpack_override_creds(desc);
+	if (IS_ERR(old_cred)) {
+		res = PTR_ERR(old_cred);
+		old_cred = NULL;
+		goto out_err;
+	}
+
+	page = __get_free_page(GFP_KERNEL);
+	if (!page)
+		goto out_err;
+
+	res = krg_proc_handler_single_getfd(tsk, ns, proc_show);
+	if (res < 0)
+		goto out_err;
+	fd = res;
+
+	for (;;) {
+		err = rpc_unpack_type(desc, count);
+		if (err)
+			goto out_err_cancel;
+		if (!count)
+			break;
+
+		res = sys_read(fd, (void *)page, count);
+
+		err = rpc_pack_type(desc, res);
+		if (err)
+			goto out_err_cancel;
+		if (res > 0) {
+			err = rpc_pack(desc, 0, (char *)page, res);
+			if (err)
+				goto out_err_cancel;
+		}
+	}
+
+out:
+	if (fd >= 0)
+		sys_close(fd);
+	if (page)
+		free_page(page);
+	if (old_cred)
+		revert_creds(old_cred);
+	put_task_struct(tsk);
+	put_pid_ns(ns);
+	if (err)
+		res = err;
+	return;
+
+out_err_cancel:
+	if (err > 0)
+		err = -EPIPE;
+	rpc_cancel(desc);
+	goto out;
+
+out_err:
+	err = rpc_pack_type(desc, res);
+	if (err)
+		goto out_err_cancel;
+	goto out;
+}
+
+static void generic_proc_show_release(struct inode *inode, struct file *file)
+{
+	struct krg_proc_single_private *private = file->private_data;
+	struct rpc_desc *desc = private->data;
+	size_t count = 0;
+	int err;
+
+	err = rpc_pack_type(desc, count);
+	if (err)
+		rpc_cancel(desc);
+	rpc_end(desc, 0);
+	kfree(private);
+}
+
+static int generic_proc_show(struct file *file,
+			     struct proc_distant_pid_info *task,
+			     char *buf, size_t count,
+			     enum rpcid req)
+{
+	struct generic_proc_show_msg msg;
+	struct krg_proc_single_private *private = file->private_data;
+	struct pid_namespace *ns = file->f_dentry->d_sb->s_fs_info;
+	struct rpc_desc *desc;
+	int bytes_read;
+	int err;
+
+	BUG_ON(task->prob_node == KERRIGHED_NODE_ID_NONE);
+	BUG_ON(!is_krg_pid_ns_root(ns));
+
+	msg.pid = task->pid;
+
+	if (!private) {
+		err = -ENOMEM;
+		private = kmalloc(sizeof(*private), GFP_KERNEL);
+		if (!private)
+			goto out_err;
+		desc = rpc_begin(req, task->prob_node);
+		if (!desc) {
+			kfree(private);
+			goto out_err;
+		}
+		private->release = generic_proc_show_release;
+		private->data = desc;
+		file->private_data = private;
+
+		err = rpc_pack_type(desc, msg);
+		if (err)
+			goto out_err_cancel;
+		err = pack_creds(desc, current_cred());
+		if (err)
+			goto out_err_cancel;
+	} else {
+		desc = private->data;
+	}
+
+	err = rpc_pack_type(desc, count);
+	if (err)
+		goto out_err_cancel;
+	err = rpc_unpack_type(desc, bytes_read);
+	if (err)
+		goto out_err_cancel;
+	if (bytes_read > 0) {
+		BUG_ON(bytes_read > count);
+		err = rpc_unpack(desc, 0, buf, bytes_read);
+	}
+	if (err)
+		goto out_err_cancel;
+
+out:
+	return bytes_read;
+
+out_err_cancel:
+	if (err > 0)
+		err = -EPIPE;
+	rpc_cancel(desc);
+out_err:
+	bytes_read = err;
+	goto out;
+}
+
+/* Entries */
+
+static void handle_read_proc_pid_status(struct rpc_desc *desc,
+					void *_msg, size_t size)
+{
+	handle_generic_proc_show(desc, _msg, proc_pid_status,
+				 REQ_PROC_PID_STATUS);
+}
+
+int krg_proc_pid_status(struct file *file, struct proc_distant_pid_info *task,
+			char *buf, size_t count)
+{
+	return generic_proc_show(file, task, buf, count, REQ_PROC_PID_STATUS);
+}
+
+static void handle_read_proc_pid_personality(struct rpc_desc *desc,
+					     void *_msg, size_t size)
+{
+	handle_generic_proc_show(desc, _msg, proc_pid_personality,
+				 REQ_PROC_PID_PERSONALITY);
+}
+
+int krg_proc_pid_personality(struct file *file,
+			     struct proc_distant_pid_info *task,
+			     char *buf, size_t count)
+{
+	return generic_proc_show(file, task, buf, count, REQ_PROC_PID_PERSONALITY);
+}
+
+static void handle_read_proc_tgid_stat(struct rpc_desc *desc,
+				       void *_msg, size_t size)
+{
+	handle_generic_proc_show(desc, _msg, proc_tgid_stat,
+				 REQ_PROC_TGID_STAT);
+}
+
+int krg_proc_tgid_stat(struct file *file, struct proc_distant_pid_info *task,
+		       char *buf, size_t count)
+{
+	return generic_proc_show(file, task, buf, count, REQ_PROC_TGID_STAT);
+}
+
+static void handle_read_proc_pid_statm(struct rpc_desc *desc,
+				       void *_msg, size_t size)
+{
+	handle_generic_proc_show(desc, _msg, proc_pid_statm,
+				 REQ_PROC_PID_STATM);
+}
+
+int krg_proc_pid_statm(struct file *file, struct proc_distant_pid_info *task,
+		       char *buf, size_t count)
+{
+	return generic_proc_show(file, task, buf, count, REQ_PROC_PID_STATM);
+}
+
+#ifdef CONFIG_STACKTRACE
+static void handle_read_proc_pid_stack(struct rpc_desc *desc,
+				       void *_msg, size_t size)
+{
+	handle_generic_proc_show(desc, _msg, proc_pid_stack,
+				 REQ_PROC_PID_STACK);
+}
+
+int krg_proc_pid_stack(struct file *file, struct proc_distant_pid_info *task,
+		       char *buf, size_t count)
+{
+	return generic_proc_show(file, task, buf, count, REQ_PROC_PID_STACK);
+}
+#endif
+
+void proc_pid_file_init(void)
+{
+	/* REG() entries */
+	rpc_register_void(REQ_PROC_PID_ENVIRON, handle_read_proc_pid_environ, 0);
+	/* INF() entries */
+	rpc_register_void(REQ_PROC_PID_CMDLINE, handle_read_proc_pid_cmdline, 0);
+	rpc_register_void(REQ_PROC_PID_AUXV, handle_read_proc_pid_auxv, 0);
+	rpc_register_void(REQ_PROC_PID_LIMITS, handle_read_proc_pid_limits, 0);
+#ifdef CONFIG_HAVE_ARCH_TRACEHOOK
+	rpc_register_void(REQ_PROC_PID_SYSCALL, handle_read_proc_pid_syscall, 0);
+#endif
+#ifdef CONFIG_KALLSYMS
+	rpc_register_void(REQ_PROC_PID_WCHAN, handle_read_proc_pid_wchan, 0);
+#endif
+#ifdef CONFIG_SCHEDSTATS
+	rpc_register_void(REQ_PROC_PID_SCHEDSTAT, handle_read_proc_pid_schedstat, 0);
+#endif
+	rpc_register_void(REQ_PROC_PID_OOM_SCORE, handle_read_proc_pid_oom_score, 0);
+#ifdef CONFIG_TASK_IO_ACCOUNTING
+	rpc_register_void(REQ_PROC_TGID_IO_ACCOUNTING,
+			  handle_read_proc_tgid_io_accounting, 0);
+#endif
+	/* ONE() entries */
+	rpc_register_void(REQ_PROC_PID_STATUS, handle_read_proc_pid_status, 0);
+	rpc_register_void(REQ_PROC_PID_PERSONALITY,
+			  handle_read_proc_pid_personality, 0);
+	rpc_register_void(REQ_PROC_TGID_STAT, handle_read_proc_tgid_stat, 0);
+	rpc_register_void(REQ_PROC_PID_STATM, handle_read_proc_pid_statm, 0);
+#ifdef CONFIG_STACKTRACE
+	rpc_register_void(REQ_PROC_PID_STACK, handle_read_proc_pid_stack, 0);
+#endif
+}
+
+void proc_pid_file_finalize(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/procfs/proc_pid_file.h android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid_file.h
--- linux-2.6.29/kerrighed/procfs/proc_pid_file.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid_file.h	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,46 @@
+/**  /proc/<pid>/<file> information management.
+ *  @file proc_pid_file.h
+ *
+ *  @author David Margery
+ */
+
+#ifndef __PROC_PID_FILE_H__
+#define __PROC_PID_FILE_H__
+
+#include <linux/fs.h>
+
+struct proc_distant_pid_info;
+
+/* REG() entries */
+extern const struct file_operations krg_proc_pid_environ_operations;
+
+/* INF() entries */
+extern const struct file_operations krg_proc_info_file_operations;
+int krg_proc_pid_cmdline(struct proc_distant_pid_info *task, char *buffer);
+int krg_proc_pid_auxv(struct proc_distant_pid_info *task, char *buffer);
+int krg_proc_pid_limits(struct proc_distant_pid_info *task, char *buffer);
+int krg_proc_pid_syscall(struct proc_distant_pid_info *task, char *buffer);
+int krg_proc_pid_wchan(struct proc_distant_pid_info *task, char *buffer);
+int krg_proc_pid_schedstat(struct proc_distant_pid_info *task, char *buffer);
+int krg_proc_pid_oom_score(struct proc_distant_pid_info *task, char *buffer);
+int krg_proc_tgid_io_accounting(struct proc_distant_pid_info *task,
+				char *buffer);
+
+/* ONE() entries */
+extern const struct file_operations krg_proc_single_file_operations;
+int krg_proc_pid_status(struct file *file, struct proc_distant_pid_info *task,
+			char *buffer, size_t count);
+int krg_proc_pid_personality(struct file *file,
+			     struct proc_distant_pid_info *task,
+			     char *buffer, size_t count);
+int krg_proc_tgid_stat(struct file *file, struct proc_distant_pid_info *task,
+		       char *buffer, size_t count);
+int krg_proc_pid_statm(struct file *file, struct proc_distant_pid_info *task,
+		       char *buffer, size_t count);
+int krg_proc_pid_stack(struct file *file, struct proc_distant_pid_info *task,
+		       char *buffer, size_t count);
+
+void proc_pid_file_init(void);
+void proc_pid_file_finalize(void);
+
+#endif /* __PROC_PID_FILE_H__ */
diff -ruN linux-2.6.29/kerrighed/procfs/proc_pid.h android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid.h
--- linux-2.6.29/kerrighed/procfs/proc_pid.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid.h	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,22 @@
+/** Global /proc/<pid> information management.
+ *  @file proc_pid.h
+ *
+ *  @author David Margery
+ */
+
+#ifndef __PROC_PID_H__
+#define __PROC_PID_H__
+
+#include <linux/proc_fs.h>
+#include <linux/fs.h>
+
+static inline
+struct proc_distant_pid_info *get_krg_proc_task(struct inode *inode)
+{
+	return &PROC_I(inode)->distant_proc;
+}
+
+int proc_pid_init(void);
+int proc_pid_finalize(void);
+
+#endif /* __PROC_PID_H__ */
diff -ruN linux-2.6.29/kerrighed/procfs/proc_pid_link.c android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid_link.c
--- linux-2.6.29/kerrighed/procfs/proc_pid_link.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid_link.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,101 @@
+/*
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2007, Louis Rilling - Kerlabs.
+ */
+
+#include <linux/proc_fs.h>
+#include <linux/procfs_internal.h>
+#include <linux/mount.h>
+#include <linux/namei.h>
+#include <linux/sched.h>
+#include <linux/cred.h>
+#include <linux/uaccess.h>
+#include <kerrighed/task.h>
+
+#include "proc_pid.h"
+#include "proc_pid_link.h"
+
+static int krg_proc_fd_access_allowed(struct inode *inode)
+{
+	struct proc_distant_pid_info *task = get_krg_proc_task(inode);
+/* 	struct task_kddm_object *obj; */
+	const struct cred *cred = current_cred();
+	int allowed = 0;
+
+/* 	obj = krg_task_readlock(task->pid); */
+/* 	if (obj) { */
+		if (((cred->uid != task->euid) ||
+/*		     (cred->uid != obj->suid) || */
+/* 		     (cred->uid != obj->uid) || */
+		     (cred->gid != task->egid)/*  || */
+/*		     (cred->gid != obj->sgid) || */
+/*		     (cred->gid != obj->gid) */) && !capable(CAP_SYS_PTRACE))
+			allowed = -EPERM;
+		if (!task->dumpable && !capable(CAP_SYS_PTRACE))
+			allowed = -EPERM;
+/* 	} */
+/* 	krg_task_unlock(task->pid); */
+	return allowed;
+}
+
+static void *krg_proc_pid_follow_link(struct dentry *dentry,
+				      struct nameidata *nd)
+{
+	struct inode *inode = dentry->d_inode;
+	int error = -EACCES;
+
+	/* We don't need a base pointer in the /proc filesystem */
+	path_put(&nd->path);
+
+	/* Are we allowed to snoop on the tasks file descriptors? */
+	if (!krg_proc_fd_access_allowed(inode))
+		goto out;
+
+	error = get_krg_proc_task(inode)->op.proc_get_link(inode, &nd->path);
+	nd->last_type = LAST_BIND;
+out:
+	return ERR_PTR(error);
+}
+
+static int krg_proc_pid_readlink(struct dentry *dentry,
+				 char __user *buffer, int buflen)
+{
+	int error = -EACCES;
+	struct inode *inode = dentry->d_inode;
+	struct path path;
+
+	/* Are we allowed to snoop on the tasks file descriptors? */
+	if (!krg_proc_fd_access_allowed(inode))
+		goto out;
+
+	error = get_krg_proc_task(inode)->op.proc_get_link(inode, &path);
+	if (error)
+		goto out;
+
+	error = do_proc_readlink(&path, buffer, buflen);
+	path_put(&path);
+out:
+	return error;
+}
+
+struct inode_operations krg_proc_pid_link_inode_operations = {
+	.readlink = krg_proc_pid_readlink,
+	.follow_link = krg_proc_pid_follow_link,
+	.setattr = proc_setattr,
+};
+
+int krg_proc_exe_link(struct inode *inode, struct path *path)
+{
+	return 0;
+}
+
+int krg_proc_cwd_link(struct inode *inode, struct path *path)
+{
+	return 0;
+}
+
+int krg_proc_root_link(struct inode *inode, struct path *path)
+{
+	/* should increment fs of task at distance */
+	return 0;
+}
diff -ruN linux-2.6.29/kerrighed/procfs/proc_pid_link.h android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid_link.h
--- linux-2.6.29/kerrighed/procfs/proc_pid_link.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/proc_pid_link.h	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,18 @@
+/**  /proc/<pid>/<link> information management.
+ *  @file proc_pid_link.h
+ *
+ *  @author David Margery
+ */
+
+#ifndef __PROC_PID_LINK_H__
+#define __PROC_PID_LINK_H__
+
+#include <linux/fs.h>
+
+extern struct inode_operations krg_proc_pid_link_inode_operations;
+
+int krg_proc_exe_link(struct inode *inode, struct path *path);
+int krg_proc_cwd_link(struct inode *inode, struct path *path);
+int krg_proc_root_link(struct inode *inode, struct path *path);
+
+#endif /* __PROC_PID_LINK_H__ */
diff -ruN linux-2.6.29/kerrighed/procfs/static_cpu_info_linker.c android_cluster/linux-2.6.29/kerrighed/procfs/static_cpu_info_linker.c
--- linux-2.6.29/kerrighed/procfs/static_cpu_info_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/static_cpu_info_linker.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,79 @@
+/** Static CPU information management.
+ *  @file static_cpu_info_linker.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <kerrighed/cpu_id.h>
+#include <asm/kerrighed/cpuinfo.h>
+#include <asm/processor.h>
+#include <linux/swap.h>
+
+#include <kddm/kddm.h>
+
+#include "static_cpu_info_linker.h"
+
+#include <kerrighed/debug.h>
+
+struct kddm_set *static_cpu_info_kddm_set;
+
+/*****************************************************************************/
+/*                                                                           */
+/*                    STATIC CPU INFO KDDM IO FUNCTIONS                      */
+/*                                                                           */
+/*****************************************************************************/
+
+kerrighed_node_t cpu_info_default_owner(struct kddm_set *set,
+					objid_t objid,
+					const krgnodemask_t *nodes,
+					int nr_nodes)
+{
+	return krg_cpu_node(objid);
+}
+
+/****************************************************************************/
+
+/* Init the cpu info IO linker */
+
+static struct iolinker_struct static_cpu_info_io_linker = {
+	.linker_name = "stat_cpu_info",
+	.linker_id = STATIC_CPU_INFO_LINKER,
+	.default_owner = cpu_info_default_owner
+};
+
+int static_cpu_info_init(void)
+{
+	krg_static_cpu_info_t *static_cpu_info;
+	int cpu_id, i;
+
+	register_io_linker(STATIC_CPU_INFO_LINKER, &static_cpu_info_io_linker);
+
+	/* Create the CPU info kddm set */
+
+	static_cpu_info_kddm_set =
+		create_new_kddm_set(kddm_def_ns,
+				    STATIC_CPU_INFO_KDDM_ID,
+				    STATIC_CPU_INFO_LINKER,
+				    KDDM_CUSTOM_DEF_OWNER,
+				    sizeof(krg_static_cpu_info_t),
+				    0);
+	if (IS_ERR(static_cpu_info_kddm_set))
+		OOM;
+
+	for_each_online_cpu (i) {
+		cpu_id = krg_cpu_id(i);
+		cpu_data(i).krg_cpu_id = cpu_id;
+
+		static_cpu_info =
+			_kddm_grab_object(static_cpu_info_kddm_set, cpu_id);
+
+		static_cpu_info->info = cpu_data(i);
+#ifndef CONFIG_USERMODE
+		static_cpu_info->info.cpu_khz = cpu_khz;
+#endif
+
+		_kddm_put_object(static_cpu_info_kddm_set, cpu_id);
+	}
+
+	return 0;
+}
diff -ruN linux-2.6.29/kerrighed/procfs/static_cpu_info_linker.h android_cluster/linux-2.6.29/kerrighed/procfs/static_cpu_info_linker.h
--- linux-2.6.29/kerrighed/procfs/static_cpu_info_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/static_cpu_info_linker.h	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,64 @@
+/** Static CPU information management.
+ *  @file static_cpu_info_linker.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef STATIC_CPU_INFO_LINKER_H
+#define STATIC_CPU_INFO_LINKER_H
+
+#include <kerrighed/cpu_id.h>
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+#include <asm/kerrighed/cpuinfo.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+/* Static CPU informations */
+
+typedef struct {
+	cpuinfo_t info;
+} krg_static_cpu_info_t;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+extern struct kddm_set *static_cpu_info_kddm_set;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+int static_cpu_info_init(void);
+
+/** Helper function to get static CPU informations.
+ *  @author Renaud Lottiaux
+ *
+ *  @param node_id   Id of the node hosting the CPU we want informations on.
+ *  @param cpu_id    Id of the CPU we want informations on.
+ *
+ *  @return  Structure containing information on the requested CPU.
+ */
+static inline krg_static_cpu_info_t *get_static_cpu_info(int node_id,
+							 int cpu_id)
+{
+	return _fkddm_get_object(static_cpu_info_kddm_set,
+				 __krg_cpu_id(node_id, cpu_id),
+				 KDDM_NO_FREEZE|KDDM_NO_FT_REQ);
+}
+
+kerrighed_node_t cpu_info_default_owner(struct kddm_set *set,
+					objid_t objid,
+					const krgnodemask_t *nodes,
+					int nr_nodes);
+
+#endif /* STATIC_CPU_INFO LINKER_H */
diff -ruN linux-2.6.29/kerrighed/procfs/static_node_info_linker.c android_cluster/linux-2.6.29/kerrighed/procfs/static_node_info_linker.c
--- linux-2.6.29/kerrighed/procfs/static_node_info_linker.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/static_node_info_linker.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,69 @@
+/** Static node information management.
+ *  @file static_node_info_linker.c
+ *
+ *  Copyright (C) 2001-2006, INRIA, Universite de Rennes 1, EDF.
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+#include <linux/swap.h>
+#include <kddm/kddm.h>
+
+#include "static_node_info_linker.h"
+
+#include <kerrighed/debug.h>
+
+struct kddm_set *static_node_info_kddm_set;
+
+/*****************************************************************************/
+/*                                                                           */
+/*                     STATIC NODE INFO KDDM IO FUNCTIONS                    */
+/*                                                                           */
+/*****************************************************************************/
+
+kerrighed_node_t node_info_default_owner(struct kddm_set *set,
+					 objid_t objid,
+					 const krgnodemask_t *nodes,
+					 int nr_nodes)
+{
+	return objid;
+}
+
+/****************************************************************************/
+
+/* Init the static node info IO linker */
+
+static struct iolinker_struct static_node_info_io_linker = {
+	.default_owner = node_info_default_owner,
+	.linker_name = "stat_node_nfo",
+	.linker_id = STATIC_NODE_INFO_LINKER,
+};
+
+int static_node_info_init()
+{
+	krg_static_node_info_t *static_node_info;
+
+	register_io_linker(STATIC_NODE_INFO_LINKER,
+			   &static_node_info_io_linker);
+
+	/* Create the static node info kddm set */
+
+	static_node_info_kddm_set =
+		create_new_kddm_set(kddm_def_ns,
+				    STATIC_NODE_INFO_KDDM_ID,
+				    STATIC_NODE_INFO_LINKER,
+				    KDDM_CUSTOM_DEF_OWNER,
+				    sizeof(krg_static_node_info_t),
+				    0);
+	if (IS_ERR(static_node_info_kddm_set))
+		OOM;
+
+	static_node_info = _kddm_grab_object(static_node_info_kddm_set,
+					     kerrighed_node_id);
+
+	static_node_info->nr_cpu = num_online_cpus();
+	static_node_info->totalram = totalram_pages;
+	static_node_info->totalhigh = totalhigh_pages;
+
+	_kddm_put_object(static_node_info_kddm_set, kerrighed_node_id);
+
+	return 0;
+}
diff -ruN linux-2.6.29/kerrighed/procfs/static_node_info_linker.h android_cluster/linux-2.6.29/kerrighed/procfs/static_node_info_linker.h
--- linux-2.6.29/kerrighed/procfs/static_node_info_linker.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/procfs/static_node_info_linker.h	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,61 @@
+/** Static node informations management.
+ *  @file static_node_info_linker.h
+ *
+ *  @author Renaud Lottiaux
+ */
+
+#ifndef STATIC_NODE_INFO_LINKER_H
+#define STATIC_NODE_INFO_LINKER_H
+
+#include <kddm/kddm.h>
+#include <kddm/object_server.h>
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                                  TYPES                                   *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+/* Static node informations */
+
+typedef struct {
+	int nr_cpu;		/* Number of CPU on the node */
+	unsigned long totalram;	/* Total usable main memory size */
+	unsigned long totalhigh;	/* Total high memory size */
+} krg_static_node_info_t;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                            EXTERN VARIABLES                              *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+extern struct kddm_set *static_node_info_kddm_set;
+
+/*--------------------------------------------------------------------------*
+ *                                                                          *
+ *                              EXTERN FUNCTIONS                            *
+ *                                                                          *
+ *--------------------------------------------------------------------------*/
+
+int static_node_info_init(void);
+
+/** Helper function to get static node informations.
+ *  @author Renaud Lottiaux
+ *
+ *  @param node_id   Id of the node we want informations on.
+ *
+ *  @return  Structure containing information on the requested node.
+ */
+static inline krg_static_node_info_t *get_static_node_info(int node_id)
+{
+	return _fkddm_get_object(static_node_info_kddm_set, node_id,
+				 KDDM_NO_FREEZE|KDDM_NO_FT_REQ);
+}
+
+kerrighed_node_t node_info_default_owner(struct kddm_set *set,
+					 objid_t objid,
+					 const krgnodemask_t *nodes,
+					 int nr_nodes);
+
+#endif /* STATIC_NODE_INFO_LINKER_H */
diff -ruN linux-2.6.29/kerrighed/scheduler/core.c android_cluster/linux-2.6.29/kerrighed/scheduler/core.c
--- linux-2.6.29/kerrighed/scheduler/core.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/core.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,194 @@
+/*
+ *  kerrighed/scheduler/core.c
+ *
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/configfs.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/scheduler/process_set.h>
+
+#include "internal.h"
+
+static struct config_item_type krg_scheduler_type = {
+	.ct_owner = THIS_MODULE,
+};
+
+struct configfs_subsystem krg_scheduler_subsys = {
+	.su_group = {
+		.cg_item = {
+			.ci_namebuf = "krg_scheduler",
+			.ci_type = &krg_scheduler_type,
+		}
+	}
+};
+
+static int add(struct hotplug_context *ctx)
+{
+	return global_config_add(ctx);
+}
+
+static int hotplug_notifier(struct notifier_block *nb,
+			    hotplug_event_t event,
+			    void *data)
+{
+	struct hotplug_context *ctx;
+	int err;
+
+	switch(event){
+	case HOTPLUG_NOTIFY_ADD:
+		ctx = data;
+		err = add(ctx);
+		break;
+	default:
+		err = 0;
+		break;
+	}
+
+	if (err)
+		return notifier_from_errno(err);
+	return NOTIFY_OK;
+}
+
+static int post_add(struct hotplug_context *ctx)
+{
+	int err;
+
+	err = scheduler_post_add(ctx);
+	if (err)
+		return err;
+	return global_config_post_add(ctx);
+}
+
+static int post_hotplug_notifier(struct notifier_block *nb,
+				 hotplug_event_t event,
+				 void *data)
+{
+	struct hotplug_context *ctx;
+	int err;
+
+	switch(event){
+	case HOTPLUG_NOTIFY_ADD:
+		ctx = data;
+		err = post_add(ctx);
+		break;
+	default:
+		err = 0;
+		break;
+	}
+
+	if (err)
+		return notifier_from_errno(err);
+	return NOTIFY_OK;
+}
+
+int init_scheduler(void)
+{
+	int ret;
+	struct config_group **defs = NULL;
+
+	/* per task informations framework */
+	ret = krg_sched_info_start();
+	if (ret)
+		goto err_krg_sched_info;
+
+	/* initialize global mechanisms to replicate configfs operations */
+	ret = global_lock_start();
+	if (ret)
+		goto err_global_lock;
+	ret = string_list_start();
+	if (ret)
+		goto err_string_list;
+	ret = global_config_start();
+	if (ret)
+		goto err_global_config;
+	ret = remote_pipe_start();
+	if (ret)
+		goto err_remote_pipe;
+
+	/* initialize and register configfs subsystem. */
+	config_group_init(&krg_scheduler_subsys.su_group);
+	mutex_init(&krg_scheduler_subsys.su_mutex);
+
+	/* add probes, sched_policies to scheduler. */
+	defs = kcalloc(3, sizeof (struct config_group *), GFP_KERNEL);
+
+	if (defs == NULL) {
+		printk(KERN_ERR "[%s] error: cannot allocate memory!\n",
+			"scheduler_module_init");
+		ret = -ENOMEM;
+		goto err_kcalloc;
+	}
+
+	/* initialize probes and scheduling policies subgroup. */
+	defs[0] = scheduler_probe_start();
+	defs[1] = scheduler_start();
+	defs[2] = NULL;
+
+	if (defs[0]==NULL || defs[1]==NULL) {
+		printk(KERN_ERR "[%s] error: Could not initialize one of the"
+			" subgroups!\n", __PRETTY_FUNCTION__);
+		ret = -EFAULT;
+		goto err_init;
+	}
+
+	krg_scheduler_subsys.su_group.default_groups = defs;
+
+	ret = configfs_register_subsystem(&krg_scheduler_subsys);
+
+	if (ret) {
+		printk(KERN_ERR "[%s] error %d: cannot register subsystem!\n",
+			__PRETTY_FUNCTION__, ret);
+		goto err_register;
+	}
+
+	ret = register_hotplug_notifier(hotplug_notifier,
+					HOTPLUG_PRIO_SCHED);
+	if (ret)
+		goto err_hotplug;
+
+	ret = register_hotplug_notifier(post_hotplug_notifier,
+					HOTPLUG_PRIO_SCHED_POST);
+	if (ret)
+		goto err_hotplug;
+
+	printk(KERN_INFO "scheduler initialization succeeded!\n");
+	return 0;
+
+err_hotplug:
+
+	configfs_unregister_subsystem(&krg_scheduler_subsys);
+err_register:
+
+err_init:
+	if (defs[1])
+		scheduler_exit();
+	if (defs[0])
+		scheduler_probe_exit();
+	kfree(defs);
+err_kcalloc:
+
+	remote_pipe_exit();
+err_remote_pipe:
+
+	global_config_exit();
+err_global_config:
+
+	string_list_exit();
+err_string_list:
+
+	global_lock_exit();
+err_global_lock:
+
+	krg_sched_info_exit();
+err_krg_sched_info:
+
+	return ret;
+}
+
+void cleanup_scheduler(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/scheduler/filter.c android_cluster/linux-2.6.29/kerrighed/scheduler/filter.c
--- linux-2.6.29/kerrighed/scheduler/filter.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/filter.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,110 @@
+/*
+ *  kerrighed/scheduler/filter.c
+ *
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/scheduler/pipe.h>
+#include <kerrighed/scheduler/port.h>
+#include <kerrighed/scheduler/filter.h>
+
+static int scheduler_filter_attribute_array_length(
+	struct scheduler_filter_attribute **attrs)
+{
+	int nr = 0;
+	if (attrs)
+		while (attrs[nr])
+			nr++;
+	return nr;
+}
+
+int scheduler_filter_type_register(struct scheduler_filter_type *type)
+{
+	struct configfs_attribute **tmp_attrs = NULL;
+	int nr_attrs, i;
+	int err;
+
+	if (!type->source_type.get_value
+	    || !type->port_type.new || !type->port_type.destroy)
+		return -EINVAL;
+
+	nr_attrs = scheduler_filter_attribute_array_length(type->attrs);
+	if (nr_attrs) {
+		err = -ENOMEM;
+		tmp_attrs = kmalloc(sizeof(*tmp_attrs) * (nr_attrs + 1),
+				    GFP_KERNEL);
+		if (!tmp_attrs)
+			goto err_attrs;
+		for (i = 0; i < nr_attrs; i++)
+			tmp_attrs[i] = &type->attrs[i]->port_attr.config;
+		tmp_attrs[nr_attrs] = NULL;
+	}
+	err = scheduler_port_type_register(&type->port_type, tmp_attrs);
+	kfree(tmp_attrs);
+
+out:
+	return err;
+err_attrs:
+	goto out;
+}
+EXPORT_SYMBOL(scheduler_filter_type_register);
+
+void scheduler_filter_type_unregister(struct scheduler_filter_type *type)
+{
+	scheduler_port_type_unregister(&type->port_type);
+}
+EXPORT_SYMBOL(scheduler_filter_type_unregister);
+
+int scheduler_filter_init(struct scheduler_filter *filter,
+			  const char *name,
+			  struct scheduler_filter_type *type,
+			  struct config_group **default_groups)
+{
+	scheduler_source_init(&filter->source, &type->source_type);
+	return scheduler_port_init(&filter->port, name, &type->port_type,
+				   &filter->source,
+				   default_groups);
+}
+EXPORT_SYMBOL(scheduler_filter_init);
+
+void scheduler_filter_cleanup(struct scheduler_filter *filter)
+{
+	scheduler_port_cleanup(&filter->port);
+	scheduler_source_cleanup(&filter->source);
+}
+EXPORT_SYMBOL(scheduler_filter_cleanup);
+
+int scheduler_filter_simple_source_get_value(struct scheduler_source *source,
+					     void *value_p, unsigned int nr,
+					     const void *in_value_p,
+					     unsigned int in_nr)
+{
+	struct scheduler_filter *filter;
+	filter = container_of(source, struct scheduler_filter, source);
+	return scheduler_port_get_value(&filter->port,
+					value_p, nr, in_value_p, in_nr);
+}
+EXPORT_SYMBOL(scheduler_filter_simple_source_get_value);
+
+ssize_t
+scheduler_filter_simple_source_show_value(struct scheduler_source *source,
+					  char *page)
+{
+	struct scheduler_filter *filter;
+	filter = container_of(source, struct scheduler_filter, source);
+	return scheduler_port_show_value(&filter->port, page);
+}
+EXPORT_SYMBOL(scheduler_filter_simple_source_show_value);
+
+void scheduler_filter_simple_sink_update_value(struct scheduler_sink *sink,
+					       struct scheduler_source *source)
+{
+	struct scheduler_filter *filter;
+	filter = container_of(sink, struct scheduler_filter, port.sink);
+	scheduler_source_publish(&filter->source);
+}
+EXPORT_SYMBOL(scheduler_filter_simple_sink_update_value);
diff -ruN linux-2.6.29/kerrighed/scheduler/filters/freq_limit_filter.c android_cluster/linux-2.6.29/kerrighed/scheduler/filters/freq_limit_filter.c
--- linux-2.6.29/kerrighed/scheduler/filters/freq_limit_filter.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/filters/freq_limit_filter.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,212 @@
+/*
+ *  kerrighed/scheduler/filters/freq_limit_filter.c
+ *
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/ktime.h>
+#include <kerrighed/scheduler/filter.h>
+#include <kerrighed/scheduler/port.h>
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Louis Rilling <Louis.Rilling@kerlabs.com>");
+MODULE_DESCRIPTION("Filter to limit the frequency of events");
+
+struct config_group;
+
+struct freq_limit_filter {
+	struct scheduler_filter filter;
+	u64 min_interval_nsec;
+	struct scheduler_port last_event_port;
+	struct scheduler_port events_on_going_port;
+	struct config_group *default_groups[3];
+};
+
+static inline
+struct freq_limit_filter *to_freq_limit_filter(struct scheduler_filter *filter)
+{
+	return container_of(filter, struct freq_limit_filter, filter);
+}
+
+DEFINE_SCHEDULER_FILTER_ATTRIBUTE_SHOW(min_interval, filter, attr, page)
+{
+	struct freq_limit_filter *f = to_freq_limit_filter(filter);
+	u64 min_interval_nsec;
+
+	/*
+	 * Access to 64 bits is not atomic on 32 bits x86 so locking is
+	 * required.
+	 */
+	scheduler_filter_lock(filter);
+	min_interval_nsec = f->min_interval_nsec;
+	scheduler_filter_unlock(filter);
+	return sprintf(page, "%llu\n", min_interval_nsec);
+}
+
+DEFINE_SCHEDULER_FILTER_ATTRIBUTE_STORE(min_interval, filter, attr, buffer, size)
+{
+	struct freq_limit_filter *f = to_freq_limit_filter(filter);
+	char *pos;
+	u64 min_interval;
+
+	min_interval = simple_strtoull(buffer, &pos, 10);
+	if (((*pos == '\n' && pos - buffer == size - 1)
+	     || (*pos == '\0' && pos - buffer == size))
+	    && pos != buffer) {
+		scheduler_filter_lock(filter);
+		f->min_interval_nsec = min_interval;
+		scheduler_filter_unlock(filter);
+		return size;
+	}
+	return -EINVAL;
+}
+
+static BEGIN_SCHEDULER_FILTER_ATTRIBUTE(min_interval_attr, min_interval, 0664),
+	.SCHEDULER_FILTER_ATTRIBUTE_SHOW(min_interval),
+	.SCHEDULER_FILTER_ATTRIBUTE_STORE(min_interval),
+END_SCHEDULER_FILTER_ATTRIBUTE(min_interval);
+
+static struct scheduler_filter_attribute *freq_limit_attrs[] = {
+	&min_interval_attr,
+	NULL
+};
+
+static BEGIN_SCHEDULER_PORT_TYPE(last_event_port),
+	.SCHEDULER_PORT_VALUE_TYPE(last_event_port, ktime_t),
+END_SCHEDULER_PORT_TYPE(last_event_port);
+static BEGIN_SCHEDULER_PORT_TYPE(events_on_going_port),
+	.SCHEDULER_PORT_VALUE_TYPE(events_on_going_port, int),
+END_SCHEDULER_PORT_TYPE(events_on_going_port);
+
+DEFINE_SCHEDULER_FILTER_UPDATE_VALUE(freq_limit_filter, filter)
+{
+	struct freq_limit_filter *f = to_freq_limit_filter(filter);
+	ktime_t last_event;
+	int on_going;
+	ktime_t now;
+	struct timespec now_ts;
+	u64 interval;
+	u64 min_interval;
+	int ret;
+
+	scheduler_filter_lock(filter);
+	min_interval = f->min_interval_nsec;
+	scheduler_filter_unlock(filter);
+
+	if (!min_interval)
+		goto propagate;
+
+	ret = scheduler_port_get_value(&f->last_event_port,
+				       &last_event, 1, NULL, 0);
+	if (ret < 1)
+		return;
+
+	ktime_get_ts(&now_ts);
+	now = timespec_to_ktime(now_ts);
+
+	interval = (u64) ktime_to_ns(ktime_sub(now, last_event));
+	if (interval < min_interval)
+		return;
+
+	ret = scheduler_port_get_value(&f->events_on_going_port,
+				       &on_going, 1, NULL, 0);
+	if (ret == 1 && on_going)
+		return;
+
+propagate:
+	scheduler_filter_simple_update_value(filter);
+}
+
+/* Forward declaration */
+static struct scheduler_filter_type freq_limit_filter_type;
+
+DEFINE_SCHEDULER_FILTER_NEW(freq_limit_filter, name)
+{
+	struct freq_limit_filter *f = kmalloc(sizeof(*f), GFP_KERNEL);
+	int err;
+
+	if (!f)
+		goto err_freq_limit;
+	f->min_interval_nsec = 0;
+	err = scheduler_port_init(&f->last_event_port, "last_event",
+				  &last_event_port_type, NULL, NULL);
+	if (err)
+		goto err_last_event;
+	err = scheduler_port_init(&f->events_on_going_port, "events_on_going",
+				  &events_on_going_port_type, NULL, NULL);
+	if (err)
+		goto err_events_on_going;
+	f->default_groups[0] = scheduler_port_config_group(&f->last_event_port);
+	f->default_groups[1] =
+		scheduler_port_config_group(&f->events_on_going_port);
+	f->default_groups[2] = NULL;
+	err = scheduler_filter_init(&f->filter, name, &freq_limit_filter_type,
+				    f->default_groups);
+	if (err)
+		goto err_filter;
+
+	return &f->filter;
+
+err_filter:
+	scheduler_port_cleanup(&f->events_on_going_port);
+err_events_on_going:
+	scheduler_port_cleanup(&f->last_event_port);
+err_last_event:
+	kfree(f);
+err_freq_limit:
+	return NULL;
+}
+
+DEFINE_SCHEDULER_FILTER_DESTROY(freq_limit_filter, filter)
+{
+	struct freq_limit_filter *f = to_freq_limit_filter(filter);
+
+	scheduler_filter_cleanup(&f->filter);
+	scheduler_port_cleanup(&f->events_on_going_port);
+	scheduler_port_cleanup(&f->last_event_port);
+	kfree(f);
+}
+
+static BEGIN_SCHEDULER_FILTER_TYPE(freq_limit_filter),
+	.SCHEDULER_FILTER_UPDATE_VALUE(freq_limit_filter),
+	.SCHEDULER_FILTER_SOURCE_VALUE_TYPE(freq_limit_filter, unsigned long),
+	.SCHEDULER_FILTER_PORT_VALUE_TYPE(freq_limit_filter, unsigned long),
+	.SCHEDULER_FILTER_ATTRIBUTES(freq_limit_filter, freq_limit_attrs),
+END_SCHEDULER_FILTER_TYPE(freq_limit_filter);
+
+int freq_limit_start(void)
+{
+	int err;
+
+	err = scheduler_port_type_init(&last_event_port_type, NULL);
+	if (err)
+		goto err_last_event;
+	err = scheduler_port_type_init(&events_on_going_port_type, NULL);
+	if (err)
+		goto err_events_on_going;
+	err = scheduler_filter_type_register(&freq_limit_filter_type);
+	if (err)
+		goto err_register;
+out:
+	return err;
+
+err_register:
+	scheduler_port_type_cleanup(&events_on_going_port_type);
+err_events_on_going:
+	scheduler_port_type_cleanup(&last_event_port_type);
+err_last_event:
+	goto out;
+}
+
+void freq_limit_exit(void)
+{
+	scheduler_filter_type_unregister(&freq_limit_filter_type);
+	scheduler_port_type_cleanup(&events_on_going_port_type);
+	scheduler_port_type_cleanup(&last_event_port_type);
+}
+
+module_init(freq_limit_start);
+module_exit(freq_limit_exit);
diff -ruN linux-2.6.29/kerrighed/scheduler/filters/Makefile android_cluster/linux-2.6.29/kerrighed/scheduler/filters/Makefile
--- linux-2.6.29/kerrighed/scheduler/filters/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/filters/Makefile	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,5 @@
+obj-$(CONFIG_KRG_SCHED_THRESHOLD_FILTER) += threshold_filter.o
+obj-$(CONFIG_KRG_SCHED_FREQ_LIMIT_FILTER) += freq_limit_filter.o
+obj-$(CONFIG_KRG_SCHED_REMOTE_CACHE_FILTER) += remote_cache_filter.o
+
+EXTRA_CFLAGS += -Wall -Werror
diff -ruN linux-2.6.29/kerrighed/scheduler/filters/remote_cache_filter.c android_cluster/linux-2.6.29/kerrighed/scheduler/filters/remote_cache_filter.c
--- linux-2.6.29/kerrighed/scheduler/filters/remote_cache_filter.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/filters/remote_cache_filter.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,282 @@
+/*
+ *  kerrighed/scheduler/filters/remote_cache_filter.c
+ *
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/module.h>
+#include <linux/workqueue.h>
+#include <linux/jiffies.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/scheduler/filter.h>
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Louis Rilling <Louis.Rilling@kerlabs.com>");
+MODULE_DESCRIPTION("Filter to proactively cache remote values");
+
+struct remote_cache_filter {
+	struct scheduler_filter filter;
+	unsigned long remote_values[KERRIGHED_MAX_NODES];
+	krgnodemask_t available_values;
+	unsigned long polling_period; /* in jiffies */
+	kerrighed_node_t current_node;
+	struct delayed_work polling_work;
+	int active; /* Is it able to collect values? */
+};
+
+static inline
+struct remote_cache_filter *
+to_remote_cache_filter(struct scheduler_filter *filter)
+{
+	return container_of(filter, struct remote_cache_filter, filter);
+}
+
+static inline void rc_lock(struct remote_cache_filter *rc_filter)
+{
+	scheduler_filter_lock(&rc_filter->filter);
+}
+
+static inline void rc_unlock(struct remote_cache_filter *rc_filter)
+{
+	scheduler_filter_unlock(&rc_filter->filter);
+}
+
+static void schedule_next_poll(struct remote_cache_filter *rc_filter)
+{
+	unsigned long delay;
+
+	delay = rc_filter->polling_period;
+	if (rc_filter->current_node != KERRIGHED_NODE_ID_NONE)
+		/*
+		 * Last polling phase could not finish within period. Schedule
+		 * next phase ASAP
+		 */
+		delay = 1;
+	else if (!delay)
+		/*
+		 * We are forced to schedule a poll in order to make
+		 * cancel_rearming_delayed_work() do its job.
+		 *
+		 * Schedule it every hour
+		 */
+		delay = msecs_to_jiffies(3600000);
+	schedule_delayed_work(&rc_filter->polling_work, delay);
+}
+
+static void reschedule_next_poll(struct remote_cache_filter *rc_filter)
+{
+	cancel_delayed_work(&rc_filter->polling_work);
+	schedule_next_poll(rc_filter);
+}
+
+DEFINE_SCHEDULER_FILTER_ATTRIBUTE_SHOW(polling_period, filter, attr, page)
+{
+	struct remote_cache_filter *f = to_remote_cache_filter(filter);
+	unsigned long period;
+
+	rc_lock(f);
+	period = f->polling_period;
+	rc_unlock(f);
+	return sprintf(page, "%u", jiffies_to_msecs(period));
+}
+
+DEFINE_SCHEDULER_FILTER_ATTRIBUTE_STORE(polling_period, filter, attr, page, count)
+{
+	struct remote_cache_filter *f = to_remote_cache_filter(filter);
+	unsigned long new_period;
+	char *last_read;
+
+	new_period = simple_strtoul(page, &last_read, 0);
+	if (last_read - page + 1 < count
+	    || (last_read[1] != '\0' && last_read[1] != '\n'))
+		return -EINVAL;
+	new_period = msecs_to_jiffies(new_period);
+	rc_lock(f);
+	f->polling_period = new_period;
+	reschedule_next_poll(f);
+	rc_unlock(f);
+
+	return count;
+}
+
+static BEGIN_SCHEDULER_FILTER_ATTRIBUTE(polling_period_attr, polling_period, 0666),
+	.SCHEDULER_FILTER_ATTRIBUTE_SHOW(polling_period),
+	.SCHEDULER_FILTER_ATTRIBUTE_STORE(polling_period),
+END_SCHEDULER_FILTER_ATTRIBUTE(polling_period);
+
+static struct scheduler_filter_attribute *remote_cache_attrs[] = {
+	&polling_period_attr,
+	NULL
+};
+
+/*
+ * Gets called:
+ * - from the scheduler framework, but only if it holds a reference,
+ * - or from the polling worker.
+ * So, when the destroy method is called, can only be called from the polling
+ * worker.
+ */
+static int try_get_remote_values(struct remote_cache_filter *f)
+{
+	kerrighed_node_t current_node = f->current_node;
+	int nr = 0;
+	int ret = 0;
+
+	while (current_node != KERRIGHED_NODE_ID_NONE) {
+		ret = scheduler_filter_simple_get_remote_value(
+			&f->filter,
+			current_node,
+			&f->remote_values[current_node], 1,
+			NULL, 0);
+		if (ret == -EAGAIN)
+			break;
+		nr++;
+		if (ret > 0)
+			krgnode_set(current_node, f->available_values);
+		else
+			krgnode_clear(current_node, f->available_values);
+		current_node = krgnode_next_online(current_node);
+		if (current_node == KERRIGHED_MAX_NODES)
+			current_node = KERRIGHED_NODE_ID_NONE;
+	}
+	f->current_node = current_node;
+
+	if (ret == -EACCES)
+		f->active = 0;
+
+	return nr;
+}
+
+static void get_remote_values(struct remote_cache_filter *rc_filter)
+{
+	kerrighed_node_t first_node;
+
+	if (rc_filter->current_node == KERRIGHED_NODE_ID_NONE) {
+		first_node = nth_online_krgnode(0);
+		if (first_node != KERRIGHED_MAX_NODES) {
+			rc_filter->current_node = first_node;
+			try_get_remote_values(rc_filter);
+		}
+	}
+}
+
+static void polling_worker(struct work_struct *work)
+{
+	struct remote_cache_filter *f =
+		container_of(work,
+			     struct remote_cache_filter, polling_work.work);
+
+	rc_lock(f);
+	schedule_next_poll(f);
+	get_remote_values(f);
+	rc_unlock(f);
+}
+
+DEFINE_SCHEDULER_FILTER_UPDATE_VALUE(remote_cache_filter, filter)
+{
+	struct remote_cache_filter *f = to_remote_cache_filter(filter);
+	int nr;
+
+	rc_lock(f);
+	nr = try_get_remote_values(f);
+	rc_unlock(f);
+
+	/*
+	 * Propagate updates from the connected local source.
+	 *
+	 * We may miss some if incidentally a remote value becomes available at
+	 * the same time. Let's hope this is not to bad...
+	 */
+	if (!nr)
+		/* Update comes from the connected local source */
+		scheduler_filter_simple_update_value(filter);
+}
+
+DEFINE_SCHEDULER_FILTER_GET_REMOTE_VALUE(remote_cache_filter, filter,
+					 node,
+					 unsigned long, value_p, nr,
+					 unsigned int, param_p, nr_param)
+{
+	struct remote_cache_filter *f = to_remote_cache_filter(filter);
+	int ret = 0;
+
+	rc_lock(f);
+	if (!f->active) {
+		/*
+		 * Do not wait for the next worker activation to begin reading
+		 * remote values
+		 */
+		f->active = 1;
+		reschedule_next_poll(f);
+		get_remote_values(f);
+	}
+	if (krgnode_isset(node, f->available_values)) {
+		value_p[0] = f->remote_values[node];
+		ret = 1;
+	}
+	rc_unlock(f);
+
+	return ret;
+}
+
+/* Forward declaration */
+static struct scheduler_filter_type remote_cache_filter_type;
+
+DEFINE_SCHEDULER_FILTER_NEW(remote_cache_filter, name)
+{
+	struct remote_cache_filter *f = kmalloc(sizeof(*f), GFP_KERNEL);
+	int err;
+
+	if (!f)
+		goto err_f;
+	err = scheduler_filter_init(&f->filter, name, &remote_cache_filter_type,
+				    NULL);
+	if (err)
+		goto err_filter;
+	krgnodes_clear(f->available_values);
+	f->polling_period = 0;
+	f->current_node = KERRIGHED_NODE_ID_NONE;
+	INIT_DELAYED_WORK(&f->polling_work, polling_worker);
+	f->active = 0;
+	schedule_next_poll(f);
+
+	return &f->filter;
+
+err_filter:
+	kfree(f);
+err_f:
+	return NULL;
+}
+
+DEFINE_SCHEDULER_FILTER_DESTROY(remote_cache_filter, filter)
+{
+	struct remote_cache_filter *f = to_remote_cache_filter(filter);
+	cancel_rearming_delayed_work(&f->polling_work);
+	scheduler_filter_cleanup(filter);
+	kfree(f);
+}
+
+static BEGIN_SCHEDULER_FILTER_TYPE(remote_cache_filter),
+	.SCHEDULER_FILTER_UPDATE_VALUE(remote_cache_filter),
+	.SCHEDULER_FILTER_GET_REMOTE_VALUE(remote_cache_filter),
+	.SCHEDULER_FILTER_SOURCE_VALUE_TYPE(remote_cache_filter, unsigned long),
+	.SCHEDULER_FILTER_PORT_VALUE_TYPE(remote_cache_filter, unsigned long),
+	.SCHEDULER_FILTER_ATTRIBUTES(remote_cache_filter, remote_cache_attrs),
+END_SCHEDULER_FILTER_TYPE(remote_cache_filter);
+
+static int remote_cache_start(void)
+{
+	return scheduler_filter_type_register(&remote_cache_filter_type);
+}
+
+static void remote_cache_exit(void)
+{
+	scheduler_filter_type_unregister(&remote_cache_filter_type);
+}
+
+module_init(remote_cache_start);
+module_exit(remote_cache_exit);
diff -ruN linux-2.6.29/kerrighed/scheduler/filters/threshold_filter.c android_cluster/linux-2.6.29/kerrighed/scheduler/filters/threshold_filter.c
--- linux-2.6.29/kerrighed/scheduler/filters/threshold_filter.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/filters/threshold_filter.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,122 @@
+/*
+ *  kerrighed/scheduler/filters/threshold_filter.c
+ *
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <kerrighed/scheduler/filter.h>
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Louis Rilling <Louis.Rilling@kerlabs.com>");
+MODULE_DESCRIPTION("Filter to propagate updates of values above a threshold");
+
+struct threshold_filter {
+	struct scheduler_filter filter;
+	unsigned long threshold __attribute__((aligned(sizeof(unsigned long))));
+};
+
+/*
+ * The threshold attribute is not protected by scheduler_filter_lock() since
+ * atomic memory access is sufficient.
+ */
+
+static inline
+struct threshold_filter *to_threshold_filter(struct scheduler_filter *filter)
+{
+	return container_of(filter, struct threshold_filter, filter);
+}
+
+DEFINE_SCHEDULER_FILTER_ATTRIBUTE_SHOW(threshold, filter, attr, page)
+{
+	struct threshold_filter *f = to_threshold_filter(filter);
+	return sprintf(page, "%lu", f->threshold);
+}
+
+DEFINE_SCHEDULER_FILTER_ATTRIBUTE_STORE(threshold, filter, attr, page, count)
+{
+	struct threshold_filter *f = to_threshold_filter(filter);
+	unsigned long new_value;
+	char *last_read;
+
+	new_value = simple_strtoul(page, &last_read, 0);
+	if (last_read - page + 1 < count
+	    || (last_read[1] != '\0' && last_read[1] != '\n'))
+		return -EINVAL;
+	f->threshold = new_value;
+	return count;
+}
+
+static BEGIN_SCHEDULER_FILTER_ATTRIBUTE(threshold_attr, threshold, 0666),
+	.SCHEDULER_FILTER_ATTRIBUTE_SHOW(threshold),
+	.SCHEDULER_FILTER_ATTRIBUTE_STORE(threshold),
+END_SCHEDULER_FILTER_ATTRIBUTE(threshold);
+
+static struct scheduler_filter_attribute *threshold_attrs[] = {
+	&threshold_attr,
+	NULL
+};
+
+DEFINE_SCHEDULER_FILTER_UPDATE_VALUE(threshold_filter, filter)
+{
+	struct threshold_filter *f = to_threshold_filter(filter);
+	unsigned long value;
+	ssize_t ret;
+
+	ret = scheduler_filter_simple_get_value(filter, &value, 1);
+	if (ret > 0 && value >= f->threshold)
+		scheduler_filter_simple_update_value(filter);
+}
+
+/* Forward declaration */
+static struct scheduler_filter_type threshold_filter_type;
+
+DEFINE_SCHEDULER_FILTER_NEW(threshold_filter, name)
+{
+	struct threshold_filter *f = kmalloc(sizeof(*f), GFP_KERNEL);
+	int err;
+
+	if (!f)
+		goto err_f;
+	err = scheduler_filter_init(&f->filter, name, &threshold_filter_type,
+				    NULL);
+	if (err)
+		goto err_filter;
+	f->threshold = 0;
+
+	return &f->filter;
+
+err_filter:
+	kfree(f);
+err_f:
+	return NULL;
+}
+
+DEFINE_SCHEDULER_FILTER_DESTROY(threshold_filter, filter)
+{
+	struct threshold_filter *f = to_threshold_filter(filter);
+	scheduler_filter_cleanup(filter);
+	kfree(f);
+}
+
+static BEGIN_SCHEDULER_FILTER_TYPE(threshold_filter),
+	.SCHEDULER_FILTER_UPDATE_VALUE(threshold_filter),
+	.SCHEDULER_FILTER_SOURCE_VALUE_TYPE(threshold_filter, unsigned long),
+	.SCHEDULER_FILTER_PORT_VALUE_TYPE(threshold_filter, unsigned long),
+	.SCHEDULER_FILTER_ATTRIBUTES(threshold_filter, threshold_attrs),
+END_SCHEDULER_FILTER_TYPE(threshold_filter);
+
+static int threshold_start(void)
+{
+	return scheduler_filter_type_register(&threshold_filter_type);
+}
+
+static void threshold_exit(void)
+{
+	scheduler_filter_type_unregister(&threshold_filter_type);
+}
+
+module_init(threshold_start);
+module_exit(threshold_exit);
diff -ruN linux-2.6.29/kerrighed/scheduler/global_config.c android_cluster/linux-2.6.29/kerrighed/scheduler/global_config.c
--- linux-2.6.29/kerrighed/scheduler/global_config.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/global_config.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,1681 @@
+/*
+ *  kerrighed/scheduler/global_config.c
+ *
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ */
+
+/*
+ * Helper functions to make configfs operations on kerrighed's schedulers global
+ *
+ * The principle is to replicate the operations done at user-level (mkdir,
+ * rmdir, symlink, unlink, and write), and to infer from the context if the
+ * operation is directly done by user-level, or by the replication engine. The
+ * current criterion is whether current is a kernel thread (assuming an RPC
+ * handler) or not.
+ *
+ * To ensure that all operations on a given item are done in the same order on
+ * all nodes, they are globally serialized.  However, since many configfs
+ * callbacks are called with mutex held on directories, we cannot be sure that
+ * two concurrent global operations on a same item will not deadlock. For this
+ * reason we use a global lock that implements only try_lock and unlock
+ * operations. If try_lock fails, the operation fails and the user is requested
+ * to try again. The above deadlock should be avoidable with a more globalized
+ * vfs.
+ */
+
+#include <linux/configfs.h>
+#include <linux/fs.h>
+#include <linux/mount.h>
+#include <linux/namei.h>
+#include <linux/fs_struct.h>
+#include <linux/gfp.h>
+#include <linux/cluster_barrier.h>
+#include <linux/mutex.h>
+#include <linux/rwsem.h>
+#include <linux/spinlock.h>
+#include <linux/string.h>
+#include <linux/workqueue.h>
+#include <linux/list.h>
+#include <linux/jiffies.h>
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/workqueue.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/ghost.h>
+#endif
+#include <kerrighed/scheduler/global_config.h>
+#include <net/krgrpc/rpc.h>
+#include <net/krgrpc/rpcid.h>
+#include <kddm/kddm.h>
+
+#include <asm/fcntl.h>
+#include <asm/system.h>
+
+#include "internal.h"
+#include "hashed_string_list.h"
+#include "string_list.h"
+#include "global_lock.h"
+
+struct global_config_attr {
+	struct list_head list;
+	struct list_head global_list;
+	struct config_item *item;
+	struct configfs_attribute *attr;
+	void *value;
+	size_t size;
+};
+
+static struct kddm_set *global_items_set;
+static LIST_HEAD(items_head);
+
+static struct global_config_item_operations *global_item_ops[] = {
+	&probe_source_global_item_ops,
+	&probe_global_item_ops,
+	&port_global_item_ops,
+	&policy_global_item_ops,
+	&process_set_global_item_ops,
+	&scheduler_global_item_ops,
+};
+static LIST_HEAD(attrs_head);
+static DEFINE_SPINLOCK(attrs_lock);
+static DECLARE_RWSEM(attrs_rwsem);
+
+static struct cluster_barrier *global_config_barrier;
+
+static struct vfsmount *scheduler_fs_mount; /* vfsmount attached to configfs */
+static int mount_count;
+
+int global_config_freeze(void)
+{
+	return global_lock_readlock(0);
+}
+
+void global_config_thaw(void)
+{
+	global_lock_unlock(0);
+}
+
+static inline int in_krg_scheduler_subsys(struct config_item *item)
+{
+	return item && item != &krg_scheduler_subsys.su_group.cg_item;
+}
+
+/* Two following functions adapted from configfs/symlink.c */
+
+static int item_path_length(struct config_item *item)
+{
+	struct config_item *p = item;
+	int length = 1;
+	do {
+		length += strlen(config_item_name(p)) + 1;
+		p = p->ci_parent;
+	} while (in_krg_scheduler_subsys(p));
+	return length;
+}
+
+static void fill_item_path(struct config_item *item, char *buffer, int length)
+{
+	struct config_item *p;
+
+	--length;
+	for (p = item; in_krg_scheduler_subsys(p); p = p->ci_parent) {
+		int cur = strlen(config_item_name(p));
+
+		/* back up enough to print this bus id with '/' */
+		length -= cur;
+		strncpy(buffer + length, config_item_name(p), cur);
+		*(buffer + --length) = '/';
+	}
+}
+
+/**
+ * Returns the absolute path combining parent and name, assuming that root is at
+ * the configfs scheduler subsystem entry.
+ *
+ * @param parent       base config_item
+ * @param name         component to catenate to parent's path
+ *
+ * @return	       pointer to a newly allocated string containing the
+ *                     absolute path. The path must be freed with put_path.
+ */
+static char *get_full_path(struct config_item *parent, const char *name)
+{
+	size_t parent_len = item_path_length(parent);
+	size_t full_len = parent_len - 1;
+	char *path;
+
+	if (name)
+		full_len += 1 + strlen(name);
+	path = kmalloc(full_len + 1, GFP_KERNEL);
+	if (!path)
+		return NULL;
+	fill_item_path(parent, path, parent_len);
+	if (name) {
+		path[parent_len - 1] = '/';
+		strcpy(path + parent_len, name);
+	} else
+		path[parent_len - 1] = '\0';
+	return path;
+}
+
+static void put_path(const char *path)
+{
+	kfree(path);
+}
+
+/**
+ * Prepares a directory operation by taking the mutex on the parent inode of
+ * an entry and returning a dentry for the entry.
+ *
+ * @param child_name   path to the entry, assumed absolute from configfs
+ *		       scheduler subsystem entry.
+ *
+ * @return	       a valid dentry to the target entry, or error. The valid
+ *		       dentry must be released with put_child_dentry.
+ */
+static struct dentry *get_child_dentry(const char *child_name)
+{
+	struct dentry *d_dir;
+	struct dentry *d_child;
+	const char *last_child_comp;
+	const char *real_child_name = child_name;
+	int err;
+
+	d_dir = dget(krg_scheduler_subsys.su_group.cg_item.ci_dentry);
+
+	last_child_comp = strrchr(child_name, '/');
+	if (last_child_comp) {
+		struct nameidata nd;
+
+		err = vfs_path_lookup(d_dir, scheduler_fs_mount,
+				      child_name, LOOKUP_PARENT, &nd);
+
+		dput(d_dir);
+
+		if (err)
+			return ERR_PTR(err);
+
+		d_dir = dget(nd.path.dentry);
+		path_put(&nd.path);
+		BUG_ON(!last_child_comp[1]);
+		real_child_name = last_child_comp + 1;
+	}
+
+	mutex_lock_nested(&d_dir->d_inode->i_mutex, I_MUTEX_PARENT);
+	d_child = lookup_one_len(real_child_name, d_dir, strlen(real_child_name));
+	if (IS_ERR(d_child))
+		mutex_unlock(&d_dir->d_inode->i_mutex);
+	dput(d_dir);
+	return d_child;
+}
+
+static void put_child_dentry(struct dentry *d_child)
+{
+	struct dentry *d_dir;
+
+	d_dir = dget(d_child->d_parent);
+	dput(d_child);
+	mutex_unlock(&d_dir->d_inode->i_mutex);
+	dput(d_dir);
+}
+
+/**
+ * Change current's root to configfs scheduler subsystem'root, and save
+ * previous root in parameters.
+ *
+ * @param prev_root    valid pointer to a struct path
+ *
+ * @return	       prev_rootmnt and prev_root are filled with the previous
+ *		       root. They must be restored with chroot_restore, or
+ *		       mntput/dput when not needed anymore.
+ */
+static void chroot_to_scheduler_subsystem(struct path *prev_root)
+{
+	struct path new_root;
+
+	/*
+	 * These two values won't change unless a pivot_root is running ...
+	 * but we assume that this can not happen.
+	 * Locking is more used for memory barriers than for anything else.
+	 */
+	read_lock(&current->fs->lock);
+	*prev_root = current->fs->root;
+	path_get(prev_root);
+	read_unlock(&current->fs->lock);
+
+	new_root.mnt = scheduler_fs_mount;
+	new_root.dentry = krg_scheduler_subsys.su_group.cg_item.ci_dentry;
+	set_fs_root(current->fs, &new_root);
+}
+
+static void chroot_restore(struct path *prev_root)
+{
+	set_fs_root(current->fs, prev_root);
+	path_put(prev_root);
+}
+
+/* Low level handling of global config operations */
+
+enum config_op {
+	CO_MKDIR,
+	CO_RMDIR,
+	CO_SYMLINK,
+	CO_UNLINK,
+	CO_WRITE,
+};
+
+static enum config_op reverse_op(enum config_op op)
+{
+	switch (op) {
+	case CO_MKDIR:
+		return CO_RMDIR;
+	case CO_SYMLINK:
+		return CO_UNLINK;
+	default:
+		BUG();
+	}
+}
+
+struct config_op_message {
+	enum config_op op;
+};
+
+static struct rpc_desc *__global_config_op_begin(krgnodemask_t *nodes,
+						 enum config_op op)
+{
+	struct config_op_message msg = {
+		.op = op
+	};
+	struct rpc_desc *desc;
+	int err;
+
+	desc = rpc_begin_m(GLOBAL_CONFIG_OP, nodes);
+	if (!desc)
+		return ERR_PTR(-ENOMEM);
+
+	err = rpc_pack_type(desc, msg);
+	if (err) {
+		rpc_cancel(desc);
+		rpc_end(desc, 0);
+		return ERR_PTR(err);
+	} else {
+		return desc;
+	}
+}
+
+/**
+ * Prepare to broadcast a global config operation to all *other* nodes.
+ *
+ * @param op	       op code of the operation
+ * @param nodes	       valid pointer to a nodes set
+ *
+ * @return	       a valid rpc_desc to do operation-specific communications,
+ *		       or error. nodes is filled with the nodes contacted for
+ *		       the operation.
+ */
+static struct rpc_desc *global_config_op_begin(enum config_op op,
+					       krgnodemask_t *nodes)
+{
+	krgnodemask_t _nodes = krgnode_online_map;
+	struct rpc_desc *desc;
+
+	krgnode_clear(kerrighed_node_id, _nodes);
+	desc = __global_config_op_begin(&_nodes, op);
+	if (!IS_ERR(desc))
+		*nodes = _nodes;
+	return desc;
+}
+
+/**
+ * Close a global config operation by retrieving the result from all contacted
+ * nodes.
+ *
+ * @param desc	       rpc_desc as returned by global_config_op_begin.
+ *		       Will be closed before returning a result.
+ * @param nodes	       valid pointer to a nodes set, previously filled by
+ *		       global_config_op_begin
+ *
+ * @return	       0 if the operation succeeded on all contacted nodes, or
+ *                     error
+ */
+static int global_config_op_end(struct rpc_desc *desc, krgnodemask_t *nodes)
+{
+	int res = 0;
+	kerrighed_node_t node;
+	int err;
+
+	for_each_krgnode_mask(node, *nodes) {
+		err = rpc_unpack_type_from(desc, node, res);
+		if (!err && res) {
+			rpc_cancel(desc);
+			goto out;
+		}
+	}
+
+out:
+	rpc_end(desc, 0);
+
+	return res;
+}
+
+static void handle_global_config_write(struct rpc_desc *desc,
+				       void *_msg, size_t size);
+static void handle_global_config_dir_op(struct rpc_desc *desc,
+					void *_msg, size_t size);
+
+/**
+ * Generic RPC handler for global config operations
+ */
+static void handle_global_config_op(struct rpc_desc *desc,
+				    void *_msg, size_t size)
+{
+	struct config_op_message *msg = _msg;
+
+	if (msg->op == CO_WRITE)
+		handle_global_config_write(desc, _msg, size);
+	else
+		handle_global_config_dir_op(desc, _msg, size);
+}
+
+/**
+ * Helper function to send a string
+ *
+ * @param desc	       RPC descriptor to send on
+ * @param string       string to send
+ *
+ * @return	       0 is success, or error
+ */
+static int pack_string(struct rpc_desc *desc, const char *string)
+{
+	size_t len = strlen(string);
+	int err;
+
+	err = rpc_pack_type(desc, len);
+	if (err)
+		goto out;
+	err = rpc_pack(desc, 0, string, len + 1);
+out:
+	return err;
+}
+
+/**
+ * Helper function to receive a string
+ *
+ * @param desc	       RPC descriptor to receive from
+ *
+ * @return	       a valid string pointer or error. The string must be
+ *		       freed with put_string.
+ */
+static char *unpack_get_string(struct rpc_desc *desc)
+{
+	size_t len;
+	char *string;
+	int err;
+
+	err = rpc_unpack_type(desc, len);
+	if (err)
+		goto err;
+	err = -ENOMEM;
+	string = kmalloc(len + 1, GFP_KERNEL);
+	if (!string)
+		goto err;
+	err = rpc_unpack(desc, 0, string, len + 1);
+	if (err)
+		goto err_string;
+out:
+	return string;
+
+err_string:
+	kfree(string);
+err:
+	string = ERR_PTR(err);
+	goto out;
+}
+
+static void put_string(char *string)
+{
+	kfree(string);
+}
+
+static
+int
+do_global_config_write(struct rpc_desc *desc, krgnodemask_t *nodes,
+		       struct config_item *item,
+		       struct configfs_attribute *attr,
+		       const char *page, size_t count)
+{
+	char *path;
+	int err;
+
+	err = -ENOMEM;
+	path = get_full_path(item, attr->ca_name);
+	if (!path)
+		goto err_cancel;
+	err = pack_string(desc, path);
+	put_path(path);
+	if (err)
+		goto err_cancel;
+
+	err = rpc_pack_type(desc, count);
+	if (err)
+		goto err_cancel;
+	err = rpc_pack(desc, 0, page, count);
+	if (err)
+		goto err_cancel;
+
+	return global_config_op_end(desc, nodes);
+
+err_cancel:
+	rpc_cancel(desc);
+	rpc_end(desc, 0);
+	return err;
+}
+
+static int global_config_write(struct config_item *item,
+			       struct configfs_attribute *attr,
+			       const char *page, size_t count)
+{
+	struct rpc_desc *desc;
+	krgnodemask_t nodes;
+
+	desc = global_config_op_begin(CO_WRITE, &nodes);
+	if (IS_ERR(desc))
+		return PTR_ERR(desc);
+	return do_global_config_write(desc, &nodes, item, attr, page, count);
+}
+
+static int __global_config_write(krgnodemask_t *nodes,
+				 struct config_item *item,
+				 struct configfs_attribute *attr,
+				 const char *page, size_t count)
+{
+	struct rpc_desc *desc;
+
+	desc = __global_config_op_begin(nodes, CO_WRITE);
+	if (IS_ERR(desc))
+		return PTR_ERR(desc);
+	return do_global_config_write(desc, nodes, item, attr, page, count);
+}
+
+/**
+ * RPC handler for global attribute store
+ */
+static void handle_global_config_write(struct rpc_desc *desc,
+				       void *_msg, size_t size)
+{
+	struct path old_root;
+	struct file *file;
+	loff_t pos = 0;
+	char *path;
+	void *buf;
+	size_t count;
+	ssize_t ret;
+	int err;
+
+	path = unpack_get_string(desc);
+	if (IS_ERR(path))
+		goto err_path;
+	err = rpc_unpack_type(desc, count);
+	if (err)
+		goto err_count;
+	buf = kmalloc(count, GFP_KERNEL);
+	if (!buf)
+		goto err_count;
+	err = rpc_unpack(desc, 0, buf, count);
+	if (err)
+		goto err_buf;
+
+	chroot_to_scheduler_subsystem(&old_root);
+
+	file = filp_open(path, O_WRONLY, 0);
+	if (IS_ERR(file)) {
+		err = PTR_ERR(file);
+		goto chroot_restore;
+	}
+	ret = vfs_write(file, buf, count, &pos);
+	err = filp_close(file, NULL);
+
+	if (ret != count) {
+		if (ret >= 0)
+			err = -ENOSPC;
+		else
+			err = ret;
+	}
+chroot_restore:
+	chroot_restore(&old_root);
+
+	rpc_pack_type(desc, err);
+
+	kfree(buf);
+	put_string(path);
+out:
+	return;
+
+err_buf:
+	kfree(buf);
+err_count:
+	put_string(path);
+err_path:
+	rpc_cancel(desc);
+	goto out;
+}
+
+static int do_global_config_dir_op(struct rpc_desc *desc, krgnodemask_t *nodes,
+				   enum config_op op,
+				   const char *name, const char *old_name)
+{
+	int err;
+
+	err = pack_string(desc, name);
+	if (err)
+		goto err_cancel;
+	if (!old_name) {
+		BUG_ON(op == CO_SYMLINK);
+		goto out_end;
+	}
+	BUG_ON(op != CO_SYMLINK);
+	err = pack_string(desc, old_name);
+	if (err)
+		goto err_cancel;
+out_end:
+	err = global_config_op_end(desc, nodes);
+out:
+	return err;
+
+err_cancel:
+	rpc_cancel(desc);
+	rpc_end(desc, 0);
+	goto out;
+}
+
+static int __global_config_dir_op(krgnodemask_t *nodes, enum config_op op,
+				  const char *name, const char *old_name)
+{
+	struct rpc_desc *desc;
+	int err;
+
+	desc = __global_config_op_begin(nodes, op);
+	if (IS_ERR(desc)) {
+		err = PTR_ERR(desc);
+		goto out;
+	}
+
+	err = do_global_config_dir_op(desc, nodes, op, name, old_name);
+
+out:
+	return err;
+}
+
+/**
+ * Generic function to broadcast a global directory operation
+ * (mkdir, rmdir, symlink, unlink)
+ *
+ * @param op		op code of the operation
+ * @param name		path to the target entry of the operation, from
+ *			configfs scheduler subsystem directory.
+ * @param old_name	target path for a symlink, or NULL
+ *
+ * @return		0 on success, or error
+ */
+static int global_config_dir_op(enum config_op op,
+				const char *name, const char *old_name)
+{
+	struct rpc_desc *desc;
+	krgnodemask_t nodes;
+	int err;
+
+	desc = global_config_op_begin(op, &nodes);
+	if (IS_ERR(desc)) {
+		err = PTR_ERR(desc);
+		goto out;
+	}
+
+	err = do_global_config_dir_op(desc, &nodes, op, name, old_name);
+
+out:
+	return err;
+}
+
+static int handle_global_config_symlink(struct inode *dir,
+					struct dentry *d_child,
+					const char *old_name)
+{
+	struct path old_root;
+	int err;
+
+	/*
+	 * Temporarily change current's root to configfs'one so that configfs
+	 * can retrieve the right target item
+	 */
+	chroot_to_scheduler_subsystem(&old_root);
+	err = vfs_symlink(d_child->d_parent->d_inode, d_child,
+			  old_name);
+	chroot_restore(&old_root);
+
+	return err;
+}
+
+/**
+ * Generic RPC handler for a global directory operation
+ * (mkdir, rmdir, symlink, unlink)
+ * The directory operation is made as if a user did the operation locally.
+ */
+static void handle_global_config_dir_op(struct rpc_desc *desc,
+					void *_msg, size_t size)
+{
+	const struct config_op_message *msg = _msg;
+	char *name;
+	char *old_name = NULL;
+	struct dentry *d_child;
+	int err;
+
+	name = unpack_get_string(desc);
+	if (IS_ERR(name))
+		goto err_name;
+
+	d_child = get_child_dentry(name);
+	if (IS_ERR(d_child)) {
+		err = PTR_ERR(d_child);
+		goto out_pack;
+	}
+
+	switch (msg->op) {
+	case CO_MKDIR:
+		err = vfs_mkdir(d_child->d_parent->d_inode, d_child, 0);
+		break;
+	case CO_RMDIR:
+		err = vfs_rmdir(d_child->d_parent->d_inode, d_child);
+		break;
+	case CO_SYMLINK:
+		old_name = unpack_get_string(desc);
+		if (IS_ERR(old_name))
+			goto err_old_name;
+
+		err = handle_global_config_symlink(d_child->d_parent->d_inode,
+						   d_child,
+						   old_name);
+		break;
+	case CO_UNLINK:
+		err = vfs_unlink(d_child->d_parent->d_inode, d_child);
+		break;
+	default:
+		BUG();
+	}
+
+	put_child_dentry(d_child);
+
+out_pack:
+	rpc_pack_type(desc, err);
+	put_string(old_name);
+	put_string(name);
+out:
+	return;
+
+err_old_name:
+	put_child_dentry(d_child);
+	put_string(name);
+err_name:
+	rpc_cancel(desc);
+	goto out;
+}
+
+static void delayed_drop_work(struct work_struct *work);
+
+/**
+ * Initialize a global_config_item
+ *
+ * @param item		item to initialize
+ * @param ops		operations associated with this item
+ */
+void global_config_item_init(
+	struct global_config_item *item,
+	const struct global_config_drop_operations *ops)
+{
+	INIT_DELAYED_WORK(&item->drop_work, delayed_drop_work);
+	item->drop_ops = ops;
+	item->path = NULL;
+	item->target_path = NULL;
+}
+
+/**
+ * Generic function to prepare a global config mkdir or symlink
+ *
+ * @param parent	item under which the operation is done
+ * @param name		name of the new entry
+ *
+ * @return		a valid pointer or NULL, to be passed to create_end or
+ *			create_error, or error
+ */
+static
+struct string_list_object *create_begin(struct config_item *parent,
+					const char *name)
+{
+	struct string_list_object *list;
+	char *path;
+	int err;
+
+	if (current->flags & PF_KTHREAD)
+		return NULL;
+
+	err = global_lock_try_writelock(0);
+	if (err)
+		goto err_lock;
+
+	path = get_full_path(parent, name);
+	if (!path)
+		goto err_path;
+
+	list = hashed_string_list_lock_hash(global_items_set, path);
+	BUG_ON(!list);
+	if (IS_ERR(list))
+		goto err_list;
+	err = -EAGAIN;
+	if (string_list_is_element(list, path))
+		/* A previous drop is pending. Let it terminate. */
+		goto err_is_element;
+	kfree(path);
+out:
+	return list;
+err_is_element:
+	hashed_string_list_unlock_hash(global_items_set, list);
+err_list:
+	kfree(path);
+err_path:
+	global_lock_unlock(0);
+err_lock:
+	list = ERR_PTR(err);
+	goto out;
+}
+
+static void local_commit(struct global_config_item *item,
+			 const char *path, const char *target_path)
+{
+	/* See smp_wmb() in local_drop() */
+	smp_read_barrier_depends();
+	item->path = path;
+	item->target_path = target_path;
+	list_add_tail(&item->list, &items_head);
+}
+
+/*
+ * Same as create_end() below, except that concurrent operations are kept
+ * disabled.
+ * Caller is responsible for calling __create_end() afterwards.
+ */
+static int __create_commit(enum config_op op,
+			   struct string_list_object *list,
+			   struct config_item *parent,
+			   struct global_config_item *item,
+			   const char *name,
+			   const char *old_name)
+{
+	char *path;
+	int err;
+
+	err = -ENOMEM;
+	path = get_full_path(parent, name);
+	if (!path)
+		goto out;
+
+	if (!list) {
+		local_commit(item, path, old_name);
+		return 0;
+	}
+
+	err = string_list_add_element(list, path);
+	if (err)
+		goto err_list_add;
+
+	err = global_config_dir_op(op, path, old_name);
+	if (err)
+		goto err_dir_op;
+
+	local_commit(item, path, old_name);
+
+out:
+	return err;
+
+err_dir_op:
+	global_config_dir_op(reverse_op(op), path, NULL);
+	string_list_remove_element(list, path);
+err_list_add:
+	kfree(path);
+	goto out;
+}
+
+/*
+ * Last step of a global create. Re-enables concurrent operations.
+ *
+ * @param list		pointer returned by create_begin
+ */
+static void __create_end(struct string_list_object *list)
+{
+	if (list) {
+		hashed_string_list_unlock_hash(global_items_set, list);
+		global_lock_unlock(0);
+	}
+}
+
+/**
+ * Generic function to commit a global config mkdir or symlink
+ *
+ * @param op		op code of the operation
+ * @param list		pointer returned by create_begin
+ * @param parent	item under which the operation is done
+ * @param item		pointer to the global_config_item for the new entry,
+ *			previously initialized with global_config_item_init
+ * @param name		name of the new entry
+ * @param old_name	name of the target of a the symlink or NULL for mkdir
+ *
+ * @return		0 on success, or error
+ */
+static int create_end(enum config_op op,
+		      struct string_list_object *list,
+		      struct config_item *parent,
+		      struct global_config_item *item,
+		      const char *name,
+		      const char *old_name)
+{
+	int err;
+
+	err = __create_commit(op, list, parent, item, name, old_name);
+	__create_end(list);
+
+	return err;
+}
+
+/**
+ * Generic function to cleanup a global config operation if an error occurs
+ * before calling create_end
+ *
+ * @param list		pointer returned by create_begin
+ * @param name		name of the new entry
+ */
+static void create_error(struct string_list_object *list,
+			 const char *name)
+{
+	if (list) {
+		hashed_string_list_unlock_hash(global_items_set, list);
+		global_lock_unlock(0);
+	}
+}
+
+/**
+ * Function that prepares a global mkdir
+ *
+ * @param parent	item under which the operation is done
+ * @param name		name of the new sub-directory
+ *
+ * @return		valid pointer or NULL to be passed to
+ *			global_config_make_item_end or
+ *			global_config_make_item_error, or error
+ */
+struct string_list_object *
+global_config_make_item_begin(struct config_item *parent, const char *name)
+{
+	return create_begin(parent, name);
+}
+
+/*
+ * Same as global_config_make_item_end() below, except that concurrent
+ * operations are kept disabled.
+ * Caller is responsible for calling __global_config_make_item_end() afterwards.
+ */
+int __global_config_make_item_commit(struct string_list_object *list,
+				     struct config_item *parent,
+				     struct global_config_item *item,
+				     const char *name)
+{
+	return __create_commit(CO_MKDIR, list, parent, item, name, NULL);
+}
+
+/*
+ * Last step of a global make_item. Re-enables concurrent operations.
+ *
+ * @param list		pointer returned by global_config_make_item_begin()
+ */
+void __global_config_make_item_end(struct string_list_object *list)
+{
+	__create_end(list);
+}
+
+/**
+ * Commit a global config mkdir
+ *
+ * @param list		pointer returned by global_config_make_item_begin
+ * @param parent	item under which the operation is done
+ * @param item		pointer to the global_config_item for the new dir,
+ *			previously initialized with global_config_item_init
+ * @param name		name of the new dir
+ *
+ * @return		0 on success, or error
+ */
+int global_config_make_item_end(struct string_list_object *list,
+				struct config_item *parent,
+				struct global_config_item *item,
+				const char *name)
+{
+	return create_end(CO_MKDIR, list, parent, item, name, NULL);
+}
+
+/**
+ * Cleanup a global mkdir if an error occurs before calling
+ * global_config_make_item_end
+ *
+ * @param list		pointer returned by global_config_make_item_begin
+ * @param name		name of the new dir
+ */
+void global_config_make_item_error(struct string_list_object *list,
+				   const char *name)
+{
+	create_error(list, name);
+}
+
+/**
+ * Function that prepares a global symlink
+ *
+ * @param parent	item under which the link is created
+ * @param name		name of the new link
+ * @param target	target item of the new link
+ *
+ * @return		valid pointer or NULL to be passed to
+ *			global_config_allow_link_end or
+ *			global_config_allow_link_error, or error
+ */
+struct string_list_object *
+global_config_allow_link_begin(struct config_item *parent,
+				    const char *name,
+				    struct config_item *target)
+{
+	return create_begin(parent, name);
+}
+
+/*
+ * Same as global_config_allow_link_end() below, except that concurrent
+ * operations are kept disabled.
+ * Caller is responsible for calling __global_config_allow_link_end()
+ * afterwards.
+ */
+int __global_config_allow_link_commit(struct string_list_object *list,
+					   struct config_item *parent,
+					   struct global_config_item *item,
+					   const char *name,
+					   struct config_item *target)
+{
+	char *path;
+	int err;
+
+	path = get_full_path(target, NULL);
+	if (!path)
+		return -ENOMEM;
+	err = __create_commit(CO_SYMLINK, list, parent, item, name, path);
+	if (err)
+		put_path(path);
+
+	return err;
+}
+
+/*
+ * Last step of a global allow_link(). Re-enables concurrent operations.
+ *
+ * @param list		pointer returned by global_config_allow_link_begin()
+ */
+void __global_config_allow_link_end(struct string_list_object *list)
+{
+	__create_end(list);
+}
+
+/**
+ * Commit a global config symlink
+ *
+ * @param list		pointer returned by global_config_allow_link_begin
+ * @param parent	item under which the new link is created
+ * @param item		pointer to the global_config_item for the new link,
+ *			previously initialized with global_config_item_init
+ * @param name		name of the new link
+ * @param target	target item of the new link
+ *
+ * @return		0 on success, or error
+ */
+int global_config_allow_link_end(struct string_list_object *list,
+				      struct config_item *parent,
+				      struct global_config_item *item,
+				      const char *name,
+				      struct config_item *target)
+{
+	int err;
+
+	err = __global_config_allow_link_commit(list,
+						     parent,
+						     item,
+						     name,
+						     target);
+	__global_config_allow_link_end(list);
+
+	return err;
+}
+
+/**
+ * Cleanup a global symlink if an error occurs before calling
+ * global_config_allow_link_end
+ *
+ * @param list		pointer returned by global_config_allow_link_begin
+ * @param name		name of the new link
+ * @param target	target item of the new link
+ */
+void global_config_allow_link_error(struct string_list_object *list,
+					 const char *name,
+					 struct config_item *target)
+{
+	create_error(list, name);
+}
+
+/*
+ * Common handling of global rmdir and unlink
+ *
+ * Since rmdir and unlink cannot fail once configfs drop_item or drop_link
+ * callbacks are called, we must repeatingly defer the global operation until we
+ * manage to get the global lock.
+ */
+
+static struct timespec drop_delay = {
+	.tv_sec = 1,
+	.tv_nsec = 0
+};
+
+static void delay_drop(struct global_config_item *item)
+{
+	queue_delayed_work(krg_wq, &item->drop_work,
+			   timespec_to_jiffies(&drop_delay));
+}
+
+static void local_drop(struct global_config_item *item)
+{
+	const char *path = item->path;
+
+	list_del(&item->list);
+
+	put_path(item->target_path);
+	item->path = NULL;
+	/*
+	 * Ensure that all conditions in item's create function that may become
+	 * true after drop function see that assignment before setting another
+	 * path (in __create_commit())
+	 * This is needed when another node re-creates the item after having
+	 * deleted it, because the local node takes no lock in both cases.
+	 */
+	smp_wmb();
+	put_path(path);
+
+	item->drop_ops->drop_func(item);
+}
+
+static void global_drop(struct global_config_item *item)
+{
+	const struct global_config_drop_operations *drop_ops = item->drop_ops;
+	const char *name = item->path;
+	struct string_list_object *list;
+	int err;
+
+	err = global_lock_try_writelock(0);
+	if (err) {
+		delay_drop(item);
+		return;
+	}
+
+	list = hashed_string_list_lock_hash(global_items_set, name);
+	BUG_ON(!list);
+	if (IS_ERR(list))
+		goto err_list;
+	string_list_remove_element(list, name);
+
+	if (drop_ops->is_symlink)
+		global_config_dir_op(CO_UNLINK, name, NULL);
+	else
+		global_config_dir_op(CO_RMDIR, name, NULL);
+
+	local_drop(item);
+
+	hashed_string_list_unlock_hash(global_items_set, list);
+
+out:
+	global_lock_unlock(0);
+	return;
+
+err_list:
+	delay_drop(item);
+	goto out;
+}
+
+static void delayed_drop_work(struct work_struct *work)
+{
+	struct global_config_item *item =
+		container_of(work,
+			     struct global_config_item,
+			     drop_work.work);
+
+	global_drop(item);
+}
+
+/**
+ * Notify a global rmdir or unlink. The drop may be delayed, so the item should
+ * not be freed before the drop callback is called.
+ *
+ * @param item		global_config_item used for the dropped entry
+ */
+void global_config_drop(struct global_config_item *item)
+{
+	if (!(current->flags & PF_KTHREAD))
+		global_drop(item);
+	else
+		local_drop(item);
+}
+
+static void global_config_attrs_init(struct global_config_attrs *attrs)
+{
+	INIT_LIST_HEAD(&attrs->head);
+	attrs->valid = 1;
+}
+
+static void global_config_attrs_cleanup(struct global_config_attrs *attrs)
+{
+	struct global_config_attr *attr, *tmp;
+
+	down_read(&attrs_rwsem);
+
+	spin_lock(&attrs_lock);
+	attrs->valid = 0;
+	spin_unlock(&attrs_lock);
+
+	list_for_each_entry_safe(attr, tmp, &attrs->head, list) {
+		list_del(&attr->list);
+		list_del(&attr->global_list);
+		kfree(attr->value);
+		kfree(attr);
+	}
+
+	up_read(&attrs_rwsem);
+}
+
+static
+inline
+struct global_config_item_operations *
+to_global_config_item_ops(struct configfs_item_operations *ops)
+{
+	return container_of(ops, struct global_config_item_operations, config);
+}
+
+void global_config_attrs_init_r(struct config_group *group)
+{
+	struct config_item *item = &group->cg_item;
+	struct global_config_item_operations *ops;
+	struct config_group **pos;
+	int i;
+
+	pos = group->default_groups;
+	if (pos)
+		for (; *pos; pos++)
+			global_config_attrs_init_r(*pos);
+
+	for (i = 0; i < ARRAY_SIZE(global_item_ops); i++) {
+		ops = global_item_ops[i];
+		if (item->ci_type->ct_item_ops == &ops->config)
+			global_config_attrs_init(ops->global_attrs(item));
+	}
+}
+
+void global_config_attrs_cleanup_r(struct config_group *group)
+{
+	struct config_item *item = &group->cg_item;
+	struct global_config_item_operations *ops;
+	struct config_group **pos;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(global_item_ops); i++) {
+		ops = global_item_ops[i];
+		if (item->ci_type->ct_item_ops == &ops->config)
+			global_config_attrs_cleanup(ops->global_attrs(item));
+	}
+
+	pos = group->default_groups;
+	if (pos)
+		for (; *pos; pos++)
+			global_config_attrs_cleanup_r(*pos);
+}
+
+/**
+ * Prepare a global store operation on an attribute.
+ *
+ * @param item		item owning the attribute
+ *
+ * @return		a valid pointer or NULL to be passed to
+ *			global_config_attr_store_end or
+ *			global_config_attr_store_error, or error
+ */
+struct string_list_object *
+global_config_attr_store_begin(struct config_item *item)
+{
+	struct string_list_object *list;
+	char *path;
+
+	if (current->flags & PF_KTHREAD)
+		return NULL;
+
+	path = get_full_path(item, NULL);
+	if (!path)
+		return ERR_PTR(-ENOMEM);
+
+	down_read(&attrs_rwsem);
+
+	list = hashed_string_list_lock_hash(global_items_set, path);
+	BUG_ON(!list);
+	put_path(path);
+	if (IS_ERR(list))
+		up_read(&attrs_rwsem);
+
+	return list;
+}
+
+static ssize_t attr_store_record(struct config_item *item,
+				 struct configfs_attribute *attr,
+				 const char *page, size_t count)
+{
+	struct global_config_attrs *attrs;
+	struct global_config_attr *a;
+	void *value;
+	int err;
+
+	value = kmalloc(count, GFP_KERNEL);
+	if (!value)
+		return -ENOMEM;
+	memcpy(value, page, count);
+
+	attrs = to_global_config_item_ops(item->ci_type->ct_item_ops)->global_attrs(item);
+
+	err = -ENOENT;
+	spin_lock(&attrs_lock);
+	if (!attrs->valid)
+		goto out_unlock;
+
+	list_for_each_entry(a, &attrs->head, list) {
+		if (a->attr == attr) {
+			kfree(a->value);
+			a->value = value;
+			a->size = count;
+			list_move_tail(&a->global_list, &attrs_head);
+			err = 0;
+			goto out_unlock;
+		}
+	}
+	spin_unlock(&attrs_lock);
+
+	err = -ENOMEM;
+	a = kmalloc(sizeof(*a), GFP_KERNEL);
+	if (!a)
+		goto out;
+	a->item = item;
+	a->attr = attr;
+	a->value = value;
+	a->size = count;
+
+	err = -ENOENT;
+	spin_lock(&attrs_lock);
+	if (attrs->valid) {
+		list_add(&a->list, &attrs->head);
+		list_add_tail(&a->global_list, &attrs_head);
+		err = 0;
+	} else {
+		kfree(a);
+	}
+out_unlock:
+	spin_unlock(&attrs_lock);
+
+out:
+	if (err)
+		kfree(value);
+
+	return err ? : count;
+}
+
+/**
+ * Commit a global store on an attribute
+ *
+ * @param list		pointer returned by global_config_attr_store_begin
+ * @param item		item owning the attribute
+ * @param attr		attribute to modify
+ * @param page		buffer containing the value to store
+ * @param count		number of bytes to store, as can really be stored
+ *			(result from the local store operation for instance).
+ *
+ * @return		number of bytes written on success, or error
+ */
+ssize_t global_config_attr_store_end(struct string_list_object *list,
+				     struct config_item *item,
+				     struct configfs_attribute *attr,
+				     const char *page, size_t count)
+{
+	ssize_t err = 0;
+
+	if (!list)
+		return attr_store_record(item, attr, page, count);
+
+	if (!count)
+		goto out_unlock;
+
+	err = global_config_write(item, attr, page, count);
+	if (!err)
+		err = count;
+
+out_unlock:
+	if (err >= 0)
+		err = attr_store_record(item, attr, page, count);
+
+	hashed_string_list_unlock_hash(global_items_set, list);
+	up_read(&attrs_rwsem);
+
+	return err;
+}
+
+/**
+ * Cleanup a global attribute store if an error occurs before calling
+ * global_config_attr_store_end
+ *
+ * @param list		pointer returned by global_config_attr_store_begin
+ * @param item		item owning the attribute
+ */
+void global_config_attr_store_error(struct string_list_object *list,
+				    struct config_item *item)
+{
+	if (list) {
+		hashed_string_list_unlock_hash(global_items_set, list);
+		up_read(&attrs_rwsem);
+	}
+}
+
+int global_config_pack_item(struct rpc_desc *desc, struct config_item *item)
+{
+	char *path = get_full_path(item, NULL);
+	int err;
+
+	if (!path)
+		return -ENOMEM;
+	err = pack_string(desc, path);
+	put_path(path);
+
+	return err;
+}
+
+static struct config_item *get_item(const char *path)
+{
+	struct config_group *parent;
+	struct config_item *child;
+	char *__path;
+	char *parent_root, *next_root;
+
+	child = &krg_scheduler_subsys.su_group.cg_item;
+	/* Get rid of special case "/" */
+	BUG_ON(!path[0]);
+	if (!path[1])
+		goto out;
+
+	__path = kstrdup(path, GFP_KERNEL);
+	if (!__path)
+		return ERR_PTR(-ENOMEM);
+
+	/*
+	 * The algorithm to walk the tree is not safe for a general case
+	 * configfs tree, but it is safe with krg_scheduler subtree since all
+	 * directories are config_groups.
+	 */
+
+	parent_root = __path;
+
+	mutex_lock(&krg_scheduler_subsys.su_mutex);
+	do {
+		parent = to_config_group(child);
+		next_root = strchr(parent_root + 1, '/');
+		if (next_root)
+			*next_root = '\0';
+		child = config_group_find_item(parent, parent_root + 1);
+		if (!child)
+			break;
+		parent_root = next_root;
+	} while (parent_root);
+	if (child)
+		config_item_get(child);
+	mutex_unlock(&krg_scheduler_subsys.su_mutex);
+
+	kfree(__path);
+	if (!child)
+		return ERR_PTR(-ENOENT);
+out:
+	return child;
+}
+
+struct config_item *global_config_unpack_get_item(struct rpc_desc *desc)
+{
+	struct config_item *item;
+	char *path = unpack_get_string(desc);
+
+	if (IS_ERR(path))
+		return (struct config_item *) path;
+	item = get_item(path);
+	put_string(path);
+
+	return item;
+}
+
+#ifdef CONFIG_KRG_EPM
+
+int export_global_config_item(struct epm_action *action, ghost_t *ghost,
+			      struct config_item *item)
+{
+	char *path = get_full_path(item, NULL);
+	size_t len = strlen(path);
+	int err;
+
+	if (!path)
+		return -ENOMEM;
+	err = ghost_write(ghost, &len, sizeof(len));
+	if (err)
+		goto put;
+	err = ghost_write(ghost, path, len + 1);
+	if (err)
+		goto put;
+put:
+	put_path(path);
+
+	return err;
+}
+
+int import_global_config_item(struct epm_action *action, ghost_t *ghost,
+			      struct config_item **item_p)
+{
+	struct config_item *item;
+	char *path;
+	size_t len;
+	int err;
+
+	err = ghost_read(ghost, &len, sizeof(len));
+	if (err)
+		goto out;
+	err = -ENOMEM;
+	path = kmalloc(len + 1, GFP_KERNEL);
+	if (!path)
+		goto out;
+	err = ghost_read(ghost, path, len + 1);
+	if (err)
+		goto out_free;
+
+	item = get_item(path);
+	/*
+	 * Do not set err if item is error, so that caller can distinguish ghost
+	 * error from item lookup error
+	 */
+	*item_p = item;
+
+out_free:
+	kfree(path);
+out:
+	return err;
+}
+
+#endif /* CONFIG_KRG_EPM */
+
+static int replicate_config(kerrighed_node_t node)
+{
+	krgnodemask_t nodes = krgnodemask_of_node(node);
+	struct global_config_item *item;
+	struct global_config_attr *attr;
+	enum config_op op;
+	int err = 0;
+
+	list_for_each_entry(item, &items_head, list) {
+		if (item->drop_ops->is_symlink)
+			op = CO_SYMLINK;
+		else
+			op = CO_MKDIR;
+		err = __global_config_dir_op(&nodes, op, item->path, item->target_path);
+		if (err)
+			goto cleanup;
+	}
+
+	list_for_each_entry(attr, &attrs_head, global_list) {
+		err = __global_config_write(&nodes,
+					    attr->item, attr->attr,
+					    attr->value, attr->size);
+		if (err)
+			goto cleanup;
+	}
+
+out:
+	return err;
+
+cleanup:
+	/* Do our best to cleanup */
+	list_for_each_entry_continue_reverse(item, &items_head, list) {
+		if (item->drop_ops->is_symlink)
+			op = CO_UNLINK;
+		else
+			op = CO_RMDIR;
+		__global_config_dir_op(&nodes, op, item->path, NULL);
+	}
+	goto out;
+}
+
+int global_config_add(struct hotplug_context *ctx)
+{
+	krgnodemask_t nodes;
+	kerrighed_node_t node, master;
+	int err, err2 = 0;
+
+	krgnodes_or(nodes, ctx->node_set.v, krgnode_online_map);
+	master = first_krgnode(nodes);
+
+	if (master == kerrighed_node_id) {
+		err = global_config_freeze();
+		if (err)
+			goto out;
+	}
+
+	down_write(&attrs_rwsem);
+
+	rpc_enable(GLOBAL_CONFIG_OP);
+
+	err = cluster_barrier(global_config_barrier, &nodes, master);
+	if (err)
+		goto out_check;
+
+	/* There is no config to replicate at cluster start. */
+	if (first_krgnode(krgnode_online_map) == kerrighed_node_id) {
+		BUG_ON(krgnode_isset(kerrighed_node_id, ctx->node_set.v));
+		for_each_krgnode_mask(node, ctx->node_set.v) {
+			err = replicate_config(node);
+			if (err)
+				break;
+		}
+	}
+
+	err2 = cluster_barrier(global_config_barrier, &nodes, master);
+
+out_check:
+	err = err ? : err2;
+	if (err) {
+		if (krgnode_isset(kerrighed_node_id, ctx->node_set.v))
+			rpc_disable(GLOBAL_CONFIG_OP);
+		up_write(&attrs_rwsem);
+		if (master == kerrighed_node_id)
+			global_config_thaw();
+	}
+out:
+	return err;
+}
+
+int global_config_post_add(struct hotplug_context *ctx)
+{
+	BUG_ON(!krgnodes_subset(ctx->node_set.v, krgnode_online_map));
+	up_write(&attrs_rwsem);
+	if (first_krgnode(krgnode_online_map) == kerrighed_node_id)
+		global_config_thaw();
+
+	return 0;
+}
+
+/**
+ * Initialize the global config subsystem
+ */
+int global_config_start(void)
+{
+	struct file_system_type *fs_type;
+	int err;
+
+	/* retrieve a vfsmount structure for configfs */
+	fs_type = get_fs_type("configfs");
+	if (!fs_type)
+		return -ENODEV;
+	err = simple_pin_fs(fs_type, &scheduler_fs_mount, &mount_count);
+	if (err)
+		goto err_pin_fs;
+
+	global_items_set = hashed_string_list_create(GLOBAL_CONFIG_KDDM_SET_ID);
+	if (IS_ERR(global_items_set)) {
+		err = PTR_ERR(global_items_set);
+		goto err_set;
+	}
+
+	global_config_barrier = alloc_cluster_barrier(SCHED_HOTPLUG_BARRIER);
+	if (IS_ERR(global_config_barrier)) {
+		err = PTR_ERR(global_config_barrier);
+		goto err_barrier;
+	}
+
+	err = rpc_register_void(GLOBAL_CONFIG_OP, handle_global_config_op, 0);
+	if (err)
+		goto err_rpc;
+
+out:
+	return err;
+
+err_rpc:
+
+err_barrier:
+
+err_set:
+
+	simple_release_fs(&scheduler_fs_mount, &mount_count);
+err_pin_fs:
+	goto out;
+}
+
+/**
+ * Cleanup the global config subsystem.
+ * Difficult to write indeed ...
+ */
+void global_config_exit(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/scheduler/global_lock.c android_cluster/linux-2.6.29/kerrighed/scheduler/global_lock.c
--- linux-2.6.29/kerrighed/scheduler/global_lock.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/global_lock.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,132 @@
+/*
+ *  kerrighed/scheduler/global_lock.c
+ *
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ */
+
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <kddm/kddm.h>
+#include <asm/system.h>
+
+static struct kddm_set *lock_set;
+
+#define ZERO_SIZE_LOCK_OBJECT	((void *) 0xe5e5e5e5)
+
+/* Avoid using memory for 0-sized objects */
+static int global_lock_alloc_object(struct kddm_obj *obj_entry,
+				    struct kddm_set *set,
+				    objid_t objid)
+{
+	obj_entry->object = ZERO_SIZE_LOCK_OBJECT;
+	return 0;
+}
+
+/* Avoid a useless rpc_pack() ... */
+static int global_lock_export_object(struct rpc_desc *desc,
+				     struct kddm_set *set,
+				     struct kddm_obj *obj_entry,
+				     objid_t objid,
+				     int flags)
+{
+	return 0;
+}
+
+/* ... and its useless rpc_unpack() counterpart */
+static int global_lock_import_object(struct rpc_desc *desc,
+				     struct kddm_set *set,
+				     struct kddm_obj *obj_entry,
+				     objid_t objid,
+				     int flags)
+{
+	return 0;
+}
+
+/* Do not try kfree(ZERO_SIZE_LOCK_OBJECT) */
+static int global_lock_remove_object(void *object,
+				     struct kddm_set *set,
+				     objid_t objid)
+{
+	return 0;
+}
+
+static struct iolinker_struct global_lock_io_linker = {
+	.linker_name   = "global lock",
+	.linker_id     = GLOBAL_LOCK_LINKER,
+	.alloc_object  = global_lock_alloc_object,
+	.export_object = global_lock_export_object,
+	.import_object = global_lock_import_object,
+	.remove_object = global_lock_remove_object
+};
+
+int global_lock_try_writelock(unsigned long lock_id)
+{
+	void *ret = _kddm_try_grab_object(lock_set, lock_id);
+	int retval;
+
+	if (likely(ret == ZERO_SIZE_LOCK_OBJECT))
+		retval = 0;
+	else if (likely(ret == ERR_PTR(-EBUSY)))
+		retval = -EAGAIN;
+	else if (!ret)
+		retval = -ENOMEM;
+	else
+		retval = PTR_ERR(ret);
+
+	return retval;
+}
+
+int global_lock_writelock(unsigned long lock_id)
+{
+	void *ret = _kddm_grab_object(lock_set, lock_id);
+	int retval;
+
+	if (likely(ret == ZERO_SIZE_LOCK_OBJECT))
+		retval = 0;
+	else if (!ret)
+		retval = -ENOMEM;
+	else
+		retval = PTR_ERR(ret);
+
+	return retval;
+}
+
+int global_lock_readlock(unsigned long lock_id)
+{
+	void *ret = _kddm_get_object(lock_set, lock_id);
+	int retval;
+
+	if (likely(ret == ZERO_SIZE_LOCK_OBJECT))
+		retval = 0;
+	else if (!ret)
+		retval = -ENOMEM;
+	else
+		retval = PTR_ERR(ret);
+
+	return retval;
+}
+
+void global_lock_unlock(unsigned long lock_id)
+{
+	_kddm_put_object(lock_set, lock_id);
+}
+
+int global_lock_start(void)
+{
+	register_io_linker(GLOBAL_LOCK_LINKER, &global_lock_io_linker);
+
+	lock_set = create_new_kddm_set(kddm_def_ns, GLOBAL_LOCK_KDDM_SET_ID,
+				       GLOBAL_LOCK_LINKER,
+				       KDDM_RR_DEF_OWNER,
+				       0, KDDM_LOCAL_EXCLUSIVE);
+	BUG_ON(!lock_set);
+	if (IS_ERR(lock_set))
+		return PTR_ERR(lock_set);
+
+	return 0;
+}
+
+void global_lock_exit(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/scheduler/global_lock.h android_cluster/linux-2.6.29/kerrighed/scheduler/global_lock.h
--- linux-2.6.29/kerrighed/scheduler/global_lock.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/global_lock.h	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,9 @@
+#ifndef __GLOBAL_LOCK_H__
+#define __GLOBAL_LOCK_H__
+
+int global_lock_try_writelock(unsigned long lock_id);
+int global_lock_writelock(unsigned long lock_id);
+int global_lock_readlock(unsigned long lock_id);
+void global_lock_unlock(unsigned long lock_id);
+
+#endif /* __GLOBAL_LOCK_H__ */
diff -ruN linux-2.6.29/kerrighed/scheduler/hashed_string_list.c android_cluster/linux-2.6.29/kerrighed/scheduler/hashed_string_list.c
--- linux-2.6.29/kerrighed/scheduler/hashed_string_list.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/hashed_string_list.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,54 @@
+/*
+ *  kerrighed/scheduler/hashed_string_list.c
+ *
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ */
+
+#include <linux/string.h>
+#include <kddm/kddm.h>
+
+#include "string_list.h"
+
+struct kddm_set *hashed_string_list_create(kddm_set_id_t kddm_set_id)
+{
+	return create_new_kddm_set(kddm_def_ns, kddm_set_id,
+				   STRING_LIST_LINKER,
+				   KDDM_RR_DEF_OWNER,
+				   0,
+				   KDDM_LOCAL_EXCLUSIVE);
+}
+
+static unsigned long get_hash(const char *string)
+{
+	unsigned long hash = 0;
+	const char *limit = string + strlen(string) - sizeof(hash);
+	const unsigned long *pos;
+
+	for (pos = (const unsigned long *) string; (char *) pos <= limit; pos++)
+		hash = hash ^ *pos;
+
+	if ((char *) (pos - 1) < limit) {
+		unsigned long last_hash = 0;
+
+		strcpy((char *) &last_hash, (const char *) pos);
+		hash = hash ^ last_hash;
+	}
+
+	return hash;
+}
+
+struct string_list_object *
+hashed_string_list_lock_hash(struct kddm_set *kddm_set, const char *element)
+{
+	return string_list_create_writelock(kddm_set, get_hash(element));
+}
+
+void hashed_string_list_unlock_hash(struct kddm_set *kddm_set,
+				    struct string_list_object *string_list)
+{
+	if (string_list_empty(string_list))
+		string_list_unlock_and_destroy(kddm_set, string_list);
+	else
+		string_list_unlock(kddm_set, string_list);
+}
diff -ruN linux-2.6.29/kerrighed/scheduler/hashed_string_list.h android_cluster/linux-2.6.29/kerrighed/scheduler/hashed_string_list.h
--- linux-2.6.29/kerrighed/scheduler/hashed_string_list.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/hashed_string_list.h	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,16 @@
+#ifndef __HASHED_STRING_LIST_H__
+#define __HASHED_STRING_LIST_H__
+
+#include <kddm/kddm_types.h>
+
+struct kddm_set;
+struct string_list_object;
+
+struct kddm_set *hashed_string_list_create(kddm_set_id_t kddm_set_id);
+
+struct string_list_object *
+hashed_string_list_lock_hash(struct kddm_set *kddm_set, const char *element);
+void hashed_string_list_unlock_hash(struct kddm_set *kddm_set,
+				    struct string_list_object *string_list);
+
+#endif /* __HASHED_STRING_LIST_H__ */
diff -ruN linux-2.6.29/kerrighed/scheduler/info.c android_cluster/linux-2.6.29/kerrighed/scheduler/info.c
--- linux-2.6.29/kerrighed/scheduler/info.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/info.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,610 @@
+/*
+ *  kerrighed/scheduler/info.c
+ *
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/nsproxy.h>
+#include <linux/slab.h>
+#include <linux/rcupdate.h>
+#include <linux/mutex.h>
+#include <linux/rculist.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/err.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/ghost.h>
+#endif
+#include <kerrighed/scheduler/info.h>
+
+struct krg_sched_info {
+	struct list_head modules;
+	struct list_head list;
+	struct task_struct *task;
+	struct rcu_head rcu;
+};
+
+static LIST_HEAD(sched_info_head);
+static DEFINE_MUTEX(sched_info_list_mutex);
+
+static struct kmem_cache *sched_info_cachep;
+
+static LIST_HEAD(modules);
+static DEFINE_SPINLOCK(modules_lock);
+static u64 version;
+
+static
+struct krg_sched_module_info_type *module_info_type_get(const char *name)
+{
+	struct krg_sched_module_info_type *type;
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(type, &modules, list)
+		if (!strcmp(type->name, name)) {
+			if (!try_module_get(type->owner))
+				type = NULL;
+			rcu_read_unlock();
+			return type;
+		}
+	rcu_read_unlock();
+	return NULL;
+}
+
+static void __add_module_info(struct krg_sched_info *info,
+			      struct krg_sched_module_info *mod_info,
+			      struct list_head *next,
+			      struct krg_sched_module_info_type *type)
+{
+	mod_info->type = type;
+	list_add(&mod_info->instance_list, &type->instance_head);
+	list_add_tail_rcu(&mod_info->info_list, next);
+}
+
+static void add_missing_mod_info(struct krg_sched_info *info,
+				 struct krg_sched_module_info_type *type)
+{
+	struct krg_sched_module_info *mod_info, *new_mod_info;
+	struct list_head *at;
+	int ret;
+
+	/* Check that no mod_info of this type already exists */
+	ret = -1;
+	rcu_read_lock();
+	list_for_each_entry_rcu(mod_info, &info->modules, info_list) {
+		ret = strcmp(type->name, mod_info->type->name);
+		if (ret <= 0)
+			break;
+	}
+	rcu_read_unlock();
+	if (ret == 0)
+		return;
+
+	/* Create one and insert it in lexicographical order */
+	new_mod_info = type->copy(info->task, NULL);
+	if (new_mod_info) {
+		at = &info->modules;
+		spin_lock_irq(&modules_lock);
+		list_for_each_entry(mod_info, &info->modules, info_list)
+			if (strcmp(type->name, mod_info->type->name) < 0) {
+				at = &mod_info->info_list;
+				break;
+			}
+		__add_module_info(info, new_mod_info, at, type);
+		spin_unlock_irq(&modules_lock);
+	}
+}
+
+int krg_sched_module_info_register(struct krg_sched_module_info_type *type)
+{
+	struct list_head *where;
+	struct krg_sched_module_info_type *pos;
+	struct krg_sched_info *info;
+	unsigned long flags;
+
+	if (!strlen(type->name))
+		return -EINVAL;
+
+	INIT_LIST_HEAD(&type->instance_head);
+
+	mutex_lock(&sched_info_list_mutex);
+	spin_lock_irqsave(&modules_lock, flags);
+	/*
+	 * Lexicographically sort the list so that import_krg_sched_info does
+	 * not need to do a complex sort of imported module infos
+	 */
+	where = &modules;
+	list_for_each_entry(pos, &modules, list) {
+		if (strcmp(pos->name, type->name) > 0)
+			where = &pos->list;
+	}
+	list_add_tail_rcu(&type->list, where);
+	spin_unlock_irqrestore(&modules_lock, flags);
+
+	/*
+	 * Matches smp_read_barrier_depends() in krg_sched_info_copy(), which
+	 * parses the modules list with RCU lock only.
+	 * list_add_tail_rcu() does not ensure that the new module is seen in
+	 * the list before version is incremented.
+	 * Ensures that version is seen incremented after the module is
+	 * registered in the list.
+	 */
+	smp_wmb();
+	version++;
+
+	list_for_each_entry(info, &sched_info_head, list)
+		add_missing_mod_info(info, type);
+	mutex_unlock(&sched_info_list_mutex);
+
+	return 0;
+}
+EXPORT_SYMBOL(krg_sched_module_info_register);
+
+/*
+ * must only be called at module unloading (See comment in
+ * krg_sched_info_copy)
+ */
+void krg_sched_module_info_unregister(struct krg_sched_module_info_type *type)
+{
+	struct krg_sched_module_info *mod_info, *tmp;
+	unsigned long flags;
+
+	spin_lock_irqsave(&modules_lock, flags);
+	list_del_rcu(&type->list);
+
+	/* Remove all existing module_info of this type from their task */
+	list_for_each_entry(mod_info, &type->instance_head, instance_list)
+		list_del_rcu(&mod_info->info_list);
+	spin_unlock_irqrestore(&modules_lock, flags);
+
+	synchronize_rcu();
+	/*
+	 * Now nobody can be using any of the module_info of this type, nor can
+	 * add a new one.
+	 */
+
+	list_for_each_entry_safe(mod_info, tmp,
+				 &type->instance_head, instance_list) {
+		list_del(&mod_info->instance_list);
+		type->free(mod_info);
+	}
+}
+EXPORT_SYMBOL(krg_sched_module_info_unregister);
+
+/* Must be called under rcu_read_lock() */
+struct krg_sched_module_info *
+krg_sched_module_info_get(struct task_struct *task,
+			  struct krg_sched_module_info_type *type)
+{
+	struct krg_sched_info *info;
+	struct krg_sched_module_info *mod_info;
+
+	info = rcu_dereference(task->krg_sched);
+	if (!info)
+		return NULL;
+	list_for_each_entry_rcu(mod_info, &info->modules, info_list)
+		if (mod_info->type == type)
+			return mod_info;
+	return NULL;
+}
+EXPORT_SYMBOL(krg_sched_module_info_get);
+
+static struct krg_sched_info *alloc_sched_info(struct task_struct *task,
+					       int gfp_flags)
+{
+	struct krg_sched_info *info;
+
+	info = kmem_cache_alloc(sched_info_cachep, gfp_flags);
+	if (info) {
+		INIT_LIST_HEAD(&info->modules);
+		info->task = task;
+	}
+	return info;
+}
+
+/* Must be called under rcu_read_lock() */
+/*
+ * We are not interested in mod_infos from unloading modules, and the mod_info
+ * returned must survive a temporary release of RCU read lock, so we grab a
+ * reference on the returned mod_info's module.
+ * Caller must release this reference after the last call to next_mod_info().
+ */
+static struct krg_sched_module_info *
+next_mod_info(struct krg_sched_info *info,
+	      struct krg_sched_module_info *mod_info)
+{
+	struct krg_sched_module_info *next_info;
+	struct list_head *pos;
+
+	if (!info)
+		return NULL;
+
+	if (mod_info) {
+		/*
+		 * Accessing mod_info remains safe as long as we hold RCU read
+		 * lock.
+		 */
+		module_put(mod_info->type->owner);
+		pos = &mod_info->info_list;
+	} else {
+		pos = &info->modules;
+	}
+
+	list_for_each_continue_rcu(pos, &info->modules) {
+		next_info = list_entry(pos,
+				       struct krg_sched_module_info,
+				       info_list);
+		if (try_module_get(next_info->type->owner))
+			return next_info;
+	}
+
+	return NULL;
+}
+
+static void add_module_info(struct krg_sched_info *info,
+			    struct krg_sched_module_info *mod_info,
+			    struct krg_sched_module_info_type *type)
+{
+	spin_lock_irq(&modules_lock);
+	__add_module_info(info, mod_info, &info->modules, type);
+	spin_unlock_irq(&modules_lock);
+}
+
+/*
+ * Algorithm similar to krg_sched_info_copy()'s normal path, with only new
+ * module infos.
+ */
+static void add_missing_mod_infos(struct krg_sched_info *info)
+{
+	struct task_struct *task;
+	struct list_head *pos;
+	struct krg_sched_module_info_type *type;
+	struct krg_sched_module_info *mod_info, *new_mod_info;
+
+	task = info->task;
+	rcu_read_lock();
+	mod_info = next_mod_info(info, NULL);
+	for (pos = rcu_dereference(modules.next);
+	     pos != &modules;
+	     pos = rcu_dereference(pos->next)) {
+		type = list_entry(pos, struct krg_sched_module_info_type, list);
+		if (!try_module_get(type->owner))
+			continue;
+		rcu_read_unlock();
+
+		if (mod_info && mod_info->type == type) {
+			rcu_read_lock();
+			mod_info = next_mod_info(info, mod_info);
+		} else {
+			new_mod_info = type->copy(task, NULL);
+			if (new_mod_info) {
+				struct list_head *at = &info->modules;
+				if (mod_info)
+					at = &mod_info->info_list;
+				spin_lock_irq(&modules_lock);
+				__add_module_info(info, new_mod_info, at, type);
+				spin_unlock_irq(&modules_lock);
+			}
+			rcu_read_lock();
+		}
+
+		module_put(type->owner);
+	}
+	rcu_read_unlock();
+	if (mod_info)
+		module_put(mod_info->type->owner);
+
+}
+
+static void complete_and_commit_sched_info(struct krg_sched_info *info,
+					   u64 start_version)
+{
+	mutex_lock(&sched_info_list_mutex);
+	if (unlikely(start_version < version))
+		add_missing_mod_infos(info);
+	list_add(&info->list, &sched_info_head);
+	mutex_unlock(&sched_info_list_mutex);
+
+	rcu_assign_pointer(info->task->krg_sched, info);
+}
+
+int krg_sched_info_copy(struct task_struct *task)
+{
+	struct krg_sched_info *info;
+	struct krg_sched_info *new_info;
+	struct list_head *pos;
+	struct krg_sched_module_info_type *type;
+	struct krg_sched_module_info *mod_info, *new_mod_info;
+	u64 start_version;
+
+	rcu_assign_pointer(task->krg_sched, NULL);
+
+	if (!task->nsproxy->krg_ns)
+		return 0;
+
+	if (krg_current) {
+		rcu_assign_pointer(task->krg_sched, krg_current->krg_sched);
+		return 0;
+	}
+
+	/* Kernel threads do not need krg_sched_info */
+	/*
+	 * This test is not really clean, since at this stage task->mm points to
+	 * the mm of the caller (parent or sister task), but we only want to
+	 * know if the new task will have an mm or not.
+	 */
+	if (task->flags & PF_KTHREAD)
+		return 0;
+
+	new_info = alloc_sched_info(task, GFP_KERNEL);
+	if (!new_info)
+		return -ENOMEM;
+
+	start_version = version;
+	/*
+	 * Matches smp_wmb() in krg_sched_module_info_register()
+	 * Ensures that version is not seen incremented before the new module
+	 * responsible for it is seen added to the modules list.
+	 */
+	smp_read_barrier_depends();
+
+	/*
+	 * Parse simultaneously the list of registered modules and the list of
+	 * current's modules to copy/create the infos for the new task
+	 *
+	 * Both lists are sorted in the same order of types, and all current's
+	 * modules have their type in the registered modules list. This is
+	 * guaranteed as long as no module calls
+	 * krg_sched_module_info_unregister() outside module
+	 * unloading. Conversely, some types may not have an info for
+	 * current.
+	 */
+
+	/*
+	 * Some modules may appear/disappear and per module internal info may be
+	 * changed or disappear. However a module info can be removed from the
+	 * list but cannot disappear as long as its module is not unloading or
+	 * RCU is locked.
+	 */
+	rcu_read_lock();
+	info = rcu_dereference(task->krg_sched);
+	mod_info = next_mod_info(info, NULL);
+	/*
+	 * We do not use list_for_each(_entry)_rcu because we must avoid any
+	 * prefetch optimization that would load an element that could be freed
+	 * while RCU is unlocked.
+	 */
+	for (pos = rcu_dereference(modules.next);
+	     pos != &modules;
+	     pos = rcu_dereference(pos->next)) {
+		type = list_entry(pos, struct krg_sched_module_info_type, list);
+		if (!try_module_get(type->owner))
+			continue;
+		/*
+		 * Now we are sure that type won't disappear, as long as
+		 * module_type_unregister is not called outside moudule
+		 * unloading.
+		 */
+		rcu_read_unlock();
+
+		if (mod_info && mod_info->type == type) {
+			new_mod_info = type->copy(task, mod_info);
+			rcu_read_lock();
+			mod_info = next_mod_info(info, mod_info);
+		} else {
+			new_mod_info = type->copy(task, NULL);
+			rcu_read_lock();
+		}
+		if (new_mod_info)
+			add_module_info(new_info, new_mod_info, type);
+
+		module_put(type->owner);
+	}
+	rcu_read_unlock();
+	if (mod_info)
+		/*
+		 * next_mod_info() took a reference on the module to survive the
+		 * temporary release of RCU read lock.
+		 */
+		module_put(mod_info->type->owner);
+
+	complete_and_commit_sched_info(new_info, start_version);
+	return 0;
+}
+
+static void free_sched_info(struct krg_sched_info *info)
+{
+	struct krg_sched_module_info *mod_info, *tmp;
+	struct krg_sched_module_info_type *type;
+	unsigned long flags;
+
+	/* Prevent an unloading module type from changing the list */
+	spin_lock_irqsave(&modules_lock, flags);
+	list_for_each_entry_safe(mod_info, tmp,
+				 &info->modules, info_list) {
+		list_del(&mod_info->instance_list);
+		list_del(&mod_info->info_list);
+		type = mod_info->type;
+		type->free(mod_info);
+	}
+	spin_unlock_irqrestore(&modules_lock, flags);
+
+	kmem_cache_free(sched_info_cachep, info);
+}
+
+static void delayed_free_sched_info(struct rcu_head *rhp)
+{
+	struct krg_sched_info *info =
+		container_of(rhp, struct krg_sched_info, rcu);
+	free_sched_info(info);
+}
+
+void krg_sched_info_free(struct task_struct *task)
+{
+	struct krg_sched_info *info = rcu_dereference(task->krg_sched);
+
+#ifdef CONFIG_KRG_EPM
+	if (krg_current)
+		return;
+#endif
+
+	if (!info)
+		return;
+
+	mutex_lock(&sched_info_list_mutex);
+	list_del(&info->list);
+	mutex_unlock(&sched_info_list_mutex);
+	rcu_assign_pointer(task->krg_sched, NULL);
+	call_rcu(&info->rcu, delayed_free_sched_info);
+}
+
+#ifdef CONFIG_KRG_EPM
+
+int export_krg_sched_info(struct epm_action *action, ghost_t *ghost,
+			  struct task_struct *task)
+{
+	struct krg_sched_info *info;
+	struct krg_sched_module_info *mod_info;
+	struct krg_sched_module_info_type *type;
+	struct list_head *mods;
+	struct list_head *pos;
+	size_t type_name_len;
+	int err;
+
+	rcu_read_lock();
+	info = rcu_dereference(task->krg_sched);
+	if (!info)
+		goto end_of_list;
+	/*
+	 * avoid using list_for_each_entry_rcu to avoid prefetching memory that
+	 * may be freed whil RCU is unlocked
+	 */
+	mods = &info->modules;
+	for (pos = rcu_dereference(mods->next);
+	     pos != mods;
+	     pos = rcu_dereference(pos->next)) {
+		mod_info = list_entry(pos, typeof(*mod_info), info_list);
+		type = mod_info->type;
+		if (!try_module_get(type->owner))
+			continue;
+		/*
+		 * Now we are sure that mod_info and type won't disappear, as
+		 * long as module_type_unregister is not called outside moudule
+		 * unloading.
+		 */
+		rcu_read_unlock();
+
+		type_name_len = strlen(type->name);
+		err = ghost_write(ghost, &type_name_len, sizeof(type_name_len));
+		if (err)
+			goto err_module;
+		err = ghost_write(ghost, type->name, type_name_len + 1);
+		if (err)
+			goto err_module;
+		err = type->export(action, ghost, mod_info);
+		if (err)
+			goto err_module;
+
+		rcu_read_lock();
+		module_put(type->owner);
+	}
+end_of_list:
+	rcu_read_unlock();
+
+	/* end-of-list marker */
+	type_name_len = 0;
+	err = ghost_write(ghost, &type_name_len, sizeof(type_name_len));
+out:
+	return err;
+
+err_module:
+	module_put(type->owner);
+	goto out;
+}
+
+int import_krg_sched_info(struct epm_action *action, ghost_t *ghost,
+			  struct task_struct *task)
+{
+	struct krg_sched_info *info;
+	struct krg_sched_module_info_type *type;
+	struct krg_sched_module_info *mod_info;
+	size_t type_name_len;
+	size_t max_type_name_len = 0;
+	char *type_name = NULL;
+	int err;
+
+	info = alloc_sched_info(task, GFP_KERNEL);
+	if (!info)
+		return -ENOMEM;
+
+	while (1) {
+		err = ghost_read(ghost, &type_name_len, sizeof(type_name_len));
+		if (err)
+			break;
+		if (!type_name_len)
+			/* end-of-list marker */
+			break;
+		if (type_name_len > max_type_name_len) {
+			kfree(type_name);
+			type_name = kmalloc(type_name_len + 1, GFP_KERNEL);
+			if (!type_name) {
+				err = -ENOMEM;
+				break;
+			}
+			max_type_name_len = type_name_len;
+		}
+		err = ghost_read(ghost, type_name, type_name_len + 1);
+		if (err)
+			break;
+
+		type = module_info_type_get(type_name);
+		if (!type) {
+			err = -ENODEV;
+			break;
+		}
+
+		mod_info = type->import(action, ghost, task);
+		if (IS_ERR(mod_info)) {
+			err = PTR_ERR(mod_info);
+			module_put(type->owner);
+			break;
+		}
+		if (mod_info)
+			add_module_info(info, mod_info, type);
+		module_put(type->owner);
+	}
+	kfree(type_name);
+
+	if (!err)
+		complete_and_commit_sched_info(info, 0);
+	else
+		free_sched_info(info);
+
+	return err;
+}
+
+void post_import_krg_sched_info(struct task_struct *task)
+{
+	mutex_lock(&sched_info_list_mutex);
+	task->krg_sched->task = task;
+	mutex_unlock(&sched_info_list_mutex);
+}
+
+void unimport_krg_sched_info(struct task_struct *task)
+{
+	krg_sched_info_free(task);
+}
+
+#endif /* CONFIG_KRG_EPM */
+
+int krg_sched_info_start(void)
+{
+	sched_info_cachep = KMEM_CACHE(krg_sched_info, SLAB_PANIC);
+
+	return 0;
+}
+
+void krg_sched_info_exit(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/scheduler/internal.h android_cluster/linux-2.6.29/kerrighed/scheduler/internal.h
--- linux-2.6.29/kerrighed/scheduler/internal.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/internal.h	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,74 @@
+#ifndef __SCHEDULER_INTERNAL_H__
+#define __SCHEDULER_INTERNAL_H__
+
+#include <linux/configfs.h>
+
+struct hotplug_context;
+
+#define PROBES_NAME "probes"
+#define SCHEDULERS_NAME "schedulers"
+
+extern struct configfs_subsystem krg_scheduler_subsys;
+
+struct global_config_attrs;
+
+struct global_config_item_operations {
+	struct configfs_item_operations config;
+	struct global_config_attrs *(*global_attrs)(struct config_item *item);
+};
+
+extern struct global_config_item_operations probe_source_global_item_ops;
+extern struct global_config_item_operations probe_global_item_ops;
+extern struct global_config_item_operations port_global_item_ops;
+extern struct global_config_item_operations policy_global_item_ops;
+extern struct global_config_item_operations process_set_global_item_ops;
+extern struct global_config_item_operations scheduler_global_item_ops;
+
+/**
+ * Checks that item is a probe source subdir of a probe.
+ * @author Louis Rilling, Marko Novak
+ *
+ * @param item		pointer to the config_item to check
+ */
+int is_scheduler_probe_source(struct config_item *item);
+
+struct scheduler_policy;
+struct scheduler_policy *scheduler_policy_new(const char *name);
+void scheduler_policy_drop(struct scheduler_policy *policy);
+
+static inline int nr_def_groups(struct config_group *def_groups[])
+{
+	int n = 0;
+	if (def_groups)
+		while (def_groups[n])
+			n++;
+	return n;
+}
+
+/* Subsystems initializers / cleaners */
+
+int krg_sched_info_start(void);
+void krg_sched_info_exit(void);
+
+int global_lock_start(void);
+void global_lock_exit(void);
+
+int string_list_start(void);
+void string_list_exit(void);
+
+int global_config_start(void);
+void global_config_exit(void);
+int global_config_add(struct hotplug_context *ctx);
+int global_config_post_add(struct hotplug_context *ctx);
+
+int remote_pipe_start(void);
+void remote_pipe_exit(void);
+
+struct config_group *scheduler_probe_start(void);
+void scheduler_probe_exit(void);
+
+struct config_group *scheduler_start(void);
+void scheduler_exit(void);
+int scheduler_post_add(struct hotplug_context *ctx);
+
+#endif /* __SCHEDULER_INTERNAL_H__ */
diff -ruN linux-2.6.29/kerrighed/scheduler/Makefile android_cluster/linux-2.6.29/kerrighed/scheduler/Makefile
--- linux-2.6.29/kerrighed/scheduler/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/Makefile	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,16 @@
+#
+# Kerrighed's configurable scheduler framework (SCHED)
+#
+
+obj-$(CONFIG_KRG_SCHED) := core.o info.o \
+	global_lock.o string_list.o hashed_string_list.o global_config.o \
+	policy.o scheduler.o \
+	pipe.o remote_pipe.o \
+	probe.o port.o filter.o \
+	process_set.o placement.o
+
+obj-$(CONFIG_KRG_SCHED) += probes/
+obj-$(CONFIG_KRG_SCHED) += filters/
+obj-$(CONFIG_KRG_SCHED) += policies/
+
+EXTRA_CFLAGS += -Wall -Werror
diff -ruN linux-2.6.29/kerrighed/scheduler/pipe.c android_cluster/linux-2.6.29/kerrighed/scheduler/pipe.c
--- linux-2.6.29/kerrighed/scheduler/pipe.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/pipe.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,365 @@
+/*
+ *  kerrighed/scheduler/pipe.c
+ *
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/module.h>
+#include <linux/configfs.h>
+#include <linux/stat.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/rcupdate.h>
+#include <linux/rculist.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <kerrighed/scheduler/global_config.h>
+#include <kerrighed/scheduler/pipe.h>
+
+void scheduler_source_init(struct scheduler_source *source,
+			   struct scheduler_source_type *type)
+{
+	source->type = type;
+	INIT_LIST_HEAD(&source->pub_sub_head);
+	spin_lock_init(&source->lock);
+}
+
+void scheduler_sink_init(struct scheduler_sink *sink,
+			 struct scheduler_sink_type *type)
+{
+	sink->type = type;
+	rcu_assign_pointer(sink->source, NULL);
+	INIT_LIST_HEAD(&sink->pub_sub_list);
+	sink->subscribed = 0;
+	scheduler_sink_remote_pipe_init(sink);
+}
+
+void scheduler_sink_cleanup(struct scheduler_sink *sink)
+{
+	scheduler_sink_remote_pipe_cleanup(sink);
+}
+
+int scheduler_source_get_value(struct scheduler_source *source,
+			       void *value_p, unsigned int nr,
+			       const void *in_value_p, unsigned int in_nr)
+{
+	struct scheduler_source_type *type = scheduler_source_type_of(source);
+
+	if (!type->get_value)
+		return -EACCES;
+	if (!nr && !in_nr)
+		return 0;
+	if ((nr && !value_p) || (in_nr && !in_value_p))
+		return -EINVAL;
+
+	return type->get_value(source, value_p, nr, in_value_p, in_nr);
+}
+
+ssize_t scheduler_source_show_value(struct scheduler_source *source, char *page)
+{
+	struct scheduler_source_type *type = scheduler_source_type_of(source);
+	ssize_t ret = -EACCES;
+
+	if (type->show_value)
+		ret = type->show_value(source, page);
+
+	return ret;
+}
+
+/*
+ * TODO: perhaps we could relax the rules for in_types: equal or one of them
+ * NULL.
+ */
+int scheduler_types_compatible(const struct scheduler_sink_type *sink_type,
+			       const struct scheduler_source_type *source_type)
+{
+	const struct get_value_types *sink_types = &sink_type->get_value_types;
+	const struct get_value_types *source_types =
+		&source_type->get_value_types;
+
+	return !(strcmp(sink_types->out_type, source_types->out_type) ||
+		 (sink_types->out_type_size != source_types->out_type_size)
+		 ||
+		 (!sink_types->in_type && source_types->in_type) ||
+		 (sink_types->in_type && !source_types->in_type)
+		 ||
+		 (sink_types->in_type &&
+		  (strcmp(sink_types->in_type, source_types->in_type) ||
+		   (sink_types->in_type_size != source_types->in_type_size))));
+}
+
+void scheduler_sink_connect(struct scheduler_sink *sink,
+			    struct scheduler_source *source,
+			    int subscribe)
+{
+	rcu_assign_pointer(sink->source, source);
+	if (subscribe) {
+		sink->subscribed = 1;
+		scheduler_source_lock(source);
+		list_add_rcu(&sink->pub_sub_list, &source->pub_sub_head);
+		scheduler_source_unlock(source);
+	}
+}
+
+void scheduler_sink_disconnect(struct scheduler_sink *sink)
+{
+	if (sink->subscribed) {
+		struct scheduler_source *source = rcu_dereference(sink->source);
+		scheduler_source_lock(source);
+		list_del_rcu(&sink->pub_sub_list);
+		scheduler_source_unlock(source);
+		sink->subscribed = 0;
+	}
+	rcu_assign_pointer(sink->source, NULL);
+	scheduler_sink_remote_pipe_disconnect(sink);
+}
+
+struct scheduler_source *
+scheduler_sink_get_peer_source(struct scheduler_sink *sink)
+{
+	return rcu_dereference(sink->source);
+}
+
+int scheduler_sink_subscribed(struct scheduler_sink *sink)
+{
+	return sink->subscribed;
+}
+
+int scheduler_source_has_subscribers(struct scheduler_source *source)
+{
+	return !list_empty(&source->pub_sub_head);
+}
+
+void scheduler_source_publish(struct scheduler_source *source)
+{
+	struct scheduler_sink *subscriber;
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(subscriber,
+				&source->pub_sub_head, pub_sub_list)
+		subscriber->type->update_value(subscriber, source);
+	rcu_read_unlock();
+}
+
+int scheduler_sink_get_value(struct scheduler_sink *sink,
+			     void *value_p, unsigned int nr,
+			     const void *in_value_p, unsigned int in_nr)
+{
+	struct scheduler_source *peer_source;
+	int ret = -EACCES;
+
+	rcu_read_lock();
+	peer_source = scheduler_sink_get_peer_source(sink);
+	if (peer_source)
+		ret = scheduler_source_get_value(peer_source,
+						 value_p, nr,
+						 in_value_p, in_nr);
+	rcu_read_unlock();
+
+	return ret;
+}
+/* Used by inline definition of scheduler_port_get_value */
+EXPORT_SYMBOL(scheduler_sink_get_value);
+
+ssize_t scheduler_sink_show_value(struct scheduler_sink *sink, char *page)
+{
+	struct scheduler_source *peer_source;
+	ssize_t ret = -EACCES;
+
+	rcu_read_lock();
+	peer_source = scheduler_sink_get_peer_source(sink);
+	if (peer_source)
+		ret = scheduler_source_show_value(peer_source, page);
+	rcu_read_unlock();
+
+	return ret;
+}
+/* Used by inline definition of scheduler_port_show_value */
+EXPORT_SYMBOL(scheduler_sink_show_value);
+
+/* Complete initialization of the value attribute of the pipe type. */
+static void value_attr_init(struct configfs_attribute *attr,
+			    struct scheduler_pipe_type *type,
+			    const char *name,
+			    int readable)
+{
+	attr->ca_name = name;
+	attr->ca_owner = type->item_type.ct_owner;
+	attr->ca_mode = 0;
+	if (readable)
+		attr->ca_mode |= S_IRUGO;
+}
+
+static int nr_attrs(struct configfs_attribute **attrs)
+{
+	int nr = 0;
+	if (attrs)
+		while (attrs[nr])
+			nr++;
+	return nr;
+}
+
+/* Default pipe attributes are packed right before the attributes array. */
+int scheduler_pipe_type_init(struct scheduler_pipe_type *type,
+			     struct configfs_attribute **attrs)
+{
+	struct configfs_attribute *next_value_attr;
+	struct configfs_attribute **tmp_attrs;
+	struct configfs_attribute **next_attr;
+	int nr_custom_attrs;
+	void *attrs_mem;
+	size_t attrs_size;
+	int err = -ENOMEM;
+
+	nr_custom_attrs = nr_attrs(attrs);
+	attrs_size = (nr_custom_attrs + 1) * sizeof(*tmp_attrs);
+	if (type->source_type) {
+		/* Store the "value" attribute at the beginning */
+		attrs_size += sizeof(struct configfs_attribute);
+		/* One more element in the attributes array */
+		attrs_size += sizeof(*tmp_attrs);
+	}
+	if (type->sink_type) {
+		/*
+		 * Store the "collected_value" attribute right after the "value"
+		 * attribute
+		 */
+		attrs_size += sizeof(struct configfs_attribute);
+		/* One more element in the attributes array */
+		attrs_size += sizeof(*tmp_attrs);
+	}
+	attrs_mem = kmalloc(attrs_size, GFP_KERNEL);
+	if (!attrs_mem)
+		goto err_attrs;
+
+	/* Reserve memory for the value attributes */
+	next_value_attr = attrs_mem;
+	if (type->source_type)
+		next_value_attr++;
+	if (type->sink_type)
+		next_value_attr++;
+	tmp_attrs = (struct configfs_attribute **) next_value_attr;
+
+	/* Initialize the value attributes */
+	next_value_attr = attrs_mem;
+	next_attr = tmp_attrs;
+	if (type->source_type) {
+		value_attr_init(next_value_attr,
+				type,
+				"value",
+				!!type->source_type->show_value);
+
+		*next_attr = next_value_attr;
+		next_value_attr++;
+		next_attr++;
+	}
+	if (type->sink_type) {
+		value_attr_init(next_value_attr, type, "collected_value", 1);
+
+		*next_attr = next_value_attr;
+		next_value_attr++;
+		next_attr++;
+	}
+
+	/* initialize configfs attributes. */
+	memcpy(next_attr, attrs, nr_custom_attrs * sizeof(*next_attr));
+	next_attr[nr_custom_attrs] = NULL;
+
+	/* initialize configfs type. */
+	type->item_type.ct_attrs = tmp_attrs;
+	return 0;
+
+err_attrs:
+	return err;
+}
+
+void scheduler_pipe_type_cleanup(struct scheduler_pipe_type *type)
+{
+	struct configfs_attribute *prev_value_attr;
+
+	prev_value_attr = (struct configfs_attribute *)
+		type->item_type.ct_attrs;
+	type->item_type.ct_attrs = NULL;
+
+	if (type->sink_type)
+		prev_value_attr--;
+	if (type->source_type)
+		prev_value_attr--;
+	kfree(prev_value_attr);
+}
+
+int scheduler_pipe_init(struct scheduler_pipe *pipe,
+			const char *name,
+			struct scheduler_pipe_type *type,
+			struct scheduler_source *source,
+			struct scheduler_sink *sink,
+			struct config_group **default_groups)
+{
+	/* initialize config_group */
+	memset(&pipe->config, 0, sizeof(pipe->config));
+	/*
+	 * Should be able to report allocation errors, but has no return
+	 * type...
+	 */
+	config_group_init_type_name(&pipe->config, name, &type->item_type);
+	pipe->config.default_groups = default_groups;
+
+	pipe->source = source;
+	pipe->sink = sink;
+
+	return 0;
+}
+
+static inline int is_source_value_attr(struct scheduler_pipe_type *type,
+				       struct configfs_attribute *attr)
+{
+	return type->source_type && attr == type->item_type.ct_attrs[0];
+}
+
+static inline int is_sink_value_attr(struct scheduler_pipe_type *type,
+				     struct configfs_attribute *attr)
+{
+	return type->sink_type
+		&& ((!type->source_type && attr == type->item_type.ct_attrs[0])
+		    || (type->source_type
+			&& attr == type->item_type.ct_attrs[1]));
+}
+
+ssize_t scheduler_pipe_show_attribute(struct scheduler_pipe *pipe,
+				      struct configfs_attribute *attr,
+				      char *page,
+				      int *handled)
+{
+	struct scheduler_pipe_type *type = scheduler_pipe_type_of(pipe);
+	int is_source_attr;
+	int is_sink_attr = 0; /* Only to prevent gcc from warning */
+	ssize_t ret = -EACCES;
+
+	/* The only attributes are values. */
+
+	is_source_attr = is_source_value_attr(type, attr);
+	if (is_source_attr)
+		ret = scheduler_source_show_value(pipe->source, page);
+	else {
+		is_sink_attr = is_sink_value_attr(type, attr);
+		if (is_sink_attr)
+			ret = scheduler_sink_show_value(pipe->sink, page);
+	}
+
+	*handled = is_source_attr || is_sink_attr;
+	return ret;
+}
+
+ssize_t scheduler_pipe_store_attribute(struct scheduler_pipe *pipe,
+				       struct configfs_attribute *attr,
+				       const char *page, size_t count,
+				       int *handled)
+{
+	struct scheduler_pipe_type *type = scheduler_pipe_type_of(pipe);
+	*handled = is_source_value_attr(type, attr)
+		|| is_sink_value_attr(type, attr);
+	return -EACCES;
+}
diff -ruN linux-2.6.29/kerrighed/scheduler/placement.c android_cluster/linux-2.6.29/kerrighed/scheduler/placement.c
--- linux-2.6.29/kerrighed/scheduler/placement.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/placement.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,63 @@
+/*
+ *  kerrighed/scheduler/placement.c
+ *
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ */
+
+#include <linux/rcupdate.h>
+#include <linux/rculist.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/scheduler/process_set.h>
+#include <kerrighed/scheduler/policy.h>
+#include <kerrighed/scheduler/scheduler.h>
+
+struct task_struct;
+
+static kerrighed_node_t scheduler_new_task_node(struct scheduler *scheduler,
+						struct task_struct *parent)
+{
+	struct scheduler_policy *p;
+	kerrighed_node_t node = KERRIGHED_NODE_ID_NONE;
+
+	p = scheduler_get_scheduler_policy(scheduler);
+	if (!p)
+		goto out;
+	node = scheduler_policy_new_task_node(p, parent);
+	scheduler_policy_put(p);
+out:
+	return node;
+}
+
+
+/*
+ * The parsing order of schedulers is:
+ * - all universal scheduler in reversed attachment order (last attached to all
+ *   processes is parsed first);
+ * - all schedulers attached to parent, in reversed attachment order.
+ *
+ * The first scheduler returning a valid node id wins.
+ */
+kerrighed_node_t new_task_node(struct task_struct *parent)
+{
+	kerrighed_node_t node = KERRIGHED_NODE_ID_NONE;
+	struct scheduler *s;
+#define QUERY_SCHEDULER(s)				   \
+		node = scheduler_new_task_node(s, parent); \
+		if (node != KERRIGHED_NODE_ID_NONE) {	   \
+			scheduler_put(s);		   \
+			goto out;			   \
+		}
+
+	rcu_read_lock();
+	do_each_scheduler_universal(s) {
+		QUERY_SCHEDULER(s);
+	} while_each_scheduler_universal(s);
+	do_each_scheduler_task(s, parent) {
+		QUERY_SCHEDULER(s);
+	} while_each_scheduler_task(s, parent);
+out:
+	rcu_read_unlock();
+
+	return node;
+}
diff -ruN linux-2.6.29/kerrighed/scheduler/policies/echo_policy.c android_cluster/linux-2.6.29/kerrighed/scheduler/policies/echo_policy.c
--- linux-2.6.29/kerrighed/scheduler/policies/echo_policy.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/policies/echo_policy.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,175 @@
+/*
+ *  kerrighed/scheduler/policies/echo_policy.c
+ *
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <kerrighed/scheduler/policy.h>
+#include <kerrighed/scheduler/port.h>
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Marko Novak <marko.novak@xlab.si>, "
+	      "Louis Rilling <Louis.Rilling@kerlabs.com>");
+MODULE_DESCRIPTION("Policy that displays collected values");
+
+struct echo_policy {
+	struct scheduler_policy policy;
+	struct scheduler_port port_mem_free;
+	struct scheduler_port port_mem_total;
+};
+
+static
+ssize_t
+scheduler_policy_attr_echo_show(struct scheduler_policy *item, char *page)
+{
+	ssize_t ret = 0;
+
+	ret = sprintf(page, "calling scheduler_policy_attr_echo_show!\n");
+
+	return ret;
+}
+
+static struct scheduler_policy_attribute echo_attr = {
+	.attr = {
+		.ca_owner = THIS_MODULE,
+		.ca_name = "echo_attr",
+		.ca_mode = S_IRUGO,
+	},
+	.show = scheduler_policy_attr_echo_show,
+};
+
+static struct scheduler_policy_attribute *echo_policy_attrs[] = {
+	&echo_attr,
+	NULL,
+};
+
+/* DEFINE_SCHEDULER_PORT_UPDATE_VALUE(port_mem_free, port) */
+/* { */
+/*         unsigned long mem_free; */
+
+/*         if (scheduler_port_get_value(port, &mem_free, 1, NULL, 0) > 0) { */
+/*                 printk(KERN_INFO "echo_policy: mem_free=%lu\n", mem_free); */
+/*         } */
+/*         else { */
+/*                 printk(KERN_ERR "echo_policy: cannot read mem_free\n"); */
+/*         } */
+/* } */
+
+static BEGIN_SCHEDULER_PORT_TYPE(port_mem_free),
+/*	.SCHEDULER_PORT_UPDATE_VALUE(port_mem_free), */
+	.SCHEDULER_PORT_VALUE_TYPE(port_mem_free, unsigned long),
+END_SCHEDULER_PORT_TYPE(port_mem_free);
+
+DEFINE_SCHEDULER_PORT_UPDATE_VALUE(port_mem_total, port)
+{
+	unsigned long mem_total;
+
+	if (scheduler_port_get_value(port, &mem_total, 1, NULL, 0) > 0) {
+		printk(KERN_INFO "echo_policy: mem_total=%lu\n", mem_total);
+	}
+	else {
+		printk(KERN_ERR "echo_policy: cannot read mem_total\n");
+	}
+}
+
+static BEGIN_SCHEDULER_PORT_TYPE(port_mem_total),
+	.SCHEDULER_PORT_UPDATE_VALUE(port_mem_total),
+	.SCHEDULER_PORT_VALUE_TYPE(port_mem_total, unsigned long),
+END_SCHEDULER_PORT_TYPE(port_mem_total);
+
+static struct scheduler_policy *echo_policy_new(const char *name);
+static void echo_policy_destroy(struct scheduler_policy *policy);
+
+static struct scheduler_policy_operations echo_policy_ops = {
+	.new = echo_policy_new,
+	.destroy = echo_policy_destroy,
+};
+
+static SCHEDULER_POLICY_TYPE(echo_policy_type, "echo_policy",
+			     &echo_policy_ops, echo_policy_attrs);
+
+static struct scheduler_policy *echo_policy_new(const char *name)
+{
+	struct echo_policy *p;
+	struct config_group *def_groups[3];
+	int err;
+
+	p = kmalloc(sizeof(*p), GFP_KERNEL);
+	if (!p)
+		goto err_echo_policy;
+
+	err = scheduler_port_init(&p->port_mem_free, "port_mem_free",
+				  &port_mem_free_type, NULL, NULL);
+	if (err)
+		goto err_mem_free;
+	err = scheduler_port_init(&p->port_mem_total, "port_mem_total",
+				  &port_mem_total_type, NULL, NULL);
+	if (err)
+		goto err_mem_total;
+
+	/* initialize default memory groups. */
+	def_groups[0] = scheduler_port_config_group(&p->port_mem_free);
+	def_groups[1] = scheduler_port_config_group(&p->port_mem_total);
+	def_groups[2] = NULL;
+
+	err = scheduler_policy_init(&p->policy, name, &echo_policy_type,
+				    def_groups);
+	if (err)
+		goto err_policy;
+
+	return &p->policy;
+
+err_policy:
+	scheduler_port_cleanup(&p->port_mem_total);
+err_mem_total:
+	scheduler_port_cleanup(&p->port_mem_free);
+err_mem_free:
+	kfree(p);
+err_echo_policy:
+	printk(KERN_ERR "error: echo_policy creation failed!\n");
+	return NULL;
+}
+
+static void echo_policy_destroy(struct scheduler_policy *policy)
+{
+	struct echo_policy *p =
+		container_of(policy, struct echo_policy, policy);
+	scheduler_policy_cleanup(policy);
+	scheduler_port_cleanup(&p->port_mem_free);
+	scheduler_port_cleanup(&p->port_mem_total);
+	kfree(p);
+}
+
+int init_module(void)
+{
+	int err;
+
+	err = scheduler_port_type_init(&port_mem_free_type, NULL);
+	if (err)
+		goto err_mem_free;
+	err = scheduler_port_type_init(&port_mem_total_type, NULL);
+	if (err)
+		goto err_mem_total;
+	err = scheduler_policy_type_register(&echo_policy_type);
+	if (err)
+		goto err_register;
+out:
+	return err;
+
+err_register:
+	scheduler_port_type_cleanup(&port_mem_total_type);
+err_mem_total:
+	scheduler_port_type_cleanup(&port_mem_free_type);
+err_mem_free:
+	goto out;
+}
+
+void cleanup_module(void)
+{
+	scheduler_policy_type_unregister(&echo_policy_type);
+	scheduler_port_type_cleanup(&port_mem_total_type);
+	scheduler_port_type_cleanup(&port_mem_free_type);
+}
diff -ruN linux-2.6.29/kerrighed/scheduler/policies/Makefile android_cluster/linux-2.6.29/kerrighed/scheduler/policies/Makefile
--- linux-2.6.29/kerrighed/scheduler/policies/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/policies/Makefile	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,6 @@
+obj-$(CONFIG_KRG_SCHED_ECHO_POLICY) += echo_policy.o
+
+obj-$(CONFIG_KRG_SCHED_MOSIX_LOAD_BALANCER) += mosix_load_balancer.o
+obj-$(CONFIG_KRG_SCHED_ROUND_ROBIN_BALANCER) += round_robin_balancer.o
+
+EXTRA_CFLAGS += -Wall -Werror
diff -ruN linux-2.6.29/kerrighed/scheduler/policies/mosix_load_balancer.c android_cluster/linux-2.6.29/kerrighed/scheduler/policies/mosix_load_balancer.c
--- linux-2.6.29/kerrighed/scheduler/policies/mosix_load_balancer.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/policies/mosix_load_balancer.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,528 @@
+/*
+ *  kerrighed/scheduler/policies/mosix_load_balancer.c
+ *
+ *  Copyright (C) 2006-2008 Louis Rilling - Kerlabs
+ */
+
+/**
+ * Simplified MOSIX load balancing scheduling policy relying on a caching
+ * module to access remote values.
+ *
+ *  @author Louis Rilling
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/pid.h>
+#include <linux/spinlock.h>
+#include <linux/rculist.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/migration.h>
+#include <kerrighed/scheduler/policy.h>
+#include <kerrighed/scheduler/port.h>
+#include <kerrighed/scheduler/scheduler.h>
+#include <kerrighed/scheduler/process_set.h>
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Louis Rilling <Louis.Rilling@kerlabs.com>");
+MODULE_DESCRIPTION("Simplified MOSIX load balancing policy");
+
+enum port_id {
+	PORT_LOCAL_LOAD,
+	PORT_REMOTE_LOAD,
+	PORT_SINGLE_PROCESS_LOAD,
+	PORT_PROCESS_LOAD,
+	PORT_MAX,
+};
+
+struct mosix_load_balancer {
+	struct scheduler_policy policy;
+	int stab_factor; /* in percents of a single process load */
+	struct scheduler_port ports[PORT_MAX];
+	unsigned long tmp_load;
+};
+
+static inline
+struct mosix_load_balancer *
+to_mosix_load_balancer(struct scheduler_policy *policy)
+{
+	return container_of(policy, struct mosix_load_balancer, policy);
+}
+
+static inline void lb_lock(struct mosix_load_balancer *lb)
+{
+	spin_lock(&lb->policy.lock);
+}
+
+static inline void lb_unlock(struct mosix_load_balancer *lb)
+{
+	spin_unlock(&lb->policy.lock);
+}
+
+static struct pid *find_target_task(struct mosix_load_balancer *lb)
+{
+	struct scheduler *scheduler =
+		scheduler_policy_get_scheduler(&lb->policy);
+	struct process_set *processes;
+	struct task_struct *p;
+	unsigned long load, highest_load, second_highest_load;
+	struct task_struct *max_p, *second_max_p;
+	struct pid *ret_pid = NULL;
+	pid_t pid;
+	int ret;
+
+	if (!scheduler)
+		goto out;
+	processes = scheduler_get_process_set(scheduler);
+	if (!processes)
+		goto put_scheduler;
+
+	highest_load = 0;
+	second_highest_load = 0;
+	max_p = second_max_p = NULL;
+
+	process_set_prepare_do_each_process(processes);
+
+	process_set_do_each_process(p, processes) {
+		if (!may_migrate(p))
+			continue;
+
+		pid = task_pid_knr(p);
+		ret = scheduler_port_get_value(&lb->ports[PORT_PROCESS_LOAD],
+					       &load, 1, &pid, 1);
+		if (ret < 1)
+			continue;
+		if (load > highest_load) {
+			second_highest_load = highest_load;
+			second_max_p = max_p;
+			max_p = p;
+			highest_load = load;
+		} else if (load > second_highest_load) {
+			second_highest_load = load;
+			second_max_p = p;
+		}
+	} process_set_while_each_process(p, processes);
+
+	if (second_max_p)
+		p = second_max_p;
+	else {
+		if (max_p)
+			p = max_p;
+		else
+			p = NULL;
+	}
+
+	if (p)
+		ret_pid = get_pid(task_pid(p));
+
+	process_set_cleanup_do_each_process(processes);
+
+	process_set_put(processes);
+put_scheduler:
+	scheduler_put(scheduler);
+
+out:
+	return ret_pid;
+}
+
+/*
+ * Give the load of a remote node with a fictive extra CPU-bound process
+ *
+ *  @author Louis Rilling
+ */
+static int get_stable_remote_load(struct mosix_load_balancer *lb,
+				  kerrighed_node_t node, unsigned long *load)
+{
+	unsigned long node_load;
+	unsigned long single_process_load;
+	int ret;
+
+	ret = scheduler_port_get_remote_value(&lb->ports[PORT_REMOTE_LOAD],
+					      node,
+					      &lb->tmp_load, 1,
+					      NULL, 0);
+	if (ret < 1)
+		goto no_load;
+	node_load = lb->tmp_load;
+	ret = scheduler_port_get_remote_value(
+		&lb->ports[PORT_SINGLE_PROCESS_LOAD],
+		node,
+		&lb->tmp_load, 1,
+		NULL, 0);
+	if (ret < 1)
+		goto no_load;
+	single_process_load = lb->tmp_load;
+
+	*load = node_load + single_process_load * lb->stab_factor / 100;
+	return 0;
+
+no_load:
+	return -EAGAIN;
+}
+
+kerrighed_node_t __find_target_node(struct mosix_load_balancer *lb,
+				    const krgnodemask_t *nodes,
+				    unsigned long high_load)
+{
+	unsigned long lowest_remote_load, remote_load;
+	kerrighed_node_t target_node = KERRIGHED_NODE_ID_NONE;
+	kerrighed_node_t i;
+	int ret;
+
+	lowest_remote_load = high_load;
+
+	__for_each_krgnode_mask(i, nodes) {
+		if (unlikely(i == kerrighed_node_id))
+			continue;
+
+		ret = get_stable_remote_load(lb, i, &remote_load);
+		if (ret)
+			continue;
+		if (remote_load < lowest_remote_load) {
+			lowest_remote_load = remote_load;
+			target_node = i;
+		}
+	}
+
+	return target_node;
+}
+
+/**
+ * If some node have an estimated processor load lower than the local one,
+ *  return the node having the lowest load;
+ *  otherwise, return KERRIGHED_NODE_ID_NONE.
+ */
+kerrighed_node_t find_target_node(struct mosix_load_balancer *lb,
+				  unsigned long current_load)
+{
+	struct scheduler *s = scheduler_policy_get_scheduler(&lb->policy);
+	krgnodemask_t nodes;
+	kerrighed_node_t target_node = KERRIGHED_NODE_ID_NONE;
+
+	if (s) {
+		scheduler_get_node_set(s, &nodes);
+		target_node = __find_target_node(lb, &nodes, current_load);
+		scheduler_put(s);
+	}
+
+	return target_node;
+}
+
+static void balance(struct mosix_load_balancer *lb, unsigned long current_load)
+{
+	struct pid *target_pid;
+	struct task_struct *target_task;
+	kerrighed_node_t target_node;
+
+	lb_lock(lb);
+
+	/* First, try to find a task that could be migrated */
+	target_pid = find_target_task(lb);
+	if (!target_pid)
+		goto out;
+
+	/* Second, check whether migrating the task could improve balance */
+	target_node = find_target_node(lb, current_load);
+	if (target_node == KERRIGHED_NODE_ID_NONE)
+		goto out_put_pid;
+
+	/* Third, migrate the selected task to the selected node */
+	rcu_read_lock();
+	target_task = pid_task(target_pid, PIDTYPE_PID);
+	if (target_task)
+		__migrate_linux_threads(target_task, MIGR_LOCAL_PROCESS,
+					target_node);
+	rcu_read_unlock();
+
+out_put_pid:
+	put_pid(target_pid);
+
+out:
+	lb_unlock(lb);
+}
+
+/* Expell migratable tasks we manage */
+static void __expell_all(struct mosix_load_balancer *lb,
+			 const krgnodemask_t *nodes)
+{
+	struct scheduler *scheduler;
+	struct process_set *processes;
+	struct task_struct *t;
+	kerrighed_node_t node;
+	kerrighed_node_t fallback_node = __first_krgnode(nodes);
+	int err;
+
+	scheduler = scheduler_policy_get_scheduler(&lb->policy);
+	if (!scheduler)
+		return;
+
+	processes = scheduler_get_process_set(scheduler);
+	if (!processes)
+		goto put_scheduler;
+
+	lb_lock(lb);
+	process_set_prepare_do_each_process(processes);
+
+	process_set_do_each_process(t, processes) {
+		if (!may_migrate(t)) {
+			printk(KERN_WARNING "mosix_load_balancer:"
+			       " task %d(%s) is not migratable!\n",
+			       task_pid_knr(t), t->comm);
+			continue;
+		}
+
+		node = __find_target_node(lb, nodes, ULONG_MAX);
+		if (node == KERRIGHED_NODE_ID_NONE)
+			node = fallback_node;
+
+		err = __migrate_linux_threads(t, MIGR_LOCAL_PROCESS, node);
+		if (err && err != -EALREADY)
+			printk(KERN_WARNING "mosix_load_balancer:"
+			       " task %d(%s) could not be migrated!\n",
+			       task_pid_knr(t), t->comm);
+	} process_set_while_each_process(t, processes);
+
+	process_set_cleanup_do_each_process(processes);
+	lb_unlock(lb);
+
+	process_set_put(processes);
+put_scheduler:
+	scheduler_put(scheduler);
+}
+
+static
+void mosix_load_balancer_update_node_set(struct scheduler_policy *policy,
+					 const krgnodemask_t *new_set,
+					 const krgnodemask_t *removed_set,
+					 const krgnodemask_t *added_set)
+{
+	struct mosix_load_balancer *lb = to_mosix_load_balancer(policy);
+	if (__krgnode_isset(kerrighed_node_id, removed_set))
+		__expell_all(lb, new_set);
+}
+
+/* scheduler_policy_attributes */
+
+static
+ssize_t stab_factor_attr_show(struct scheduler_policy *policy, char *page)
+{
+	struct mosix_load_balancer *lb = to_mosix_load_balancer(policy);
+	return sprintf(page, "%d\n", lb->stab_factor);
+}
+
+static ssize_t stab_factor_attr_store(struct scheduler_policy *policy,
+				      const char *page, size_t count)
+{
+	struct mosix_load_balancer *lb = to_mosix_load_balancer(policy);
+	char *pos;
+	unsigned int tmp;
+
+	tmp = simple_strtoul(page, &pos, 10);
+	if (((*pos == '\n' && pos - page == count - 1)
+	     || (*pos == '\0' && pos - page == count))
+	    && pos != page) {
+		lb_lock(lb);
+		lb->stab_factor = tmp;
+		lb_unlock(lb);
+		return count;
+	}
+	return -EINVAL;
+}
+
+static struct scheduler_policy_attribute stab_factor_attr = {
+	.attr = {
+		.ca_owner = THIS_MODULE,
+		.ca_name = "stab_factor",
+		.ca_mode = S_IRUGO | S_IWUSR,
+	},
+	.show = stab_factor_attr_show,
+	.store = stab_factor_attr_store,
+};
+
+static struct scheduler_policy_attribute *mosix_load_balancer_attrs[] = {
+	&stab_factor_attr,
+	NULL
+};
+
+/* local_load_port */
+
+DEFINE_SCHEDULER_PORT_UPDATE_VALUE(local_load_port, port)
+{
+	struct mosix_load_balancer *lb;
+	unsigned long new_load;
+	int ret;
+
+	lb = container_of(port, typeof(*lb), ports[PORT_LOCAL_LOAD]);
+	ret = scheduler_port_get_value(port, &new_load, 1, NULL, 0);
+	if (ret < 1)
+		return;
+
+	balance(lb, new_load);
+}
+
+static BEGIN_SCHEDULER_PORT_TYPE(local_load_port),
+	.SCHEDULER_PORT_UPDATE_VALUE(local_load_port),
+	.SCHEDULER_PORT_VALUE_TYPE(local_load_port, unsigned long),
+END_SCHEDULER_PORT_TYPE(local_load_port);
+
+/* remote_load_port */
+
+static BEGIN_SCHEDULER_PORT_TYPE(remote_load_port),
+	.SCHEDULER_PORT_VALUE_TYPE(remote_load_port, unsigned long),
+END_SCHEDULER_PORT_TYPE(remote_load_port);
+
+/* single_process_load_port */
+
+static BEGIN_SCHEDULER_PORT_TYPE(single_process_load_port),
+	.SCHEDULER_PORT_VALUE_TYPE(single_process_load_port, unsigned long),
+END_SCHEDULER_PORT_TYPE(single_process_load_port);
+
+/* process_load_port */
+
+static BEGIN_SCHEDULER_PORT_TYPE(process_load_port),
+	.SCHEDULER_PORT_VALUE_TYPE(process_load_port, unsigned long),
+	.SCHEDULER_PORT_PARAM_TYPE(process_load_port, pid_t),
+END_SCHEDULER_PORT_TYPE(process_load_port);
+
+/* scheduler_policy_type*/
+
+static struct scheduler_policy *mosix_load_balancer_new(const char *name);
+static void mosix_load_balancer_destroy(struct scheduler_policy *policy);
+
+static struct scheduler_policy_operations mosix_load_balancer_ops = {
+	.new = mosix_load_balancer_new,
+	.destroy = mosix_load_balancer_destroy,
+	.update_node_set = mosix_load_balancer_update_node_set,
+};
+
+static SCHEDULER_POLICY_TYPE(mosix_load_balancer, "mosix_load_balancer",
+			     &mosix_load_balancer_ops,
+			     mosix_load_balancer_attrs);
+
+static struct scheduler_policy *mosix_load_balancer_new(const char *name)
+{
+	struct mosix_load_balancer *lb = kmalloc(sizeof(*lb), GFP_KERNEL);
+	struct config_group *def_groups[PORT_MAX + 1];
+	int i;
+	int err;
+
+	if (!lb)
+		goto err_lb;
+
+	lb->stab_factor = 130;
+
+	err = scheduler_port_init(&lb->ports[PORT_LOCAL_LOAD],
+				  "local_load", &local_load_port_type, NULL,
+				  NULL);
+	if (err)
+		goto err_local_load;
+	err = scheduler_port_init(&lb->ports[PORT_REMOTE_LOAD],
+				  "remote_load", &remote_load_port_type, NULL,
+				  NULL);
+	if (err)
+		goto err_remote_load;
+	err = scheduler_port_init(&lb->ports[PORT_SINGLE_PROCESS_LOAD] ,
+				  "single_process_load",
+				  &single_process_load_port_type,
+				  NULL,
+				  NULL);
+	if (err)
+		goto err_single_process_load;
+	err = scheduler_port_init(&lb->ports[PORT_PROCESS_LOAD],
+				  "process_load",
+				  &process_load_port_type,
+				  NULL,
+				  NULL);
+	if (err)
+		goto err_process_load;
+
+	for (i = 0; i < PORT_MAX; i++)
+		def_groups[i] = scheduler_port_config_group(&lb->ports[i]);
+	def_groups[PORT_MAX] = NULL;
+
+	err = scheduler_policy_init(&lb->policy, name, &mosix_load_balancer,
+				    def_groups);
+	if (err)
+		goto err_policy;
+
+	return &lb->policy;
+
+err_policy:
+	scheduler_port_cleanup(&lb->ports[PORT_PROCESS_LOAD]);
+err_process_load:
+	scheduler_port_cleanup(&lb->ports[PORT_SINGLE_PROCESS_LOAD]);
+err_single_process_load:
+	scheduler_port_cleanup(&lb->ports[PORT_REMOTE_LOAD]);
+err_remote_load:
+	scheduler_port_cleanup(&lb->ports[PORT_LOCAL_LOAD]);
+err_local_load:
+	kfree(lb);
+err_lb:
+	return NULL;
+}
+
+static void mosix_load_balancer_destroy(struct scheduler_policy *policy)
+{
+	struct mosix_load_balancer *lb = to_mosix_load_balancer(policy);
+	int i;
+
+	scheduler_policy_cleanup(policy);
+	for (i = 0; i < PORT_MAX; i++)
+		scheduler_port_cleanup(&lb->ports[i]);
+	kfree(lb);
+}
+
+/* module init/exit */
+
+int mosix_load_balancer_init(void)
+{
+	int err;
+
+	err = scheduler_port_type_init(&local_load_port_type, NULL);
+	if (err)
+		goto err_local_load;
+	err = scheduler_port_type_init(&remote_load_port_type, NULL);
+	if (err)
+		goto err_remote_load;
+	err = scheduler_port_type_init(&single_process_load_port_type, NULL);
+	if (err)
+		goto err_single_process_load;
+	err = scheduler_port_type_init(&process_load_port_type, NULL);
+	if (err)
+		goto err_process_load;
+
+	err = scheduler_policy_type_register(&mosix_load_balancer);
+	if (err)
+		goto err_register;
+
+out:
+	return err;
+
+err_register:
+
+	scheduler_port_type_cleanup(&process_load_port_type);
+err_process_load:
+	scheduler_port_type_cleanup(&single_process_load_port_type);
+err_single_process_load:
+	scheduler_port_type_cleanup(&remote_load_port_type);
+err_remote_load:
+	scheduler_port_type_cleanup(&local_load_port_type);
+err_local_load:
+	goto out;
+}
+
+void mosix_load_balancer_exit(void)
+{
+	scheduler_policy_type_unregister(&mosix_load_balancer);
+	scheduler_port_type_cleanup(&process_load_port_type);
+	scheduler_port_type_cleanup(&single_process_load_port_type);
+	scheduler_port_type_cleanup(&remote_load_port_type);
+	scheduler_port_type_cleanup(&local_load_port_type);
+}
+
+module_init(mosix_load_balancer_init);
+module_exit(mosix_load_balancer_exit);
diff -ruN linux-2.6.29/kerrighed/scheduler/policies/round_robin_balancer.c android_cluster/linux-2.6.29/kerrighed/scheduler/policies/round_robin_balancer.c
--- linux-2.6.29/kerrighed/scheduler/policies/round_robin_balancer.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/policies/round_robin_balancer.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,162 @@
+/*
+ *  kerrighed/scheduler/policies/round_robin_balancer.c
+ *
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+/**
+ * Simple per node round robin placement policy.
+ *
+ * The first node chosen is the next node in ring, and subsequent nodes are
+ * obtained in node id order.
+ *
+ * E.g: let a cluster have node ids 0 2 3 5
+ *      - from node 0, the sequence will be 2 3 5 0 2 3 5 0 2 ...
+ *      - from node 2, the sequence will be 3 5 0 2 3 5 0 2 3 ...
+ *
+ * All instance are strictly independent.
+ *
+ * @author Louis Rilling
+ */
+
+#include <linux/module.h>
+#include <linux/spinlock.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/scheduler/policy.h>
+#include <kerrighed/scheduler/scheduler.h>
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Louis Rilling <Louis.Rilling@kerlabs.com>");
+MODULE_DESCRIPTION("Load balancing policy based on round robin placement");
+
+struct round_robin_balancer {
+	struct scheduler_policy policy;
+	kerrighed_node_t last_target;
+};
+
+static inline
+struct round_robin_balancer *
+to_round_robin_balancer(struct scheduler_policy *policy)
+{
+	return container_of(policy, struct round_robin_balancer, policy);
+}
+
+static inline void rrb_lock(struct round_robin_balancer *rrb)
+{
+	spin_lock(&rrb->policy.lock);
+}
+
+static inline void rrb_unlock(struct round_robin_balancer *rrb)
+{
+	spin_unlock(&rrb->policy.lock);
+}
+
+static
+kerrighed_node_t
+round_robin_balancer_new_task_node(struct scheduler_policy *policy,
+				   struct task_struct *parent)
+{
+	struct round_robin_balancer *rrb = to_round_robin_balancer(policy);
+	struct scheduler *s = scheduler_policy_get_scheduler(policy);
+	krgnodemask_t nodes;
+	kerrighed_node_t node = KERRIGHED_NODE_ID_NONE;
+
+	if (!s)
+		goto out;
+	scheduler_get_node_set(s, &nodes);
+
+	rrb_lock(rrb);
+	node = rrb->last_target;
+	if (node == KERRIGHED_NODE_ID_NONE)
+		node = kerrighed_node_id;
+	node = next_krgnode_in_ring(node, nodes);
+	rrb->last_target = node;
+	rrb_unlock(rrb);
+
+	scheduler_put(s);
+out:
+	return node;
+}
+
+/* scheduler_policy_attributes */
+
+static
+ssize_t last_target_attr_show(struct scheduler_policy *policy, char *page)
+{
+	struct round_robin_balancer *rrb = to_round_robin_balancer(policy);
+	return sprintf(page, "%d\n", rrb->last_target);
+}
+
+static struct scheduler_policy_attribute last_target_attr = {
+	.attr = {
+		.ca_owner = THIS_MODULE,
+		.ca_name = "last_target",
+		.ca_mode = S_IRUGO,
+	},
+	.show = last_target_attr_show,
+};
+
+static struct scheduler_policy_attribute *round_robin_balancer_attrs[] = {
+	&last_target_attr,
+	NULL
+};
+
+/* scheduler_policy_type*/
+
+static struct scheduler_policy *round_robin_balancer_new(const char *name);
+static void round_robin_balancer_destroy(struct scheduler_policy *policy);
+
+static struct scheduler_policy_operations round_robin_balancer_ops = {
+	.new = round_robin_balancer_new,
+	.destroy = round_robin_balancer_destroy,
+	.new_task_node = round_robin_balancer_new_task_node,
+};
+
+static SCHEDULER_POLICY_TYPE(round_robin_balancer, "round_robin_balancer",
+			     &round_robin_balancer_ops,
+			     round_robin_balancer_attrs);
+
+static struct scheduler_policy *round_robin_balancer_new(const char *name)
+{
+	struct round_robin_balancer *rrb = kmalloc(sizeof(*rrb), GFP_KERNEL);
+	int err;
+
+	if (!rrb)
+		goto err_rrb;
+	rrb->last_target = KERRIGHED_NODE_ID_NONE;
+	err = scheduler_policy_init(&rrb->policy, name, &round_robin_balancer,
+				    NULL);
+	if (err)
+		goto err_policy;
+
+	return &rrb->policy;
+
+err_policy:
+	kfree(rrb);
+err_rrb:
+	return NULL;
+}
+
+static void round_robin_balancer_destroy(struct scheduler_policy *policy)
+{
+	struct round_robin_balancer *rrb = to_round_robin_balancer(policy);
+	scheduler_policy_cleanup(policy);
+	kfree(rrb);
+}
+
+/* module init/exit */
+
+int round_robin_balancer_init(void)
+{
+	return scheduler_policy_type_register(&round_robin_balancer);
+}
+
+void round_robin_balancer_exit(void)
+{
+	scheduler_policy_type_unregister(&round_robin_balancer);
+}
+
+module_init(round_robin_balancer_init);
+module_exit(round_robin_balancer_exit);
diff -ruN linux-2.6.29/kerrighed/scheduler/policy.c android_cluster/linux-2.6.29/kerrighed/scheduler/policy.c
--- linux-2.6.29/kerrighed/scheduler/policy.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/policy.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,398 @@
+/*
+ *  kerrighed/scheduler/policy.c
+ *
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/module.h>
+#include <linux/nsproxy.h>
+#include <linux/configfs.h>
+#include <linux/spinlock.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <kerrighed/scheduler/global_config.h>
+#include <kerrighed/scheduler/policy.h>
+
+#include "internal.h"
+
+/* a spinlock protecting access to the list of scheduling policy types */
+static DEFINE_SPINLOCK(policies_lock);
+
+/* list of registered scheduling policy types */
+static LIST_HEAD(policies_list);
+
+static
+inline
+struct scheduler_policy *to_scheduler_policy(struct config_item *item)
+{
+	return container_of(to_config_group(item),
+			    struct scheduler_policy, group);
+}
+
+static
+inline
+struct scheduler_policy_attribute *
+to_scheduler_policy_attr(struct configfs_attribute *attr)
+{
+	return container_of(attr, struct scheduler_policy_attribute, attr);
+}
+
+static
+inline
+struct scheduler_policy_type *
+to_scheduler_policy_type(struct config_item_type *type)
+{
+	return container_of(type, struct scheduler_policy_type, item_type);
+}
+
+/**
+ * General function for reading scheduling policies' ConfigFS attributes.
+ * @author Marko Novak, Louis Rilling
+ */
+static ssize_t scheduler_policy_attribute_show(struct config_item *item,
+					       struct configfs_attribute *attr,
+					       char *page)
+{
+	struct scheduler_policy_attribute *policy_attr =
+		to_scheduler_policy_attr(attr);
+	struct scheduler_policy *policy = to_scheduler_policy(item);
+	ssize_t ret = -EACCES;
+
+	if (policy_attr->show) {
+		spin_lock(&policy->lock);
+		ret = policy_attr->show(policy, page);
+		spin_unlock(&policy->lock);
+	}
+
+	return ret;
+}
+
+/**
+ * General function for storing scheduling policies' ConfigFS attributes.
+ * @author Marko Novak, Louis Rilling
+ */
+static ssize_t scheduler_policy_attribute_store(struct config_item *item,
+						struct configfs_attribute *attr,
+						const char *page, size_t count)
+{
+	struct scheduler_policy_attribute *policy_attr =
+		to_scheduler_policy_attr(attr);
+	struct scheduler_policy *policy = to_scheduler_policy(item);
+	struct string_list_object *list;
+	ssize_t ret = -EACCES;
+
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return -EPERM;
+
+	if (policy_attr->store) {
+		list = global_config_attr_store_begin(item);
+		if (IS_ERR(list))
+			return PTR_ERR(list);
+
+		spin_lock(&policy->lock);
+		ret = policy_attr->store(policy, page, count);
+		spin_unlock(&policy->lock);
+
+		if (ret >= 0)
+			ret = global_config_attr_store_end(list,
+							   item, attr,
+							   page, ret);
+		else
+			global_config_attr_store_error(list, item);
+	}
+
+	return ret;
+}
+
+static void scheduler_policy_release(struct config_item *);
+
+static struct global_config_attrs *policy_global_attrs(struct config_item *item)
+{
+	return &to_scheduler_policy(item)->global_attrs;
+}
+
+/**
+ * This struct is ConfigFS-specific. See ConfigFS documentation for its
+ * explanation.
+ */
+struct global_config_item_operations policy_global_item_ops = {
+	.config = {
+		.release = scheduler_policy_release,
+		.show_attribute = scheduler_policy_attribute_show,
+		.store_attribute = scheduler_policy_attribute_store,
+	},
+	.global_attrs = policy_global_attrs,
+};
+
+/**
+ * This function initializes a new scheduling policy.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param policy	pointer to the scheduler_policy to init
+ * @param name		name of the scheduling policy. This name must be the one
+ *			provided as argument to the constructor.
+ * @param type		type of the scheduler_policy
+ * @param def_groups	NULL-terminated array of subdirs of the scheduler_policy
+ *			directory, or NULL
+ *
+ * @return		0 if successul,
+ *			-ENODEV is module is unloading (should not happen!),
+ *			-ENOMEM if not sufficient memory could be allocated.
+ */
+int scheduler_policy_init(struct scheduler_policy *policy,
+			  const char *name,
+			  struct scheduler_policy_type *type,
+			  struct config_group *def_groups[])
+{
+	struct config_group **tmp_groups = NULL;
+	int nr_groups;
+	int err;
+
+	err = -ENODEV;
+	if (!try_module_get(type->item_type.ct_owner))
+		goto err_module;
+
+	err = -ENOMEM;
+	nr_groups = nr_def_groups(def_groups);
+	if (nr_groups) {
+		tmp_groups = kmalloc(sizeof(*tmp_groups) * (nr_groups + 1),
+				     GFP_KERNEL);
+		if (!tmp_groups)
+			goto err_def_groups;
+		memcpy(tmp_groups, def_groups,
+		       sizeof(*tmp_groups) * (nr_groups + 1));
+	}
+
+	/* initialize scheduling policy. */
+	memset(policy, 0, sizeof(struct scheduler_policy));
+	config_group_init_type_name(&policy->group, name, &type->item_type);
+
+	spin_lock_init(&policy->lock);
+
+	policy->group.default_groups = tmp_groups;
+
+	return 0;
+
+err_def_groups:
+	module_put(type->item_type.ct_owner);
+err_module:
+	return err;
+}
+
+void scheduler_policy_cleanup(struct scheduler_policy *policy)
+{
+	kfree(policy->group.default_groups);
+}
+
+/*
+ * Configfs callback when the last reference on the scheduler_policy is dropped.
+ * Destroys the scheduling policy.
+ */
+static void scheduler_policy_release(struct config_item *item)
+{
+	struct scheduler_policy_type *type =
+		to_scheduler_policy_type(item->ci_type);
+	struct scheduler_policy *policy = to_scheduler_policy(item);
+
+	type->ops->destroy(policy);
+	module_put(type->item_type.ct_owner);
+}
+
+/**
+ * Finds scheduling policy type with a given name. Returns NULL if no such
+ * scheduling policy type is registered.
+ *
+ * Assumes policies_lock held.
+ */
+static
+struct scheduler_policy_type *scheduler_policy_type_find(const char *name)
+{
+	struct list_head *pos;
+	struct scheduler_policy_type *entry;
+
+	list_for_each(pos, &policies_list) {
+		entry = list_entry(pos, struct scheduler_policy_type, list);
+		if (strcmp(name, entry->name) == 0)
+			return entry;
+	}
+
+	return NULL;
+}
+
+/**
+ * Determines length of a NULL-terminated array
+ */
+static int scheduler_policy_attribute_array_length(
+	struct scheduler_policy_attribute **attrs) {
+
+	int i;
+	if (!attrs)
+		return 0;
+	for (i=0; attrs[i] != NULL; i++)
+		;
+	return i;
+}
+
+/**
+ * This function is used for registering newly added scheduling policy types.
+ * Once a type is registered, new scheduling policies of this type can be
+ * created when user does mkdir with the type name.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param type		pointer to the scheduling policy type to register.
+ *
+ * @return		0 if successful,
+ *			-EEXIST if scheduling policy type with the same name
+ *				is already registered.
+ */
+int scheduler_policy_type_register(struct scheduler_policy_type *type)
+{
+	struct configfs_attribute **tmp_attrs = NULL;
+	int num_attrs, i;
+	int ret = 0;
+
+	/* Fixup type */
+	type->item_type.ct_item_ops = &policy_global_item_ops.config;
+
+	num_attrs = scheduler_policy_attribute_array_length(type->attrs);
+	if (num_attrs) {
+		tmp_attrs = kmalloc(sizeof(*tmp_attrs) * (num_attrs + 1),
+				    GFP_KERNEL);
+		if (!tmp_attrs)
+			return -ENOMEM;
+		for (i = 0; i < num_attrs; i++)
+			tmp_attrs[i] = &type->attrs[i]->attr;
+		tmp_attrs[num_attrs] = NULL;
+	}
+
+	/* Try registering */
+	spin_lock(&policies_lock);
+	if (scheduler_policy_type_find(type->name) != NULL) {
+		ret = -EEXIST;
+	} else {
+		/*
+		 * ok, no scheduling policy with same name exists, proceed
+		 * with registration.
+		 */
+		type->item_type.ct_attrs = tmp_attrs;
+		list_add(&type->list, &policies_list);
+	}
+	spin_unlock(&policies_lock);
+	if (ret)
+		kfree(tmp_attrs);
+	else
+		printk(KERN_INFO
+		       "successfully registered scheduler_policy_type %s\n",
+		       type->name);
+
+	return ret;
+}
+
+/**
+ * This function is used for removing scheduling policy registrations.
+ * Must *only* be called at module unloading.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param type		pointer to the scheduling policy type to unregister.
+ */
+void scheduler_policy_type_unregister(struct scheduler_policy_type *type)
+{
+	spin_lock(&policies_lock);
+	list_del(&type->list);
+	spin_unlock(&policies_lock);
+
+	kfree(type->item_type.ct_attrs);
+	type->item_type.ct_attrs = NULL;
+}
+
+/**
+ * Function to create and initilialize a scheduling policy having the type
+ * named. The scheduling policy directory will be named after its type.
+ * Called by whichever subsystem that creates scheduling policies.
+ *
+ * @param name		Type name of the scheduling policy
+ *
+ * @return		Pointer to the new scheduling policy, or
+ *			NULL if failed
+ */
+struct scheduler_policy *scheduler_policy_new(const char *name)
+{
+	struct scheduler_policy_type *type;
+	struct scheduler_policy *tmp_policy;
+	int err;
+
+	spin_lock(&policies_lock);
+	type = scheduler_policy_type_find(name);
+	if (!type) {
+		spin_unlock(&policies_lock);
+
+		/*
+		 * insert scheduling policy's module into kernel space.
+		 * Note: no module locking is needed, since module is already
+		 * locked by "request_module".
+		 *
+		 * note: all the scheduling policies' files have to be copied
+		 * into "/lib/modules/<version>/extra" directory and added to
+		 * "/lib/modules/<version>/modules.dep" file.
+		 */
+		request_module("%s", name);
+
+		spin_lock(&policies_lock);
+		type = scheduler_policy_type_find(name);
+	}
+
+	/*
+	 * if scheduling policy's module didn't manage to register itself,
+	 * abort.
+	 * this usually implies an error at scheduling policy type
+	 * initialization (in "init_module" function) or that module
+	 * is already loaded in the kernel and has to be manually
+	 * unloaded first.
+	 */
+	err = -ENOENT;
+	if (!type)
+		goto err_module;
+
+	/*
+	 * configfs does try_module_get a bit too late for us because a user
+	 * might remove the policy's module while type is a pointer still
+	 * pointing to it.
+	 */
+	err = -EAGAIN;
+	if (!try_module_get(type->item_type.ct_owner))
+		goto err_module;
+	spin_unlock(&policies_lock);
+
+	tmp_policy = type->ops->new(name);
+	module_put(type->item_type.ct_owner);
+	if (!tmp_policy) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	return tmp_policy;
+
+err_module:
+	spin_unlock(&policies_lock);
+err:
+	return ERR_PTR(err);
+}
+
+/**
+ * Callback to deactivate a scheduling policy when its directory is dropped.
+ * Called by whichever subsystem that creates scheduling policies.
+ *
+ * @param policy	 The policy to drop
+ */
+void scheduler_policy_drop(struct scheduler_policy *policy)
+{
+	config_group_put(&policy->group);
+}
+
+EXPORT_SYMBOL(scheduler_policy_type_register);
+EXPORT_SYMBOL(scheduler_policy_type_unregister);
+EXPORT_SYMBOL(scheduler_policy_init);
+EXPORT_SYMBOL(scheduler_policy_cleanup);
diff -ruN linux-2.6.29/kerrighed/scheduler/port.c android_cluster/linux-2.6.29/kerrighed/scheduler/port.c
--- linux-2.6.29/kerrighed/scheduler/port.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/port.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,634 @@
+/*
+ *  kerrighed/scheduler/port.c
+ *
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/module.h>
+#include <linux/nsproxy.h>
+#include <linux/configfs.h>
+#include <linux/spinlock.h>
+#include <linux/list.h>
+#include <linux/rcupdate.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <kerrighed/scheduler/global_config.h>
+#include <kerrighed/scheduler/pipe.h>
+#include <kerrighed/scheduler/port.h>
+
+#include "internal.h"
+
+/**
+ * Get the scheduler_port_type structure embedding a config_item_type
+ *
+ * @param type		pointer to a config_item_type embedded in a
+ *			scheduler_port_type
+ *
+ * @return		pointer to the scheduler_port_type embedding type
+ */
+static inline
+struct scheduler_port_type *
+to_scheduler_port_type(struct config_item_type *type)
+{
+	return container_of(to_scheduler_pipe_type(type),
+			    struct scheduler_port_type, pipe_type);
+}
+
+/**
+ * Get the scheduler_port structure embedding a config_item
+ *
+ * @param type		pointer to a config_item embedded in a scheduler_port
+ *
+ * @return		pointer to the scheduler_port embedding item
+ */
+static inline
+struct scheduler_port *to_scheduler_port(struct config_item *item)
+{
+	return container_of(to_scheduler_pipe(item),
+			    struct scheduler_port, pipe);
+}
+
+/**
+ * Get the scheduler_port_type of a scheduler_port
+ *
+ * @param port		port which type to get
+ *
+ * @return		type of the port
+ */
+static inline
+struct scheduler_port_type *scheduler_port_type_of(struct scheduler_port *port)
+{
+	return container_of(scheduler_pipe_type_of(&port->pipe),
+			    struct scheduler_port_type, pipe_type);
+}
+
+static inline struct config_group *config_group_of(struct scheduler_port *port)
+{
+	return &port->pipe.config;
+}
+
+static inline
+struct global_config_item *global_item_of(struct scheduler_port *port)
+{
+	return &port->global_item;
+}
+
+static inline
+struct scheduler_port_attribute *
+to_scheduler_port_attribute(struct configfs_attribute *attr)
+{
+	return container_of(attr, struct scheduler_port_attribute, config);
+}
+
+/* List of registered port types */
+static LIST_HEAD(types_head);
+/* Lock protecting the port types list */
+static DEFINE_SPINLOCK(types_lock);
+
+/* Assumes types_lock held */
+static struct scheduler_port_type *port_type_find(const char *name);
+
+/**
+ * General function for reading scheduler_port's ConfigFS attributes. Falls back
+ * to the scheduler_pipe attributes methods the scheduler_pipe attributes, or to
+ * the port attribute show() operation for custom attributes.
+ * @author Marko Novak, Louis Rilling
+ */
+static ssize_t scheduler_port_attribute_show(struct config_item *item,
+					     struct configfs_attribute *attr,
+					     char *page)
+{
+	struct scheduler_port *port = to_scheduler_port(item);
+	ssize_t ret;
+	int handled;
+
+	ret = scheduler_pipe_show_attribute(&port->pipe, attr, page, &handled);
+	if (!handled) {
+		struct scheduler_port_attribute *port_attr =
+			to_scheduler_port_attribute(attr);
+		ret = -EACCES;
+
+		if (port_attr->show)
+			ret = port_attr->show(port, port_attr, page);
+	}
+
+	return ret;
+}
+
+/**
+ * General function for storing scheduler_port's ConfigFS attributes. Falls back
+ * to the store() method of the port attribute if not called for scheduler_pipe
+ * attribute.
+ */
+static ssize_t scheduler_port_attribute_store(struct config_item *item,
+					      struct configfs_attribute *attr,
+					      const char *page, size_t count)
+{
+	struct scheduler_port *port = to_scheduler_port(item);
+	struct string_list_object *list;
+        ssize_t ret;
+	int handled;
+
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return -EPERM;
+
+	list = global_config_attr_store_begin(item);
+	if (IS_ERR(list))
+		return PTR_ERR(list);
+
+	ret = scheduler_pipe_store_attribute(&port->pipe, attr, page, count, &handled);
+	if (!handled) {
+		struct scheduler_port_attribute *port_attr =
+			to_scheduler_port_attribute(attr);
+		ret = -EACCES;
+
+		if (port_attr->store)
+			ret = port_attr->store(port, port_attr, page, count);
+        }
+
+	if (ret >= 0)
+		ret = global_config_attr_store_end(list,
+						   item, attr,
+						   page, ret);
+	else
+		global_config_attr_store_error(list, item);
+
+        return ret;
+}
+
+/**
+ * Connect a scheduler_port's scheduler_sink to another scheduler_pipe having a
+ * source
+ *
+ * @param port		port having a sink to connect
+ * @param peer_pipe	pipe which source to connect to the port's sink
+ */
+static void connect(struct scheduler_port *port,
+		    struct scheduler_pipe *peer_pipe)
+{
+	int subscribe;
+	/*
+	 * Only subscribe if we are sure that port can push notifications up to
+	 * the terminating sink. This avoids having useless notification call
+	 * chains.
+	 */
+	/*
+	 * Testing whether port has subscribers is safe as long as subscribers
+	 * do not unsubscribe before destroying port. Ports and super-classes
+	 * behave so.
+	 */
+	subscribe = port->sink.type->update_value
+		&& (!port->pipe.source
+		    || scheduler_source_has_subscribers(port->pipe.source));
+	scheduler_sink_connect(&port->sink, peer_pipe->source, subscribe);
+	rcu_assign_pointer(port->peer_pipe, peer_pipe);
+}
+
+/**
+ * Disconnect a scheduler_port's scheduler_sink from its connected source
+ *
+ * @param port		port having the sink to disconnect
+ */
+static void disconnect(struct scheduler_port *port)
+{
+	rcu_assign_pointer(port->peer_pipe, NULL);
+	scheduler_sink_disconnect(&port->sink);
+	synchronize_rcu();
+}
+
+/**
+ * Tests whether a port's scheduler_sink is connected to a source.
+ * Must be called under rcu_read_lock()
+ *
+ * @param port		the port to test
+ *
+ * @return		true iff a source is connected to the port's sink
+ */
+static int connected(struct scheduler_port *port)
+{
+	return !!scheduler_sink_get_peer_source(&port->sink);
+}
+
+/**
+ * Callback called by global_config when the link/item of the peer source is
+ * globally dropped
+ */
+static void scheduler_port_peer_source_drop(struct global_config_item *item)
+{
+	struct scheduler_port *port =
+		container_of(item, struct scheduler_port, global_item);
+
+	config_group_put(config_group_of(port));
+}
+
+static void scheduler_port_drop_peer_source(struct scheduler_port *port)
+{
+	disconnect(port);
+	global_config_drop(global_item_of(port));
+}
+
+static struct global_config_drop_operations scheduler_port_link_drop_ops = {
+	.drop_func = scheduler_port_peer_source_drop,
+	.is_symlink = 1
+};
+
+/**
+ * checks if source config_item is a probe source
+ */
+/* TODO: maybe should also allow (type-compatible) ports */
+static int is_link_target_valid(struct config_item *target)
+{
+	return is_scheduler_probe_source(target);
+}
+
+/**
+ * Configfs callback called every time symbolic link creation is initiated from
+ * a scheduler_port directory.
+ */
+static int scheduler_port_allow_link(struct config_item *src,
+				     struct config_item *target,
+				     const char *name)
+{
+	struct scheduler_port *src_port = to_scheduler_port(src);
+	struct scheduler_pipe *peer_pipe;
+	struct string_list_object *list;
+	int err;
+
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return -EPERM;
+
+	/* At most one source connected at a given time */
+	rcu_read_lock();
+	if (connected(src_port)) {
+		rcu_read_unlock();
+		return -EPERM;
+	}
+	rcu_read_unlock();
+
+	if (!is_link_target_valid(target))
+		return -EINVAL;
+
+	if (!scheduler_types_compatible(
+		    src_port->sink.type,
+		    to_scheduler_pipe_type(target->ci_type)->source_type))
+		return -EINVAL;
+
+	list = global_config_allow_link_begin(src, name, target);
+	if (IS_ERR(list)) {
+		err = PTR_ERR(list);
+		goto err_global_begin;
+	}
+
+	config_item_get(src); /* To make sure a reference remains until drop is
+			       * finished. */
+
+	global_config_item_init(global_item_of(src_port),
+				&scheduler_port_link_drop_ops);
+	err = global_config_allow_link_end(list,
+						src,
+						global_item_of(src_port),
+						name,
+						target);
+	if (err)
+		goto err_global_end;
+
+	peer_pipe = to_scheduler_pipe(target);
+	connect(src_port, peer_pipe);
+	/*
+	 * always read source's value when the notification chain becomes
+	 * complete.
+	 */
+	if (scheduler_sink_subscribed(&src_port->sink))
+		src_port->sink.type->update_value(&src_port->sink, peer_pipe->source);
+out:
+	return err;
+
+err_global_end:
+	config_item_put(src);
+err_global_begin:
+	goto out;
+}
+
+static int scheduler_port_allow_drop_link(struct config_item *src,
+					  struct config_item *target)
+{
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return -EPERM;
+	return 0;
+}
+
+/**
+ * Configfs callback called every time symbolic link removal is initiated from
+ * a scheduler_port directory
+ */
+static int scheduler_port_drop_link(struct config_item *src,
+				    struct config_item *target)
+{
+	struct scheduler_port *src_port = to_scheduler_port(src);
+	scheduler_port_drop_peer_source(src_port);
+	return 0;
+}
+
+static void scheduler_port_release(struct config_item *item);
+
+static struct global_config_attrs *port_global_attrs(struct config_item *item)
+{
+	return &to_scheduler_port(item)->global_attrs;
+}
+
+struct global_config_item_operations port_global_item_ops = {
+	.config = {
+		.release = scheduler_port_release,
+		.show_attribute = scheduler_port_attribute_show,
+		.store_attribute = scheduler_port_attribute_store,
+		.allow_link = scheduler_port_allow_link,
+		.allow_drop_link = scheduler_port_allow_drop_link,
+		.drop_link = scheduler_port_drop_link,
+	},
+	.global_attrs = port_global_attrs,
+};
+
+static struct global_config_drop_operations scheduler_port_item_drop_ops = {
+	.drop_func = scheduler_port_peer_source_drop,
+};
+
+/*
+ * Configfs callback called when user does mkdir in a scheduler_port
+ * directory
+ */
+static struct config_group *
+scheduler_port_make_group(struct config_group *group, const char *name)
+{
+	struct scheduler_port *port = to_scheduler_port(&group->cg_item);
+	struct scheduler_port_type *peer_type;
+	struct scheduler_port *peer_port;
+	struct module *peer_owner = NULL;
+	struct string_list_object *global_list = NULL;
+	int err;
+
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return ERR_PTR(-EPERM);
+
+	/* At most one source connected at a given time */
+	rcu_read_lock();
+	if (connected(port)) {
+		rcu_read_unlock();
+		return ERR_PTR(-EBUSY);
+	}
+	rcu_read_unlock();
+
+	global_list = global_config_make_item_begin(&group->cg_item, name);
+	if (IS_ERR(global_list)) {
+		err = PTR_ERR(global_list);
+		goto err_global_begin;
+	}
+
+	/* Find the port type having the requested name */
+	spin_lock(&types_lock);
+	peer_type = port_type_find(name);
+	if (!peer_type) {
+		spin_unlock(&types_lock);
+
+		request_module("%s", name);
+
+		spin_lock(&types_lock);
+		peer_type = port_type_find(name);
+	}
+	if (peer_type) {
+		peer_owner = peer_type->pipe_type.item_type.ct_owner;
+		if (!peer_type->new || !try_module_get(peer_owner))
+			peer_type = NULL;
+	}
+	spin_unlock(&types_lock);
+	err = -ENOENT;
+	if (!peer_type)
+		goto err_type;
+
+	err = -EINVAL;
+	if (!scheduler_types_compatible(port->sink.type,
+					peer_type->pipe_type.source_type))
+		goto err_port;
+
+	/* Create the new port */
+	err = -ENOMEM;
+	peer_port = peer_type->new(name);
+	if (!peer_port)
+		goto err_port;
+
+	global_config_attrs_init_r(config_group_of(peer_port));
+	config_group_get(group); /* To make sure a reference remains until drop
+				  * is finished. */
+	global_config_item_init(global_item_of(port),
+				&scheduler_port_item_drop_ops);
+	err = global_config_make_item_end(global_list,
+					  &group->cg_item,
+					  global_item_of(port),
+					  name);
+	if (err) {
+		config_group_put(group);
+		global_config_attrs_cleanup_r(config_group_of(peer_port));
+		peer_type->destroy(peer_port);
+		module_put(peer_owner);
+		return ERR_PTR(err);
+	}
+
+	/* Connect the parent port's sink to the new port's source */
+	connect(port, &peer_port->pipe);
+
+	return config_group_of(peer_port);
+
+err_port:
+	module_put(peer_owner);
+err_type:
+	global_config_make_item_error(global_list, name);
+err_global_begin:
+	return ERR_PTR(err);
+}
+
+static int scheduler_port_allow_drop_item(struct config_group *group,
+					  struct config_item *item)
+{
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return -EPERM;
+	return 0;
+}
+
+/*
+ * Configfs callback called when user does rmdir in a scheduler port.
+ * Initiates the destruction of the child port.
+ */
+static void scheduler_port_drop_item(struct config_group *group,
+				     struct config_item *item)
+{
+	struct scheduler_port *port = to_scheduler_port(&group->cg_item);
+
+	global_config_attrs_cleanup_r(to_config_group(item));
+	scheduler_port_drop_peer_source(port);
+	config_item_put(item);
+}
+
+static struct configfs_group_operations port_group_ops = {
+	.make_group = scheduler_port_make_group,
+	.allow_drop_item = scheduler_port_allow_drop_item,
+	.drop_item = scheduler_port_drop_item,
+};
+
+int scheduler_port_init(struct scheduler_port *port,
+			const char *name,
+			struct scheduler_port_type *type,
+			struct scheduler_source *source,
+			struct config_group **default_groups)
+{
+	int err;
+
+	scheduler_sink_init(&port->sink, &type->sink_type);
+	err = scheduler_pipe_init(&port->pipe, name, &type->pipe_type,
+				  source, &port->sink, default_groups);
+	if (err)
+		return err;
+	port->peer_pipe = NULL;
+
+	return 0;
+}
+EXPORT_SYMBOL(scheduler_port_init);
+
+void scheduler_port_cleanup(struct scheduler_port *port)
+{
+	scheduler_pipe_cleanup(&port->pipe);
+	scheduler_sink_cleanup(&port->sink);
+}
+EXPORT_SYMBOL(scheduler_port_cleanup);
+
+/**
+ * Called by configfs when the last reference to a scheduler_port is dropped
+ */
+static void scheduler_port_release(struct config_item *item)
+{
+	struct scheduler_port *port = to_scheduler_port(item);
+	struct scheduler_port_type *type = scheduler_port_type_of(port);
+	struct module *owner = type->pipe_type.item_type.ct_owner;
+
+	if (type->destroy)
+		type->destroy(port);
+	module_put(owner);
+}
+
+/* Assumes types_lock held */
+static struct scheduler_port_type *port_type_find(const char *name)
+{
+	struct scheduler_port_type *type;
+	list_for_each_entry(type, &types_head, list)
+		if (!strcmp(type->name, name))
+			return type;
+	return NULL;
+}
+
+int scheduler_port_type_init(struct scheduler_port_type *type,
+			     struct configfs_attribute **attrs)
+{
+	struct scheduler_source_type *source_type = type->pipe_type.source_type;
+	struct module *owner = type->pipe_type.item_type.ct_owner;
+
+	/* Fixup type */
+	type->pipe_type = (struct scheduler_pipe_type)
+		SCHEDULER_PIPE_TYPE_INIT(owner,
+					 &port_global_item_ops.config,
+					 &port_group_ops,
+					 source_type,
+					 &type->sink_type);
+	return scheduler_pipe_type_init(&type->pipe_type, attrs);
+}
+EXPORT_SYMBOL(scheduler_port_type_init);
+
+void scheduler_port_type_cleanup(struct scheduler_port_type *type)
+{
+	scheduler_pipe_type_cleanup(&type->pipe_type);
+}
+EXPORT_SYMBOL(scheduler_port_type_cleanup);
+
+int scheduler_port_type_register(struct scheduler_port_type *type,
+				 struct configfs_attribute **attrs)
+{
+	int err;
+
+	err = scheduler_port_type_init(type, attrs);
+	if (err)
+		goto out;
+
+	err = -EEXIST;
+	spin_lock(&types_lock);
+	if (!port_type_find(type->name)) {
+		list_add_tail(&type->list, &types_head);
+		err = 0;
+	}
+	spin_unlock(&types_lock);
+	if (err)
+		goto err_add;
+
+out:
+	return err;
+err_add:
+	scheduler_port_type_cleanup(type);
+	goto out;
+}
+EXPORT_SYMBOL(scheduler_port_type_register);
+
+/* Must be called at module unload only */
+void scheduler_port_type_unregister(struct scheduler_port_type *type)
+{
+	spin_lock(&types_lock);
+	list_del(&type->list);
+	spin_unlock(&types_lock);
+
+	scheduler_port_type_cleanup(type);
+}
+EXPORT_SYMBOL(scheduler_port_type_unregister);
+
+int scheduler_port_get_remote_value(struct scheduler_port *port,
+				    kerrighed_node_t node,
+				    void *value_p, unsigned int nr,
+				    const void *in_value_p,
+				    unsigned int in_nr)
+{
+	struct scheduler_pipe *peer_pipe;
+	int ret = -EACCES;
+
+	rcu_read_lock();
+	peer_pipe = rcu_dereference(port->peer_pipe);
+	if (peer_pipe) {
+		struct scheduler_pipe_type *peer_type;
+
+		peer_type = scheduler_pipe_type_of(peer_pipe);
+		/*
+		 * If the peer pipe is a port, forward down until a
+		 * get_remote_value() method is defined or a source not being a
+		 * port is reached.
+		 */
+		if (peer_type->item_type.ct_item_ops == &port_global_item_ops.config) {
+			struct scheduler_port *peer_port =
+				container_of(peer_pipe,
+					     typeof(*peer_port), pipe);
+			struct scheduler_port_type *peer_port_type =
+				scheduler_port_type_of(peer_port);
+			port_get_remote_value_t *cb;
+
+			cb = peer_port_type->get_remote_value;
+			if (!cb)
+				cb = scheduler_port_get_remote_value;
+			ret = cb(peer_port, node,
+				 value_p, nr,
+				 in_value_p, in_nr);
+		} else {
+			ret = scheduler_pipe_get_remote_value(
+				&port->sink, peer_pipe,	node,
+				value_p, nr,
+				in_value_p, in_nr);
+		}
+	}
+	rcu_read_unlock();
+
+	return ret;
+}
+EXPORT_SYMBOL(scheduler_port_get_remote_value);
diff -ruN linux-2.6.29/kerrighed/scheduler/probe.c android_cluster/linux-2.6.29/kerrighed/scheduler/probe.c
--- linux-2.6.29/kerrighed/scheduler/probe.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/probe.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,920 @@
+/*
+ *  kerrighed/scheduler/probe.c
+ *
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/module.h>
+#include <linux/nsproxy.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/kmod.h>
+#include <linux/workqueue.h>
+#include <linux/kernel.h>
+#include <linux/jiffies.h>
+#include <linux/string.h>
+#include <kerrighed/krgflags.h>
+#include <kerrighed/scheduler/pipe.h>
+#include <kerrighed/scheduler/global_config.h>
+#include <kerrighed/scheduler/probe.h>
+
+#include "internal.h"
+
+/**
+ * This structure represents pluggable probes for measuring various system
+ * characteristics (e.g. CPU usage, memory usage, ...). User can implement these
+ * probes as separate Linux kernel modules and inserts them dynamcally into
+ * kernel. By doing this, it extends set of resource properties that are being
+ * measured.
+ * The probe module is loaded by issuing
+ * "mkdir /config/krg_scheduler/probes/<probe_name>" command. When probe is
+ * loaded it starts measuring its system characteristic. Probes can also be
+ * deactivated by issuing "rmdir /config/krg_scheduler/probes/<probe_name>"
+ * command from user space.
+ *
+ * @author Marko Novak, Louis Rilling
+ */
+struct scheduler_probe {
+	struct config_group group; /** representation of probe in ConfigFS. */
+
+	struct list_head list; /** list of registered probes. */
+
+	unsigned long probe_period; /** timeout between subsequent measurements.
+				      * Note: here, time is saved in jiffies.*/
+	struct delayed_work work; /** work struct for periodically performing
+				   * measurements. */
+
+	spinlock_t lock; /** lock for synchronizing probe accesses. */
+
+	struct global_config_item global_item; /** Used by global config
+						* subsystem */
+	struct global_config_attrs global_attrs;
+};
+
+static
+inline
+struct scheduler_probe *to_scheduler_probe(struct config_item *item)
+{
+	return container_of(to_config_group(item),
+			    struct scheduler_probe, group);
+}
+
+static
+inline
+struct scheduler_probe_type *
+scheduler_probe_type_of(struct scheduler_probe *probe)
+{
+	return container_of(probe->group.cg_item.ci_type,
+			    struct scheduler_probe_type, item_type);
+}
+
+static
+inline
+struct scheduler_probe_attribute *
+to_scheduler_probe_attribute(struct configfs_attribute *attr)
+{
+	return container_of(attr, struct scheduler_probe_attribute, config);
+}
+
+static
+inline
+struct scheduler_probe_source *
+to_scheduler_probe_source(struct config_item *item)
+{
+	return container_of(to_scheduler_pipe(item),
+			    struct scheduler_probe_source, pipe);
+}
+
+static
+inline
+struct scheduler_probe_source_type *
+scheduler_probe_source_type_of(struct scheduler_probe_source *probe_source)
+{
+	return container_of(scheduler_pipe_type_of(&(probe_source)->pipe),
+			    struct scheduler_probe_source_type, pipe_type);
+}
+
+static
+inline
+struct scheduler_probe_source_attribute *
+to_scheduler_probe_source_attribute(struct configfs_attribute *attr)
+{
+	return container_of(attr,
+			    struct scheduler_probe_source_attribute, config);
+}
+
+/* a spinlock protecting access to the list of registered probes. */
+static DEFINE_SPINLOCK(probes_lock);
+/* List of registered probes. */
+static LIST_HEAD(probes_list);
+
+void scheduler_probe_lock(struct scheduler_probe *probe)
+{
+	spin_lock(&probe->lock);
+}
+EXPORT_SYMBOL(scheduler_probe_lock);
+
+void scheduler_probe_unlock(struct scheduler_probe *probe)
+{
+	spin_unlock(&probe->lock);
+}
+EXPORT_SYMBOL(scheduler_probe_unlock);
+
+/**
+ * General function for reading probes' ConfigFS attributes.
+ * @author Marko Novak, Louis Rilling
+ */
+static ssize_t scheduler_probe_attribute_show(struct config_item *item,
+					      struct configfs_attribute *attr,
+					      char *page)
+{
+	struct scheduler_probe_attribute *probe_attr =
+		to_scheduler_probe_attribute(attr);
+	struct scheduler_probe *probe = to_scheduler_probe(item);
+	ssize_t ret = 0;
+
+	if (probe_attr->show) {
+		scheduler_probe_lock(probe);
+		ret = probe_attr->show(probe, page);
+		scheduler_probe_unlock(probe);
+	}
+
+	return ret;
+}
+
+/**
+ * General function for storing probes' ConfigFS attributes.
+ */
+static ssize_t scheduler_probe_attribute_store(struct config_item *item,
+					       struct configfs_attribute *attr,
+					       const char *page, size_t count)
+{
+	struct scheduler_probe_attribute *probe_attr =
+		to_scheduler_probe_attribute(attr);
+	struct scheduler_probe *probe = to_scheduler_probe(item);
+	struct string_list_object *list;
+	ssize_t ret = 0;
+
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return -EPERM;
+
+	if (probe_attr->store) {
+		list = global_config_attr_store_begin(item);
+		if (IS_ERR(list))
+			return PTR_ERR(list);
+
+		scheduler_probe_lock(probe);
+		ret = probe_attr->store(probe, page, count);
+		scheduler_probe_unlock(probe);
+
+		if (ret >= 0)
+			ret = global_config_attr_store_end(list,
+							   item, attr,
+							   page, ret);
+		else
+			global_config_attr_store_error(list, item);
+	}
+
+	return ret;
+}
+
+static struct global_config_attrs *probe_global_attrs(struct config_item *item)
+{
+	return &to_scheduler_probe(item)->global_attrs;
+}
+
+struct global_config_item_operations probe_global_item_ops = {
+	.config = {
+		.show_attribute = scheduler_probe_attribute_show,
+		.store_attribute = scheduler_probe_attribute_store,
+	},
+	.global_attrs = probe_global_attrs,
+};
+
+/**
+ * Function for reading "probe_period" attribute.
+ * @author Marko Novak, Louis Rilling
+ */
+static ssize_t scheduler_probe_attr_period_show(struct scheduler_probe *probe,
+						char *page)
+{
+	ssize_t ret;
+	/* print timeout in milliseconds */
+	ret = sprintf(page, "%u\n", jiffies_to_msecs(probe->probe_period));
+	return ret;
+}
+
+/**
+ * Function for storing "probe_period" attribute.
+ * @author Marko Novak, Louis Rilling
+ */
+static ssize_t scheduler_probe_attr_period_store(struct scheduler_probe *probe,
+						 const char *page, size_t count)
+{
+	unsigned tmp_period;
+	char *last_read;
+
+	tmp_period = simple_strtoul(page, &last_read, 0);
+	if (last_read - page + 1 < count
+	    || (last_read[1] != '\0' && last_read[1] != '\n'))
+		return -EINVAL;
+
+	probe->probe_period = msecs_to_jiffies(tmp_period);
+
+	return count;
+}
+
+/**
+ * "probe_period" attribute.
+ * @author Marko Novak, Louis Rilling
+ */
+static SCHEDULER_PROBE_ATTRIBUTE(scheduler_probe_attr_period,
+				 "probe_period",
+				 S_IRUGO | S_IWUSR,
+				 scheduler_probe_attr_period_show,
+				 scheduler_probe_attr_period_store);
+
+/**
+ * Determines length of a NULL-terminated array.
+ */
+static int probe_source_array_length(struct scheduler_probe_source **sources)
+{
+	int i;
+	if (!sources)
+		return 0;
+	for (i=0; sources[i] != NULL; i++)
+		;
+	return i;
+}
+
+static inline char *scheduler_probe_name(struct scheduler_probe *probe)
+{
+	return config_item_name(&probe->group.cg_item);
+}
+
+static void refresh_subscribers(struct scheduler_probe *p)
+{
+	int i;
+	struct scheduler_probe_source *tmp_ps;
+	struct scheduler_probe_source_type *tmp_pst;
+
+	/* check which measurements have changed since last time. */
+	for (i = 0; p->group.default_groups[i] != NULL; i++) {
+		tmp_ps = to_scheduler_probe_source(
+			&p->group.default_groups[i]->cg_item);
+		tmp_pst = scheduler_probe_source_type_of(tmp_ps);
+		if (tmp_pst->has_changed && tmp_pst->has_changed()) {
+			/*
+			 * if value has changed, run update function
+			 * of all the subscribers.
+			 */
+			scheduler_probe_unlock(p);
+			scheduler_source_publish(&tmp_ps->source);
+			scheduler_probe_lock(p);
+		}
+	}
+}
+
+/**
+ * General function for periodically performing probe measurements.
+ */
+static void probe_refresh_func(struct work_struct *work)
+{
+	struct scheduler_probe *p = container_of(
+		container_of(work, struct delayed_work, work),
+		struct scheduler_probe, work);
+	struct scheduler_probe_type *type = scheduler_probe_type_of(p);
+
+	scheduler_probe_lock(p);
+	if (type->perform_measurement) {
+		type->perform_measurement();
+		refresh_subscribers(p);
+	}
+	scheduler_probe_unlock(p);
+	/* schedule next measurement. */
+	schedule_delayed_work(&p->work, p->probe_period);
+}
+
+void scheduler_probe_source_lock(struct scheduler_probe_source *probe_source)
+{
+	scheduler_probe_lock(probe_source->parent);
+}
+EXPORT_SYMBOL(scheduler_probe_source_lock);
+
+void scheduler_probe_source_unlock(struct scheduler_probe_source *probe_source)
+{
+	scheduler_probe_unlock(probe_source->parent);
+}
+EXPORT_SYMBOL(scheduler_probe_source_unlock);
+
+static void __scheduler_probe_source_notify_update(struct work_struct *work)
+{
+	struct scheduler_probe_source *s =
+		container_of(work,
+			     struct scheduler_probe_source, notify_update_work);
+
+	scheduler_source_publish(&s->source);
+}
+
+/**
+ * Function that a probe source should call when the value changes and the probe
+ * does not have a perform_measurement() method.
+ * Does nothing if the probe provides a perform_measurement() method.
+ *
+ * @param source		Source having been updated
+ */
+void scheduler_probe_source_notify_update(struct scheduler_probe_source *source)
+{
+	struct scheduler_probe *p = source->parent;
+
+	if (scheduler_probe_type_of(p)->perform_measurement)
+		return;
+
+	schedule_work(&source->notify_update_work);
+}
+EXPORT_SYMBOL(scheduler_probe_source_notify_update);
+
+/**
+ * General function for reading probe sources' ConfigFS attributes.
+ * @author Marko Novak, Louis Rilling
+ */
+static
+ssize_t scheduler_probe_source_attribute_show(struct config_item *item,
+					      struct configfs_attribute *attr,
+					      char *page)
+{
+	struct scheduler_probe_source_attribute *source_attr;
+	struct scheduler_probe_source *ps = to_scheduler_probe_source(item);
+	ssize_t ret;
+	int handled;
+
+	ret = scheduler_pipe_show_attribute(&ps->pipe, attr, page, &handled);
+	if (!handled) {
+		ret = -EACCES;
+
+		source_attr = to_scheduler_probe_source_attribute(attr);
+		if (source_attr->show) {
+			scheduler_probe_source_lock(ps);
+			ret = source_attr->show(page);
+			scheduler_probe_source_unlock(ps);
+		}
+	}
+
+	return ret;
+}
+
+/**
+ * General function for storing probe sources' ConfigFS attributes.
+ * @author Marko Novak, Louis Rilling
+ */
+static
+ssize_t scheduler_probe_source_attribute_store(struct config_item *item,
+					       struct configfs_attribute *attr,
+					       const char *page, size_t count)
+{
+        struct scheduler_probe_source_attribute *source_attr =
+		to_scheduler_probe_source_attribute(attr);
+        struct scheduler_probe_source *ps = to_scheduler_probe_source(item);
+	struct string_list_object *list;
+	ssize_t ret;
+	int handled;
+
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return -EPERM;
+
+	list = global_config_attr_store_begin(item);
+	if (IS_ERR(list))
+		return PTR_ERR(list);
+
+	ret = scheduler_pipe_store_attribute(&ps->pipe, attr, page, count,
+					     &handled);
+	if (!handled) {
+		ret = -EACCES;
+		if (source_attr->store) {
+			scheduler_probe_source_lock(ps);
+			ret = source_attr->store(page, count);
+			scheduler_probe_source_unlock(ps);
+		}
+	}
+
+	if (ret >= 0)
+		ret = global_config_attr_store_end(list,
+						   item, attr,
+						   page, ret);
+	else
+		global_config_attr_store_error(list, item);
+
+        return ret;
+}
+
+static
+struct global_config_attrs *probe_source_global_attrs(struct config_item *item)
+{
+	return &to_scheduler_probe_source(item)->global_attrs;
+}
+
+struct global_config_item_operations probe_source_global_item_ops = {
+	.config = {
+		.show_attribute = scheduler_probe_source_attribute_show,
+		.store_attribute = scheduler_probe_source_attribute_store,
+	},
+	.global_attrs = probe_source_global_attrs,
+};
+
+static int probe_source_attribute_array_length(
+	struct scheduler_probe_source_attribute **attrs)
+{
+	int nr = 0;
+	if (attrs)
+		while (attrs[nr])
+			nr++;
+	return nr;
+}
+
+/**
+ * This function allocates memory and initializes a probe source.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param type		Type describing the probe source, defined with
+ *			SCHEDULER_PROBE_SOURCE_TYPE
+ * @param name		Name of the source's subdirectory in the probe's
+ *			directory. Must be unique for a given a probe.
+ *
+ * @return		Pointer to the created probe_source, or NULL if error
+ */
+struct scheduler_probe_source *
+scheduler_probe_source_create(struct scheduler_probe_source_type *type,
+			      const char *name)
+{
+	struct scheduler_probe_source *tmp_ps = NULL;
+	struct module *owner = type->pipe_type.item_type.ct_owner;
+	struct configfs_attribute **tmp_attrs;
+	int nr_attrs;
+	int err;
+
+	/* fixup type */
+	type->pipe_type = (struct scheduler_pipe_type)
+		SCHEDULER_PIPE_TYPE_INIT(owner,
+					 &probe_source_global_item_ops.config,
+					 NULL,
+					 &type->source_type, NULL);
+
+	nr_attrs = probe_source_attribute_array_length(type->attrs);
+	tmp_attrs = NULL;
+	if (nr_attrs) {
+		int i;
+
+		tmp_attrs = kmalloc((nr_attrs + 1) * sizeof(*tmp_attrs),
+				    GFP_KERNEL);
+		if (!tmp_attrs)
+			goto err_pipe_type;
+		for (i = 0; i < nr_attrs; i++)
+			tmp_attrs[i] = &type->attrs[i]->config;
+		tmp_attrs[nr_attrs] = NULL;
+	}
+	err = scheduler_pipe_type_init(&type->pipe_type, tmp_attrs);
+	kfree(tmp_attrs);
+	if (err)
+		goto err_pipe_type;
+
+	tmp_ps = kmalloc(sizeof(*tmp_ps), GFP_KERNEL);
+	if (!tmp_ps)
+		goto err_source;
+	/* initialize scheduler_probe_source. */
+	memset(tmp_ps, 0, sizeof(*tmp_ps));
+
+	scheduler_source_init(&tmp_ps->source, &type->source_type);
+	if (scheduler_pipe_init(&tmp_ps->pipe, name, &type->pipe_type,
+				&tmp_ps->source, NULL, NULL))
+		goto err_pipe;
+	INIT_WORK(&tmp_ps->notify_update_work,
+		  __scheduler_probe_source_notify_update);
+
+	return tmp_ps;
+
+err_pipe:
+	kfree(tmp_ps);
+err_source:
+	scheduler_pipe_type_cleanup(&type->pipe_type);
+err_pipe_type:
+	return NULL;
+}
+
+void scheduler_probe_source_free(struct scheduler_probe_source *source)
+{
+	struct scheduler_probe_source_type *type =
+		scheduler_probe_source_type_of(source);
+	scheduler_pipe_cleanup(&source->pipe);
+	scheduler_source_cleanup(&source->source);
+	kfree(source);
+	scheduler_pipe_type_cleanup(&type->pipe_type);
+}
+
+/**
+ * Checks that item is an source subdir of a probe.
+ * @author Louis Rilling, Marko Novak
+ *
+ * @param item		pointer to the config_item to check
+ */
+int is_scheduler_probe_source(struct config_item *item)
+{
+	return item->ci_type
+		&& item->ci_type->ct_item_ops ==
+			&probe_source_global_item_ops.config;
+}
+
+static void scheduler_probe_drop(struct global_config_item *);
+
+static struct global_config_drop_operations scheduler_probe_drop_ops = {
+	.drop_func = scheduler_probe_drop,
+	.is_symlink = 0
+};
+
+static
+int probe_attribute_array_length(struct scheduler_probe_attribute **attrs)
+{
+	int nr = 0;
+	if (attrs)
+		while (attrs[nr])
+			nr++;
+	return nr;
+}
+
+/**
+ * This function allocates memory for new probe and initializes it.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param name          name of the probe. This name must be unique for each
+ *			probe.
+ * @param attrs         array of probe's attributes.
+ * @param ops           pointer to probe's operations.
+ * @param owner         pointer to module that implements probe.
+ *
+ * @return              pointer to newly create probe or NULL if probe creation
+ *                      failed.
+ */
+struct scheduler_probe *
+scheduler_probe_create(struct scheduler_probe_type *type,
+		       const char *name,
+		       struct scheduler_probe_source **sources,
+		       struct config_group *def_groups[])
+{
+	int num_sources;
+	int nr_attrs;
+	int nr_groups;
+	int i;
+	struct config_group **tmp_def = NULL;
+	struct configfs_attribute **tmp_attrs = NULL;
+	struct scheduler_probe *tmp_probe = NULL;
+
+	num_sources = probe_source_array_length(sources);
+	nr_attrs = probe_attribute_array_length(type->attrs);
+	nr_groups = nr_def_groups(def_groups);
+	tmp_probe = kmalloc(sizeof(*tmp_probe), GFP_KERNEL);
+	/*
+	 * allocate 2 more elements in array of pointers: one for
+	 * probe_period attribute and one for NULL element which marks
+	 * the end of array.
+	 */
+	tmp_attrs = kmalloc(sizeof(*tmp_attrs) * (nr_attrs + 2), GFP_KERNEL);
+	tmp_def = kcalloc(num_sources + nr_groups + 1, sizeof(*tmp_def), GFP_KERNEL);
+
+	if (!tmp_probe || !tmp_attrs || !tmp_def)
+		goto out_kmalloc;
+
+	/* initialize attributes */
+	for (i = 0; i < nr_attrs; i++)
+		tmp_attrs[i] = &type->attrs[i]->config;
+	tmp_attrs[nr_attrs] = &scheduler_probe_attr_period.config;
+	tmp_attrs[nr_attrs + 1] = NULL;
+
+	/* initialize default groups */
+	for (i=0; i<num_sources; i++) {
+		tmp_def[i] = &sources[i]->pipe.config;
+
+		/* set current probe as parent of scheduler_probe_source. */
+		sources[i]->parent = tmp_probe;
+	}
+
+	/* append ports to default groups */
+	for (i = 0; i < nr_groups; i++)
+		tmp_def[num_sources + i] = def_groups[i];
+
+	tmp_def[num_sources + nr_groups] = NULL;
+
+	/* initialize probe type. */
+	type->item_type.ct_item_ops = &probe_global_item_ops.config;
+	type->item_type.ct_attrs = tmp_attrs;
+
+	/* initialize probe. */
+	memset(tmp_probe, 0, sizeof(*tmp_probe));
+	config_group_init_type_name(&tmp_probe->group, name, &type->item_type);
+	/* Make sure that item is cleaned only when freeing it */
+	config_item_get(&tmp_probe->group.cg_item);
+	tmp_probe->group.default_groups = tmp_def;
+	spin_lock_init(&tmp_probe->lock);
+	tmp_probe->probe_period =
+		msecs_to_jiffies(SCHEDULER_PROBE_DEFAULT_PERIOD);
+	global_config_item_init(&tmp_probe->global_item,
+				&scheduler_probe_drop_ops);
+
+	return tmp_probe;
+
+out_kmalloc:
+	kfree(tmp_probe);
+	kfree(tmp_attrs);
+	kfree(tmp_def);
+
+	return NULL;
+}
+
+/**
+ * This function frees all the memory taken by a probe.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param probe         pointer to probe whose memory we want to free.
+ */
+void scheduler_probe_free(struct scheduler_probe *probe)
+{
+	/*
+	 * We have to do this here because probes cannot guarantee that they
+	 * are not working before calling unregister.
+	 */
+	flush_scheduled_work();
+	config_group_put(&probe->group);
+	/*
+	 * free all the structures that were allocated during
+	 * scheduler_probe_create.
+	 */
+	kfree(probe->group.default_groups);
+	kfree(scheduler_probe_type_of(probe)->item_type.ct_attrs);
+	kfree(probe);
+}
+
+/**
+ * Finds probe with a given name. Returns NULL if no such probe is found.
+ *
+ * Assumes probes_lock held.
+ */
+static struct scheduler_probe *probe_find(const char *name)
+{
+        struct list_head *pos;
+        struct scheduler_probe *entry;
+
+        list_for_each(pos, &probes_list) {
+                entry = list_entry(pos, struct scheduler_probe, list);
+                if (strcmp(name, scheduler_probe_name(entry)) == 0)
+                        return entry;
+        }
+
+        return NULL;
+}
+
+/**
+ * This function is used for registering probe. This function has to
+ * be called at the end of "init_module" function for each probe's module.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param probe         pointer to the probe we wish to register.
+ *
+ * @return      0, if probe was successfully registered.
+ *              -EEXIST, if probe with same name is already registered.
+ */
+int scheduler_probe_register(struct scheduler_probe *probe)
+{
+	int ret = 0;
+
+	spin_lock(&probes_lock);
+	if (probe_find(scheduler_probe_name(probe)) != NULL)
+		ret = -EEXIST;
+	else
+		/*
+		 * ok, no probe with the same name exists, proceed with
+		 * registration
+		 */
+		list_add(&probe->list, &probes_list);
+	spin_unlock(&probes_lock);
+
+	return ret;
+}
+
+/**
+ * This function is used for removing probe registration. This function has to
+ * be called from "cleanup_module" function for each probe's module.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @param probe         pointer to the probe we wish to unregister.
+ */
+void scheduler_probe_unregister(struct scheduler_probe *probe)
+{
+	spin_lock(&probes_lock);
+	list_del(&probe->list);
+	spin_unlock(&probes_lock);
+}
+
+/**
+ * This is a configfs callback function, which is invoked every time user
+ * tries to create directory in "/krg_scheduler/probes/" subdirectory. It
+ * is used for loading probe's module, initializing and activating it.
+ *
+ * Note: the function is already synchronized since configfs takes care of
+ * locking.
+ */
+static struct config_group *probes_make_group(struct config_group *group,
+					      const char *name)
+{
+	struct config_group *ret;
+	struct scheduler_probe *tmp_probe;
+	struct scheduler_probe_type *type;
+	struct string_list_object *global_probes = NULL;
+	int err;
+
+	ret = ERR_PTR(-EPERM);
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		goto out;
+
+	if (!(current->flags & PF_KTHREAD)
+	    && !IS_KERRIGHED_NODE(KRGFLAGS_RUNNING))
+		goto out;
+
+	global_probes = global_config_make_item_begin(&group->cg_item, name);
+	if (IS_ERR(global_probes)) {
+		ret = (void *)(global_probes);
+		goto out;
+	}
+
+	spin_lock(&probes_lock);
+	tmp_probe = probe_find(name);
+	if (!tmp_probe) {
+		spin_unlock(&probes_lock);
+
+		/*
+		 * insert probe's module into kernel space.
+		 * Note: no module locking is needed, since module is already
+		 * locked by "request_module".
+		 *
+		 * note: all the probes' files have to be copied into
+		 * "/lib/modules/<version>/extra" directory and added
+		 * to "/lib/modules/<version>/modules.dep" file.
+		 */
+		request_module("%s", name);
+
+		spin_lock(&probes_lock);
+		tmp_probe = probe_find(name);
+	}
+
+	/*
+	 * if probe's module didn't manage to register itself, abort.
+         * this usually implies an error at probe initialization
+         * (in "init_module" function) or that module is already loaded
+         * in the kernel and has to be manually unloaded first.
+	 */
+	err = -ENOENT;
+	if (!tmp_probe)
+		goto err_module;
+
+	/*
+	 * configfs does try_module_get a bit too late for us because we will
+	 * already have scheduled probe refreshment.
+	 */
+	err = -EAGAIN;
+	if (!try_module_get(tmp_probe->group.cg_item.ci_type->ct_owner))
+		goto err_module;
+	spin_unlock(&probes_lock);
+
+	global_config_attrs_init_r(&tmp_probe->group);
+	err = global_config_make_item_end(global_probes,
+					  &group->cg_item,
+					  &tmp_probe->global_item,
+					  name);
+	if (err) {
+		global_config_attrs_cleanup_r(&tmp_probe->group);
+		module_put(tmp_probe->group.cg_item.ci_type->ct_owner);
+		goto err;
+	}
+
+	config_group_get(&tmp_probe->group);
+
+	/* perform measurement of resource properties for the first time. */
+	type = scheduler_probe_type_of(tmp_probe);
+	if (type->perform_measurement) {
+		scheduler_probe_lock(tmp_probe);
+		type->perform_measurement();
+		scheduler_probe_unlock(tmp_probe);
+		/* schedule next refreshment. */
+		INIT_DELAYED_WORK(&tmp_probe->work, probe_refresh_func);
+		schedule_delayed_work(&tmp_probe->work,
+				      tmp_probe->probe_period);
+	}
+
+	ret = &tmp_probe->group;
+
+out:
+	return ret;
+
+err_module:
+	spin_unlock(&probes_lock);
+	global_config_make_item_error(global_probes, name);
+err:
+	ret = ERR_PTR(err);
+	goto out;
+}
+
+/**
+ * Callback called by global_config when the probe is globally dropped
+ */
+static void scheduler_probe_drop(struct global_config_item *item)
+{
+	struct scheduler_probe *p = container_of(item,
+						 struct scheduler_probe,
+						 global_item);
+
+	global_config_attrs_cleanup_r(&p->group);
+	config_group_put(&p->group);
+	module_put(p->group.cg_item.ci_type->ct_owner);
+}
+
+static int probes_allow_drop_item(struct config_group *group,
+				  struct config_item *item)
+{
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return -EPERM;
+	return 0;
+}
+
+/**
+ * This is a configfs callback function, which is invoked every time user
+ * tries to remove directory in "/krg_scheduler/probes/" subdirectory.
+ * It is used for deactivating chosen probe.
+ *
+ * Note: the function is already synchronized since configfs takes care of
+ * locking.
+ */
+static void probes_drop_item(struct config_group *group,
+	struct config_item *item)
+{
+	struct scheduler_probe *p = to_scheduler_probe(item);
+
+	if (scheduler_probe_type_of(p)->perform_measurement)
+		cancel_rearming_delayed_work(&p->work);
+	global_config_drop(&p->global_item);
+}
+
+/**
+ * This struct is ConfigFS-specific. See ConfigFS documentation for its
+ * explanation.
+ */
+static struct configfs_group_operations probes_group_ops = {
+	.make_group = probes_make_group,
+	.allow_drop_item = probes_allow_drop_item,
+	.drop_item = probes_drop_item,
+};
+
+/**
+ * This struct is ConfigFS-specific. See ConfigFS documentation for its
+ * explanation.
+ */
+static struct config_item_type probes_type = {
+	.ct_group_ops = &probes_group_ops,
+	.ct_owner = THIS_MODULE,
+};
+
+/**
+ * This struct is ConfigFS-specific. See ConfigFS documentation for its
+ * explanation.
+ */
+static struct config_group probes_group = {
+	.cg_item = {
+		.ci_namebuf = PROBES_NAME,
+		.ci_type = &probes_type,
+	},
+};
+
+/**
+ * Initializes list of probes and all ConfigFS infrastructure.
+ * Registers "probes" subdirectory.
+ * author Marko Novak, Louis Rilling
+ */
+struct config_group *scheduler_probe_start(void)
+{
+	/* initialize and register configfs subsystem. */
+	config_group_init(&probes_group);
+	return &probes_group;
+}
+
+/**
+ * Unregisters "probes" subdirectory and all the ConfigFS infrastructure
+ * related to probes.
+ * @author Marko Novak, Louis Rilling
+ */
+void scheduler_probe_exit(void)
+{
+}
+
+EXPORT_SYMBOL(scheduler_probe_register);
+EXPORT_SYMBOL(scheduler_probe_unregister);
+EXPORT_SYMBOL(scheduler_probe_create);
+EXPORT_SYMBOL(scheduler_probe_free);
+EXPORT_SYMBOL(scheduler_probe_source_create);
+EXPORT_SYMBOL(scheduler_probe_source_free);
diff -ruN linux-2.6.29/kerrighed/scheduler/probes/cpu_probe.c android_cluster/linux-2.6.29/kerrighed/scheduler/probes/cpu_probe.c
--- linux-2.6.29/kerrighed/scheduler/probes/cpu_probe.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/probes/cpu_probe.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,287 @@
+/*
+ *  kerrighed/scheduler/probes/cpu_probe.c
+ *
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/kernel_stat.h>
+#include <linux/threads.h>
+#include <linux/cpumask.h>
+#include <linux/list.h>
+#include <kerrighed/scheduler/probe.h>
+#include <asm/cputime.h>
+
+#include "cpu_probe.h"
+
+static cpu_probe_data_t *probe_data;
+static cpu_probe_data_t *probe_data_prev;
+
+static clock_t *cpu_used;
+static clock_t *cpu_total;
+static clock_t *cpu_used_prev;
+static clock_t *cpu_total_prev;
+
+MODULE_LICENSE("LGPL");
+MODULE_AUTHOR("Marko Novak <marko.novak@xlab.si>, "
+	      "Louis Rilling <Louis.Rilling@kerlabs.com>");
+MODULE_DESCRIPTION("CPU load probe module.");
+
+
+static struct scheduler_probe *cpu_probe;
+static int active;
+
+#undef DEBUG_MONITOR
+
+#ifdef DEBUG_MONITOR
+#define PDEBUG(format, args...) printk(format, ## args)
+#else
+#define PDEBUG(format, args...)
+#endif
+
+/* calcilates CPU usage of i-th CPU (in percent). */
+static inline unsigned long calc_cpu_used(int i)
+{
+	PDEBUG(KERN_INFO "cpu_measurement %lu %lu\n", probe_data[i].cpu_used,
+		probe_data[i].cpu_total);
+	return probe_data[i].cpu_used*100 / probe_data[i].cpu_total;
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_GET_WITH_INPUT(cpu_probe_load,
+					     unsigned long, load_p, nr,
+					     int, idx_p, in_nr)
+{
+	int i;
+
+	if (in_nr)
+		/* Only show the cpus queried */
+		for (i = 0; i < in_nr && i < nr; i++) {
+			if (idx_p[i] < num_online_cpus())
+				*load_p++ = calc_cpu_used(idx_p[i]);
+			else
+				return -EINVAL;
+		}
+	else
+		/* Show as many CPUs as possible */
+		for (i = 0; i < nr && i < num_online_cpus(); i++)
+			*load_p++ = calc_cpu_used(i);
+	return i;
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_SHOW(cpu_probe_load, page)
+{
+	size_t size = SCHEDULER_PROBE_SOURCE_ATTR_SIZE;
+	ssize_t count;
+	int tmp_count;
+	int i;
+
+	PDEBUG(KERN_INFO "cpu_probe_source_show function called!\n");
+
+	tmp_count = snprintf(page, size, "%lu\n", calc_cpu_used(0));
+	count = tmp_count;
+	for(i = 1;
+	    tmp_count >= 0 && count + 1 < size && i < num_online_cpus();
+	    i++) {
+		tmp_count = snprintf(page + count, size - count,
+				     "%lu\n", calc_cpu_used(i));
+		if (tmp_count >= 0)
+			count += tmp_count;
+	}
+
+	return (tmp_count < 0) ? tmp_count : min((size_t) count + 1, size);
+}
+
+static void measure_cpu(void)
+{
+	int i;
+
+	PDEBUG("cpu_probe: start.\n");
+
+	for(i=0; i<num_online_cpus(); i++) {
+		cpu_used[i] =
+			cputime64_to_clock_t(kstat_cpu(i).cpustat.user) +
+			cputime64_to_clock_t(kstat_cpu(i).cpustat.nice) +
+			cputime64_to_clock_t(kstat_cpu(i).cpustat.system) +
+			cputime64_to_clock_t(kstat_cpu(i).cpustat.iowait) +
+			cputime64_to_clock_t(kstat_cpu(i).cpustat.irq) +
+			cputime64_to_clock_t(kstat_cpu(i).cpustat.softirq) +
+			cputime64_to_clock_t(kstat_cpu(i).cpustat.steal);
+		cpu_total[i] = cpu_used[i] +
+			cputime64_to_clock_t(kstat_cpu(i).cpustat.idle);
+
+		//spin_lock( &(probe_data[i].lock) );
+		probe_data[i].cpu_used = cpu_used[i] - cpu_used_prev[i];
+		probe_data[i].cpu_total = cpu_total[i] - cpu_total_prev[i];
+		//spin_unlock( &(probe_data[i].lock) );
+
+		PDEBUG("measurements for CPU%d: used=%llu total=%llu\n",
+			i, (unsigned long long)cpu_used[i],
+			(unsigned long long)cpu_total[i]);
+
+		cpu_used_prev[i] = cpu_used[i];
+		cpu_total_prev[i] = cpu_total[i];
+	}
+
+	PDEBUG("cpu_probe: done\n.");
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_HAS_CHANGED(cpu_probe_load)
+{
+	int i;
+	int isChanged = 0;
+
+	if (!active)
+		return 0;
+
+	for (i=0; i<num_online_cpus(); i++) {
+		if (probe_data[i].cpu_used!=probe_data_prev[i].cpu_used ||
+			probe_data[i].cpu_total!=probe_data_prev[i].cpu_total) {
+
+			isChanged = 1;
+			break;
+		}
+	}
+
+	if (isChanged) {
+		for (i=0; i<num_online_cpus(); i++) {
+			probe_data_prev[i] = probe_data[i];
+		}
+	}
+
+	return isChanged;
+}
+
+static BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(cpu_probe_load),
+	.SCHEDULER_PROBE_SOURCE_GET(cpu_probe_load),
+	.SCHEDULER_PROBE_SOURCE_SHOW(cpu_probe_load),
+	.SCHEDULER_PROBE_SOURCE_VALUE_TYPE(cpu_probe_load, unsigned long),
+	.SCHEDULER_PROBE_SOURCE_PARAM_TYPE(cpu_probe_load, int),
+	.SCHEDULER_PROBE_SOURCE_HAS_CHANGED(cpu_probe_load),
+END_SCHEDULER_PROBE_SOURCE_TYPE(cpu_probe_load);
+
+static struct scheduler_probe_source *cpu_probe_sources[2];
+
+static ssize_t active_attr_show(struct scheduler_probe *probe, char *page)
+{
+	return sprintf(page, "%d", active);
+}
+
+static ssize_t active_attr_store(struct scheduler_probe *probe,
+				 const char *page, size_t count)
+{
+	int new_active;
+	char *last_read;
+
+	new_active = simple_strtoul(page, &last_read, 0);
+	if (last_read - page + 1 < count
+	    || (last_read[1] != '\0' && last_read[1] != '\n'))
+		return -EINVAL;
+
+	active = !!new_active;
+	return count;
+}
+
+static SCHEDULER_PROBE_ATTRIBUTE(active_attr, "active", 0644,
+				 active_attr_show, active_attr_store);
+
+static struct scheduler_probe_attribute *cpu_probe_attrs[] = {
+	&active_attr,
+	NULL
+};
+
+static SCHEDULER_PROBE_TYPE(cpu_probe_type, cpu_probe_attrs, measure_cpu);
+
+int init_module()
+{
+	int i;
+	int err = -ENOMEM;
+
+	probe_data = (cpu_probe_data_t *)kmalloc(sizeof(cpu_probe_data_t)*num_online_cpus(), GFP_KERNEL);
+	probe_data_prev = (cpu_probe_data_t *)kmalloc(sizeof(cpu_probe_data_t)*num_online_cpus(), GFP_KERNEL);
+	cpu_used = (clock_t *)kmalloc(sizeof(clock_t)*num_online_cpus(), GFP_KERNEL);
+	cpu_total = (clock_t *)kmalloc(sizeof(clock_t)*num_online_cpus(), GFP_KERNEL);
+	cpu_used_prev = (clock_t *)kmalloc(sizeof(clock_t)*num_online_cpus(), GFP_KERNEL);
+	cpu_total_prev = (clock_t *)kmalloc(sizeof(clock_t)*num_online_cpus(), GFP_KERNEL);
+
+	if (probe_data == NULL || probe_data_prev==NULL || cpu_used==NULL ||
+	    cpu_total==NULL || cpu_used_prev==NULL || cpu_total_prev==NULL) {
+		printk(KERN_ALERT "cpu_probe initialization failed: cannot"
+		       " allocate memory for internal structures!\n");
+		goto out_kmalloc;
+	}
+
+        cpu_probe_sources[0] = scheduler_probe_source_create(
+		&cpu_probe_load_type,
+		"cpu_usage");
+	cpu_probe_sources[1] = NULL;
+
+	if (cpu_probe_sources[0] == NULL) {
+		printk(KERN_ERR "error: cannot initialize cpu_probe "
+			"attributes\n");
+		goto out_probe_sources_init;
+	}
+
+	for (i=0; i<num_online_cpus(); i++) {
+		cpu_used_prev[i] = 0;
+		cpu_total_prev[i] = 0;
+	}
+	cpu_probe = scheduler_probe_create(&cpu_probe_type, CPU_PROBE_NAME,
+					   cpu_probe_sources, NULL);
+	if (cpu_probe == NULL){
+		printk(KERN_ERR "error: cpu_probe creation failed!\n");
+		goto out_kmalloc;
+	}
+
+	// perform first measurement
+	measure_cpu();
+	for (i=0; i<num_online_cpus(); i++) {
+		probe_data_prev[i] = probe_data[i];
+	}
+
+	err = scheduler_probe_register(cpu_probe);
+	if (err)
+		goto err_register;
+
+	return 0;
+
+err_register:
+	scheduler_probe_free(cpu_probe);
+out_probe_sources_init:
+	if (cpu_probe_sources[0] != NULL)
+		scheduler_probe_source_free(cpu_probe_sources[0]);
+
+out_kmalloc:
+	if (probe_data)
+		kfree(probe_data);
+	if (probe_data_prev)
+		kfree(probe_data_prev);
+	if (cpu_used)
+		kfree(cpu_used);
+	if (cpu_total)
+		kfree(cpu_total);
+	if (cpu_used_prev)
+		kfree(cpu_used_prev);
+	if (cpu_total_prev)
+		kfree(cpu_total_prev);
+
+	return err;
+}
+
+void cleanup_module()
+{
+	int i;
+
+	PDEBUG(KERN_INFO "cpu_probe cleanup function called!\n");
+	scheduler_probe_unregister(cpu_probe);
+	scheduler_probe_free(cpu_probe);
+	for (i = 0; cpu_probe_sources[i] != NULL; i++)
+		scheduler_probe_source_free(cpu_probe_sources[i]);
+	kfree(probe_data);
+	kfree(probe_data_prev);
+	kfree(cpu_used);
+	kfree(cpu_total);
+	kfree(cpu_used_prev);
+	kfree(cpu_total_prev);
+}
diff -ruN linux-2.6.29/kerrighed/scheduler/probes/cpu_probe.h android_cluster/linux-2.6.29/kerrighed/scheduler/probes/cpu_probe.h
--- linux-2.6.29/kerrighed/scheduler/probes/cpu_probe.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/probes/cpu_probe.h	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,11 @@
+#ifndef __KRG_CPU_PROBE_H__
+#define __KRG_CPU_PROBE_H__
+
+#define CPU_PROBE_NAME "cpu_probe"
+
+typedef struct cpu_probe_data {
+	clock_t cpu_used;
+	clock_t cpu_total;
+} cpu_probe_data_t;
+
+#endif /* __KRG_CPU_PROBE_H__ */
diff -ruN linux-2.6.29/kerrighed/scheduler/probes/Makefile android_cluster/linux-2.6.29/kerrighed/scheduler/probes/Makefile
--- linux-2.6.29/kerrighed/scheduler/probes/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/probes/Makefile	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,7 @@
+obj-$(CONFIG_KRG_SCHED_CPU_PROBE) += cpu_probe.o
+obj-$(CONFIG_KRG_SCHED_MEM_PROBE) += mem_probe.o
+
+obj-$(CONFIG_KRG_SCHED_MIGRATION_PROBE) += migration_probe.o
+obj-$(CONFIG_KRG_SCHED_MOSIX_PROBE) += mosix_probe.o
+
+EXTRA_CFLAGS += -Werror -Wall
diff -ruN linux-2.6.29/kerrighed/scheduler/probes/mem_probe.c android_cluster/linux-2.6.29/kerrighed/scheduler/probes/mem_probe.c
--- linux-2.6.29/kerrighed/scheduler/probes/mem_probe.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/probes/mem_probe.c	2014-05-27 23:04:10.474026927 -0700
@@ -0,0 +1,247 @@
+/*
+ *  kerrighed/scheduler/probes/mem_probe.c
+ *
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <kerrighed/scheduler/probe.h>
+
+#include "mem_probe.h"
+
+#define K(x) ((x) << (PAGE_SHIFT - 10))
+
+static mem_probe_data_t probe_data;
+static mem_probe_data_t probe_data_prev;
+
+MODULE_LICENSE("LGPL");
+MODULE_AUTHOR("Marko Novak <marko.novak@xlab.si>, "
+	      "Louis Rilling <Louis.Rilling@kerlabs.com>");
+MODULE_DESCRIPTION("Memory probe module.");
+
+static struct scheduler_probe *mem_probe;
+static int mem_free_active = 1;
+static int mem_total_active = 1;
+
+#undef DEBUG_MONITOR
+
+#ifdef DEBUG_MONITOR
+#define PDEBUG(format, args...) printk(format, ## args)
+#else
+#define PDEBUG(format, args...)
+#endif
+
+DEFINE_SCHEDULER_PROBE_SOURCE_GET(mem_free, unsigned long, value_p, nr)
+{
+	//spin_lock(&probe_data.lock);
+	*value_p = K(probe_data.ram_free);
+	//spin_unlock(&probe_data.lock);
+	return 1;
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_SHOW(mem_free, page)
+{
+	ssize_t ret;
+
+	//spin_lock(&probe_data.lock);
+	ret = sprintf(page, "%lu\n", K(probe_data.ram_free));
+	//spin_unlock(&probe_data.lock);
+
+	return ret;
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_GET(mem_total, unsigned long, value_p, nr)
+{
+	//spin_lock(&probe_data.lock);
+	*value_p = K(probe_data.ram_total);
+	//spin_unlock(&probe_data.lock);
+	return 1;
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_SHOW(mem_total, page)
+{
+	ssize_t ret;
+
+	//spin_lock(&probe_data.lock);
+	ret = sprintf(page, "%lu\n", K(probe_data.ram_total));
+	//spin_unlock(&probe_data.lock);
+
+	return ret;
+}
+
+static void measure_mem(void)
+{
+	struct sysinfo meminfo;
+
+	PDEBUG("mem_probe: start.\n");
+
+	si_meminfo(&meminfo);
+
+	//spin_lock(&probe_data.lock);
+	probe_data.ram_free = meminfo.freeram;
+	probe_data.ram_total = meminfo.totalram;
+	//spin_unlock( &(probe_data.lock) );
+
+	PDEBUG("mem_probe: finished.\n");
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_HAS_CHANGED(mem_total)
+{
+	int isChanged = 0;
+
+	if (mem_total_active &&
+	    probe_data.ram_total != probe_data_prev.ram_total) {
+		isChanged = 1;
+
+		probe_data_prev.ram_total = probe_data.ram_total;
+	}
+
+	return isChanged;
+}
+
+static ssize_t mem_total_active_attr_show(char *page)
+{
+	return sprintf(page, "%d", mem_total_active);
+}
+
+static ssize_t mem_total_active_attr_store(const char *page, size_t count)
+{
+	int new_active;
+	char *last_read;
+
+	new_active = simple_strtoul(page, &last_read, 0);
+	if (last_read - page + 1 < count
+	    || (last_read[1] != '\0' && last_read[1] != '\n'))
+		return -EINVAL;
+
+	mem_total_active = !!new_active;
+	return count;
+}
+
+static SCHEDULER_PROBE_SOURCE_ATTRIBUTE(mem_total_active_attr, "active", 0644,
+					mem_total_active_attr_show,
+					mem_total_active_attr_store);
+
+static struct scheduler_probe_source_attribute *mem_total_attrs[] = {
+	&mem_total_active_attr,
+	NULL
+};
+
+static BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(mem_total),
+	.SCHEDULER_PROBE_SOURCE_GET(mem_total),
+	.SCHEDULER_PROBE_SOURCE_SHOW(mem_total),
+	.SCHEDULER_PROBE_SOURCE_VALUE_TYPE(mem_total, unsigned long),
+	.SCHEDULER_PROBE_SOURCE_HAS_CHANGED(mem_total),
+	.SCHEDULER_PROBE_SOURCE_ATTRS(mem_total, mem_total_attrs),
+END_SCHEDULER_PROBE_SOURCE_TYPE(mem_total);
+
+DEFINE_SCHEDULER_PROBE_SOURCE_HAS_CHANGED(mem_free)
+{
+        int isChanged = 0;
+
+        if (mem_free_active
+	    && probe_data.ram_free != probe_data_prev.ram_free) {
+                isChanged = 1;
+
+                probe_data_prev.ram_free = probe_data.ram_free;
+        }
+
+        return isChanged;
+}
+
+static ssize_t mem_free_active_attr_show(char *page)
+{
+	return sprintf(page, "%d", mem_free_active);
+}
+
+static ssize_t mem_free_active_attr_store(const char *page, size_t count)
+{
+	int new_active;
+	char *last_read;
+
+	new_active = simple_strtoul(page, &last_read, 0);
+	if (last_read - page + 1 < count
+	    || (last_read[1] != '\0' && last_read[1] != '\n'))
+		return -EINVAL;
+
+	mem_free_active = !!new_active;
+	return count;
+}
+
+static SCHEDULER_PROBE_SOURCE_ATTRIBUTE(mem_free_active_attr, "active", 0644,
+					mem_free_active_attr_show,
+					mem_free_active_attr_store);
+
+static struct scheduler_probe_source_attribute *mem_free_attrs[] = {
+	&mem_free_active_attr,
+	NULL
+};
+
+static BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(mem_free),
+	.SCHEDULER_PROBE_SOURCE_GET(mem_free),
+	.SCHEDULER_PROBE_SOURCE_SHOW(mem_free),
+	.SCHEDULER_PROBE_SOURCE_VALUE_TYPE(mem_free, unsigned long),
+	.SCHEDULER_PROBE_SOURCE_HAS_CHANGED(mem_free),
+	.SCHEDULER_PROBE_SOURCE_ATTRS(mem_free, mem_free_attrs),
+END_SCHEDULER_PROBE_SOURCE_TYPE(mem_free);
+
+static struct scheduler_probe_source *mem_probe_sources[3];
+
+static SCHEDULER_PROBE_TYPE(mem_probe_type, NULL, measure_mem);
+
+int init_module()
+{
+	int err = -ENOMEM;
+
+	mem_probe_sources[0] = scheduler_probe_source_create(&mem_free_type,
+							     "ram_free");
+	mem_probe_sources[1] = scheduler_probe_source_create(&mem_total_type,
+							     "ram_total");
+	mem_probe_sources[2] = NULL;
+
+	if (mem_probe_sources[0]==NULL || mem_probe_sources[1]==NULL) {
+		printk(KERN_ERR "error: cannot initialize mem_probe "
+			"attributes\n");
+		goto out_kmalloc;
+	}
+
+	mem_probe = scheduler_probe_create(&mem_probe_type, MEM_PROBE_NAME,
+					   mem_probe_sources, NULL);
+	if (mem_probe == NULL) {
+		printk(KERN_ERR "error: mem_probe creation failed!\n");
+		goto out_kmalloc;
+	}
+
+	// perform first measurement
+	measure_mem();
+	probe_data_prev = probe_data;
+
+	err = scheduler_probe_register(mem_probe);
+	if (err)
+		goto err_register;
+
+	return 0;
+
+err_register:
+	scheduler_probe_free(mem_probe);
+out_kmalloc:
+	if (mem_probe_sources[0] != NULL)
+		scheduler_probe_source_free(mem_probe_sources[0]);
+	if (mem_probe_sources[1] != NULL)
+		scheduler_probe_source_free(mem_probe_sources[1]);
+
+	return err;
+}
+
+void cleanup_module()
+{
+	int i;
+	PDEBUG(KERN_INFO "mem_probe cleanup function called!\n");
+	scheduler_probe_unregister(mem_probe);
+	scheduler_probe_free(mem_probe);
+	for (i = 0; mem_probe_sources[i] != NULL; i++)
+		scheduler_probe_source_free(mem_probe_sources[i]);
+}
diff -ruN linux-2.6.29/kerrighed/scheduler/probes/mem_probe.h android_cluster/linux-2.6.29/kerrighed/scheduler/probes/mem_probe.h
--- linux-2.6.29/kerrighed/scheduler/probes/mem_probe.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/probes/mem_probe.h	2014-05-27 23:04:10.478026844 -0700
@@ -0,0 +1,11 @@
+#ifndef __KERRIGHED_MONITOR_MEM_PROBE_H__
+#define __KERRIGHED_MONITOR_MEM_PROBE_H__
+
+#define MEM_PROBE_NAME "mem_probe"
+
+typedef struct mem_probe_data {
+	unsigned long ram_free;
+	unsigned long ram_total;
+} mem_probe_data_t;
+
+#endif /* __KERRIGHED_MONITOR_MEM_PROBE_H__ */
diff -ruN linux-2.6.29/kerrighed/scheduler/probes/migration_probe.c android_cluster/linux-2.6.29/kerrighed/scheduler/probes/migration_probe.c
--- linux-2.6.29/kerrighed/scheduler/probes/migration_probe.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/probes/migration_probe.c	2014-05-27 23:04:10.478026844 -0700
@@ -0,0 +1,208 @@
+/*
+ *  kerrighed/scheduler/probes/migration_probe.c
+ *
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ *
+ *  Based on former analyzer.c by Renaud Lottiaux
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ */
+
+#include <linux/module.h>
+#include <linux/ktime.h>
+#include <kerrighed/migration.h>
+#include <kerrighed/scheduler/probe.h>
+#include <asm/atomic.h>
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Louis Rilling <Louis.Rilling@kerlabs.com>");
+MODULE_DESCRIPTION("Probe tracking migrations");
+
+static ktime_t last_migration_raised;
+static atomic_t migration_on_going = ATOMIC_INIT(0);
+
+static struct scheduler_probe *migration_probe;
+static struct scheduler_probe_source *migration_probe_sources[3];
+
+DEFINE_SCHEDULER_PROBE_SOURCE_GET(migration_probe_last_migration,
+				  ktime_t, last_p, nr)
+{
+	if (likely(nr)) {
+		/*
+		 * Note: scheduler_probe_source is already locked by the
+		 * framework.
+		 */
+		*last_p = last_migration_raised;
+		return 1;
+	}
+	return 0;
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_SHOW(migration_probe_last_migration, page)
+{
+	/* TODO: Should convert to wall time to have a meaning in userspace */
+	return sprintf(page, "%lld\n", ktime_to_ns(last_migration_raised));
+}
+
+static BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(migration_probe_last_migration),
+	.SCHEDULER_PROBE_SOURCE_GET(migration_probe_last_migration),
+	.SCHEDULER_PROBE_SOURCE_SHOW(migration_probe_last_migration),
+	.SCHEDULER_PROBE_SOURCE_VALUE_TYPE(migration_probe_last_migration,
+					   ktime_t),
+END_SCHEDULER_PROBE_SOURCE_TYPE(migration_probe_last_migration);
+
+DEFINE_SCHEDULER_PROBE_SOURCE_GET(migration_probe_migration_ongoing,
+				  int, count_p, nr)
+{
+	if (likely(nr)) {
+		*count_p = atomic_read(&migration_on_going);
+		return 1;
+	}
+	return 0;
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_SHOW(migration_probe_migration_ongoing, page)
+{
+	return sprintf(page, "%d\n", atomic_read(&migration_on_going));
+}
+
+static BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(migration_probe_migration_ongoing),
+	.SCHEDULER_PROBE_SOURCE_GET(migration_probe_migration_ongoing),
+	.SCHEDULER_PROBE_SOURCE_SHOW(migration_probe_migration_ongoing),
+	.SCHEDULER_PROBE_SOURCE_VALUE_TYPE(migration_probe_migration_ongoing,
+					   int),
+END_SCHEDULER_PROBE_SOURCE_TYPE(migration_probe_migration_ongoing);
+
+static SCHEDULER_PROBE_TYPE(migration_probe_type, NULL, NULL);
+
+static void migration_probe_migration_start(struct task_struct *task)
+{
+	struct timespec now_ts;
+
+	atomic_inc(&migration_on_going);
+	scheduler_probe_source_notify_update(migration_probe_sources[1]);
+
+	ktime_get_ts(&now_ts);
+	scheduler_probe_source_lock(migration_probe_sources[0]);
+	last_migration_raised = timespec_to_ktime(now_ts);
+	scheduler_probe_source_unlock(migration_probe_sources[0]);
+	scheduler_probe_source_notify_update(migration_probe_sources[0]);
+}
+
+static void kmcb_migration_start(unsigned long arg)
+{
+	migration_probe_migration_start((struct task_struct *) arg);
+}
+
+static void migration_probe_migration_end(struct task_struct *task)
+{
+	if (!task) {
+		atomic_dec(&migration_on_going);
+		scheduler_probe_source_notify_update(
+			migration_probe_sources[1]);
+	}
+}
+
+static void kmcb_migration_end(unsigned long arg)
+{
+	migration_probe_migration_end((struct task_struct *) arg);
+}
+
+static void kmcb_migration_aborted(unsigned long arg)
+{
+	migration_probe_migration_end(NULL);
+}
+
+static int probe_not_registered;
+
+int migration_probe_start(void)
+{
+	int err = -ENOMEM;
+	char *err_msg = NULL;
+
+	migration_probe_sources[0] = scheduler_probe_source_create(
+		&migration_probe_last_migration_type,
+		"last_migration");
+	if (!migration_probe_sources[0])
+		goto err_last_migration;
+	migration_probe_sources[1] = scheduler_probe_source_create(
+		&migration_probe_migration_ongoing_type,
+		"migration_on_going");
+	if (!migration_probe_sources[1])
+		goto err_migration_on_going;
+	migration_probe_sources[2] = NULL;
+
+	migration_probe = scheduler_probe_create(&migration_probe_type,
+						 "migration_probe",
+						 migration_probe_sources,
+						 NULL);
+	if (!migration_probe)
+		goto err_probe;
+
+	/*
+	 * We cannot call unregister in init, so the system may have to live
+	 * with partial hook init until module is unloaded.
+	 */
+	err = module_hook_register(&kmh_migration_start, kmcb_migration_start,
+				   THIS_MODULE);
+	if (err)
+		goto err_hooks;
+	err = module_hook_register(&kmh_migration_end, kmcb_migration_end,
+				   THIS_MODULE);
+	if (err)
+		goto err_other_hooks;
+	err = module_hook_register(&kmh_migration_aborted,
+				   kmcb_migration_aborted,
+				   THIS_MODULE);
+	if (err)
+		goto err_other_hooks;
+
+	err = scheduler_probe_register(migration_probe);
+	if (err)
+		goto err_register;
+
+out:
+	return err;
+
+err_hooks:
+	scheduler_probe_free(migration_probe);
+err_probe:
+	scheduler_probe_source_free(migration_probe_sources[1]);
+err_migration_on_going:
+	scheduler_probe_source_free(migration_probe_sources[0]);
+err_last_migration:
+	goto out;
+
+err_other_hooks:
+	if (!err_msg)
+		err_msg = "inconsistent hooks initialization";
+err_register:
+	probe_not_registered = 1;
+	if (!err_msg)
+		err_msg = "could not register probe";
+
+	printk(KERN_ERR "[%s] error %d: %s!\n"
+	       "Module cannot cleanly self-unload.\n"
+	       "Please unload the module.\n",
+	       __PRETTY_FUNCTION__, err, err_msg);
+	err = 0;
+	goto out;
+}
+
+void migration_probe_exit(void)
+{
+	int i;
+
+	if (!probe_not_registered)
+		scheduler_probe_unregister(migration_probe);
+
+	module_hook_unregister(&kmh_migration_aborted, kmcb_migration_aborted);
+	module_hook_unregister(&kmh_migration_end, kmcb_migration_end);
+	module_hook_unregister(&kmh_migration_start, kmcb_migration_start);
+
+	scheduler_probe_free(migration_probe);
+	for (i = 0; migration_probe_sources[i] != NULL; i++)
+		scheduler_probe_source_free(migration_probe_sources[i]);
+}
+
+module_init(migration_probe_start);
+module_exit(migration_probe_exit);
diff -ruN linux-2.6.29/kerrighed/scheduler/probes/mosix_probe.c android_cluster/linux-2.6.29/kerrighed/scheduler/probes/mosix_probe.c
--- linux-2.6.29/kerrighed/scheduler/probes/mosix_probe.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/probes/mosix_probe.c	2014-05-27 23:04:10.478026844 -0700
@@ -0,0 +1,767 @@
+/*
+ *  kerrighed/scheduler/probes/mosix_probe.c
+ *
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ *
+ *  Based on Kerrighed/modules/scheduler_old/mosix_probe.c:
+ *  Copyright (C) 1999-2006 INRIA, Universite de Rennes 1, EDF
+ *  Copyright (C) 2006-2007 Louis Rilling - Kerlabs
+ */
+
+/**
+ *  Processor load computation.
+ *  @file mosix_probe.c
+ *
+ *  Implementation of processor load computation functions.
+ *  It is a simplified version of the MOSIX functions.
+ *
+ *  Original work by Amnon Shiloh and Amnon Barak.
+ *
+ *  @author Louis Rilling, Renaud Lottiaux, Marko Novak
+ */
+
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/rcupdate.h>
+#include <linux/jiffies.h>
+#include <kerrighed/pid.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/scheduler/info.h>
+#include <kerrighed/scheduler/hooks.h>
+#include <kerrighed/scheduler/probe.h>
+
+#include "mosix_probe.h"
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Louis Rilling <Louis.Rilling@kerlabs.com>");
+MODULE_DESCRIPTION("CPU load probe based on MOSIX algorithms");
+
+/* Compute processor load every second */
+#define CF HZ
+
+/* Speed of the standard processor */
+/* #define STD_SPD 10000 */
+/* Not used. Set this to 1 avoid long overflow in load compuation. */
+#define STD_SPD 1
+
+/* #define PROCESS_MEAN_LOAD_SCALE 4 */
+#define PROCESS_MEAN_LOAD_SCALE 1
+
+/* Values taken from MOSIX */
+#define MEAN_LOAD_SCALE 128
+
+#define MEAN_LOAD_DECAY 5
+#define MEAN_LOAD_NEW_DATA 3
+
+#define UPPER_LOAD_DECAY 7
+#define UPPER_LOAD_NEW_DATA 1
+
+#define CPU_USE_DECAY 3
+#define CPU_USE_NEW_DATA 1
+
+struct mosix_probe_info {
+	struct krg_sched_module_info module_info;
+	unsigned int load;    /**< Estimated load:
+			       * approx. 4 * ticks used in CF ticks */
+	unsigned int last_on; /**< load_ticks + 1 when last put on runqueue,
+			       * 0 if not on runqueue */
+	unsigned int ran;     /**< number of ticks used
+			       * since last call to mp_calc_load */
+};
+
+static inline
+struct mosix_probe_info *
+to_mosix_probe_info(struct krg_sched_module_info *sched_info)
+{
+	return container_of(sched_info, struct mosix_probe_info, module_info);
+}
+
+struct mosix_probe_data mosix_data;
+/* struct mosix_probe_data mosix_data_prev; */
+
+/* Load informations readable from outside in the cluster */
+unsigned long cpu_speed = 1;  /* Speed of each CPU */
+
+unsigned long mean_load = 0; /* Value scaled by MEAN_LOAD_SCALE */
+unsigned long upper_load = 0;
+
+/* Load accumulators which are the basis to compute the machine load */
+unsigned long load_adder;
+unsigned int load_ticks = CF;
+
+static struct scheduler_probe *mosix_probe;
+enum mosix_probe_source_t {
+	VALUE_MEAN_LOAD,
+	VALUE_UPPER_LOAD,
+	VALUE_NORM_MEAN_LOAD,
+	VALUE_NORM_UPPER_LOAD,
+	VALUE_SINGLE_PROCESS_LOAD,
+	VALUE_NORM_SINGLE_PROCESS_LOAD,
+	VALUE_PROCESS_LOAD,
+	NR_VALUES,
+};
+static struct scheduler_probe_source *mosix_probe_sources[NR_VALUES + 1];
+
+
+/* static u64 curr_jiffies; */
+/* static u64 prev_jiffies; */
+
+/**
+ *  Function to initialize load informations of a process.
+ *
+ *  @param task		task on which info relates
+ *  @param info		task info structure for mosix probe
+ */
+static void mosix_probe_init_info(struct task_struct *task,
+				  struct mosix_probe_info *info)
+{
+	info->load = 0;
+	if (task->state == TASK_RUNNING)
+		info->last_on = load_ticks + 1;
+	else
+		info->last_on = 0;
+	info->ran = 0;
+}
+
+static struct krg_sched_module_info *
+mosix_probe_info_copy(struct task_struct *task,
+		      struct krg_sched_module_info *info)
+{
+	struct mosix_probe_info *new_info;
+
+	new_info = kmalloc(sizeof(*new_info), GFP_KERNEL);
+	if (new_info) {
+		mosix_probe_init_info(task, new_info);
+		return &new_info->module_info;
+	}
+	return NULL;
+}
+
+static void mosix_probe_info_free(struct krg_sched_module_info *info)
+{
+	kfree(to_mosix_probe_info(info));
+}
+
+static int mosix_probe_info_export(struct epm_action *action,
+				   struct ghost *ghost,
+				   struct krg_sched_module_info *info)
+{
+	/* nothing to do */
+	return 0;
+}
+
+static struct krg_sched_module_info *
+mosix_probe_info_import(struct epm_action *action,
+			struct ghost *ghost,
+			struct task_struct *task)
+{
+	return mosix_probe_info_copy(task, NULL);
+}
+
+static struct krg_sched_module_info_type mosix_probe_module_info_type = {
+	.name = "mosix probe",
+	.owner = THIS_MODULE,
+	.copy = mosix_probe_info_copy,
+	.free = mosix_probe_info_free,
+	.export = mosix_probe_info_export,
+	.import = mosix_probe_info_import
+};
+
+/* Must be called under rcu_read_lock() */
+static struct mosix_probe_info *get_mosix_probe_info(struct task_struct *task)
+{
+	struct krg_sched_module_info *mod_info;
+
+	mod_info = krg_sched_module_info_get(task,
+					     &mosix_probe_module_info_type);
+	if (mod_info)
+		return to_mosix_probe_info(mod_info);
+	else
+		return NULL;
+}
+
+static inline unsigned long new_mean_load(unsigned long old_load,
+					  unsigned long new_load)
+{
+	return ((old_load * MEAN_LOAD_DECAY + new_load * MEAN_LOAD_NEW_DATA)
+		/ (MEAN_LOAD_DECAY + MEAN_LOAD_NEW_DATA));
+}
+
+static inline unsigned long new_upper_load(unsigned long old_load,
+					   unsigned long new_load)
+{
+	return ((old_load * UPPER_LOAD_DECAY + new_load * UPPER_LOAD_NEW_DATA)
+		/ (UPPER_LOAD_DECAY + UPPER_LOAD_NEW_DATA));
+}
+
+static inline unsigned new_cpu_use(unsigned old_use,
+				   unsigned new_use)
+{
+	return ((old_use * CPU_USE_DECAY + new_use * CPU_USE_NEW_DATA
+		 + CPU_USE_DECAY + CPU_USE_NEW_DATA - 1)
+		/ (CPU_USE_DECAY + CPU_USE_NEW_DATA));
+}
+
+/**
+ *  Function to update the processor load generated by each process, according
+ *  to their stats.
+ *
+ *  @param ticks   Number of clock ticks since the last update.
+ */
+static void update_processes_load(unsigned int ticks)
+{
+	struct task_struct *tsk;
+	struct mosix_probe_info *p;
+
+	rcu_read_lock();
+
+	for_each_process(tsk) {
+		if (unlikely(tsk->exit_state))
+			continue;
+		if (unlikely(!(task_pid_knr(tsk) & GLOBAL_PID_MASK)))
+			continue;
+		if (unlikely(!(p = get_mosix_probe_info(tsk))))
+			continue;
+
+		if (p->last_on) {
+			p->ran += ticks + 1 - p->last_on;
+			p->last_on = 1;
+		}
+
+		p->load = new_mean_load(p->load,
+					(p->ran * PROCESS_MEAN_LOAD_SCALE * CF)
+					/ ticks);
+		p->ran = 0;
+	}
+
+	rcu_read_unlock();
+}
+
+/**
+ *  Function to compute load stats of the last execution period of a process.
+ *  We only monitor Kerrighed processes.
+ *
+ *  @param tsk   Process concerned.
+ */
+static void mp_process_off(struct task_struct *tsk)
+{
+	struct mosix_probe_info *p;
+
+	rcu_read_lock();
+	p = get_mosix_probe_info(tsk);
+	if (p) {
+		p->ran += load_ticks + 1 - p->last_on;
+		p->last_on = 0;
+	}
+	rcu_read_unlock();
+}
+
+static void kmcb_process_off(unsigned long arg)
+{
+	mp_process_off((struct task_struct *) arg);
+}
+
+/**
+ *  Function to initialize load stats of a process for a new execution period.
+ *  We only monitor kerrighed processes.
+ *
+ *  @param tsk   Process concerned.
+ */
+static void mp_process_on(struct task_struct *tsk)
+{
+	struct mosix_probe_info *p;
+
+	rcu_read_lock();
+	p = get_mosix_probe_info(tsk);
+	if (p)
+		p->last_on = load_ticks + 1;
+	rcu_read_unlock();
+}
+
+static void kmcb_process_on(unsigned long arg)
+{
+	mp_process_on((struct task_struct *) arg);
+}
+
+/**
+ *  Function to update the processor load of the node.
+ *  It is called approximatively every CF clock ticks. (CF being equal
+ *  to HZ, called every second)
+ */
+static void mp_calc_load(void)
+{
+	unsigned long scaled_load;
+	unsigned long load;
+	unsigned int ticks;
+/*	unsigned use; */
+
+	load = load_adder;   /* Accumulated number of processes
+			      * since last call */
+	ticks = load_ticks;  /* Number of clock ticks since last call */
+/*	use = cpu_use;       /\* Accumulated (ticks * num_cpu) efficiently used  */
+/*				since last call -- not really computed *\/ */
+
+	/* Reset the stats for the next period */
+	load_adder = 0;
+	load_ticks = 0;
+/*	cpu_use = 0; */
+
+	/* Make the load time-of-measure independent */
+	load = (load * CF) / ticks;
+
+	scaled_load = load * MEAN_LOAD_SCALE;
+
+	if (scaled_load > mean_load)
+		mean_load = new_mean_load(mean_load, scaled_load);
+	else
+		mean_load = scaled_load;
+
+	if (load >= upper_load)
+		upper_load = load;
+	else
+		upper_load = new_upper_load(upper_load, load);
+
+	mosix_data.mosix_mean_load = (mean_load + MEAN_LOAD_SCALE / 2) / MEAN_LOAD_SCALE;
+/*		+ mosix_single_process_load; */
+	mosix_data.mosix_upper_load = upper_load;
+/*		+ mosix_single_process_load; */
+
+	mosix_data.mosix_norm_mean_load = (mosix_data.mosix_mean_load * STD_SPD) / cpu_speed;
+	mosix_data.mosix_norm_upper_load = (mosix_data.mosix_upper_load * STD_SPD) / cpu_speed;
+
+	update_processes_load(ticks);
+}
+
+/**
+ *  Function to accumulate load stats at each clock tick.
+ *  Called each time calc_load is called.
+ *  This function is called in the timer interrupt,
+ *  with the xtime_lock write lock held.
+ *
+ *  @param ticks   Clock ticks since last called.
+ */
+static void kmcb_accumulate_load(unsigned long ticks)
+{
+	unsigned long load;
+
+	load_adder += nr_running();
+/*	cpu_use += ticks * num_online_cpus(); */
+	load_ticks += ticks;
+
+	/* CF is equal to HZ, which means load is computed every second */
+	if (load_ticks >= CF) {
+		mp_calc_load();
+
+		load = mosix_data.mosix_mean_load;
+
+		scheduler_probe_source_notify_update(
+			mosix_probe_sources[VALUE_MEAN_LOAD]);
+		scheduler_probe_source_notify_update(
+			mosix_probe_sources[VALUE_UPPER_LOAD]);
+		scheduler_probe_source_notify_update(
+			mosix_probe_sources[VALUE_NORM_MEAN_LOAD]);
+		scheduler_probe_source_notify_update(
+			mosix_probe_sources[VALUE_NORM_UPPER_LOAD]);
+/*		if (load >= ALARM_THRESHOLD) */
+/*			send_alarm_to_analyzer(); */
+	}
+}
+
+static void mosix_probe_init_variables(void)
+{
+	load_adder = CF * (nr_running() - 1);
+/*	accurate_use = CF * num_online_cpus(); */
+/*	cpu_use = CF * num_online_cpus(); */
+/*	mosix_data.mosix_single_process_load = CF / num_online_cpus(); */
+	mosix_data.mosix_single_process_load = CF;
+	mosix_data.mosix_norm_single_process_load =
+		mosix_data.mosix_single_process_load * STD_SPD / cpu_speed;
+
+/*         |+ called each time calc_load is called +| */
+/*         hook_register(&kh_calc_load, kcb_accumulate_load); */
+/*         |+ called when a process is added to the run queue +| */
+/*         hook_register(&kh_process_on, kcb_process_on); */
+/*         |+ called when a process is removed from the run queue +| */
+/*         hook_register(&kh_process_off, kcb_process_off); */
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_GET(value_mean_load, unsigned long, value_p, nr)
+{
+	*value_p = mosix_data.mosix_mean_load;
+	return 1;
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_SHOW(value_mean_load, page)
+{
+	return sprintf(page, "%lu\n", mosix_data.mosix_mean_load);
+}
+
+/* DEFINE_SCHEDULER_PROBE_SOURCE_HAS_CHANGED(value_mean_load) */
+/* { */
+/*	int isChanged = 0; */
+/*	if (mosix_data.mosix_mean_load != mosix_data_prev.mosix_mean_load) { */
+/*		isChanged = 1; */
+/*		mosix_data_prev.mosix_mean_load = mosix_data.mosix_mean_load; */
+/*	} */
+
+/*	return isChanged; */
+/* } */
+
+static BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(value_mean_load),
+	.SCHEDULER_PROBE_SOURCE_GET(value_mean_load),
+	.SCHEDULER_PROBE_SOURCE_SHOW(value_mean_load),
+	.SCHEDULER_PROBE_SOURCE_VALUE_TYPE(value_mean_load, unsigned long),
+/*	.SCHEDULER_PROBE_SOURCE_HAS_CHANGED(value_mean_load), */
+END_SCHEDULER_PROBE_SOURCE_TYPE(value_mean_load);
+
+DEFINE_SCHEDULER_PROBE_SOURCE_GET(value_upper_load, unsigned long, value_p, nr)
+{
+	*value_p = upper_load;
+	return 1;
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_SHOW(value_upper_load, page)
+{
+        return sprintf(page, "%lu\n", mosix_data.mosix_upper_load);
+}
+
+/* DEFINE_SCHEDULER_PROBE_SOURCE_HAS_CHANGED(value_upper_load) */
+/* { */
+/*         int isChanged = 0; */
+/*         if (mosix_data.mosix_upper_load != mosix_data_prev.mosix_upper_load) { */
+/*                 isChanged = 1; */
+/*                 mosix_data_prev.mosix_upper_load = mosix_data.mosix_upper_load; */
+/*         } */
+
+/*         return isChanged; */
+
+/* } */
+
+static BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(value_upper_load),
+	.SCHEDULER_PROBE_SOURCE_GET(value_upper_load),
+	.SCHEDULER_PROBE_SOURCE_SHOW(value_upper_load),
+	.SCHEDULER_PROBE_SOURCE_VALUE_TYPE(value_upper_load, unsigned long),
+/*	.SCHEDULER_PROBE_SOURCE_HAS_CHANGED(value_upper_load), */
+END_SCHEDULER_PROBE_SOURCE_TYPE(value_upper_load);
+
+DEFINE_SCHEDULER_PROBE_SOURCE_GET(value_single_process_load,
+				  unsigned long, value_p, nr)
+{
+	*value_p = mosix_data.mosix_single_process_load;
+	return 1;
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_SHOW(value_single_process_load, page)
+{
+        return sprintf(page, "%lu\n", mosix_data.mosix_single_process_load);
+}
+
+/* DEFINE_SCHEDULER_PROBE_SOURCE_HAS_CHANGED(value_single_process_load) */
+/* { */
+/*         int isChanged = 0; */
+/*         if (mosix_data.mosix_single_process_load !=  */
+/*		mosix_data_prev.mosix_single_process_load) { */
+
+/*                 isChanged = 1; */
+/*                 mosix_data_prev.mosix_single_process_load =  */
+/*			mosix_data.mosix_single_process_load; */
+/*         } */
+
+/*         return isChanged; */
+
+/* } */
+
+static BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(value_single_process_load),
+	.SCHEDULER_PROBE_SOURCE_GET(value_single_process_load),
+	.SCHEDULER_PROBE_SOURCE_SHOW(value_single_process_load),
+	.SCHEDULER_PROBE_SOURCE_VALUE_TYPE(value_single_process_load,
+					   unsigned long),
+/*	.SCHEDULER_PROBE_SOURCE_HAS_CHANGED(value_single_process_load), */
+END_SCHEDULER_PROBE_SOURCE_TYPE(value_single_process_load);
+
+DEFINE_SCHEDULER_PROBE_SOURCE_GET(value_norm_mean_load,
+				  unsigned long, value_p, nr)
+{
+	*value_p = mosix_data.mosix_norm_mean_load;
+	return 1;
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_SHOW(value_norm_mean_load, page)
+{
+        return sprintf(page, "%lu\n", mosix_data.mosix_norm_mean_load);
+}
+
+/* DEFINE_SCHEDULER_PROBE_SOURCE_HAS_CHANGED(value_norm_mean_load) */
+/* { */
+/*         int isChanged = 0; */
+/*         if (mosix_data.mosix_norm_mean_load !=  */
+/*		mosix_data_prev.mosix_norm_mean_load) { */
+
+/*                 isChanged = 1; */
+/*                 mosix_data_prev.mosix_norm_mean_load =  */
+/*			mosix_data.mosix_norm_mean_load; */
+/*         } */
+
+/*         return isChanged; */
+/* } */
+
+static BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(value_norm_mean_load),
+	.SCHEDULER_PROBE_SOURCE_GET(value_norm_mean_load),
+	.SCHEDULER_PROBE_SOURCE_SHOW(value_norm_mean_load),
+	.SCHEDULER_PROBE_SOURCE_VALUE_TYPE(value_norm_mean_load, unsigned long),
+/*	.SCHEDULER_PROBE_SOURCE_HAS_CHANGED(value_norm_mean_load), */
+END_SCHEDULER_PROBE_SOURCE_TYPE(value_norm_mean_load);
+
+DEFINE_SCHEDULER_PROBE_SOURCE_GET(value_norm_upper_load,
+				  unsigned long, value_p, nr)
+{
+	*value_p = mosix_data.mosix_norm_upper_load;
+	return 1;
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_SHOW(value_norm_upper_load, page)
+{
+        return sprintf(page, "%lu\n", mosix_data.mosix_norm_upper_load);
+}
+
+/* DEFINE_SCHEDULER_PROBE_SOURCE_HAS_CHANGED(value_norm_upper_load) */
+/* { */
+/*         int isChanged = 0; */
+/*         if (mosix_data.mosix_norm_upper_load != */
+/*                 mosix_data_prev.mosix_norm_upper_load) { */
+
+/*                 isChanged = 1; */
+/*                 mosix_data_prev.mosix_norm_upper_load = */
+/*                         mosix_data.mosix_norm_upper_load; */
+/*         } */
+
+/*         return isChanged; */
+/* } */
+
+static BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(value_norm_upper_load),
+	.SCHEDULER_PROBE_SOURCE_GET(value_norm_upper_load),
+	.SCHEDULER_PROBE_SOURCE_SHOW(value_norm_upper_load),
+	.SCHEDULER_PROBE_SOURCE_VALUE_TYPE(value_norm_upper_load,
+					   unsigned long),
+/*	.SCHEDULER_PROBE_SOURCE_HAS_CHANGED(value_norm_upper_load), */
+END_SCHEDULER_PROBE_SOURCE_TYPE(value_norm_upper_load);
+
+DEFINE_SCHEDULER_PROBE_SOURCE_GET(value_norm_single_process_load,
+		       unsigned long, value_p, nr)
+{
+	*value_p = mosix_data.mosix_norm_single_process_load;
+	return 1;
+}
+
+DEFINE_SCHEDULER_PROBE_SOURCE_SHOW(value_norm_single_process_load, page)
+{
+        return sprintf(page, "%lu\n",
+		       mosix_data.mosix_norm_single_process_load);
+}
+
+/* DEFINE_SCHEDULER_PROBE_SOURCE_HAS_CHANGED(value_norm_single_process_load) */
+/* { */
+/*         int isChanged = 0; */
+/*         if (mosix_data.mosix_norm_single_process_load != */
+/*                 mosix_data_prev.mosix_norm_single_process_load) { */
+
+/*                 isChanged = 1; */
+/*                 mosix_data_prev.mosix_norm_single_process_load = */
+/*                         mosix_data.mosix_norm_single_process_load; */
+/*         } */
+
+/*         return isChanged; */
+/* } */
+
+static BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(value_norm_single_process_load),
+	.SCHEDULER_PROBE_SOURCE_GET(value_norm_single_process_load),
+	.SCHEDULER_PROBE_SOURCE_SHOW(value_norm_single_process_load),
+	.SCHEDULER_PROBE_SOURCE_VALUE_TYPE(value_norm_single_process_load,
+					   unsigned long),
+/*	.SCHEDULER_PROBE_SOURCE_HAS_CHANGED(value_norm_single_process_load), */
+END_SCHEDULER_PROBE_SOURCE_TYPE(value_norm_single_process_load);
+
+DEFINE_SCHEDULER_PROBE_SOURCE_GET_WITH_INPUT(value_process_load,
+					     unsigned long, value_p, nr,
+					     pid_t, in_value_p, in_nr)
+{
+	pid_t pid;
+	struct task_struct *task;
+	struct mosix_probe_info *info = NULL;
+	int i;
+
+	rcu_read_lock();
+	for (i = 0; i < in_nr && i < nr; i++) {
+		pid = in_value_p[i];
+
+		task = find_task_by_kpid(pid);
+		if (!task)
+			break;
+		info = get_mosix_probe_info(task);
+		if (!info)
+			break;
+
+		value_p[i] = info->load;
+	}
+	rcu_read_unlock();
+
+	return i;
+}
+
+static BEGIN_SCHEDULER_PROBE_SOURCE_TYPE(value_process_load),
+	.SCHEDULER_PROBE_SOURCE_GET(value_process_load),
+	.SCHEDULER_PROBE_SOURCE_VALUE_TYPE(value_process_load, unsigned long),
+	.SCHEDULER_PROBE_SOURCE_PARAM_TYPE(value_process_load, pid_t),
+END_SCHEDULER_PROBE_SOURCE_TYPE(value_process_load);
+
+/* static void measure_mosix(void) */
+/* { */
+/*	curr_jiffies = get_jiffies_64(); */
+/*	load_adder += nr_running(); */
+/*	load_ticks += (curr_jiffies - prev_jiffies); */
+/*	mp_calc_load(); */
+/*	prev_jiffies = curr_jiffies; */
+/* } */
+
+static SCHEDULER_PROBE_TYPE(mosix_probe_type, NULL, NULL /* measure_mosix */);
+
+static int mod_info_not_registered;
+static int probe_not_registered;
+
+int mosix_probe_init(void)
+{
+	int err = -ENOMEM;
+	char *err_msg = NULL;
+	int i;
+
+	mosix_probe_init_variables();
+
+	mosix_probe_sources[VALUE_MEAN_LOAD] =
+		scheduler_probe_source_create(&value_mean_load_type,
+					      "mean_load");
+        mosix_probe_sources[VALUE_UPPER_LOAD] =
+		scheduler_probe_source_create(&value_upper_load_type,
+					      "upper_load");
+        mosix_probe_sources[VALUE_SINGLE_PROCESS_LOAD] =
+		scheduler_probe_source_create(&value_single_process_load_type,
+					      "single_process_load");
+        mosix_probe_sources[VALUE_NORM_MEAN_LOAD] =
+		scheduler_probe_source_create(&value_norm_mean_load_type,
+					      "norm_mean_load");
+        mosix_probe_sources[VALUE_NORM_UPPER_LOAD] =
+		scheduler_probe_source_create(&value_norm_upper_load_type,
+					      "norm_upper_load");
+        mosix_probe_sources[VALUE_NORM_SINGLE_PROCESS_LOAD] =
+		scheduler_probe_source_create(
+			&value_norm_single_process_load_type,
+			"norm_single_process_load");
+        mosix_probe_sources[VALUE_PROCESS_LOAD] =
+		scheduler_probe_source_create(&value_process_load_type,
+					      "process_load");
+	mosix_probe_sources[NR_VALUES] = NULL;
+
+	for (i=0; i<NR_VALUES; i++) {
+		if (mosix_probe_sources[i] == NULL) {
+			printk(KERN_ERR "error: cannot initialize mosix probe "
+			       "values\n");
+			goto out_kmalloc;
+		}
+	}
+
+	mosix_probe = scheduler_probe_create(&mosix_probe_type,
+					     MOSIX_PROBE_NAME,
+					     mosix_probe_sources,
+					     NULL);
+	if (mosix_probe == NULL) {
+		printk(KERN_ERR "error: mosix_probe creation failed\n");
+		goto out_kmalloc;
+	}
+
+/*         |+ perform first measurement +| */
+/*         measure_mosix(); */
+/*         mosix_data_prev = mosix_data; */
+
+/*	curr_jiffies = get_jiffies_64(); */
+/*	prev_jiffies = get_jiffies_64(); */
+
+	/*
+	 * We cannot call unregister in init, so the system may have to live
+	 * with partial hook init until module is unloaded.
+	 */
+	err = module_hook_register(&kmh_process_on, kmcb_process_on,
+				   THIS_MODULE);
+	if (err)
+		goto err_hooks;
+	err = module_hook_register(&kmh_process_off, kmcb_process_off,
+				   THIS_MODULE);
+	if (err)
+		goto err_other_hooks;
+	err = module_hook_register(&kmh_calc_load, kmcb_accumulate_load,
+				   THIS_MODULE);
+	if (err)
+		goto err_other_hooks;
+
+	err = krg_sched_module_info_register(&mosix_probe_module_info_type);
+	if (err)
+		goto err_mod_info;
+
+	err = scheduler_probe_register(mosix_probe);
+	if (err)
+		goto err_register;
+
+out:
+	return err;
+
+err_hooks:
+	scheduler_probe_free(mosix_probe);
+out_kmalloc:
+	for (i=0; i<NR_VALUES; i++)
+		if (mosix_probe_sources[i])
+			scheduler_probe_source_free(mosix_probe_sources[i]);
+	goto out;
+
+err_other_hooks:
+	if (!err_msg)
+		err_msg = "inconsistent hooks initialization";
+err_mod_info:
+	mod_info_not_registered = 1;
+	if (!err_msg)
+		err_msg = "could not finish module initialization";
+err_register:
+	probe_not_registered = 1;
+	if (!err_msg)
+		err_msg = "could not register probe";
+
+	printk(KERN_ERR "[%s] error %d: %s!\n"
+	       "Module cannot cleanly self-unload.\n"
+	       "Please unload the module.\n",
+	       __PRETTY_FUNCTION__, err, err_msg);
+	err = 0;
+	goto out;
+}
+
+void mosix_probe_exit(void)
+{
+	int i=0;
+
+	if (!probe_not_registered)
+		scheduler_probe_unregister(mosix_probe);
+
+	if (!mod_info_not_registered)
+		krg_sched_module_info_unregister(&mosix_probe_module_info_type);
+
+	module_hook_unregister(&kmh_calc_load, kmcb_accumulate_load);
+	module_hook_unregister(&kmh_process_off, kmcb_process_off);
+	module_hook_unregister(&kmh_process_on, kmcb_process_on);
+
+	scheduler_probe_free(mosix_probe);
+	while (mosix_probe_sources[i] != NULL) {
+		scheduler_probe_source_free(mosix_probe_sources[i]);
+		i++;
+	}
+}
+
+module_init(mosix_probe_init);
+module_exit(mosix_probe_exit);
diff -ruN linux-2.6.29/kerrighed/scheduler/probes/mosix_probe.h android_cluster/linux-2.6.29/kerrighed/scheduler/probes/mosix_probe.h
--- linux-2.6.29/kerrighed/scheduler/probes/mosix_probe.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/probes/mosix_probe.h	2014-05-27 23:04:10.478026844 -0700
@@ -0,0 +1,29 @@
+/**
+ *  Processor load computation.
+ *  @file mosix_probe.h
+ *
+ *  Implementation of processor load computation functions.
+ *  It is a simplified version of the MOSIX functions.
+ *
+ *  Original work by Amnon Shiloh and Amnon Barak.
+ *
+ *  @author Louis Rilling, ported to configfs by Marko Novak and Louis Rilling
+ */
+
+#ifndef __KRG_MOSIX_PROBE_H__
+#define __KRG_MOSIX_PROBE_H__
+
+#define MOSIX_PROBE_NAME "mosix_probe"
+
+struct mosix_probe_data {
+	unsigned long mosix_mean_load; /* Load computed and used locally:
+	                                * increases slowly, decreases quickly */
+	unsigned long mosix_upper_load; /* Load given to the outside: increases
+					 * quickly, decreases slowly */
+	unsigned long mosix_single_process_load;
+	unsigned long mosix_norm_mean_load; /* Normalized mean load */
+	unsigned long mosix_norm_upper_load; /* Normalized upper load */
+	unsigned long mosix_norm_single_process_load;
+};
+
+#endif /* __KRG_MOSIX_PROBE_H__ */
diff -ruN linux-2.6.29/kerrighed/scheduler/process_set.c android_cluster/linux-2.6.29/kerrighed/scheduler/process_set.c
--- linux-2.6.29/kerrighed/scheduler/process_set.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/process_set.c	2014-05-27 23:04:10.478026844 -0700
@@ -0,0 +1,823 @@
+/*
+ *  kerrighed/scheduler/process_set.c
+ *
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/kernel.h>
+#include <linux/nsproxy.h>
+#include <linux/pid.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/rcupdate.h>
+#include <linux/rculist.h>
+#include <linux/mutex.h>
+#include <linux/module.h>
+#include <linux/err.h>
+#include <kerrighed/pid.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/ghost.h>
+#include <kerrighed/action.h>
+#endif
+#include <kerrighed/scheduler/global_config.h>
+#include <kerrighed/scheduler/process_set.h>
+
+#include "internal.h"
+
+static inline struct process_set *to_process_set(struct config_item *item)
+{
+	return container_of(to_config_group(item), struct process_set, group);
+}
+
+static
+inline
+struct process_subset *to_process_subset(struct config_item *item)
+{
+	return container_of(to_config_group(item),
+			    struct process_subset, group);
+}
+
+static
+inline
+struct process_set_element *to_process_set_element(struct config_item *item)
+{
+	return container_of(item, struct process_set_element, item);
+}
+
+/**
+ * Internal structure for representing process set attributes.
+ */
+struct pset_attribute {
+	struct configfs_attribute attr;
+
+	ssize_t (*show)(struct process_set *, char *);
+	ssize_t (*store)(struct process_set *, const char *, size_t);
+};
+
+static
+inline
+struct pset_attribute *to_pset_attribute(struct configfs_attribute *attr)
+{
+	return container_of(attr, struct pset_attribute, attr);
+}
+
+static const char *process_subset_names[] = {
+	[PIDTYPE_PID] = "single_processes",
+	[PIDTYPE_PGID] = "process_groups",
+	[PIDTYPE_SID] = "process_sessions"
+};
+
+LIST_HEAD(process_set_handle_all_head);
+DEFINE_SPINLOCK(process_set_link_lock);
+
+static void process_set_element_destroy(struct process_set_element *pset_el);
+
+/* Configfs callback when ref count of a process_set_element reaches 0 */
+static void process_set_element_release(struct config_item *item)
+{
+	struct process_set_element *pset_el = to_process_set_element(item);
+	process_set_element_destroy(pset_el);
+}
+
+static struct configfs_item_operations process_element_item_ops = {
+	.release = process_set_element_release,
+};
+
+static struct config_item_type process_element_type = {
+        .ct_owner = THIS_MODULE,
+	.ct_item_ops = &process_element_item_ops,
+};
+
+/**
+ * Callback called by global_config when the element is globally dropped
+ */
+static void process_set_element_drop(struct global_config_item *item)
+{
+	struct process_set_element *pset_el =
+		container_of(item, struct process_set_element, global_item);
+
+	config_item_put(&pset_el->item);
+}
+
+static struct global_config_drop_operations process_set_element_drop_ops = {
+	.drop_func = process_set_element_drop,
+};
+
+/**
+ * Create a new process_set_element
+ *
+ * @param id		id of the new element
+ *
+ * @return		pointer to the new element, or
+ *			NULL
+ */
+static struct process_set_element *process_set_element_new(pid_t id)
+{
+	struct process_set_element *pset_el = kmalloc(sizeof(*pset_el),
+						      GFP_KERNEL);
+	if (!pset_el)
+		goto err_alloc;
+
+	memset(pset_el, 0, sizeof(*pset_el));
+	if (config_item_set_name(&pset_el->item, "%d", id))
+		goto err_name;
+	pset_el->item.ci_type = &process_element_type;
+	config_item_init(&pset_el->item);
+	pset_el->id = id;
+	pset_el->pid = NULL;
+	pset_el->in_subset = 0;
+	global_config_item_init(&pset_el->global_item,
+				&process_set_element_drop_ops);
+
+	return pset_el;
+
+err_name:
+	kfree(pset_el);
+err_alloc:
+	return NULL;
+}
+
+/**
+ * Free all memory allocated for a process_set_element
+ *
+ * @param pset_el	process_set_element to free
+ */
+static void process_set_element_destroy(struct process_set_element *pset_el)
+{
+	kfree(pset_el);
+}
+
+/**
+ * Link a process_set_element to the local matching struct pid
+ *
+ * @param pset_el	process_set_element to link
+ * @param pid		struct pid to link the process_set_element with
+ * @param type		pid_type of the process_set_element
+ */
+static inline
+void __process_set_element_link(struct process_set_element *pset_el,
+				struct pid *pid, enum pid_type type)
+{
+	pset_el->pid = get_pid(pid);
+	spin_lock(&process_set_link_lock);
+	hlist_add_head_rcu(&pset_el->pid_node,
+			   &pset_el->pid->process_sets[type]);
+	spin_unlock(&process_set_link_lock);
+}
+
+/**
+ * Link a process_set_element to the local matching struct pid, if it exists
+ *
+ * @param pset_el	process_set_element to link
+ * @param type		pid_type of the process_set_element
+ */
+static inline void process_set_element_link(struct process_set_element *pset_el,
+					    enum pid_type type)
+{
+	struct pid *pid;
+	rcu_read_lock();
+	pid = find_kpid(pset_el->id);
+	if (pid)
+		__process_set_element_link(pset_el, pid, type);
+	rcu_read_unlock();
+}
+
+/**
+ * Unlink a process_set_element from the local matching pid, if it exists
+ *
+ * @param pset_el	process_set_element to unlink
+ */
+static inline
+void process_set_element_unlink(struct process_set_element *pset_el)
+{
+	if (pset_el->pid) {
+		spin_lock(&process_set_link_lock);
+		hlist_del_rcu(&pset_el->pid_node);
+		spin_unlock(&process_set_link_lock);
+		put_pid(pset_el->pid);
+		pset_el->pid = NULL;
+	}
+}
+
+/**
+ * Checks whether a process_set_element is in a subset
+ * WARNING: caller must handle race conditions with process_subset_add_element
+ * and process_subset_remove_element
+ *
+ * @param pset_el	process_set_element to test
+ *
+ * @return		non 0 if process_set_element is an subset,
+ *			0 otherwise
+ */
+static inline
+int process_set_element_in_subset(struct process_set_element *pset_el)
+{
+	return pset_el->in_subset;
+}
+
+/**
+ * Checks whether a process_set_element is linked to a pid
+ * WARNING: caller must handle race conditions with process_set_element_link
+ * and process_set_element_unlink
+ *
+ * @param pset_el	process_set_element to test
+ *
+ * @return		non 0 if process_set_element is linked to a pid,
+ *			0 otherwise
+ */
+static inline
+int process_set_element_linked(struct process_set_element *pset_el)
+{
+	return !!pset_el->pid;
+}
+
+static inline enum pid_type process_subset_type(struct process_subset *psubset)
+{
+	struct process_set *pset;
+	pset = to_process_set(psubset->group.cg_item.ci_parent);
+	return psubset - pset->subsets;
+}
+
+/**
+ * Low-level function to add an element to a process subset
+ *
+ * @param psubset	subset to host the new element
+ * @param pset_el	element to add the subset
+ */
+static inline
+void process_subset_add_element(struct process_subset *psubset,
+				struct process_set_element *pset_el)
+{
+	list_add_rcu(&pset_el->list, &psubset->elements_head);
+	pset_el->in_subset = 1;
+	process_set_element_link(pset_el, process_subset_type(psubset));
+}
+
+/**
+ * Low-level function to remove an element from a process subset
+ *
+ * @param psubset	subset hosting the element to remove
+ * @param pset_el	element to remove from the subset
+ */
+static inline
+void process_subset_remove_element(struct process_subset *psubset,
+				   struct process_set_element *pset_el)
+{
+	process_set_element_unlink(pset_el);
+	pset_el->in_subset = 0;
+	list_del_rcu(&pset_el->list);
+}
+
+/**
+ * Check whether a process subset is empty
+ *
+ * @param psubset	subset to check
+ *
+ * @return		non 0 if psubset is empty,
+ *			0 otherwise
+ */
+static inline int process_subset_empty(struct process_subset *psubset)
+{
+	return list_empty(&psubset->elements_head);
+}
+
+/**
+ * Adds ID to a subset.
+ */
+static
+struct config_item *process_subset_make_item(struct config_group *group,
+					      const char *name)
+{
+	struct config_item *ret;
+	struct process_set_element *pset_el;
+	struct process_set *pset = to_process_set(group->cg_item.ci_parent);
+	struct process_subset *psubset = to_process_subset(&group->cg_item);
+	struct string_list_object *global_ids;
+	pid_t id;
+	int err;
+
+	err = -EPERM;
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		goto err;
+
+	global_ids = global_config_make_item_begin(&group->cg_item, name);
+	if (IS_ERR(global_ids)) {
+		err = PTR_ERR(global_ids);
+		goto err;
+	}
+
+	/* convert string to PID number */
+	/* TODO: ensure that name is exactly an integer */
+	err = -EINVAL;
+	if (sscanf(name, "%d", &id) != 1)
+		goto err_id;
+
+	err = -ENOMEM;
+	pset_el = process_set_element_new(id);
+	if (!pset_el)
+		goto err_pset_el;
+
+	/* add ID to the list */
+	/*
+	 * if particular scheduler already handles all the processes, there is
+	 * no use in adding IDs to the list.
+	 */
+	err = -EPERM;
+	process_set_lock(pset);
+	if (process_set_contains_all(pset)) {
+		process_set_unlock(pset);
+		goto err_handle_all;
+	}
+	process_subset_add_element(psubset, pset_el);
+	process_set_unlock(pset);
+
+	err = global_config_make_item_end(global_ids,
+					  &group->cg_item,
+					  &pset_el->global_item,
+					  config_item_name(&pset_el->item));
+	if (err) {
+		/*
+		 * TODO: may make handle_all setting fail even if it succeeded
+		 * on another node
+		 */
+		process_set_lock(pset);
+		process_subset_remove_element(psubset, pset_el);
+		process_set_unlock(pset);
+		synchronize_rcu();
+		config_item_put(&pset_el->item);
+		goto err;
+	}
+
+	ret = &pset_el->item;
+
+	return ret;
+
+err_handle_all:
+	config_item_put(&pset_el->item);
+err_pset_el:
+err_id:
+	global_config_make_item_error(global_ids, name);
+err:
+	return ERR_PTR(err);
+}
+
+static int process_subset_allow_drop_item(struct config_group *group,
+					  struct config_item *item)
+{
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return -EPERM;
+	return 0;
+}
+
+/**
+ * Removes ID from a subset.
+ */
+static void process_subset_drop_item(struct config_group *group,
+				     struct config_item *item)
+{
+	struct process_subset *psubset = to_process_subset(&group->cg_item);
+	struct process_set *pset = to_process_set(group->cg_item.ci_parent);
+	struct process_set_element *pset_el = to_process_set_element(item);
+
+	process_set_lock(pset);
+	process_subset_remove_element(psubset, pset_el);
+	process_set_unlock(pset);
+	synchronize_rcu();
+
+	global_config_drop(&pset_el->global_item);
+}
+
+static struct configfs_group_operations process_subset_group_ops = {
+	.make_item = process_subset_make_item,
+	.allow_drop_item = process_subset_allow_drop_item,
+	.drop_item = process_subset_drop_item,
+};
+
+static struct config_item_type process_subset_item_type = {
+	.ct_owner = THIS_MODULE,
+	.ct_group_ops = &process_subset_group_ops,
+};
+
+/**
+ * Initialize a process_subset
+ *
+ * @param psubset	process_subset to initialize
+ * @param name		directory name of the subset
+ */
+static void process_subset_init(struct process_subset *psubset,
+				const char *name)
+{
+	memset(&psubset->group, 0, sizeof(psubset->group));
+	config_group_init_type_name(&psubset->group,
+				    name, &process_subset_item_type);
+	INIT_LIST_HEAD(&psubset->elements_head);
+}
+
+/**
+ * Cleanup a process_subset
+ * Must be called before freeing the object containing the subset
+ *
+ * @param psubset	process_subset to cleanup
+ */
+static void process_subset_cleanup(struct process_subset *psubset)
+{
+	config_group_put(&psubset->group);
+}
+
+/**
+ * Shows value of "handle_all" attribute.
+ */
+static ssize_t pset_handle_all_show(struct process_set *pset, char *page)
+{
+	ssize_t ret;
+	/*
+	 * We do not really care about locking here, since things may change
+	 * before userspace gets the result anyway.
+	 */
+	ret = sprintf(page, "%d\n", process_set_contains_all(pset));
+	return ret;
+}
+
+/**
+ * Stores value of "handle_all" attribute.
+ */
+static ssize_t pset_handle_all_store(struct process_set *pset,
+				     const char *page, size_t count)
+{
+	short int val;
+	enum pid_type type;
+	ssize_t ret;
+
+	process_set_lock(pset);
+
+	/*
+	 * Do not accept to handle all processes as long as specific ones live
+	 * in the set
+	 */
+	for (type = 0; type < PIDTYPE_MAX; type++)
+		if (!process_subset_empty(&pset->subsets[type])) {
+			ret = -EPERM;
+			goto out;
+		}
+
+	if (sscanf(page, "%hd", &val) != 1) {
+		ret = -EINVAL;
+	} else {
+		spin_lock(&process_set_link_lock);
+		if (val <= 0 && process_set_contains_all(pset)) {
+			/*
+			 * if user inserted 0 or negative number, corresponding
+			 * scheduler doesn't handle all processes.
+			 */
+			pset->handle_all = 0;
+			list_del_rcu(&pset->handle_all_list);
+		} else if (val > 0 && !process_set_contains_all(pset)) {
+			/*
+			 * if user inserted positive number, corresponding
+			 * scheduler handles all processes.
+			 */
+			list_add_rcu(&pset->handle_all_list,
+				     &process_set_handle_all_head);
+			pset->handle_all = 1;
+		}
+		spin_unlock(&process_set_link_lock);
+		ret = count;
+	}
+
+out:
+	process_set_unlock(pset);
+	/*
+	 * Be sure to not re-add pset to the handle all list before concurrent
+	 * list traversals end
+	 */
+	synchronize_rcu();
+	return ret;
+}
+
+/**
+ * The "handle_all" attribute determines if particular set contains all
+ * processes.
+ */
+static struct pset_attribute pset_handle_all = {
+	.attr = {
+		.ca_name = "handle_all",
+		.ca_owner = THIS_MODULE,
+		.ca_mode = S_IRUGO | S_IWUSR,
+	},
+	.show = pset_handle_all_show,
+	.store = pset_handle_all_store,
+};
+
+static struct configfs_attribute *pset_attributes[] = {
+	&pset_handle_all.attr,
+	NULL,
+};
+
+/**
+ * This is general function for showing values of process set attributes.
+ */
+static ssize_t pset_attribute_show(struct config_item *item,
+				   struct configfs_attribute *attr,
+				   char *page)
+{
+	struct pset_attribute *pset_attr = to_pset_attribute(attr);
+	struct process_set *pset = to_process_set(item);
+	ssize_t ret = 0;
+
+	if (pset_attr->show)
+		ret = pset_attr->show(pset, page);
+
+	return ret;
+}
+
+/**
+ * This is general function for storing values of process set attributes.
+ */
+static ssize_t pset_attribute_store(struct config_item *item,
+				    struct configfs_attribute *attr,
+				    const char *page, size_t count)
+{
+	struct pset_attribute *pset_attr = to_pset_attribute(attr);
+	struct process_set *pset = to_process_set(item);
+	ssize_t ret = 0;
+
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return -EPERM;
+
+	if (pset_attr->store) {
+		struct string_list_object *list;
+
+		list = global_config_attr_store_begin(item);
+		if (IS_ERR(list))
+			return PTR_ERR(list);
+
+		ret = pset_attr->store(pset, page, count);
+
+		if (ret >= 0)
+			ret = global_config_attr_store_end(list,
+							   item, attr,
+							   page, ret);
+		else
+			global_config_attr_store_error(list, item);
+	}
+
+	return ret;
+}
+
+static void process_set_release(struct config_item *item);
+
+static
+struct global_config_attrs *process_set_global_attrs(struct config_item *item)
+{
+	return &to_process_set(item)->global_attrs;
+}
+
+struct global_config_item_operations process_set_global_item_ops = {
+	.config = {
+		.release = process_set_release,
+		.show_attribute = pset_attribute_show,
+		.store_attribute = pset_attribute_store,
+	},
+	.global_attrs = process_set_global_attrs,
+};
+
+static struct config_item_type pset_type = {
+        .ct_owner = THIS_MODULE,
+	.ct_item_ops = &process_set_global_item_ops.config,
+	.ct_attrs = pset_attributes,
+};
+
+/**
+ * This function allocates memory for new process set and initializes it.
+ * Note: at the beginning the process set doesn't contain any processes nor
+ * process groups.
+ * @author Marko Novak, Louis Rilling
+ *
+ * @return              pointer to newly created process set or NULL if
+ *                      creation failed.
+ */
+struct process_set *process_set_create(void)
+{
+	struct process_set *pset;
+	enum pid_type type;
+
+	pset = kmalloc(sizeof(struct process_set), GFP_KERNEL);
+	if (!pset)
+		goto err_kmalloc;
+
+	/* initialize process set. */
+	memset(&pset->group, 0, sizeof(pset->group));
+	config_group_init_type_name(&pset->group, "process_set", &pset_type);
+	for (type = 0; type < PIDTYPE_MAX; type++) {
+		process_subset_init(&pset->subsets[type],
+				    process_subset_names[type]);
+		pset->def_groups[type] = &pset->subsets[type].group;
+	}
+	pset->def_groups[PIDTYPE_MAX] = NULL;
+	pset->group.default_groups = pset->def_groups;
+
+	/*
+	 * by default, particular scheduling policy doesn't handle any
+	 * process.
+	 */
+	pset->handle_all = 0;
+
+	spin_lock_init(&pset->lock);
+
+	return pset;
+
+err_kmalloc:
+	return NULL;
+}
+
+static void delayed_process_set_put(struct rcu_head *rcu)
+{
+	struct process_set *pset = container_of(rcu, struct process_set, rcu);
+	process_set_put(pset);
+}
+
+void process_set_drop(struct process_set *pset)
+{
+	process_set_lock(pset);
+	if (pset->handle_all) {
+		pset->handle_all = 0;
+		spin_lock(&process_set_link_lock);
+		list_del_rcu(&pset->handle_all_list);
+		spin_unlock(&process_set_link_lock);
+	}
+	process_set_unlock(pset);
+	call_rcu(&pset->rcu, delayed_process_set_put);
+}
+
+/**
+ * ConfigFS callback when the last reference on a process set is dropped
+ * Frees all the memory allocated for a process set
+ */
+static void process_set_release(struct config_item *item)
+{
+	struct process_set *pset = to_process_set(item);
+	enum pid_type type;
+	for (type = 0; type < PIDTYPE_MAX; type++)
+		process_subset_cleanup(&pset->subsets[type]);
+	kfree(pset);
+}
+
+#ifdef CONFIG_KRG_EPM
+
+/*
+ * Ghost export / import functions
+ *
+ * The exporting procedure to follow is:
+ * - call export_process_set_links_start
+ * - call export_process_set_links for each desired pid_type
+ * - call export_process_set_links_end
+ */
+
+int export_process_set_links_start(struct epm_action *action, ghost_t *ghost,
+				   struct task_struct *task)
+{
+	if (action->type != EPM_MIGRATE && action->type != EPM_REMOTE_CLONE)
+		return 0;
+	return global_config_freeze();
+}
+
+int export_process_set_links(struct epm_action *action, ghost_t *ghost,
+			     struct pid *pid, enum pid_type type)
+{
+	struct process_set_element **elements;
+	struct process_set_element *pset_el;
+	struct hlist_node *pos;
+	int nr_links, nr;
+	int err;
+
+	if (action->type != EPM_MIGRATE && action->type != EPM_REMOTE_CLONE)
+		return 0;
+
+	/*
+	 * process_set_elements found in pid->process_set_links will remain
+	 * linked until we release the subsystem mutex, since all
+	 * link/unlink are done in make_item/drop_item operations or in
+	 * import_process_set_links with the mutex held.
+	 */
+	mutex_lock(&krg_scheduler_subsys.su_mutex);
+
+	nr_links = 0;
+	/*
+	 * No need to acquire process_set_link_lock since all mutations of
+	 * process set links are protected by krg_scheduler_subsys.su_mutex
+	 */
+	hlist_for_each(pos, &pid->process_sets[type])
+		nr_links++;
+
+	err = -ENOMEM;
+	elements = kmalloc(sizeof(*elements) * nr_links, GFP_KERNEL);
+	if (!elements)
+		goto out_unlock;
+
+	nr = 0;
+	/*
+	 * Traverse the list in reverse order so that import restores the list
+	 * in the same order as the one of this node
+	 */
+	if (nr_links) {
+		struct hlist_head *head = &pid->process_sets[type];
+		struct hlist_node **pnext;
+
+		/* The list has at least one element. */
+		/* Find the last element */
+		for (pos = head->first; pos->next; pos = pos->next);
+		/* Start from last element and stop when head is reached */
+		for (pnext = &pos->next;
+		     pnext != &head->first &&
+			     ({ pos = container_of(pnext, struct hlist_node, next); 1; });
+		     pnext = pos->pprev) {
+			pset_el = hlist_entry(pos,
+					      struct process_set_element, pid_node);
+			elements[nr++] = pset_el;
+		}
+	}
+	BUG_ON(nr != nr_links);
+
+	err = ghost_write(ghost, &nr_links, sizeof(nr_links));
+	if (err)
+		goto out_free;
+
+	for (nr = 0; nr < nr_links; nr++) {
+		/* Export the globalized process_set_element */
+		err = export_global_config_item(action, ghost,
+						&elements[nr]->item);
+		if (err)
+			break;
+	}
+
+out_free:
+	kfree(elements);
+out_unlock:
+	mutex_unlock(&krg_scheduler_subsys.su_mutex);
+
+	return err;
+}
+
+void export_process_set_links_end(struct epm_action *action, ghost_t *ghost,
+				  struct task_struct *task)
+{
+	if (action->type == EPM_MIGRATE || action->type == EPM_REMOTE_CLONE)
+		global_config_thaw();
+}
+
+int import_process_set_links(struct epm_action *action, ghost_t *ghost,
+			     struct pid *pid, enum pid_type type)
+{
+	struct process_set_element *pset_el;
+	struct config_item *item;
+	int nr_links, nr;
+	int err;
+
+	if (action->type != EPM_MIGRATE && action->type != EPM_REMOTE_CLONE)
+		return 0;
+
+	err = ghost_read(ghost, &nr_links, sizeof(nr_links));
+	if (err)
+		goto out;
+
+	for (nr = 0; nr < nr_links; nr++) {
+		err = import_global_config_item(action, ghost, &item);
+		if (err)
+			break;
+		/*
+		 * Some imported process_set_element may not exist anymore, so
+		 * do not make import fail in that case.
+		 */
+		if (IS_ERR(item))
+			continue;
+
+		pset_el = to_process_set_element(item);
+		/*
+		 * Taking the subsystem mutex blocks all other calls of
+		 * __process_set_element_link and process_set_element_unlink
+		 */
+		mutex_lock(&krg_scheduler_subsys.su_mutex);
+		/*
+		 * item must not be added to a pid list of links unless it is
+		 * still linked in configfs.
+		 */
+		/*
+		 * Note: here we may race with a process set traversal, even if
+		 * it acquired the process set lock before. We can live with
+		 * this since the incoming task may be considered as not
+		 * completely there yet.
+		 */
+		if (process_set_element_in_subset(pset_el)
+		    && !process_set_element_linked(pset_el))
+			__process_set_element_link(pset_el, pid, type);
+		mutex_unlock(&krg_scheduler_subsys.su_mutex);
+
+		config_item_put(item);
+	}
+
+out:
+	return err;
+}
+
+#endif /* CONFIG_KRG_EPM */
diff -ruN linux-2.6.29/kerrighed/scheduler/remote_pipe.c android_cluster/linux-2.6.29/kerrighed/scheduler/remote_pipe.c
--- linux-2.6.29/kerrighed/scheduler/remote_pipe.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/remote_pipe.c	2014-05-27 23:04:10.478026844 -0700
@@ -0,0 +1,262 @@
+/*
+ *  kerrighed/scheduler/remote_pipe.c
+ *
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ */
+
+#include <linux/kernel.h>
+#include <linux/configfs.h>
+#include <linux/slab.h>
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/workqueue.h>
+#include <kerrighed/scheduler/pipe.h>
+#include <kerrighed/scheduler/global_config.h>
+#include <kerrighed/scheduler/remote_pipe.h>
+#include <net/krgrpc/rpc.h>
+#include <net/krgrpc/rpcid.h>
+
+static void handle_pipe_get_remote_value(struct rpc_desc *desc)
+{
+	struct scheduler_pipe *pipe;
+	struct scheduler_source *source;
+	struct get_value_types *types;
+	struct config_item *item;
+	unsigned int nr, in_nr;
+	void *value_p, *in_value_p;
+	int ret;
+	int err;
+
+	item = global_config_unpack_get_item(desc);
+	err = PTR_ERR(item);
+	if (IS_ERR(item))
+		goto err_cancel;
+	pipe = to_scheduler_pipe(item);
+	source = pipe->source;
+	types = &scheduler_source_type_of(source)->get_value_types;
+	err = rpc_unpack_type(desc, nr);
+	if (err)
+		goto err_put_item;
+	err = rpc_unpack_type(desc, in_nr);
+	if (err)
+		goto err_put_item;
+	err = -ENOMEM;
+	value_p = NULL;
+	if (nr) {
+		value_p = kmalloc(nr * types->out_type_size, GFP_KERNEL);
+		if (!value_p)
+			goto err_put_item;
+	}
+	in_value_p = NULL;
+	if (in_nr) {
+		size_t in_size;
+		in_size = in_nr * types->in_type_size;
+		in_value_p = kmalloc(in_size, GFP_KERNEL);
+		if (!in_value_p)
+			goto err_free_value_p;
+		err = rpc_unpack(desc, 0, in_value_p, in_size);
+		if (err)
+			goto err_free_in_value_p;
+	}
+
+	ret = scheduler_source_get_value(source, value_p, nr,
+					 in_value_p, in_nr);
+
+	err = rpc_pack_type(desc, ret);
+	if (err)
+		goto err_free_in_value_p;
+	if (ret > 0) {
+		err = rpc_pack(desc, 0,
+			       value_p,
+			       ret * types->out_type_size);
+		if (err)
+			goto err_free_in_value_p;
+	}
+
+	kfree(in_value_p);
+	kfree(value_p);
+	config_item_put(item);
+
+	return;
+
+err_free_in_value_p:
+	kfree(in_value_p);
+err_free_value_p:
+	kfree(value_p);
+err_put_item:
+	config_item_put(item);
+err_cancel:
+	rpc_cancel(desc);
+}
+
+static void pipe_get_remote_value_worker(struct work_struct *work)
+{
+	struct remote_pipe_desc *show_desc =
+		container_of(work, typeof(*show_desc), work);
+	struct scheduler_sink *sink =
+		container_of(show_desc, typeof(*sink), remote_pipe);
+	size_t value_type_size = sink->type->get_value_types.out_type_size;
+	int ret;
+	int err;
+
+	err = rpc_unpack_type(show_desc->desc, ret);
+	if (err)
+		goto err_cancel;
+	show_desc->ret = ret;
+	if (ret <= 0)
+		goto end_request;
+	err = rpc_unpack(show_desc->desc, 0,
+			 show_desc->value_p, value_type_size * ret);
+	if (err)
+		goto err_cancel;
+
+end_request:
+	err = rpc_end(show_desc->desc, 0);
+
+	spin_lock(&show_desc->lock);
+	show_desc->pending = 0;
+	spin_unlock(&show_desc->lock);
+	sink->type->update_value(sink, NULL);
+	return;
+
+err_cancel:
+	rpc_cancel(show_desc->desc);
+	if (err == RPC_EPIPE)
+		err = -EPIPE;
+	BUG_ON(err >= 0);
+	show_desc->ret = err;
+	goto end_request;
+}
+
+static int start_pipe_get_remote_value(
+	struct scheduler_sink *sink,
+	struct scheduler_pipe *local_pipe,
+	kerrighed_node_t node,
+	void *value_p, unsigned int nr,
+	const void *in_value_p, unsigned int in_nr)
+{
+	struct remote_pipe_desc *show_desc = &sink->remote_pipe;
+	struct rpc_desc *desc;
+	size_t in_size = sink->type->get_value_types.in_type_size;
+	int err;
+
+	if (!krgnode_online(node))
+		return -EINVAL;
+	if (node == kerrighed_node_id)
+		return scheduler_source_get_value(local_pipe->source,
+						  value_p, nr,
+						  in_value_p, in_nr);
+	desc = rpc_begin(SCHED_PIPE_GET_REMOTE_VALUE, node);
+	if (!desc)
+		return -ENOMEM;
+	err = global_config_pack_item(desc, &local_pipe->config.cg_item);
+	if (err)
+		goto err_cancel;
+	err = rpc_pack_type(desc, nr);
+	if (err)
+		goto err_cancel;
+	err = rpc_pack_type(desc, in_nr);
+	if (err)
+		goto err_cancel;
+	if (in_nr) {
+		err = rpc_pack(desc, 0, in_value_p, in_nr * in_size);
+		if (err)
+			goto err_cancel;
+	}
+
+	show_desc->pending = 1;
+	show_desc->desc = desc;
+	show_desc->node = node;
+	show_desc->value_p = value_p;
+	queue_work(krg_wq, &show_desc->work);
+	return -EAGAIN;
+
+err_cancel:
+	rpc_cancel(desc);
+	rpc_end(desc, 0);
+	BUG_ON(err == -EAGAIN);
+	return err;
+}
+
+int scheduler_pipe_get_remote_value(
+	struct scheduler_sink *sink,
+	struct scheduler_pipe *local_pipe,
+	kerrighed_node_t node,
+	void *value_p, unsigned int nr,
+	const void *in_value_p, unsigned int in_nr)
+{
+	struct remote_pipe_desc *show_desc = &sink->remote_pipe;
+	int ret;
+
+	if (!scheduler_source_type_of(local_pipe->source)->get_value)
+		return -EACCES;
+	if (!sink->type->update_value)
+		return -EINVAL;
+	if (!nr && !in_nr)
+		return 0;
+	if ((nr && !value_p) || (in_nr && !in_value_p))
+		return -EINVAL;
+
+	spin_lock(&show_desc->lock);
+	if (show_desc->node != KERRIGHED_NODE_ID_NONE
+	    && (show_desc->node != node
+		|| show_desc->value_p != value_p)) {
+		ret = -EINVAL;
+	} else if (show_desc->pending) {
+		ret = -EAGAIN;
+	} else if (show_desc->node == KERRIGHED_NODE_ID_NONE) {
+		/* Start a new asynchronous request */
+		ret = start_pipe_get_remote_value(sink,
+						  local_pipe,
+						  node,
+						  value_p, nr,
+						  in_value_p, in_nr);
+	} else {
+		/* A result is ready and matches the current call. */
+		show_desc->node = KERRIGHED_NODE_ID_NONE;
+		ret = min((int) nr, show_desc->ret);
+	}
+	spin_unlock(&show_desc->lock);
+
+	return ret;
+}
+
+void scheduler_sink_remote_pipe_init(struct scheduler_sink *sink)
+{
+	struct remote_pipe_desc *show_desc = &sink->remote_pipe;
+
+	show_desc->pending = 0;
+	INIT_WORK(&show_desc->work, pipe_get_remote_value_worker);
+	show_desc->node = KERRIGHED_NODE_ID_NONE;
+	spin_lock_init(&show_desc->lock);
+}
+
+void scheduler_sink_remote_pipe_disconnect(struct scheduler_sink *sink)
+{
+	/* Wait for (any) request worker to terminate */
+	/*
+	 * Lockdep warns against a possible circular locking dependency:
+	 * (a)workqueue_mutex --> (a)kerrighed_init_sem --> (a)(b)mm->mmap_sem -->
+	 * (b)(c)inode->i_mutex --> (c)workqueue_mutex, with (a) being called only once
+	 * when creating kthreadd, (b) being called in sys_mmap(), and (c) being called
+	 * in current path.
+	 * Fortunately, the three chains do not happen to merge in any path :)
+	 */
+	lockdep_off();
+	flush_workqueue(krg_wq);
+	lockdep_on();
+}
+
+int remote_pipe_start(void)
+{
+	return rpc_register(SCHED_PIPE_GET_REMOTE_VALUE,
+			    handle_pipe_get_remote_value, 0);
+}
+
+void remote_pipe_exit(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/scheduler/scheduler.c android_cluster/linux-2.6.29/kerrighed/scheduler/scheduler.c
--- linux-2.6.29/kerrighed/scheduler/scheduler.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/scheduler.c	2014-05-27 23:04:10.478026844 -0700
@@ -0,0 +1,835 @@
+/*
+ *  kerrighed/scheduler/scheduler.c
+ *
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ */
+
+#include <linux/configfs.h>
+#include <linux/module.h>
+#include <linux/nsproxy.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+#include <linux/spinlock.h>
+#include <linux/mutex.h>
+#include <linux/err.h>
+#include <kerrighed/krgflags.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/scheduler/policy.h>
+#include <kerrighed/scheduler/process_set.h>
+#include <kerrighed/scheduler/global_config.h>
+
+#include "internal.h"
+
+/*
+ * Structure representing a scheduler.
+ * Created each time a user does mkdir in "schedulers" subsystem directory,
+ * and destroyed after user does rmdir on the matching directory.
+ */
+struct scheduler {
+	struct config_group group;	 /** configfs reprensentation */
+	struct scheduler_policy *policy; /** scheduling policy attached to this
+					  * scheduler */
+	struct process_set *processes;	 /** set of processes managed by this
+					  * scheduler */
+	struct config_group *default_groups[2]; /** default subdirs */
+	struct global_config_item global_item; /** global_config subsystem */
+	struct global_config_attrs global_attrs;
+
+	krgnodemask_t node_set;
+	unsigned node_set_exclusive:1;
+	unsigned node_set_max_fit:1;
+
+	struct list_head list;
+
+	spinlock_t lock;
+};
+
+static inline struct scheduler *to_scheduler(struct config_item *item)
+{
+	return container_of(item, struct scheduler, group.cg_item);
+}
+
+#define SCHEDULER_ATTR_SIZE 4096
+
+struct scheduler_attribute {
+	struct configfs_attribute config;
+	ssize_t (*show)(struct scheduler *,
+			char *);
+	ssize_t (*store)(struct scheduler *,
+			 const char *,
+			 size_t count);
+};
+
+static inline
+struct scheduler_attribute *
+to_scheduler_attribute(struct configfs_attribute *attr)
+{
+	return container_of(attr, struct scheduler_attribute, config);
+}
+
+static LIST_HEAD(schedulers_head);
+static DEFINE_SPINLOCK(schedulers_list_lock);
+static DEFINE_MUTEX(schedulers_list_mutex);
+
+static krgnodemask_t shared_set;
+
+void scheduler_get(struct scheduler *scheduler)
+{
+	if (scheduler)
+		config_group_get(&scheduler->group);
+}
+EXPORT_SYMBOL(scheduler_get);
+
+void scheduler_put(struct scheduler *scheduler)
+{
+	if (scheduler)
+		config_group_put(&scheduler->group);
+}
+EXPORT_SYMBOL(scheduler_put);
+
+static inline struct scheduler *get_parent_scheduler(struct config_item *item)
+{
+	struct config_item *scheduler_item;
+	scheduler_item = config_item_get(item->ci_parent);
+	if (scheduler_item)
+		return to_scheduler(scheduler_item);
+	return NULL;
+}
+
+struct scheduler *
+scheduler_policy_get_scheduler(struct scheduler_policy *policy)
+{
+	return get_parent_scheduler(&policy->group.cg_item);
+}
+EXPORT_SYMBOL(scheduler_policy_get_scheduler);
+
+struct scheduler *process_set_get_scheduler(struct process_set *pset)
+{
+	return get_parent_scheduler(&pset->group.cg_item);
+}
+EXPORT_SYMBOL(process_set_get_scheduler);
+
+struct scheduler_policy *
+scheduler_get_scheduler_policy(struct scheduler *scheduler)
+{
+	struct scheduler_policy *policy;
+
+	spin_lock(&scheduler->lock);
+	scheduler_policy_get(scheduler->policy);
+	policy = scheduler->policy;
+	spin_unlock(&scheduler->lock);
+
+	return policy;
+}
+EXPORT_SYMBOL(scheduler_get_scheduler_policy);
+
+struct process_set *scheduler_get_process_set(struct scheduler *scheduler)
+{
+	struct process_set *pset;
+
+	spin_lock(&scheduler->lock);
+	process_set_get(scheduler->processes);
+	pset = scheduler->processes;
+	spin_unlock(&scheduler->lock);
+
+	return pset;
+}
+EXPORT_SYMBOL(scheduler_get_process_set);
+
+static inline const krgnodemask_t *get_node_set(struct scheduler *scheduler)
+{
+	if (scheduler->node_set_max_fit) {
+		if (scheduler->node_set_exclusive)
+			return &krgnode_online_map;
+		else
+			return &shared_set;
+	} else {
+		return &scheduler->node_set;
+	}
+}
+
+static
+inline void set_node_set(struct scheduler *scheduler, const krgnodemask_t *set)
+{
+	BUG_ON(scheduler->node_set_max_fit);
+	__krgnodes_copy(&scheduler->node_set, set);
+}
+
+void scheduler_get_node_set(struct scheduler *scheduler,
+			    krgnodemask_t *node_set)
+{
+	spin_lock(&schedulers_list_lock);
+	spin_lock(&scheduler->lock);
+	__krgnodes_copy(node_set, get_node_set(scheduler));
+	spin_unlock(&scheduler->lock);
+	spin_unlock(&schedulers_list_lock);
+}
+EXPORT_SYMBOL(scheduler_get_node_set);
+
+static ssize_t scheduler_show_attribute(struct config_item *item,
+				        struct configfs_attribute *attr,
+				        char *page)
+{
+	struct scheduler_attribute *sa = to_scheduler_attribute(attr);
+	ssize_t ret = -EACCES;
+	if (sa->show)
+		ret = sa->show(to_scheduler(item), page);
+	return ret;
+}
+
+static ssize_t scheduler_store_attribute(struct config_item *item,
+					 struct configfs_attribute *attr,
+					 const char *page,
+					 size_t count)
+{
+	struct scheduler_attribute *sa = to_scheduler_attribute(attr);
+	struct string_list_object *list;
+	ssize_t ret = -EACCES;
+
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return -EPERM;
+
+	if (sa->store) {
+		list = global_config_attr_store_begin(item);
+		if (IS_ERR(list))
+			return PTR_ERR(list);
+
+		ret = sa->store(to_scheduler(item), page, count);
+
+		if (ret >= 0)
+			ret = global_config_attr_store_end(list,
+							   item, attr,
+							   page, ret);
+		else
+			global_config_attr_store_error(list, item);
+	}
+
+	return ret;
+}
+
+static void scheduler_free(struct scheduler *);
+
+/*
+ * Configfs callback when the last reference on a scheduler is dropped.
+ * Destroys the scheduler.
+ */
+static void scheduler_release(struct config_item *item)
+{
+	struct scheduler *s = to_scheduler(item);
+	scheduler_free(s);
+}
+
+static
+struct global_config_attrs *scheduler_global_attrs(struct config_item *item)
+{
+	return &to_scheduler(item)->global_attrs;
+}
+
+struct global_config_item_operations scheduler_global_item_ops = {
+	.config = {
+		.show_attribute = scheduler_show_attribute,
+		.store_attribute = scheduler_store_attribute,
+		.release = scheduler_release,
+	},
+	.global_attrs = scheduler_global_attrs,
+};
+
+/**
+ * Callback called by global_config when the scheduler_policy of a scheduler is
+ * globally dropped
+ */
+static void policy_global_drop(struct global_config_item *item)
+{
+	struct scheduler_policy *policy =
+		container_of(item, struct scheduler_policy, global_item);
+	global_config_attrs_cleanup_r(&policy->group);
+	scheduler_policy_drop(policy);
+}
+
+static struct global_config_drop_operations policy_global_drop_ops = {
+	.drop_func = policy_global_drop,
+	.is_symlink = 0
+};
+
+/**
+ * This is a configfs callback function, which is invoked every time user tries
+ * to create a directory in a scheduler directory ("schedulers/<scheduler>"
+ * directories).  It is used for loading scheduling policy's module, creating
+ * and activating a new scheduler_policy having the type matching the new
+ * directory name.
+ */
+static struct config_group *scheduler_make_group(struct config_group *group,
+						 const char *name)
+{
+	struct scheduler *s = to_scheduler(&group->cg_item);
+	struct config_group *ret;
+	struct scheduler_policy *policy;
+	struct string_list_object *global_policies;
+	int err;
+
+	ret = ERR_PTR(-EPERM);
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		goto out;
+
+	/* Cannot manage several scheduling policies yet */
+	ret = ERR_PTR(-EBUSY);
+	if (s->policy)
+		goto out;
+
+	global_policies = global_config_make_item_begin(&group->cg_item, name);
+	if (IS_ERR(global_policies)) {
+		ret = (void *)global_policies;
+		goto out;
+	}
+
+	policy = scheduler_policy_new(name);
+	if (IS_ERR(policy)) {
+		err = PTR_ERR(policy);
+		goto err_policy;
+	}
+	global_config_attrs_init_r(&policy->group);
+	global_config_item_init(&policy->global_item,
+				&policy_global_drop_ops);
+	err = global_config_make_item_end(global_policies,
+					  &group->cg_item,
+					  &policy->global_item,
+					  name);
+	if (err)
+		goto err_global_end;
+
+	spin_lock(&s->lock);
+	s->policy = policy;
+	spin_unlock(&s->lock);
+	ret = &policy->group;
+
+out:
+	return ret;
+
+err_policy:
+	global_config_make_item_error(global_policies, name);
+	ret = ERR_PTR(err);
+	goto out;
+
+err_global_end:
+	global_config_attrs_cleanup_r(&policy->group);
+	scheduler_policy_drop(policy);
+	ret = ERR_PTR(err);
+	goto out;
+}
+
+static int scheduler_allow_drop_item(struct config_group *group,
+				     struct config_item *item)
+{
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return -EPERM;
+	return 0;
+}
+
+/*
+ * Configfs callback called when the scheduling policy directory of a scheduler
+ * is removed.
+ */
+static void scheduler_drop_item(struct config_group *group,
+				struct config_item *item)
+{
+	struct scheduler *s = to_scheduler(&group->cg_item);
+	struct scheduler_policy *p =
+		container_of(item, struct scheduler_policy, group.cg_item);
+	spin_lock(&s->lock);
+	s->policy = NULL;
+	spin_unlock(&s->lock);
+	global_config_drop(&p->global_item);
+}
+
+/**
+ * This struct is ConfigFS-specific. See ConfigFS documentation for its
+ * explanation.
+ */
+static struct configfs_group_operations scheduler_group_ops = {
+	.make_group = scheduler_make_group,
+	.allow_drop_item = scheduler_allow_drop_item,
+	.drop_item = scheduler_drop_item,
+};
+
+/* Scheduler attributes */
+
+static ssize_t node_set_show(struct scheduler *s, char *page)
+{
+	krgnodemask_t set;
+
+	scheduler_get_node_set(s, &set);
+	return krgnodelist_scnprintf(page, SCHEDULER_ATTR_SIZE, set);
+}
+
+static int node_set_may_be_exclusive(const struct scheduler *s,
+				     const krgnodemask_t *node_set);
+
+static void policy_update_node_set(struct scheduler *scheduler,
+				   const krgnodemask_t *removed_set,
+				   const krgnodemask_t *added_set)
+{
+	struct scheduler_policy *policy;
+
+	policy = scheduler_get_scheduler_policy(scheduler);
+	if (policy) {
+		scheduler_policy_update_node_set(policy,
+						 get_node_set(scheduler),
+						 removed_set,
+						 added_set);
+		scheduler_policy_put(policy);
+	}
+}
+
+static int do_update_node_set(struct scheduler *s,
+			      const krgnodemask_t *new_set,
+			      bool max_fit)
+{
+	krgnodemask_t removed_set, added_set;
+	const krgnodemask_t *old_set;
+	struct scheduler_policy *policy = NULL;
+	int err = -EBUSY;
+
+	mutex_lock(&schedulers_list_mutex);
+	spin_lock(&schedulers_list_lock);
+
+	if (max_fit) {
+		if (s->node_set_exclusive)
+			new_set = &krgnode_online_map;
+		else
+			new_set = &shared_set;
+	} else if (!new_set) {
+		new_set = get_node_set(s);
+	}
+
+	old_set = get_node_set(s);
+	krgnodes_andnot(removed_set, *old_set, *new_set);
+	krgnodes_andnot(added_set, *new_set, *old_set);
+
+	if (s->node_set_exclusive) {
+		if (!node_set_may_be_exclusive(s, new_set))
+			goto unlock;
+		krgnodes_andnot(shared_set, shared_set, added_set);
+		krgnodes_or(shared_set, shared_set, removed_set);
+	} else {
+		if (!krgnodes_subset(*new_set, shared_set))
+			goto unlock;
+	}
+	err = 0;
+
+	spin_lock(&s->lock);
+	s->node_set_max_fit = max_fit;
+	if (!max_fit)
+		set_node_set(s, new_set);
+	policy = s->policy;
+	scheduler_policy_get(policy);
+	spin_unlock(&s->lock);
+unlock:
+	spin_unlock(&schedulers_list_lock);
+
+	if (!err)
+		policy_update_node_set(s, &removed_set, &added_set);
+	mutex_unlock(&schedulers_list_mutex);
+
+	return err;
+}
+
+static
+ssize_t node_set_store(struct scheduler *s, const char *page, size_t count)
+{
+	krgnodemask_t new_set;
+	int err;
+	ssize_t ret;
+
+	err = krgnodelist_parse(page, new_set);
+	if (err) {
+		ret = err;
+	} else {
+		if (krgnodes_subset(new_set, krgnode_online_map)) {
+			err = do_update_node_set(s, &new_set, false);
+			ret = err ? err : count;
+		} else {
+			ret = -EINVAL;
+		}
+	}
+	return ret;
+}
+
+static struct scheduler_attribute node_set = {
+	.config = {
+		.ca_name = "node_set",
+		.ca_owner = THIS_MODULE,
+		.ca_mode = S_IRUGO | S_IWUSR,
+	},
+	.show = node_set_show,
+	.store = node_set_store
+};
+
+static ssize_t node_set_exclusive_show(struct scheduler *s, char *page)
+{
+	return sprintf(page, "%u", s->node_set_exclusive);
+}
+
+static int node_set_may_be_exclusive(const struct scheduler *s,
+			    const krgnodemask_t *node_set)
+{
+	struct scheduler *pos;
+
+	list_for_each_entry(pos, &schedulers_head, list)
+		if (pos != s
+		    && (pos->node_set_exclusive || !pos->node_set_max_fit)
+		    && krgnodes_intersects(*node_set, *get_node_set(pos)))
+			return 0;
+	return 1;
+}
+
+static int make_node_set_exclusive(struct scheduler *s)
+{
+	const krgnodemask_t *set = get_node_set(s);
+	int err = 0;
+
+	if (s->node_set_exclusive)
+		goto out;
+
+	if (!node_set_may_be_exclusive(s, set)) {
+		err = -EBUSY;
+		goto out;
+	}
+
+	krgnodes_andnot(shared_set, shared_set, *set);
+	s->node_set_exclusive = 1;
+
+out:
+	return err;
+}
+
+static void make_node_set_not_exclusive(struct scheduler *s)
+{
+	if (s->node_set_exclusive) {
+		krgnodes_or(shared_set, shared_set, *get_node_set(s));
+		s->node_set_exclusive = 0;
+	}
+}
+
+static
+ssize_t
+node_set_exclusive_store(struct scheduler *s, const char *page, size_t count)
+{
+	int new_state;
+	char *last_read;
+	krgnodemask_t added, removed;
+	bool changed;
+	int err;
+
+	new_state = simple_strtoul(page, &last_read, 0);
+	if (last_read - page + 1 < count
+	    || (last_read[1] != '\0' && last_read[1] != '\n'))
+		return -EINVAL;
+
+	mutex_lock(&schedulers_list_mutex);
+	spin_lock(&schedulers_list_lock);
+	if (new_state) {
+		krgnodes_clear(added);
+		krgnodes_copy(removed, *get_node_set(s));
+		changed = !s->node_set_exclusive;
+		err = make_node_set_exclusive(s);
+		changed = changed && !err;
+	} else {
+		krgnodes_copy(added, *get_node_set(s));
+		krgnodes_clear(removed);
+		changed = s->node_set_exclusive;
+		make_node_set_not_exclusive(s);
+		err = 0;
+	}
+	spin_unlock(&schedulers_list_lock);
+
+	if (changed) {
+		list_for_each_entry(s, &schedulers_head, list)
+			if (s->node_set_max_fit)
+				policy_update_node_set(s, &removed, &added);
+	}
+	mutex_unlock(&schedulers_list_mutex);
+
+	return err ? err : count;
+}
+
+static struct scheduler_attribute node_set_exclusive = {
+	.config = {
+		.ca_name = "node_set_exclusive",
+		.ca_owner = THIS_MODULE,
+		.ca_mode = S_IRUGO | S_IWUSR,
+	},
+	.show = node_set_exclusive_show,
+	.store = node_set_exclusive_store
+};
+
+static ssize_t node_set_max_fit_show(struct scheduler *s, char *page)
+{
+	return sprintf(page, "%u", s->node_set_max_fit);
+}
+
+static
+ssize_t
+node_set_max_fit_store(struct scheduler *s, const char *page, size_t count)
+{
+	int new_state;
+	char *last_read;
+	int err;
+
+	new_state = simple_strtoul(page, &last_read, 0);
+	if (last_read - page + 1 < count
+	    || (last_read[1] != '\0' && last_read[1] != '\n'))
+		return -EINVAL;
+
+	err = do_update_node_set(s, NULL, new_state);
+	return err ? err : count;
+}
+
+static struct scheduler_attribute node_set_max_fit = {
+	.config = {
+		.ca_name = "node_set_max_fit",
+		.ca_owner = THIS_MODULE,
+		.ca_mode = S_IRUGO | S_IWUSR,
+	},
+	.show = node_set_max_fit_show,
+	.store = node_set_max_fit_store
+};
+
+static struct configfs_attribute *scheduler_attrs[] = {
+	&node_set.config,
+	&node_set_exclusive.config,
+	&node_set_max_fit.config,
+	NULL
+};
+
+/**
+ * This struct is ConfigFS-specific. See ConfigFS documentation for its
+ * explanation.
+ */
+static struct config_item_type scheduler_type = {
+	.ct_owner = THIS_MODULE,
+	.ct_item_ops = &scheduler_global_item_ops.config,
+	.ct_group_ops = &scheduler_group_ops,
+	.ct_attrs = scheduler_attrs
+};
+
+/**
+ * Create a scheduler with no processes attached and no scheduling policy.
+ *
+ * @param name		Name of the directory containing the scheduler
+ *
+ * @return		pointer to the new scheduler, or
+ *			NULL if error
+ */
+static struct scheduler *scheduler_create(const char *name)
+{
+	struct scheduler *s = kmalloc(sizeof(*s), GFP_KERNEL);
+
+	if (!s)
+		return NULL;
+	memset(&s->group, 0, sizeof(s->group));
+	config_group_init_type_name(&s->group, name, &scheduler_type);
+	s->policy = NULL;
+	s->processes = process_set_create();
+	if (!s->processes) {
+		config_group_put(&s->group);
+		return NULL;
+	}
+	s->node_set_exclusive = 0;
+	s->node_set_max_fit = 1;
+	s->default_groups[0] = &s->processes->group;
+	s->default_groups[1] = NULL;
+	s->group.default_groups = s->default_groups;
+	spin_lock_init(&s->lock);
+	return s;
+}
+
+/**
+ * Free a scheduler
+ */
+static void scheduler_free(struct scheduler *scheduler)
+{
+	kfree(scheduler);
+}
+
+static void scheduler_deactivate(struct scheduler *scheduler)
+{
+	spin_lock(&scheduler->lock);
+	process_set_drop(scheduler->processes);
+	scheduler->processes = NULL;
+	spin_unlock(&scheduler->lock);
+}
+
+/* Global_config callback when the scheduler directory is globally removed */
+static void scheduler_drop(struct global_config_item *item)
+{
+	struct scheduler *scheduler =
+		container_of(item, struct scheduler, global_item);
+
+	global_config_attrs_cleanup_r(&scheduler->group);
+
+	mutex_lock(&schedulers_list_mutex);
+	spin_lock(&schedulers_list_lock);
+	list_del(&scheduler->list);
+	make_node_set_not_exclusive(scheduler);
+	spin_unlock(&schedulers_list_lock);
+	mutex_unlock(&schedulers_list_mutex);
+
+	config_group_put(&scheduler->group);
+}
+
+static struct global_config_drop_operations scheduler_drop_ops = {
+	.drop_func = scheduler_drop,
+	.is_symlink = 0
+};
+
+/*
+ * Configfs callback called when a user creates a directory under "schedulers"
+ * subsystem directory. This creates a new scheduler.
+ */
+static struct config_group *schedulers_make_group(struct config_group *group,
+						  const char *name)
+{
+	struct config_group *ret;
+	struct scheduler *s;
+	struct string_list_object *global_names;
+	int err;
+
+	ret = ERR_PTR(-EPERM);
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		goto out;
+
+	if (!(current->flags & PF_KTHREAD)
+	    && !IS_KERRIGHED_NODE(KRGFLAGS_RUNNING))
+		goto out;
+
+	global_names = global_config_make_item_begin(&group->cg_item, name);
+	if (IS_ERR(global_names)) {
+		ret = (void *)global_names;
+		goto out;
+	}
+
+	err = -ENOMEM;
+	s = scheduler_create(name);
+	if (!s)
+		goto err_scheduler;
+	global_config_attrs_init_r(&s->group);
+	global_config_item_init(&s->global_item, &scheduler_drop_ops);
+	err = __global_config_make_item_commit(global_names,
+					       &group->cg_item,
+					       &s->global_item,
+					       name);
+	if (err)
+		goto err_global_end;
+	mutex_lock(&schedulers_list_mutex);
+	spin_lock(&schedulers_list_lock);
+	list_add(&s->list, &schedulers_head);
+	spin_unlock(&schedulers_list_lock);
+	mutex_unlock(&schedulers_list_mutex);
+	__global_config_make_item_end(global_names);
+
+	ret = &s->group;
+
+out:
+	return ret;
+
+err_scheduler:
+	global_config_make_item_error(global_names, name);
+	ret = ERR_PTR(err);
+	goto out;
+
+err_global_end:
+	__global_config_make_item_end(global_names);
+	global_config_attrs_cleanup_r(&s->group);
+	scheduler_deactivate(s);
+	config_group_put(&s->group);
+	ret = ERR_PTR(err);
+	goto out;
+}
+
+static int schedulers_allow_drop_item(struct config_group *group,
+				      struct config_item *item)
+{
+	if (!(current->flags & PF_KTHREAD) && !current->nsproxy->krg_ns)
+		return -EPERM;
+	return 0;
+}
+
+/* Configfs callback when a scheduler's directory is removed */
+static void schedulers_drop_item(struct config_group *group,
+				 struct config_item *item)
+{
+	struct scheduler *s = to_scheduler(item);
+
+	scheduler_deactivate(s);
+	global_config_drop(&s->global_item);
+}
+
+static struct configfs_group_operations schedulers_group_ops = {
+	.make_group = schedulers_make_group,
+	.allow_drop_item = schedulers_allow_drop_item,
+	.drop_item = schedulers_drop_item,
+};
+
+static struct config_item_type schedulers_type = {
+	.ct_owner = THIS_MODULE,
+	.ct_group_ops = &schedulers_group_ops,
+};
+
+/**
+ * This struct is ConfigFS-specific. See ConfigFS documentation for its
+ * explanation.
+ */
+static struct config_group schedulers_group = {
+	.cg_item = {
+		.ci_namebuf = SCHEDULERS_NAME,
+		.ci_type = &schedulers_type,
+	},
+};
+
+int scheduler_post_add(struct hotplug_context *ctx)
+{
+	const krgnodemask_t *added = &ctx->node_set.v;
+	krgnodemask_t removed = KRGNODE_MASK_NONE;
+	struct scheduler *s;
+
+	mutex_lock(&schedulers_list_mutex);
+
+	list_for_each_entry(s, &schedulers_head, list)
+		if (s->node_set_exclusive && s->node_set_max_fit) {
+			policy_update_node_set(s, &removed, added);
+			goto unlock;
+		}
+
+	spin_lock(&schedulers_list_lock);
+	krgnodes_or(shared_set, shared_set, *added);
+	spin_unlock(&schedulers_list_lock);
+
+	list_for_each_entry(s, &schedulers_head, list)
+		if (s->node_set_max_fit)
+			policy_update_node_set(s, &removed, added);
+
+unlock:
+	mutex_unlock(&schedulers_list_mutex);
+
+	return 0;
+}
+
+/**
+ * Initializes the "schedulers" subsystem directory.
+ * @author Marko Novak, Louis Rilling
+ */
+struct config_group *scheduler_start(void)
+{
+	/* initialize configfs entry */
+	config_group_init(&schedulers_group);
+	return &schedulers_group;
+}
+
+void scheduler_exit(void)
+{
+	printk(KERN_WARNING "[%s] WARNING: loosing memory!\n",
+	       __PRETTY_FUNCTION__);
+}
diff -ruN linux-2.6.29/kerrighed/scheduler/string_list.c android_cluster/linux-2.6.29/kerrighed/scheduler/string_list.c
--- linux-2.6.29/kerrighed/scheduler/string_list.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/string_list.c	2014-05-27 23:04:10.478026844 -0700
@@ -0,0 +1,250 @@
+/*
+ *  kerrighed/scheduler/string_list.c
+ *
+ *  Copyright (C) 2007-2008 Louis Rilling - Kerlabs
+ *  Copyright (C) 2007 Marko Novak - Xlab
+ */
+
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+#include <linux/list.h>
+#include <linux/types.h>
+#include <net/krgrpc/rpc.h>
+#include <kddm/kddm.h>
+
+struct string_list_object {
+	struct list_head head;
+	unsigned long id;
+};
+
+struct string_list_element {
+	struct list_head list;
+	char string[];
+};
+
+static struct kmem_cache *string_list_cachep;
+
+static int string_list_alloc_object(struct kddm_obj *obj_entry,
+				    struct kddm_set *set,
+				    objid_t objid)
+{
+	struct string_list_object *obj;
+	int retval;
+
+	retval = -ENOMEM;
+	obj = kmem_cache_alloc(string_list_cachep, GFP_KERNEL);
+	if (!obj)
+		goto out;
+	INIT_LIST_HEAD(&obj->head);
+	obj->id = objid;
+
+	obj_entry->object = obj;
+	retval = 0;
+out:
+	return retval;
+}
+
+static struct string_list_element *element_alloc(size_t string_length)
+{
+	struct string_list_element *elt;
+	size_t size = offsetof(typeof(*elt), string) + string_length + 1;
+
+	elt = kmalloc(size, GFP_KERNEL);
+	return elt;
+}
+
+static void element_free(struct string_list_element *element)
+{
+	kfree(element);
+}
+
+static void string_list_make_empty(struct string_list_object *object)
+{
+	struct string_list_element *elt, *tmp;
+
+	list_for_each_entry_safe(elt, tmp, &object->head, list) {
+		list_del(&elt->list);
+		element_free(elt);
+	}
+}
+
+static int string_list_import_object(struct rpc_desc *desc,
+				     struct kddm_set *set,
+				     struct kddm_obj *obj_entry,
+				     objid_t objid,
+				     int flags)
+{
+	struct string_list_object *obj = obj_entry->object;
+	struct string_list_element *elt;
+	int nr_elt;
+	size_t len;
+	int err;
+
+	string_list_make_empty(obj);
+
+	err = rpc_unpack_type(desc, nr_elt);
+	if (err)
+		goto out;
+
+	for (; nr_elt > 0; nr_elt--) {
+		err = rpc_unpack_type(desc, len);
+		if (err)
+			break;
+		elt = element_alloc(len);
+		if (!elt) {
+			err = -ENOMEM;
+			break;
+		}
+		err = rpc_unpack(desc, 0, elt->string, len + 1);
+		if (err) {
+			element_free(elt);
+			break;
+		}
+		list_add_tail(&elt->list, &obj->head);
+	}
+
+out:
+	return err;
+}
+
+static int string_list_export_object(struct rpc_desc *desc,
+				     struct kddm_set *set,
+				     struct kddm_obj *obj_entry,
+				     objid_t objid,
+				     int flags)
+{
+	struct string_list_object *obj = obj_entry->object;
+	struct string_list_element *elt;
+	int nr_elt = 0;
+	size_t len;
+	int err;
+
+	list_for_each_entry(elt, &obj->head, list)
+		nr_elt++;
+	err = rpc_pack_type(desc, nr_elt);
+	if (err)
+		goto out;
+
+	list_for_each_entry(elt, &obj->head, list) {
+		len = strlen(elt->string);
+		err = rpc_pack_type(desc, len);
+		if (err)
+			break;
+		err = rpc_pack(desc, 0, elt->string, len + 1);
+		if (err)
+			break;
+	}
+
+out:
+	return err;
+}
+
+static int string_list_remove_object(void *object,
+				     struct kddm_set *set,
+				     objid_t objid)
+{
+	string_list_make_empty(object);
+	return 0;
+}
+
+static struct iolinker_struct string_list_io_linker = {
+	.linker_name   = "string_list",
+	.linker_id     = STRING_LIST_LINKER,
+	.alloc_object  = string_list_alloc_object,
+	.export_object = string_list_export_object,
+	.import_object = string_list_import_object,
+	.remove_object = string_list_remove_object
+};
+
+struct string_list_object *string_list_create_writelock(
+	struct kddm_set *kddm_set,
+	objid_t objid)
+{
+	return _kddm_grab_object(kddm_set, objid);
+}
+
+struct string_list_object *string_list_writelock(struct kddm_set *kddm_set,
+						 objid_t objid)
+{
+	return _kddm_grab_object_no_ft(kddm_set, objid);
+}
+
+void string_list_unlock(struct kddm_set *kddm_set,
+			struct string_list_object *object)
+{
+	_kddm_put_object(kddm_set, object->id);
+}
+
+void string_list_unlock_and_destroy(struct kddm_set *kddm_set,
+				    struct string_list_object *object)
+{
+	_kddm_remove_frozen_object(kddm_set, object->id);
+}
+
+int string_list_add_element(struct string_list_object *object,
+			    const char *element)
+{
+	struct string_list_element *elt;
+
+	elt = element_alloc(strlen(element));
+	if (!elt)
+		return -ENOMEM;
+	strcpy(elt->string, element);
+	list_add_tail(&elt->list, &object->head);
+	return 0;
+}
+
+static
+struct string_list_element *
+string_list_find_element(struct string_list_object *object, const char *element)
+{
+	struct string_list_element *elt;
+
+	list_for_each_entry(elt, &object->head, list)
+		if (!strcmp(element, elt->string))
+			return elt;
+	return NULL;
+}
+
+int string_list_remove_element(struct string_list_object *object,
+			       const char *element)
+{
+	struct string_list_element *elt = string_list_find_element(object,
+								   element);
+
+	if (elt) {
+		list_del(&elt->list);
+		element_free(elt);
+		return 0;
+	}
+
+	return -ENOENT;
+}
+
+int string_list_is_element(struct string_list_object *object,
+			   const char *element)
+{
+	struct string_list_element *elt = string_list_find_element(object,
+								   element);
+
+	return !!elt;
+}
+
+int string_list_empty(struct string_list_object *object)
+{
+	return list_empty(&object->head);
+}
+
+int string_list_start(void)
+{
+	string_list_cachep = KMEM_CACHE(string_list_object, SLAB_PANIC);
+
+	register_io_linker(STRING_LIST_LINKER, &string_list_io_linker);
+
+	return 0;
+}
+
+void string_list_exit(void)
+{
+}
diff -ruN linux-2.6.29/kerrighed/scheduler/string_list.h android_cluster/linux-2.6.29/kerrighed/scheduler/string_list.h
--- linux-2.6.29/kerrighed/scheduler/string_list.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/kerrighed/scheduler/string_list.h	2014-05-27 23:04:10.478026844 -0700
@@ -0,0 +1,26 @@
+#ifndef __STRING_LIST_H__
+#define __STRING_LIST_H__
+
+#include <kddm/kddm_types.h>
+
+struct kddm_set;
+struct string_list_object;
+
+struct string_list_object *
+string_list_create_writelock(struct kddm_set *kddm_set, objid_t objid);
+struct string_list_object *string_list_writelock(struct kddm_set *kddm_set,
+						 objid_t objid);
+void string_list_unlock(struct kddm_set *kddm_set,
+			struct string_list_object *object);
+void string_list_unlock_and_destroy(struct kddm_set *kddm_set,
+				    struct string_list_object *object);
+
+int string_list_add_element(struct string_list_object *object,
+			    const char *element);
+int string_list_remove_element(struct string_list_object *object,
+			       const char *element);
+int string_list_is_element(struct string_list_object *object,
+			   const char *element);
+int string_list_empty(struct string_list_object *object);
+
+#endif /* __STRING_LIST_H__ */
diff -ruN linux-2.6.29/lib/cluster_barrier.c android_cluster/linux-2.6.29/lib/cluster_barrier.c
--- linux-2.6.29/lib/cluster_barrier.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/lib/cluster_barrier.c	2014-05-27 23:04:10.478026844 -0700
@@ -0,0 +1,214 @@
+/** Cluster wide barrier
+ *  @file cluster_barrier.c
+ *
+ *  Implementation of a cluster wide barrier.
+ *
+ *  Copyright (C) 2009, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/cluster_barrier.h>
+#include <linux/hashtable.h>
+#include <linux/unique_id.h>
+#include <net/krgrpc/rpc.h>
+
+#include <kerrighed/types.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/krginit.h>
+
+#define TABLE_SIZE 128
+
+static unique_id_root_t barrier_id_root;
+static hashtable_t *barrier_table;
+
+struct barrier_id {
+	unique_id_t id;
+	int toggle;
+};
+
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                             INTERFACE FUNCTIONS                           */
+/*                                                                           */
+/*****************************************************************************/
+
+
+struct cluster_barrier *alloc_cluster_barrier(unique_id_t key)
+{
+	struct cluster_barrier *barrier;
+	int r, i;
+
+	if (!key)
+		key = get_unique_id(&barrier_id_root);
+
+	if (hashtable_find (barrier_table, key))
+		return ERR_PTR(-EEXIST);
+
+	barrier = kmalloc (sizeof(struct cluster_barrier), GFP_KERNEL);
+	if (!barrier)
+		return ERR_PTR(-ENOMEM);
+
+	for (i = 0; i < 2; i++) {
+		krgnodes_clear (barrier->core[i].nodes_in_barrier);
+		krgnodes_clear (barrier->core[i].nodes_to_wait);
+		init_waitqueue_head(&barrier->core[i].waiting_tsk);
+		barrier->core[i].in_barrier = 0;
+	}
+	spin_lock_init(&barrier->lock);
+	barrier->id.key = key;
+	barrier->id.toggle = 0;
+
+	r = hashtable_add (barrier_table, barrier->id.key, barrier);
+	if (r) {
+		kfree (barrier);
+		return ERR_PTR(r);
+	}
+
+	return barrier;
+}
+
+void free_cluster_barrier(struct cluster_barrier *barrier)
+{
+	hashtable_remove (barrier_table, barrier->id.key);
+
+	kfree(barrier);
+}
+
+int cluster_barrier(struct cluster_barrier *barrier,
+		    krgnodemask_t *nodes,
+		    kerrighed_node_t master)
+{
+	struct cluster_barrier_core *core_bar;
+	struct cluster_barrier_id id;
+	struct rpc_desc *desc;
+	int err = 0;
+
+	BUG_ON (!__krgnode_isset(kerrighed_node_id, nodes));
+	BUG_ON (!__krgnode_isset(master, nodes));
+
+	spin_lock(&barrier->lock);
+	barrier->id.toggle = (barrier->id.toggle + 1) % 2;
+	id = barrier->id;
+	core_bar = &barrier->core[id.toggle];
+	if (core_bar->in_barrier)
+		err = -EBUSY;
+	core_bar->in_barrier = 1;
+	spin_unlock(&barrier->lock);
+	if (err)
+		return err;
+
+	desc = rpc_begin(RPC_ENTER_BARRIER, master);
+
+	rpc_pack_type (desc, id);
+	rpc_pack(desc, 0, nodes, sizeof(krgnodemask_t));
+
+	rpc_end(desc, 0);
+
+	/* Wait for the barrier to complete */
+
+	wait_event (core_bar->waiting_tsk, (core_bar->in_barrier == 0));
+
+	return 0;
+}
+
+/*****************************************************************************/
+/*                                                                           */
+/*                              REQUEST HANDLERS                             */
+/*                                                                           */
+/*****************************************************************************/
+
+
+static int handle_enter_barrier(struct rpc_desc* desc,
+				void *_msg, size_t size)
+{
+	struct cluster_barrier_id *id = ((struct cluster_barrier_id *) _msg);
+	struct cluster_barrier_core *core_bar;
+	struct cluster_barrier *barrier;
+	krgnodemask_t nodes;
+
+	rpc_unpack(desc, 0, &nodes, sizeof(krgnodemask_t));
+
+	barrier = hashtable_find (barrier_table, id->key);
+	BUG_ON(!barrier);
+
+	core_bar = &barrier->core[id->toggle];
+
+	if (krgnodes_empty(core_bar->nodes_to_wait)) {
+		krgnodes_copy(core_bar->nodes_in_barrier, nodes);
+		krgnodes_copy(core_bar->nodes_to_wait, nodes);
+	}
+	else
+		BUG_ON(!krgnodes_equal(core_bar->nodes_in_barrier, nodes));
+
+	krgnode_clear(desc->client, core_bar->nodes_to_wait);
+
+	if (krgnodes_empty(core_bar->nodes_to_wait)) {
+                rpc_async_m(RPC_EXIT_BARRIER, &core_bar->nodes_in_barrier,
+			    id, sizeof (struct cluster_barrier_id));
+	}
+
+	return 0;
+}
+
+
+static int handle_exit_barrier(struct rpc_desc* desc,
+			       void *_msg, size_t size)
+{
+	struct cluster_barrier_id *id = ((struct cluster_barrier_id *) _msg);
+	struct cluster_barrier_core *core_bar;
+	struct cluster_barrier *barrier;
+
+	barrier = hashtable_find (barrier_table, id->key);
+	BUG_ON(!barrier);
+
+	core_bar = &barrier->core[id->toggle];
+	core_bar->in_barrier = 0;
+
+	wake_up (&core_bar->waiting_tsk);
+
+	return 0;
+}
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                               INIT / FINALIZE                             */
+/*                                                                           */
+/*****************************************************************************/
+
+static int barrier_notification(struct notifier_block *nb,
+				hotplug_event_t event,
+				void *data)
+{
+	switch(event){
+	case HOTPLUG_NOTIFY_ADD:
+		rpc_enable(RPC_ENTER_BARRIER);
+		rpc_enable(RPC_EXIT_BARRIER);
+		break;
+
+	case HOTPLUG_NOTIFY_REMOVE:
+		/* TODO */
+		break;
+
+	case HOTPLUG_NOTIFY_FAIL:
+		/* TODO */
+		break;
+
+	default:
+		BUG();
+	}
+
+	return NOTIFY_OK;
+}
+
+void init_cluster_barrier(void)
+{
+	init_and_set_unique_id_root(&barrier_id_root, CLUSTER_BARRIER_MAX);
+	barrier_table = hashtable_new(TABLE_SIZE);
+
+	rpc_register_int (RPC_ENTER_BARRIER, handle_enter_barrier, 0);
+	rpc_register_int (RPC_EXIT_BARRIER, handle_exit_barrier, 0);
+
+	register_hotplug_notifier(barrier_notification, HOTPLUG_PRIO_BARRIER);
+}
diff -ruN linux-2.6.29/lib/hashtable.c android_cluster/linux-2.6.29/lib/hashtable.c
--- linux-2.6.29/lib/hashtable.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/lib/hashtable.c	2014-05-27 23:04:10.478026844 -0700
@@ -0,0 +1,449 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ *  Copyright (C) 2008, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/module.h>
+#include <linux/hashtable.h>
+
+
+/*****************************************************************************/
+/*                                                                           */
+/*                           KERRIGHED HASH TABLES                           */
+/*                                                                           */
+/*****************************************************************************/
+
+
+static struct hash_list HASH_LISTHEAD_NEW = { 0, NULL, NULL };
+
+
+/** Add a new element in a hash table linked list.
+ *  @author Viet Hoa Dinh
+ * 
+ *  The function must be called with the lock taken.
+ *
+ *  @param table  The table to add the element in.
+ *  @param hash   The element key.
+ *  @param data   The element to add in the table
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+static inline int hash_list_add(hashtable_t * table,
+                                unsigned long hash,
+				void * data)
+{
+	struct hash_list * ht;
+	int index;
+
+	index = hash % table->hashtable_size;
+
+	ht = kmalloc(sizeof(struct hash_list), GFP_ATOMIC);
+	if (ht == NULL)
+		return -ENOMEM;
+
+	ht->hash = hash;
+	ht->data = data;
+	ht->next = table->table[index].next;
+
+	table->table[index].next = ht;
+
+	return 0;
+}
+
+
+
+/** Remove an element from a hash table linked list.
+ *  @author Viet Hoa Dinh
+ *
+ *  The function must be called with the lock taken.
+ *
+ *  @param table  The table to remove the element from.
+ *  @param hash   The element key.
+ *
+ *  @return  0 if everything ok.
+ *           Negative value otherwise.
+ */
+static inline void *hash_list_remove(hashtable_t * table,
+				     unsigned long hash)
+{
+	struct hash_list * elem;
+	void *data;
+	int index;
+
+	index = hash % table->hashtable_size;
+
+	for(elem = &table->table[index]; elem->next != NULL; elem = elem->next) {
+		if (elem->next->hash == hash) {
+			struct hash_list * hash_data;
+
+			hash_data = elem->next;
+			data = hash_data->data;
+			elem->next = elem->next->next;
+
+			kfree(hash_data);
+			return data;
+		}
+	}
+
+	return NULL;
+}
+
+
+
+/** Free a hash table linked list.
+ *  @author Viet Hoa Dinh
+ *
+ *  The function must be called with the lock taken.
+ *
+ *  @param list  List to free.
+ */
+static inline void hash_list_free(struct hash_list * list)
+{
+	struct hash_list * elem;
+	struct hash_list * next;
+
+	next = list;
+	while (next != NULL) {
+		elem = next;
+		next = elem->next;
+		kfree(elem);
+	}
+}
+
+
+
+/** Find an element in a hash table linked list.
+ *  @author Viet Hoa Dinh
+ *
+ *  The function must be called with the lock taken.
+ *
+ *  @param head   The linked list to find the element in.
+ *  @param hash   The element key.
+ *
+ *  @return  A pointer to the found data.
+ *           NULL if the data is not found.
+ */
+static inline void * hash_list_find(struct hash_list * head,
+				    unsigned long hash)
+{
+	struct hash_list * elem;
+
+	for(elem = head; elem != NULL; elem = elem->next) {
+		if (elem->hash == hash)
+			return elem->data;
+	}
+
+	return NULL;
+}
+
+
+/** Find an element in a hash table linked list or the next elem in hash order.
+ *  @author Renaud Lottiaux
+ *
+ *  @param head   The linked list to find the element in.
+ *  @param hash   The element key.
+ *
+ *  @return  A pointer to the found data.
+ *           NULL if the data is not found.
+ *           The hash of the found data is returned in hash_found.
+ */
+static inline void * hash_list_find_equal_or_next(struct hash_list * head,
+						  unsigned long hash,
+						  unsigned long *hash_found)
+{
+	struct hash_list * elem;
+	void *found = NULL;
+
+	*hash_found = -1UL;
+	for(elem = head; elem != NULL; elem = elem->next) {
+		if (elem->hash == hash) {
+			*hash_found = elem->hash;
+			return elem->data;
+		}
+
+		if (elem->hash > hash &&
+		    elem->hash <= *hash_found) {
+			*hash_found = elem->hash;
+			found = elem->data;
+		}
+	}
+
+	return found;
+}
+
+
+/** Create a new hash table */
+
+hashtable_t *_hashtable_new(unsigned long hashtable_size)
+{
+	hashtable_t * ht;
+	int i;
+
+	ht = kmalloc(sizeof(hashtable_t), GFP_KERNEL);
+	if (ht == NULL)
+		return NULL;
+
+	ht->table = kmalloc(sizeof(struct hash_list) * hashtable_size,
+			    GFP_KERNEL);
+
+	if (ht->table == NULL)
+		return NULL;
+
+	ht->hashtable_size = hashtable_size;
+
+	for(i = 0; i < hashtable_size; i++)
+		ht->table[i] = HASH_LISTHEAD_NEW;
+
+	return ht;
+}
+EXPORT_SYMBOL(_hashtable_new);
+
+
+/** Free a hash table */
+
+void hashtable_free(hashtable_t * table)
+{
+	int i;
+	unsigned long flags;
+
+	spin_lock_irqsave (&table->lock, flags);
+
+	for(i = 0; i < table->hashtable_size; i++)
+		hash_list_free(table->table[i].next);
+
+	kfree (table->table);
+	spin_unlock_irqrestore (&table->lock, flags);
+
+	kfree(table);
+}
+EXPORT_SYMBOL(hashtable_free);
+
+
+/** Add an element in a hash table */
+
+int __hashtable_add(hashtable_t * table,
+		    unsigned long hash,
+		    void * data)
+{
+	int index;
+	int r = 0;
+
+	index = hash % table->hashtable_size;
+
+	if (table->table[index].data == NULL) {
+		table->table[index].hash = hash;
+		table->table[index].data = data;
+		table->table[index].next = NULL;
+	}
+	else
+		r = hash_list_add(table, hash, data);
+
+	return r;
+}
+EXPORT_SYMBOL(__hashtable_add);
+
+int __hashtable_add_unique(hashtable_t * table,
+			   unsigned long hash,
+			   void * data)
+{
+	int index;
+	int r = 0;
+
+	index = hash % table->hashtable_size;
+
+	if (!table->table[index].data) {
+		table->table[index].hash = hash;
+		table->table[index].data = data;
+		table->table[index].next = NULL;
+	}
+	else if (hash_list_find(&table->table[index], hash))
+		r = -EEXIST;
+	else
+		r = hash_list_add(table, hash, data);
+
+	return r;
+}
+EXPORT_SYMBOL(__hashtable_add_unique);
+
+/** Remove an element from a hash table */
+
+void *__hashtable_remove(hashtable_t * table,
+			 unsigned long hash)
+{
+	int index;
+	struct hash_list * next;
+	void *data = NULL;
+
+	index = hash % table->hashtable_size;
+
+	if (table->table[index].hash == hash) {
+		data = table->table[index].data;
+
+		if ((next = table->table[index].next) != NULL) {
+			table->table[index].hash = next->hash;
+			table->table[index].data = next->data;
+			table->table[index].next = next->next;
+			kfree(next);
+		}
+		else {
+			table->table[index].hash = 0;
+			table->table[index].data = NULL;
+		}
+	}
+	else
+		data = hash_list_remove(table, hash);
+
+	return data;
+}
+EXPORT_SYMBOL(__hashtable_remove);
+
+
+/** Find an element in a hash table */
+
+void * __hashtable_find(hashtable_t * table,
+			unsigned long hash)
+{
+	int index;
+
+	index = hash % table->hashtable_size;
+
+	return hash_list_find(&table->table[index], hash);
+}
+EXPORT_SYMBOL(__hashtable_find);
+
+
+/** Find an element in a hash table */
+
+void * __hashtable_find_next(hashtable_t * table,
+			     unsigned long hash,
+			     unsigned long *hash_found)
+{
+	unsigned long nearest_possible;
+	unsigned long nearest_found, i;
+	int index;
+	void *found_data = NULL, *data;
+
+	if (hash == -1UL)
+		return NULL;
+
+	nearest_found = -1UL;
+	nearest_possible = hash + 1;
+
+	for (i = hash + 1; i <= hash + table->hashtable_size; i++) {
+		index = i % table->hashtable_size;
+		data = hash_list_find_equal_or_next (&table->table[index],
+						     i, hash_found);
+
+		if (data && (*hash_found <= nearest_found)) {
+			nearest_found = *hash_found;
+			found_data = data;
+
+			if (nearest_found == nearest_possible)
+				goto done;
+		}
+
+		nearest_possible++;
+	}
+
+done:
+	*hash_found = nearest_found;
+	return found_data;
+}
+EXPORT_SYMBOL(__hashtable_find_next);
+
+
+/** Apply a function of each hash table key */
+
+void __hashtable_foreach_key(hashtable_t * table,
+			     void (* func)(unsigned long, void *),
+			     void * data)
+{
+	unsigned long index;
+	struct hash_list *cur, *elem;
+
+	for(index = 0; index < table->hashtable_size; index ++) {
+		cur = &table->table[index];
+		if (cur->data != NULL) {
+			func(cur->hash, data);
+			for(elem = cur->next; elem != NULL; elem = elem->next)
+				func(elem->hash, data);
+		}
+	}
+}
+EXPORT_SYMBOL(__hashtable_foreach_key);
+
+
+/** Apply a function of each hash table element */
+
+void __hashtable_foreach_data(hashtable_t * table,
+			      void (* func)(void *, void *),
+			      void * data)
+{
+	unsigned long index;
+	struct hash_list *cur, *elem;
+
+	for(index = 0; index < table->hashtable_size; index ++) {
+		cur = &table->table[index];
+		if (cur->data != NULL) {
+			func(cur->data, data);
+			for(elem = cur->next; elem != NULL; elem = elem->next)
+				func(elem->data, data);
+		}
+	}
+}
+EXPORT_SYMBOL(__hashtable_foreach_data);
+
+
+/** Apply a function to each hash table pair (key, element) */
+
+void __hashtable_foreach_key_data(hashtable_t * table,
+				  void (* func)(unsigned long, void *, void *),
+				  void * data)
+{
+	unsigned long index;
+	struct hash_list *cur, *elem;
+
+	for(index = 0; index < table->hashtable_size; index ++) {
+		cur = &table->table[index];
+		if (cur->data != NULL) {
+			func(cur->hash, cur->data, data);
+			for(elem = cur->next; elem != NULL; elem = elem->next)
+				func(elem->hash, elem->data, data);
+		}
+	}
+}
+EXPORT_SYMBOL(__hashtable_foreach_key_data);
+
+
+void * hashtable_find_data(hashtable_t * table,
+			   int (* func)(void *, void *),
+			   void * data)
+{
+	unsigned long index;
+	struct hash_list *cur, *elem;
+	unsigned long flags;
+	void * res = NULL;
+
+	spin_lock_irqsave (&table->lock, flags);
+
+	for(index = 0; index < table->hashtable_size; index ++) {
+		cur = &table->table[index];
+		if (cur->data != NULL) {
+			if (func(cur->data, data)) {
+				res = cur->data;
+				goto found;
+			}
+			for(elem = cur->next; elem != NULL; elem = elem->next)
+				if (func(elem->data, data)) {
+					res = elem->data;
+					goto found;
+				}
+		}
+	}
+
+found:
+	spin_unlock_irqrestore (&table->lock, flags);
+	return res;
+}
+EXPORT_SYMBOL(hashtable_find_data);
diff -ruN linux-2.6.29/lib/Kconfig android_cluster/linux-2.6.29/lib/Kconfig
--- linux-2.6.29/lib/Kconfig	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/lib/Kconfig	2014-05-27 23:04:10.478026844 -0700
@@ -174,4 +174,10 @@
        bool "Disable obsolete cpumask functions" if DEBUG_PER_CPU_MAPS
        depends on EXPERIMENTAL && BROKEN
 
+config MODULE_HOOK
+	bool "Enable module hooks"
+	help
+	  Enable modules to register/unregister hooks at various places.
+	  In-tree users are Kerrighed's scheduler probes.
+
 endmenu
diff -ruN linux-2.6.29/lib/Makefile android_cluster/linux-2.6.29/lib/Makefile
--- linux-2.6.29/lib/Makefile	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/lib/Makefile	2014-05-27 23:04:10.478026844 -0700
@@ -13,6 +13,12 @@
 	 sha1.o irq_regs.o reciprocal_div.o argv_split.o \
 	 proportions.o prio_heap.o ratelimit.o show_mem.o is_single_threaded.o
 
+ifdef CONFIG_KRGRPC
+lib-y += hashtable.o unique_id.o cluster_barrier.o remote_sleep.o
+endif
+
+lib-$(CONFIG_MODULE_HOOK) += module_hook.o
+
 lib-$(CONFIG_MMU) += ioremap.o
 lib-$(CONFIG_SMP) += cpumask.o
 
diff -ruN linux-2.6.29/lib/module_hook.c android_cluster/linux-2.6.29/lib/module_hook.c
--- linux-2.6.29/lib/module_hook.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/lib/module_hook.c	2014-05-27 23:04:10.482026760 -0700
@@ -0,0 +1,61 @@
+/*
+ *  lib/module_hook.c
+ *
+ *  Copyright (C) 2007 Louis Rilling - Kerlabs
+ */
+
+#include <linux/module.h>
+#include <linux/spinlock.h>
+#include <linux/errno.h>
+#include <linux/module_hook.h>
+
+static DEFINE_SPINLOCK(hooks_lock);
+
+int module_hook_register(struct module_hook_desc *desc,
+			 module_hook_cb_t *callback,
+			 struct module *owner)
+{
+	unsigned long flags;
+	int err = -EEXIST;
+
+	spin_lock_irqsave(&hooks_lock, flags);
+	if (!desc->callback) {
+		desc->callback = callback;
+		desc->owner = owner;
+		err = 0;
+	}
+	spin_unlock_irqrestore(&hooks_lock, flags);
+
+	return err;
+}
+EXPORT_SYMBOL(module_hook_register);
+
+/* Must only be called at module unloading */
+void module_hook_unregister(struct module_hook_desc *desc,
+			    module_hook_cb_t *callback)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&hooks_lock, flags);
+	if (desc->callback == callback)
+		desc->callback = NULL;
+	spin_unlock_irqrestore(&hooks_lock, flags);
+}
+EXPORT_SYMBOL(module_hook_unregister);
+
+void module_hook_call(struct module_hook_desc *desc, unsigned long arg)
+{
+	unsigned long flags;
+	int exec = 0;
+
+	spin_lock_irqsave(&hooks_lock, flags);
+	if (desc->callback && try_module_get(desc->owner))
+		exec = 1;
+	spin_unlock_irqrestore(&hooks_lock, flags);
+
+	if (exec) {
+		desc->callback(arg);
+		module_put(desc->owner);
+	}
+}
+EXPORT_SYMBOL(module_hook_call);
diff -ruN linux-2.6.29/lib/remote_sleep.c android_cluster/linux-2.6.29/lib/remote_sleep.c
--- linux-2.6.29/lib/remote_sleep.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/lib/remote_sleep.c	2014-05-27 23:04:10.482026760 -0700
@@ -0,0 +1,76 @@
+/*
+ *  lib/remote_sleep.c
+ *
+ *  Copyright (C) 2010 Louis Rilling - Kerlabs
+ */
+#include <linux/sched.h>
+#include <linux/signal.h>
+#include <linux/errno.h>
+#include <net/krgrpc/rpc.h>
+
+int remote_sleep_prepare(struct rpc_desc *desc)
+{
+	int dummy, err;
+
+	current->sighand->action[SIGINT - 1].sa.sa_handler = SIG_DFL;
+	err = rpc_pack_type(desc, dummy);
+	if (err)
+		ignore_signals(current);
+
+	return err;
+}
+
+void remote_sleep_finish(void)
+{
+	ignore_signals(current);
+}
+
+int unpack_remote_sleep_res_prepare(struct rpc_desc *desc)
+{
+	int dummy, err;
+
+	err = rpc_unpack_type(desc, dummy);
+	if (err > 0)
+		err = -EPIPE;
+	return err;
+}
+
+/**
+ *  Unpack the result value of a remote, sleepable, interruptible operation
+ *  @author Renaud Lottiaux
+ *
+ *  @param desc		The RPC descriptor to get the result from.
+ *  @param res		Pointer to store the result.
+ *  @param size		Size of the result.
+ *
+ *  @return		0 in case of success, or negative error code.
+ */
+int unpack_remote_sleep_res(struct rpc_desc *desc, void *res, size_t size)
+{
+	int err, flags;
+
+	flags = RPC_FLAGS_INTR;
+	for (;;) {
+		err = rpc_unpack(desc, flags, res, size);
+		switch (err) {
+			case RPC_EOK:
+				return 0;
+			case RPC_EINTR:
+				BUG_ON(flags != RPC_FLAGS_INTR);
+				rpc_signal(desc, SIGINT);
+				/*
+				 * We do not need to explicitly receive SIGACK,
+				 * since the server will return the result
+				 * anyway.
+				 */
+				flags = 0;
+				break;
+			case RPC_EPIPE:
+				return -EPIPE;
+			default:
+				BUG();
+		}
+	}
+
+	return err;
+}
diff -ruN linux-2.6.29/lib/unique_id.c android_cluster/linux-2.6.29/lib/unique_id.c
--- linux-2.6.29/lib/unique_id.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/lib/unique_id.c	2014-05-27 23:04:10.486026678 -0700
@@ -0,0 +1,92 @@
+/** Unique id generator
+ *  @file unique_id.c
+ *
+ *  Implementation of unique id generator. This mechanism generates
+ *  locally, an indentifier which is unique in the cluster.
+ *
+ *  Copyright (C) 2006-2007, Renaud Lottiaux, Kerlabs.
+ */
+
+#include <linux/hardirq.h>
+#include <linux/module.h>
+#include <linux/unique_id.h>
+#include <kerrighed/krginit.h>
+
+#define INITVAL 1
+
+unique_id_root_t mm_unique_id_root = {
+	.local_unique_id = ATOMIC_LONG_INIT(INITVAL),
+};
+
+/** Initialize a unique id root.
+ *  @author Renaud Lottiaux
+ *
+ *  @param root   The root to initialize
+ *  @return       0 if everything ok.
+ *                Negative value otherwise.
+ */
+int init_unique_id_root(unique_id_root_t *root)
+{
+	/* Value 0 is reserved for UNIQUE_ID_NONE */
+
+	atomic_long_set (&root->local_unique_id, INITVAL);
+
+	return 0;
+}
+EXPORT_SYMBOL(init_unique_id_root);
+
+
+
+/** Initialize a unique id root with a given init value.
+ *  @author Renaud Lottiaux
+ *
+ *  @param root   The root to initialize
+ *  @param base   Init of value for the key generator.
+ *  @return       0 if everything ok.
+ *                Negative value otherwise.
+ */
+int init_and_set_unique_id_root(unique_id_root_t *root, int base)
+{
+	atomic_long_set (&root->local_unique_id, base + INITVAL);
+
+	return 0;
+}
+EXPORT_SYMBOL(init_and_set_unique_id_root);
+
+
+
+/** Generate a unique id from a given root.
+ *  @author Renaud Lottiaux
+ *
+ *  @param root   The root of the unique id to generate.
+ *  @return       A unique id !
+ */
+unique_id_t get_unique_id(unique_id_root_t *root)
+{
+	unique_id_t unique_id ;
+
+	/* If the unique ID root has not been initialized... */
+	if (atomic_long_read(&root->local_unique_id) == 0)
+		return UNIQUE_ID_NONE;
+
+	unique_id = atomic_long_inc_return (&root->local_unique_id);
+
+	/* Check if there is a loop in the identitier generator */
+
+	if ((unique_id & UNIQUE_ID_LOCAL_MASK) == 0)
+		panic ("Unique id generator loop !\n");
+
+	/* Combine local unique id and local node id to generate a
+	   identifier which is unique cluster wide */
+
+	unique_id = unique_id | ((unsigned long)kerrighed_node_id << UNIQUE_ID_NODE_SHIFT);
+
+	return unique_id;
+}
+EXPORT_SYMBOL(get_unique_id);
+
+
+void init_unique_ids(void)
+{
+	init_unique_id_root(&mm_unique_id_root);
+}
diff -ruN linux-2.6.29/.mailmap android_cluster/linux-2.6.29/.mailmap
--- linux-2.6.29/.mailmap	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/.mailmap	1969-12-31 16:00:00.000000000 -0800
@@ -1,107 +0,0 @@
-#
-# This list is used by git-shortlog to fix a few botched name translations
-# in the git archive, either because the author's full name was messed up
-# and/or not always written the same way, making contributions from the
-# same person appearing not to be so or badly displayed.
-#
-# repo-abbrev: /pub/scm/linux/kernel/git/
-#
-
-Aaron Durbin <adurbin@google.com>
-Adam Oldham <oldhamca@gmail.com>
-Adam Radford <aradford@gmail.com>
-Adrian Bunk <bunk@stusta.de>
-Alan Cox <alan@lxorguk.ukuu.org.uk>
-Alan Cox <root@hraefn.swansea.linux.org.uk>
-Aleksey Gorelov <aleksey_gorelov@phoenix.com>
-Al Viro <viro@ftp.linux.org.uk>
-Al Viro <viro@zenIV.linux.org.uk>
-Andreas Herrmann <aherrman@de.ibm.com>
-Andrew Morton <akpm@osdl.org>
-Andrew Vasquez <andrew.vasquez@qlogic.com>
-Andy Adamson <andros@citi.umich.edu>
-Arnaud Patard <arnaud.patard@rtp-net.org>
-Arnd Bergmann <arnd@arndb.de>
-Axel Dyks <xl@xlsigned.net>
-Ben Gardner <bgardner@wabtec.com>
-Ben M Cahill <ben.m.cahill@intel.com>
-Björn Steinbrink <B.Steinbrink@gmx.de>
-Brian Avery <b.avery@hp.com>
-Brian King <brking@us.ibm.com>
-Christoph Hellwig <hch@lst.de>
-Corey Minyard <minyard@acm.org>
-David Brownell <david-b@pacbell.net>
-David Woodhouse <dwmw2@shinybook.infradead.org>
-Dmitry Eremin-Solenikov <dbaryshkov@gmail.com>
-Domen Puncer <domen@coderock.org>
-Douglas Gilbert <dougg@torque.net>
-Ed L. Cashin <ecashin@coraid.com>
-Evgeniy Polyakov <johnpol@2ka.mipt.ru>
-Felipe W Damasio <felipewd@terra.com.br>
-Felix Kuhling <fxkuehl@gmx.de>
-Felix Moeller <felix@derklecks.de>
-Filipe Lautert <filipe@icewall.org>
-Franck Bui-Huu <vagabon.xyz@gmail.com>
-Frank Zago <fzago@systemfabricworks.com>
-Greg Kroah-Hartman <greg@echidna.(none)>
-Greg Kroah-Hartman <gregkh@suse.de>
-Greg Kroah-Hartman <greg@kroah.com>
-Henk Vergonet <Henk.Vergonet@gmail.com>
-Henrik Kretzschmar <henne@nachtwindheim.de>
-Herbert Xu <herbert@gondor.apana.org.au>
-Jacob Shin <Jacob.Shin@amd.com>
-James Bottomley <jejb@mulgrave.(none)>
-James Bottomley <jejb@titanic.il.steeleye.com>
-James E Wilson <wilson@specifix.com>
-James Ketrenos <jketreno@io.(none)>
-Jean Tourrilhes <jt@hpl.hp.com>
-Jeff Garzik <jgarzik@pretzel.yyz.us>
-Jens Axboe <axboe@suse.de>
-Jens Osterkamp <Jens.Osterkamp@de.ibm.com>
-John Stultz <johnstul@us.ibm.com>
-Juha Yrjola <at solidboot.com>
-Juha Yrjola <juha.yrjola@nokia.com>
-Juha Yrjola <juha.yrjola@solidboot.com>
-Kay Sievers <kay.sievers@vrfy.org>
-Kenneth W Chen <kenneth.w.chen@intel.com>
-Koushik <raghavendra.koushik@neterion.com>
-Leonid I Ananiev <leonid.i.ananiev@intel.com>
-Linas Vepstas <linas@austin.ibm.com>
-Mark Brown <broonie@sirena.org.uk>
-Matthieu CASTET <castet.matthieu@free.fr>
-Michael Buesch <mb@bu3sch.de>
-Michael Buesch <mbuesch@freenet.de>
-Michel Dänzer <michel@tungstengraphics.com>
-Mitesh shah <mshah@teja.com>
-Morten Welinder <terra@gnome.org>
-Morten Welinder <welinder@anemone.rentec.com>
-Morten Welinder <welinder@darter.rentec.com>
-Morten Welinder <welinder@troll.com>
-Nguyen Anh Quynh <aquynh@gmail.com>
-Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
-Patrick Mochel <mochel@digitalimplant.org>
-Peter A Jonsson <pj@ludd.ltu.se>
-Peter Oruba <peter@oruba.de>
-Peter Oruba <peter.oruba@amd.com>
-Praveen BP <praveenbp@ti.com>
-Rajesh Shah <rajesh.shah@intel.com>
-Ralf Baechle <ralf@linux-mips.org>
-Ralf Wildenhues <Ralf.Wildenhues@gmx.de>
-Rémi Denis-Courmont <rdenis@simphalempin.com>
-Rudolf Marek <R.Marek@sh.cvut.cz>
-Rui Saraiva <rmps@joel.ist.utl.pt>
-Sachin P Sant <ssant@in.ibm.com>
-Sam Ravnborg <sam@mars.ravnborg.org>
-Sascha Hauer <s.hauer@pengutronix.de>
-S.Çağlar Onur <caglar@pardus.org.tr>
-Simon Kelley <simon@thekelleys.org.uk>
-Stéphane Witzmann <stephane.witzmann@ubpmes.univ-bpclermont.fr>
-Stephen Hemminger <shemminger@osdl.org>
-Tejun Heo <htejun@gmail.com>
-Thomas Graf <tgraf@suug.ch>
-Tony Luck <tony.luck@intel.com>
-Tsuneo Yoshioka <Tsuneo.Yoshioka@f-secure.com>
-Uwe Kleine-König <ukleinek@informatik.uni-freiburg.de>
-Uwe Kleine-König <ukl@pengutronix.de>
-Uwe Kleine-König <Uwe.Kleine-Koenig@digi.com>
-Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
diff -ruN linux-2.6.29/Makefile android_cluster/linux-2.6.29/Makefile
--- linux-2.6.29/Makefile	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/Makefile	2014-05-27 23:04:07.810082292 -0700
@@ -1,7 +1,7 @@
 VERSION = 2
 PATCHLEVEL = 6
 SUBLEVEL = 29
-EXTRAVERSION =
+EXTRAVERSION = -krg
 NAME = Temporary Tasmanian Devil
 
 # *DOCUMENTATION*
@@ -636,7 +636,7 @@
 
 
 ifeq ($(KBUILD_EXTMOD),)
-core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/ block/
+core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/ block/ kerrighed/ kddm/
 
 vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, $(init-y) $(init-m) \
 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
diff -ruN linux-2.6.29/mm/backing-dev.c android_cluster/linux-2.6.29/mm/backing-dev.c
--- linux-2.6.29/mm/backing-dev.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/backing-dev.c	2014-05-27 23:04:10.490026595 -0700
@@ -291,7 +291,11 @@
  * write congestion.  If no backing_devs are congested then just wait for the
  * next write to be completed.
  */
+#ifdef CONFIG_KRG_EPM
+static long __congestion_wait(int rw, long timeout)
+#else
 long congestion_wait(int rw, long timeout)
+#endif
 {
 	long ret;
 	DEFINE_WAIT(wait);
@@ -302,5 +306,20 @@
 	finish_wait(wqh, &wait);
 	return ret;
 }
+
+#ifdef CONFIG_KRG_EPM
+long congestion_wait(int rw, long timeout)
+{
+	struct task_struct *krg_cur;
+	long ret;
+
+	krg_cur = krg_current;
+	krg_current = NULL;
+	ret = __congestion_wait(rw, timeout);
+	krg_current = krg_cur;
+
+	return ret;
+}
+#endif
 EXPORT_SYMBOL(congestion_wait);
 
diff -ruN linux-2.6.29/mm/memcontrol.c android_cluster/linux-2.6.29/mm/memcontrol.c
--- linux-2.6.29/mm/memcontrol.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/memcontrol.c	2014-05-27 23:04:10.490026595 -0700
@@ -587,7 +587,11 @@
 					unsigned long *scanned, int order,
 					int mode, struct zone *z,
 					struct mem_cgroup *mem_cont,
+#ifdef CONFIG_KRG_MM
+					int active, int file, int kddm)
+#else
 					int active, int file)
+#endif
 {
 	unsigned long nr_taken = 0;
 	struct page *page;
@@ -598,7 +602,12 @@
 	int nid = z->zone_pgdat->node_id;
 	int zid = zone_idx(z);
 	struct mem_cgroup_per_zone *mz;
+#ifdef CONFIG_KRG_MM
+	int lru = BUILD_LRU_ID(!!active, !!file, !!kddm);
+	BUG_ON(kddm && file);
+#else
 	int lru = LRU_FILE * !!file + !!active;
+#endif
 
 	BUG_ON(!mem_cont);
 	mz = mem_cgroup_zoneinfo(mem_cont, nid, zid);
diff -ruN linux-2.6.29/mm/memory.c android_cluster/linux-2.6.29/mm/memory.c
--- linux-2.6.29/mm/memory.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/memory.c	2014-05-27 23:04:10.490026595 -0700
@@ -61,7 +61,9 @@
 #include <asm/tlb.h>
 #include <asm/tlbflush.h>
 #include <asm/pgtable.h>
-
+#ifdef CONFIG_KRG_MM
+#include <kerrighed/page_table_tree.h>
+#endif
 #include "internal.h"
 
 #ifndef CONFIG_NEED_MULTIPLE_NODES
@@ -537,11 +539,17 @@
  * already present in the new task to be cleared in the whole range
  * covered by this vma.
  */
-
+#ifdef CONFIG_KRG_MM
+static inline void
+copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+		pte_t *dst_pte, pte_t *src_pte, struct vm_area_struct *vma,
+		unsigned long addr, int *rss, int anon_only)
+#else
 static inline void
 copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		pte_t *dst_pte, pte_t *src_pte, struct vm_area_struct *vma,
 		unsigned long addr, int *rss)
+#endif
 {
 	unsigned long vm_flags = vma->vm_flags;
 	pte_t pte = *src_pte;
@@ -549,6 +557,12 @@
 
 	/* pte contains position in swap or file, so copy. */
 	if (unlikely(!pte_present(pte))) {
+#ifdef CONFIG_KRG_MM
+		if (pte_obj_entry(src_pte)) {
+			pte_clear(dst_mm, addr, dst_pte);
+			return;
+		}
+#endif
 		if (!pte_file(pte)) {
 			swp_entry_t entry = pte_to_swp_entry(pte);
 
@@ -594,6 +608,12 @@
 
 	page = vm_normal_page(vma, addr, pte);
 	if (page) {
+#ifdef CONFIG_KRG_MM
+		if (anon_only && !PageAnon(page)) {
+			pte_clear(dst_mm, addr, dst_pte);
+			return;
+		}
+#endif
 		get_page(page);
 		page_dup_rmap(page, vma, addr);
 		rss[!!PageAnon(page)]++;
@@ -603,9 +623,15 @@
 	set_pte_at(dst_mm, addr, dst_pte, pte);
 }
 
+#ifdef CONFIG_KRG_MM
+static int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+		pmd_t *dst_pmd, pmd_t *src_pmd, struct vm_area_struct *vma,
+		unsigned long addr, unsigned long end, int anon_only)
+#else
 static int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		pmd_t *dst_pmd, pmd_t *src_pmd, struct vm_area_struct *vma,
 		unsigned long addr, unsigned long end)
+#endif
 {
 	pte_t *src_pte, *dst_pte;
 	spinlock_t *src_ptl, *dst_ptl;
@@ -637,7 +663,12 @@
 			progress++;
 			continue;
 		}
+#ifdef CONFIG_KRG_MM
+		copy_one_pte(dst_mm, src_mm, dst_pte, src_pte, vma, addr, rss,
+			     anon_only);
+#else
 		copy_one_pte(dst_mm, src_mm, dst_pte, src_pte, vma, addr, rss);
+#endif
 		progress += 8;
 	} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);
 
@@ -652,9 +683,15 @@
 	return 0;
 }
 
+#ifdef CONFIG_KRG_MM
+static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+		pud_t *dst_pud, pud_t *src_pud, struct vm_area_struct *vma,
+		unsigned long addr, unsigned long end, int anon_only)
+#else
 static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		pud_t *dst_pud, pud_t *src_pud, struct vm_area_struct *vma,
 		unsigned long addr, unsigned long end)
+#endif
 {
 	pmd_t *src_pmd, *dst_pmd;
 	unsigned long next;
@@ -667,16 +704,27 @@
 		next = pmd_addr_end(addr, end);
 		if (pmd_none_or_clear_bad(src_pmd))
 			continue;
+#ifdef CONFIG_KRG_MM
+		if (copy_pte_range(dst_mm, src_mm, dst_pmd, src_pmd,
+				   vma, addr, next, anon_only))
+#else
 		if (copy_pte_range(dst_mm, src_mm, dst_pmd, src_pmd,
 						vma, addr, next))
+#endif
 			return -ENOMEM;
 	} while (dst_pmd++, src_pmd++, addr = next, addr != end);
 	return 0;
 }
 
+#ifdef CONFIG_KRG_MM
+static inline int copy_pud_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+		pgd_t *dst_pgd, pgd_t *src_pgd, struct vm_area_struct *vma,
+		unsigned long addr, unsigned long end, int anon_only)
+#else
 static inline int copy_pud_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		pgd_t *dst_pgd, pgd_t *src_pgd, struct vm_area_struct *vma,
 		unsigned long addr, unsigned long end)
+#endif
 {
 	pud_t *src_pud, *dst_pud;
 	unsigned long next;
@@ -689,15 +737,25 @@
 		next = pud_addr_end(addr, end);
 		if (pud_none_or_clear_bad(src_pud))
 			continue;
+#ifdef CONFIG_KRG_MM
+		if (copy_pmd_range(dst_mm, src_mm, dst_pud, src_pud,
+				   vma, addr, next, anon_only))
+#else
 		if (copy_pmd_range(dst_mm, src_mm, dst_pud, src_pud,
 						vma, addr, next))
+#endif
 			return -ENOMEM;
 	} while (dst_pud++, src_pud++, addr = next, addr != end);
 	return 0;
 }
 
+#ifdef CONFIG_KRG_MM
+int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+		struct vm_area_struct *vma, int anon_only)
+#else
 int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		struct vm_area_struct *vma)
+#endif
 {
 	pgd_t *src_pgd, *dst_pgd;
 	unsigned long next;
@@ -745,8 +803,13 @@
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(src_pgd))
 			continue;
+#ifdef CONFIG_KRG_MM
+		if (unlikely(copy_pud_range(dst_mm, src_mm, dst_pgd, src_pgd,
+					    vma, addr, next, anon_only))) {
+#else
 		if (unlikely(copy_pud_range(dst_mm, src_mm, dst_pgd, src_pgd,
 					    vma, addr, next))) {
+#endif
 			ret = -ENOMEM;
 			break;
 		}
@@ -778,6 +841,18 @@
 			continue;
 		}
 
+#ifdef CONFIG_KRG_MM
+		if (vma->vm_mm->anon_vma_kddm_set &&
+		    !(details && (details->check_mapping ||
+				  details->nonlinear_vma ))) {
+			KRGFCT(kh_zap_pte)(vma->vm_mm, addr, pte);
+			ptent = *pte;
+			if (pte_none(ptent)) {
+				(*zap_work)--;
+				continue;
+			}
+		}
+#endif
 		(*zap_work) -= PAGE_SIZE;
 
 		if (pte_present(ptent)) {
@@ -1894,6 +1969,9 @@
 	int reuse = 0, ret = 0;
 	int page_mkwrite = 0;
 	struct page *dirty_page = NULL;
+#ifdef CONFIG_KRG_MM
+	int need_vma_link_check = (vma->anon_vma == NULL);
+#endif
 
 	old_page = vm_normal_page(vma, address, orig_pte);
 	if (!old_page) {
@@ -1909,7 +1987,14 @@
 			goto reuse;
 		goto gotten;
 	}
-
+#ifdef CONFIG_KRG_MM
+	else {
+		if (vma->vm_flags & VM_KDDM) {
+			page_cache_get(old_page);
+			goto gotten;
+		}
+	}
+#endif
 	/*
 	 * Take out anonymous pages first, anonymous shared vmas are
 	 * not dirty accountable.
@@ -1991,6 +2076,31 @@
 
 	if (unlikely(anon_vma_prepare(vma)))
 		goto oom;
+
+#ifdef CONFIG_KRG_MM
+	if (need_vma_link_check && mm->anon_vma_kddm_set)
+		krg_check_vma_link(vma);
+	if (vma->vm_ops && vma->vm_ops->wppage) {
+		new_page = vma->vm_ops->wppage(vma, address & PAGE_MASK,
+					       old_page);
+		/* Check if we have called the regular SHM wppage code.
+		 * If we did so, continue with regular kernel code.
+		 */
+		if (new_page == ERR_PTR(EPERM))
+			goto continue_wppage;
+
+		if (!new_page)
+			goto oom;
+
+		if (old_page)
+			page_cache_release(old_page);
+
+		ret |= VM_FAULT_WRITE;
+		return ret;
+	}
+continue_wppage:
+#endif /* CONFIG_KRG_MM */
+
 	VM_BUG_ON(old_page == ZERO_PAGE(0));
 	new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
 	if (!new_page)
@@ -2391,9 +2501,15 @@
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_sem still held, but pte unmapped and unlocked.
  */
+#ifdef CONFIG_KRG_MM
+int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,
+		 unsigned long address, pte_t *page_table, pmd_t *pmd,
+		 int write_access, pte_t orig_pte)
+#else
 static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		unsigned long address, pte_t *page_table, pmd_t *pmd,
 		int write_access, pte_t orig_pte)
+#endif
 {
 	spinlock_t *ptl;
 	struct page *page;
@@ -2401,10 +2517,25 @@
 	pte_t pte;
 	struct mem_cgroup *ptr = NULL;
 	int ret = 0;
+#ifdef CONFIG_KRG_MM
+	struct kddm_obj *obj_entry = NULL;
+#endif
 
 	if (!pte_unmap_same(mm, pmd, page_table, orig_pte))
 		goto out;
 
+#ifdef CONFIG_KRG_MM
+	if (pte_obj_entry(&orig_pte)) {
+		obj_entry = get_obj_entry_from_pte(mm, address, &orig_pte,
+						   NULL);
+		page = obj_entry->object;
+		if (swap_pte_page(page))
+			entry = get_swap_entry_from_page(page);
+		else
+			entry.val = page_private(page);
+	}
+	else
+#endif
 	entry = pte_to_swp_entry(orig_pte);
 	if (is_migration_entry(entry)) {
 		migration_entry_wait(mm, pmd, address);
@@ -2477,6 +2608,19 @@
 		write_access = 0;
 	}
 	flush_icache_page(vma, page);
+#ifdef CONFIG_KRG_MM
+	if (obj_entry) {
+		BUG_ON (!swap_pte_obj_entry(&orig_pte));
+		wait_lock_kddm_page(page);
+		BUG_ON(page->obj_entry && page->obj_entry != obj_entry);
+		if (swap_pte_page(obj_entry->object))
+			obj_entry->object = page;
+                page->obj_entry = obj_entry;
+                atomic_inc(&page->_kddm_count);
+		unlock_kddm_page(page);
+                pte = pte_wrprotect(pte);
+	}
+#endif
 	set_pte_at(mm, address, page_table, pte);
 	page_add_anon_rmap(page, vma, address);
 	/* It's better to call commit-charge after rmap is established */
@@ -2592,10 +2736,33 @@
 	vmf.flags = flags;
 	vmf.page = NULL;
 
+#ifdef CONFIG_KRG_MM
+	vmf.pte = orig_pte;
+
+	if (flags & FAULT_FLAG_WRITE
+	    && !vma->anon_vma
+	    && !(vma->vm_flags & VM_SHARED)) {
+		if (unlikely(anon_vma_prepare(vma))) {
+			anon = 1;
+			return VM_FAULT_OOM;
+		}
+		if (mm->anon_vma_kddm_set)
+			krg_check_vma_link(vma);
+	}
+#endif
 	ret = vma->vm_ops->fault(vma, &vmf);
 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))
 		return ret;
 
+#ifdef CONFIG_KRG_MM
+	/*
+	 * If we are in a KDDM linked VMA, all the mapping job has been done
+	 * by the Kerrighed MM layer.
+	 */
+	if (vma->vm_flags & VM_KDDM)
+		return ret;
+#endif
+
 	/*
 	 * For consistency in subsequent calls, make the faulted page always
 	 * locked.
@@ -2794,7 +2961,11 @@
 
 	entry = *pte;
 	if (!pte_present(entry)) {
+#ifdef CONFIG_KRG_MM
+		if (pte_none(entry) || pte_obj_entry(pte)) {
+#else
 		if (pte_none(entry)) {
+#endif
 			if (vma->vm_ops) {
 				if (likely(vma->vm_ops->fault))
 					return do_linear_fault(mm, vma, address,
diff -ruN linux-2.6.29/mm/mempool.c android_cluster/linux-2.6.29/mm/mempool.c
--- linux-2.6.29/mm/mempool.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/mempool.c	2014-05-27 23:04:10.494026511 -0700
@@ -205,6 +205,9 @@
 	unsigned long flags;
 	wait_queue_t wait;
 	gfp_t gfp_temp;
+#ifdef CONFIG_KRG_EPM
+	struct task_struct *krg_cur;
+#endif
 
 	might_sleep_if(gfp_mask & __GFP_WAIT);
 
@@ -232,6 +235,10 @@
 	if (!(gfp_mask & __GFP_WAIT))
 		return NULL;
 
+#ifdef CONFIG_KRG_EPM
+	krg_cur = krg_current;
+	krg_current = NULL;
+#endif
 	/* Now start performing page reclaim */
 	gfp_temp = gfp_mask;
 	init_wait(&wait);
@@ -245,6 +252,9 @@
 		io_schedule_timeout(5*HZ);
 	}
 	finish_wait(&pool->wait, &wait);
+#ifdef CONFIG_KRG_EPM
+	krg_current = krg_cur;
+#endif
 
 	goto repeat_alloc;
 }
diff -ruN linux-2.6.29/mm/mmap.c android_cluster/linux-2.6.29/mm/mmap.c
--- linux-2.6.29/mm/mmap.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/mmap.c	2014-06-09 18:19:41.948330441 -0700
@@ -33,6 +33,11 @@
 #include <asm/tlb.h>
 #include <asm/mmu_context.h>
 
+#ifdef CONFIG_KRG_MM
+#include <kerrighed/krgsyms.h>
+#include <kerrighed/dynamic_node_info_linker.h>
+#endif
+
 #include "internal.h"
 
 #ifndef arch_mmap_check
@@ -43,9 +48,11 @@
 #define arch_rebalance_pgtables(addr, len)		(addr)
 #endif
 
+#ifndef CONFIG_KRG_MM
 static void unmap_region(struct mm_struct *mm,
 		struct vm_area_struct *vma, struct vm_area_struct *prev,
 		unsigned long start, unsigned long end);
+#endif
 
 /*
  * WARNING: the debugging will use recursive algorithms so never enable this
@@ -105,6 +112,10 @@
 int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin)
 {
 	unsigned long free, allowed;
+#ifdef CONFIG_KRG_MM
+	krg_dynamic_node_info_t *dyn_info;
+	kerrighed_node_t node;
+#endif
 
 	vm_acct_memory(pages);
 
@@ -161,6 +172,24 @@
 		if (free > pages)
 			return 0;
 
+#ifdef CONFIG_KRG_MM
+		/* Now, check for cluster wide memory space if the process
+		 * has the USE_REMOTE_MEMORY capability.
+		 */
+		if (!can_use_krg_cap(current, CAP_USE_REMOTE_MEMORY))
+			goto error;
+
+		for_each_online_krgnode(node) {
+			if (node == kerrighed_node_id)
+				continue;
+			dyn_info = get_dynamic_node_info(node);
+			free += dyn_info->freeram - dyn_info->freeram / 32;
+			free += dyn_info->nr_file_pages;
+			if (free > pages)
+				return 0;
+		}
+#endif
+
 		goto error;
 	}
 
@@ -228,7 +257,10 @@
 /*
  * Close a vm structure and free it, returning the next.
  */
-static struct vm_area_struct *remove_vma(struct vm_area_struct *vma)
+#ifndef CONFIG_KRG_MM
+static
+#endif
+struct vm_area_struct *remove_vma(struct vm_area_struct *vma)
 {
 	struct vm_area_struct *next = vma->vm_next;
 
@@ -245,6 +277,14 @@
 	return next;
 }
 
+#ifdef CONFIG_KRG_MM
+unsigned long __sys_brk(struct mm_struct *mm, unsigned long brk,
+			unsigned long lock_limit, unsigned long data_limit)
+{
+	unsigned long rlim = data_limit, retval;
+	unsigned long newbrk, oldbrk;
+	unsigned long min_brk;
+#else
 SYSCALL_DEFINE1(brk, unsigned long, brk)
 {
 	unsigned long rlim, retval;
@@ -253,6 +293,7 @@
 	unsigned long min_brk;
 
 	down_write(&mm->mmap_sem);
+#endif
 
 #ifdef CONFIG_COMPAT_BRK
 	min_brk = mm->end_code;
@@ -268,7 +309,9 @@
 	 * segment grow beyond its set limit the in case where the limit is
 	 * not page aligned -Ram Gupta
 	 */
+#ifndef CONFIG_KRG_MM
 	rlim = current->signal->rlim[RLIMIT_DATA].rlim_cur;
+#endif
 	if (rlim < RLIM_INFINITY && (brk - mm->start_brk) +
 			(mm->end_data - mm->start_data) > rlim)
 		goto out;
@@ -290,16 +333,45 @@
 		goto out;
 
 	/* Ok, looks good - let it rip. */
+#ifdef CONFIG_KRG_MM
+	if (__do_brk(mm, oldbrk, newbrk-oldbrk, lock_limit) != oldbrk)
+#else
 	if (do_brk(oldbrk, newbrk-oldbrk) != oldbrk)
+#endif
 		goto out;
 set_brk:
 	mm->brk = brk;
 out:
 	retval = mm->brk;
+#ifndef CONFIG_KRG_MM
 	up_write(&mm->mmap_sem);
+#endif
 	return retval;
 }
 
+#ifdef CONFIG_KRG_MM
+SYSCALL_DEFINE1(brk, unsigned long, brk)
+{
+	struct mm_struct *mm = current->mm;
+	unsigned long retval;
+
+	down_write(&mm->mmap_sem);
+
+	retval = __sys_brk(mm, brk,
+			   current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur,
+			   current->signal->rlim[RLIMIT_DATA].rlim_cur);
+
+	if (mm->anon_vma_kddm_set)
+		krg_do_brk(mm, brk,
+			   current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur,
+			   current->signal->rlim[RLIMIT_DATA].rlim_cur);
+
+	up_write(&mm->mmap_sem);
+
+	return retval;
+}
+#endif
+
 #ifdef DEBUG_MM_RB
 static int browse_rb(struct rb_root *root)
 {
@@ -411,7 +483,10 @@
 	rb_insert_color(&vma->vm_rb, &mm->mm_rb);
 }
 
-static void __vma_link_file(struct vm_area_struct *vma)
+#ifndef CONFIG_KRG_MM
+static
+#endif
+void __vma_link_file(struct vm_area_struct *vma)
 {
 	struct file *file;
 
@@ -659,7 +734,11 @@
 }
 
 /* Flags that can be inherited from an existing mapping when merging */
+#ifdef CONFIG_KRG_MM
+#define VM_MERGEABLE_FLAGS (VM_CAN_NONLINEAR|VM_KDDM)
+#else
 #define VM_MERGEABLE_FLAGS (VM_CAN_NONLINEAR)
+#endif
 
 /*
  * If the vma has a ->close operation then the driver probably needs to release
@@ -674,6 +753,11 @@
 		return 0;
 	if (vma->vm_ops && vma->vm_ops->close)
 		return 0;
+
+#ifdef CONFIG_KRG_MM
+	if (!(vma->vm_flags & VM_KDDM) && (vm_flags & VM_KDDM))
+		return 0;
+#endif
 	return 1;
 }
 
@@ -1102,11 +1186,26 @@
 	return (vm_flags & (VM_NORESERVE | VM_SHARED | VM_WRITE)) == VM_WRITE;
 }
 
+#ifdef CONFIG_KRG_MM
+unsigned long mmap_region(struct file *file, unsigned long addr,
+			  unsigned long len, unsigned long flags,
+			  unsigned int vm_flags, unsigned long pgoff)
+{
+	return __mmap_region(current->mm, file, addr, len, flags, vm_flags,
+			     pgoff, 0);
+}
+unsigned long __mmap_region(struct mm_struct *mm, struct file *file,
+			    unsigned long addr, unsigned long len,
+			    unsigned long flags, unsigned int vm_flags,
+			    unsigned long pgoff, int handler_call)
+{
+#else
 unsigned long mmap_region(struct file *file, unsigned long addr,
 			  unsigned long len, unsigned long flags,
 			  unsigned int vm_flags, unsigned long pgoff)
 {
 	struct mm_struct *mm = current->mm;
+#endif
 	struct vm_area_struct *vma, *prev;
 	int correct_wcount = 0;
 	int error;
@@ -1147,7 +1246,11 @@
 	 */
 	if (accountable_mapping(file, vm_flags)) {
 		charged = len >> PAGE_SHIFT;
+#ifdef CONFIG_KRG_MM
+		if (security_vm_enough_memory_mm(mm, charged))
+#else
 		if (security_vm_enough_memory(charged))
+#endif
 			return -ENOMEM;
 		vm_flags |= VM_ACCOUNT;
 	}
@@ -1231,6 +1334,10 @@
 		mm->locked_vm += (len >> PAGE_SHIFT) - nr_pages;
 	} else if ((flags & MAP_POPULATE) && !(flags & MAP_NONBLOCK))
 		make_pages_present(addr, addr + len);
+#ifdef CONFIG_KRG_MM
+	if (!handler_call && mm->anon_vma_kddm_set)
+		krg_do_mmap_region(vma, flags, vm_flags);
+#endif
 	return addr;
 
 unmap_and_free_vma:
@@ -1435,6 +1542,24 @@
 		mm->free_area_cache = mm->mmap_base;
 }
 
+#ifdef CONFIG_KRG_MM
+unsigned long
+get_unmapped_area(struct file *file, unsigned long addr, unsigned long len,
+		  unsigned long pgoff, unsigned long flags)
+{
+	return __get_unmapped_area (current->mm, file, addr, len, pgoff, flags);
+}
+
+unsigned long
+	__get_unmapped_area(struct mm_struct *mm, struct file *file,
+			    unsigned long addr, unsigned long len,
+			    unsigned long pgoff, unsigned long flags)
+{
+	unsigned long (*get_area)(struct file *, unsigned long,
+				  unsigned long, unsigned long, unsigned long);
+
+	get_area = mm->get_unmapped_area;
+#else
 unsigned long
 get_unmapped_area(struct file *file, unsigned long addr, unsigned long len,
 		unsigned long pgoff, unsigned long flags)
@@ -1443,6 +1568,7 @@
 				  unsigned long, unsigned long, unsigned long);
 
 	get_area = current->mm->get_unmapped_area;
+#endif
 	if (file && file->f_op && file->f_op->get_unmapped_area)
 		get_area = file->f_op->get_unmapped_area;
 	addr = get_area(file, addr, len, pgoff, flags);
@@ -1684,9 +1810,12 @@
 {
 	return expand_downwards(vma, address);
 }
-
 #ifdef CONFIG_STACK_GROWSUP
+#ifdef CONFIG_KRG_MM
+int __expand_stack(struct vm_area_struct *vma, unsigned long address)
+#else
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
+#endif
 {
 	return expand_upwards(vma, address);
 }
@@ -1709,7 +1838,11 @@
 	return prev;
 }
 #else
+#ifdef CONFIG_KRG_MM
+int __expand_stack(struct vm_area_struct *vma, unsigned long address)
+#else
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
+#endif
 {
 	return expand_downwards(vma, address);
 }
@@ -1745,7 +1878,10 @@
  *
  * Called with the mm semaphore held.
  */
-static void remove_vma_list(struct mm_struct *mm, struct vm_area_struct *vma)
+#ifndef CONFIG_KRG_MM
+static
+#endif
+void remove_vma_list(struct mm_struct *mm, struct vm_area_struct *vma)
 {
 	/* Update high watermark before we lower total_vm */
 	update_hiwater_vm(mm);
@@ -1764,7 +1900,10 @@
  *
  * Called with the mm semaphore held.
  */
-static void unmap_region(struct mm_struct *mm,
+#ifndef CONFIG_KRG_MM
+static
+#endif
+void unmap_region(struct mm_struct *mm,
 		struct vm_area_struct *vma, struct vm_area_struct *prev,
 		unsigned long start, unsigned long end)
 {
@@ -1786,7 +1925,10 @@
  * Create a list of vma's touched by the unmap, removing them from the mm's
  * vma list as we go..
  */
-static void
+#ifndef CONFIG_KRG_MM
+static
+#endif
+void
 detach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,
 	struct vm_area_struct *prev, unsigned long end)
 {
@@ -1955,6 +2097,12 @@
 	down_write(&mm->mmap_sem);
 	ret = do_munmap(mm, addr, len);
 	up_write(&mm->mmap_sem);
+
+#ifdef CONFIG_KRG_MM
+	if (!ret && mm->anon_vma_kddm_set)
+		krg_do_munmap(mm, addr, len);
+#endif
+
 	return ret;
 }
 
@@ -1973,9 +2121,20 @@
  *  anonymous maps.  eventually we may be able to do some
  *  brk-specific accounting here.
  */
+#ifdef CONFIG_KRG_MM
+unsigned long do_brk(unsigned long addr, unsigned long len)
+{
+	return __do_brk(current->mm, addr, len,
+			current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur);
+}
+unsigned long __do_brk(struct mm_struct * mm, unsigned long addr,
+		       unsigned long len, unsigned long _lock_limit)
+{
+#else
 unsigned long do_brk(unsigned long addr, unsigned long len)
 {
 	struct mm_struct * mm = current->mm;
+#endif
 	struct vm_area_struct * vma, * prev;
 	unsigned long flags;
 	struct rb_node ** rb_link, * rb_parent;
@@ -2009,7 +2168,11 @@
 		unsigned long locked, lock_limit;
 		locked = len >> PAGE_SHIFT;
 		locked += mm->locked_vm;
+#ifdef CONFIG_KRG_MM
+		lock_limit = _lock_limit;
+#else
 		lock_limit = current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur;
+#endif
 		lock_limit >>= PAGE_SHIFT;
 		if (locked > lock_limit && !capable(CAP_IPC_LOCK))
 			return -EAGAIN;
@@ -2039,7 +2202,11 @@
 	if (mm->map_count > sysctl_max_map_count)
 		return -ENOMEM;
 
+#ifdef CONFIG_KRG_MM
+	if (security_vm_enough_memory_mm(mm, len >> PAGE_SHIFT))
+#else
 	if (security_vm_enough_memory(len >> PAGE_SHIFT))
+#endif
 		return -ENOMEM;
 
 	/* Can we just expand an old private anonymous mapping? */
@@ -2065,6 +2232,10 @@
 	vma->vm_page_prot = vm_get_page_prot(flags);
 	vma_link(mm, vma, prev, rb_link, rb_parent);
 out:
+#ifdef CONFIG_KRG_MM
+	if (mm->anon_vma_kddm_set)
+		krg_check_vma_link(vma);
+#endif
 	mm->total_vm += len >> PAGE_SHIFT;
 	if (flags & VM_LOCKED) {
 		if (!mlock_vma_pages_range(vma, addr, addr + len))
@@ -2072,7 +2243,6 @@
 	}
 	return addr;
 }
-
 EXPORT_SYMBOL(do_brk);
 
 /* Release all mmaps. */
@@ -2264,11 +2434,26 @@
 {
 }
 
-static struct vm_operations_struct special_mapping_vmops = {
+#ifndef CONFIG_KRG_MM
+static
+#endif
+struct vm_operations_struct special_mapping_vmops = {
 	.close = special_mapping_close,
 	.fault = special_mapping_fault,
 };
 
+#ifdef CONFIG_KRG_MM
+int special_mapping_vm_ops_krgsyms_register(void)
+{
+	return krgsyms_register(KRGSYMS_VM_OPS_SPECIAL_MAPPING, &special_mapping_vmops);
+}
+
+int special_mapping_vm_ops_krgsyms_unregister(void)
+{
+	return krgsyms_unregister(KRGSYMS_VM_OPS_SPECIAL_MAPPING);
+}
+#endif
+
 /*
  * Called with mm->mmap_sem held for writing.
  * Insert a new vma covering the given region, with the given flags.
diff -ruN linux-2.6.29/mm/mprotect.c android_cluster/linux-2.6.29/mm/mprotect.c
--- linux-2.6.29/mm/mprotect.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/mprotect.c	2014-05-27 23:04:10.494026511 -0700
@@ -218,9 +218,27 @@
 	return error;
 }
 
+#ifdef CONFIG_KRG_MM
 SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
 		unsigned long, prot)
 {
+	int res;
+
+	res = do_mprotect(current->mm, start, len, prot, current->personality);
+
+	if (!res && current->mm->anon_vma_kddm_set)
+		krg_do_mprotect(current->mm, start, len, prot,
+				current->personality);
+
+	return res;
+}
+int do_mprotect(struct mm_struct *mm, unsigned long start, size_t len,
+		unsigned long prot, int personality)
+#else
+SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
+		unsigned long, prot)
+#endif
+{
 	unsigned long vm_flags, nstart, end, tmp, reqprot;
 	struct vm_area_struct *vma, *prev;
 	int error = -EINVAL;
@@ -244,14 +262,22 @@
 	/*
 	 * Does the application expect PROT_READ to imply PROT_EXEC:
 	 */
+#ifdef CONFIG_KRG_MM
+	if ((prot & PROT_READ) && (personality & READ_IMPLIES_EXEC))
+#else
 	if ((prot & PROT_READ) && (current->personality & READ_IMPLIES_EXEC))
+#endif
 		prot |= PROT_EXEC;
 
 	vm_flags = calc_vm_prot_bits(prot);
 
+#ifdef CONFIG_KRG_MM
+	down_write(&mm->mmap_sem);
+	vma = find_vma_prev(mm, start, &prev);
+#else
 	down_write(&current->mm->mmap_sem);
-
 	vma = find_vma_prev(current->mm, start, &prev);
+#endif
 	error = -ENOMEM;
 	if (!vma)
 		goto out;
@@ -313,6 +339,10 @@
 		}
 	}
 out:
+#ifdef CONFIG_KRG_MM
+	up_write(&mm->mmap_sem);
+#else
 	up_write(&current->mm->mmap_sem);
+#endif
 	return error;
 }
diff -ruN linux-2.6.29/mm/mremap.c android_cluster/linux-2.6.29/mm/mremap.c
--- linux-2.6.29/mm/mremap.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/mremap.c	2014-06-09 18:19:41.948330441 -0700
@@ -254,11 +254,30 @@
  * MREMAP_FIXED option added 5-Dec-1999 by Benjamin LaHaise
  * This option implies MREMAP_MAYMOVE.
  */
+#ifdef CONFIG_KRG_MM
+unsigned long do_mremap(unsigned long addr,
+			unsigned long old_len, unsigned long new_len,
+			unsigned long flags, unsigned long new_addr)
+{
+	unsigned long dummy = 0;
+
+	return __do_mremap(current->mm, addr, old_len, new_len, flags, new_addr,
+			   &dummy,
+			   current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur);
+}
+
+unsigned long __do_mremap(struct mm_struct *mm, unsigned long addr,
+			  unsigned long old_len, unsigned long new_len,
+			  unsigned long flags, unsigned long new_addr,
+			  unsigned long *_new_addr, unsigned long _lock_limit)
+{
+#else
 unsigned long do_mremap(unsigned long addr,
 	unsigned long old_len, unsigned long new_len,
 	unsigned long flags, unsigned long new_addr)
 {
 	struct mm_struct *mm = current->mm;
+#endif
 	struct vm_area_struct *vma;
 	unsigned long ret = -EINVAL;
 	unsigned long charged = 0;
@@ -344,7 +363,11 @@
 	if (vma->vm_flags & VM_LOCKED) {
 		unsigned long locked, lock_limit;
 		locked = mm->locked_vm << PAGE_SHIFT;
+#ifdef CONFIG_KRG_MM
+		lock_limit = _lock_limit;
+#else
 		lock_limit = current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur;
+#endif
 		locked += new_len - old_len;
 		ret = -EAGAIN;
 		if (locked > lock_limit && !capable(CAP_IPC_LOCK))
@@ -357,7 +380,11 @@
 
 	if (vma->vm_flags & VM_ACCOUNT) {
 		charged = (new_len - old_len) >> PAGE_SHIFT;
+#ifdef CONFIG_KRG_MM
+		if (security_vm_enough_memory_mm(mm, charged))
+#else
 		if (security_vm_enough_memory(charged))
+#endif
 			goto out_nc;
 	}
 
@@ -400,8 +427,18 @@
 			if (vma->vm_flags & VM_MAYSHARE)
 				map_flags |= MAP_SHARED;
 
+#ifdef CONFIG_KRG_MM
+			if (*_new_addr == 0) {
+				new_addr = get_unmapped_area(vma->vm_file, 0,
+					     new_len, vma->vm_pgoff, map_flags);
+				*_new_addr = new_addr;
+			}
+			else
+				new_addr = *_new_addr;
+#else
 			new_addr = get_unmapped_area(vma->vm_file, 0, new_len,
 						vma->vm_pgoff, map_flags);
+#endif
 			if (new_addr & ~PAGE_MASK) {
 				ret = new_addr;
 				goto out;
@@ -425,9 +462,20 @@
 		unsigned long, new_addr)
 {
 	unsigned long ret;
+	unsigned long _new_addr = 0;
 
 	down_write(&current->mm->mmap_sem);
-	ret = do_mremap(addr, old_len, new_len, flags, new_addr);
+	ret = __do_mremap(current->mm, addr, old_len, new_len, flags, new_addr,
+			  &_new_addr,
+			  current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur);
 	up_write(&current->mm->mmap_sem);
+
+#ifdef CONFIG_KRG_MM
+	if (!(ret & ~PAGE_MASK) && current->mm->anon_vma_kddm_set)
+		krg_do_mremap(current->mm, addr, old_len, new_len, flags,
+			      new_addr, _new_addr,
+			      current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur);
+#endif
+
 	return ret;
 }
diff -ruN linux-2.6.29/mm/mremap.c.old android_cluster/linux-2.6.29/mm/mremap.c.old
--- linux-2.6.29/mm/mremap.c.old	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/mm/mremap.c.old	2014-05-27 23:04:10.494026511 -0700
@@ -0,0 +1,633 @@
+/*
+ *	mm/mremap.c
+ *
+ *	(C) Copyright 1996 Linus Torvalds
+ *
+ *	Address space accounting code	<alan@lxorguk.ukuu.org.uk>
+ *	(C) Copyright 2002 Red Hat Inc, All Rights Reserved
+ */
+
+#include <linux/mm.h>
+#include <linux/hugetlb.h>
+#include <linux/slab.h>
+#include <linux/shm.h>
+#include <linux/mman.h>
+#include <linux/swap.h>
+#include <linux/capability.h>
+#include <linux/fs.h>
+#include <linux/highmem.h>
+#include <linux/security.h>
+#include <linux/syscalls.h>
+#include <linux/mmu_notifier.h>
+
+#include <asm/uaccess.h>
+#include <asm/cacheflush.h>
+#include <asm/tlbflush.h>
+
+#include "internal.h"
+
+static pmd_t *get_old_pmd(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+
+	pgd = pgd_offset(mm, addr);
+	if (pgd_none_or_clear_bad(pgd))
+		return NULL;
+
+	pud = pud_offset(pgd, addr);
+	if (pud_none_or_clear_bad(pud))
+		return NULL;
+
+	pmd = pmd_offset(pud, addr);
+	if (pmd_none_or_clear_bad(pmd))
+		return NULL;
+
+	return pmd;
+}
+
+static pmd_t *alloc_new_pmd(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+
+	pgd = pgd_offset(mm, addr);
+	pud = pud_alloc(mm, pgd, addr);
+	if (!pud)
+		return NULL;
+
+	pmd = pmd_alloc(mm, pud, addr);
+	if (!pmd)
+		return NULL;
+
+	if (!pmd_present(*pmd) && __pte_alloc(mm, pmd, addr))
+		return NULL;
+
+	return pmd;
+}
+
+static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,
+		unsigned long old_addr, unsigned long old_end,
+		struct vm_area_struct *new_vma, pmd_t *new_pmd,
+		unsigned long new_addr)
+{
+	struct address_space *mapping = NULL;
+	struct mm_struct *mm = vma->vm_mm;
+	pte_t *old_pte, *new_pte, pte;
+	spinlock_t *old_ptl, *new_ptl;
+	unsigned long old_start;
+
+	old_start = old_addr;
+	mmu_notifier_invalidate_range_start(vma->vm_mm,
+					    old_start, old_end);
+	if (vma->vm_file) {
+		/*
+		 * Subtle point from Rajesh Venkatasubramanian: before
+		 * moving file-based ptes, we must lock vmtruncate out,
+		 * since it might clean the dst vma before the src vma,
+		 * and we propagate stale pages into the dst afterward.
+		 */
+		mapping = vma->vm_file->f_mapping;
+		spin_lock(&mapping->i_mmap_lock);
+		if (new_vma->vm_truncate_count &&
+		    new_vma->vm_truncate_count != vma->vm_truncate_count)
+			new_vma->vm_truncate_count = 0;
+	}
+
+	/*
+	 * We don't have to worry about the ordering of src and dst
+	 * pte locks because exclusive mmap_sem prevents deadlock.
+	 */
+	old_pte = pte_offset_map_lock(mm, old_pmd, old_addr, &old_ptl);
+ 	new_pte = pte_offset_map_nested(new_pmd, new_addr);
+	new_ptl = pte_lockptr(mm, new_pmd);
+	if (new_ptl != old_ptl)
+		spin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);
+	arch_enter_lazy_mmu_mode();
+
+	for (; old_addr < old_end; old_pte++, old_addr += PAGE_SIZE,
+				   new_pte++, new_addr += PAGE_SIZE) {
+		if (pte_none(*old_pte))
+			continue;
+		pte = ptep_clear_flush(vma, old_addr, old_pte);
+		pte = move_pte(pte, new_vma->vm_page_prot, old_addr, new_addr);
+		set_pte_at(mm, new_addr, new_pte, pte);
+	}
+
+	arch_leave_lazy_mmu_mode();
+	if (new_ptl != old_ptl)
+		spin_unlock(new_ptl);
+	pte_unmap_nested(new_pte - 1);
+	pte_unmap_unlock(old_pte - 1, old_ptl);
+	if (mapping)
+		spin_unlock(&mapping->i_mmap_lock);
+	mmu_notifier_invalidate_range_end(vma->vm_mm, old_start, old_end);
+}
+
+#define LATENCY_LIMIT	(64 * PAGE_SIZE)
+
+unsigned long move_page_tables(struct vm_area_struct *vma,
+		unsigned long old_addr, struct vm_area_struct *new_vma,
+		unsigned long new_addr, unsigned long len)
+{
+	unsigned long extent, next, old_end;
+	pmd_t *old_pmd, *new_pmd;
+
+	old_end = old_addr + len;
+	flush_cache_range(vma, old_addr, old_end);
+
+	for (; old_addr < old_end; old_addr += extent, new_addr += extent) {
+		cond_resched();
+		next = (old_addr + PMD_SIZE) & PMD_MASK;
+		if (next - 1 > old_end)
+			next = old_end;
+		extent = next - old_addr;
+		old_pmd = get_old_pmd(vma->vm_mm, old_addr);
+		if (!old_pmd)
+			continue;
+		new_pmd = alloc_new_pmd(vma->vm_mm, new_addr);
+		if (!new_pmd)
+			break;
+		next = (new_addr + PMD_SIZE) & PMD_MASK;
+		if (extent > next - new_addr)
+			extent = next - new_addr;
+		if (extent > LATENCY_LIMIT)
+			extent = LATENCY_LIMIT;
+		move_ptes(vma, old_pmd, old_addr, old_addr + extent,
+				new_vma, new_pmd, new_addr);
+	}
+
+	return len + old_addr - old_end;	/* how much done */
+}
+
+static unsigned long move_vma(struct vm_area_struct *vma,
+		unsigned long old_addr, unsigned long old_len,
+		unsigned long new_len, unsigned long new_addr)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	struct vm_area_struct *new_vma;
+	unsigned long vm_flags = vma->vm_flags;
+	unsigned long new_pgoff;
+	unsigned long moved_len;
+	unsigned long excess = 0;
+	unsigned long hiwater_vm;
+	int split = 0;
+
+	/*
+	 * We'd prefer to avoid failure later on in do_munmap:
+	 * which may split one vma into three before unmapping.
+	 */
+	if (mm->map_count >= sysctl_max_map_count - 3)
+		return -ENOMEM;
+
+	new_pgoff = vma->vm_pgoff + ((old_addr - vma->vm_start) >> PAGE_SHIFT);
+	new_vma = copy_vma(&vma, new_addr, new_len, new_pgoff);
+	if (!new_vma)
+		return -ENOMEM;
+
+	moved_len = move_page_tables(vma, old_addr, new_vma, new_addr, old_len);
+	if (moved_len < old_len) {
+		/*
+		 * On error, move entries back from new area to old,
+		 * which will succeed since page tables still there,
+		 * and then proceed to unmap new area instead of old.
+		 */
+		move_page_tables(new_vma, new_addr, vma, old_addr, moved_len);
+		vma = new_vma;
+		old_len = new_len;
+		old_addr = new_addr;
+		new_addr = -ENOMEM;
+	}
+
+	/* Conceal VM_ACCOUNT so old reservation is not undone */
+	if (vm_flags & VM_ACCOUNT) {
+		vma->vm_flags &= ~VM_ACCOUNT;
+		excess = vma->vm_end - vma->vm_start - old_len;
+		if (old_addr > vma->vm_start &&
+		    old_addr + old_len < vma->vm_end)
+			split = 1;
+	}
+
+	/*
+	 * If we failed to move page tables we still do total_vm increment
+	 * since do_munmap() will decrement it by old_len == new_len.
+	 *
+	 * Since total_vm is about to be raised artificially high for a
+	 * moment, we need to restore high watermark afterwards: if stats
+	 * are taken meanwhile, total_vm and hiwater_vm appear too high.
+	 * If this were a serious issue, we'd add a flag to do_munmap().
+	 */
+	hiwater_vm = mm->hiwater_vm;
+	mm->total_vm += new_len >> PAGE_SHIFT;
+	vm_stat_account(mm, vma->vm_flags, vma->vm_file, new_len>>PAGE_SHIFT);
+
+	if (do_munmap(mm, old_addr, old_len) < 0) {
+		/* OOM: unable to split vma, just get accounts right */
+		vm_unacct_memory(excess >> PAGE_SHIFT);
+		excess = 0;
+	}
+	mm->hiwater_vm = hiwater_vm;
+
+	/* Restore VM_ACCOUNT if one or two pieces of vma left */
+	if (excess) {
+		vma->vm_flags |= VM_ACCOUNT;
+		if (split)
+			vma->vm_next->vm_flags |= VM_ACCOUNT;
+	}
+
+	if (vm_flags & VM_LOCKED) {
+		mm->locked_vm += new_len >> PAGE_SHIFT;
+		if (new_len > old_len)
+			mlock_vma_pages_range(new_vma, new_addr + old_len,
+						       new_addr + new_len);
+	}
+
+	return new_addr;
+}
+
+/*
+ * Expand (or shrink) an existing mapping, potentially moving it at the
+ * same time (controlled by the MREMAP_MAYMOVE flag and available VM space)
+ *
+ * MREMAP_FIXED option added 5-Dec-1999 by Benjamin LaHaise
+ * This option implies MREMAP_MAYMOVE.
+ */
+#ifdef CONFIG_KRG_MM
+
+unsigned long __do_mremap(struct mm_struct *mm, unsigned long addr,
+			  unsigned long old_len, unsigned long new_len,
+			  unsigned long flags, unsigned long new_addr,
+			  unsigned long *_new_addr, unsigned long _lock_limit)
+{
+	struct vm_area_struct *vma;
+	unsigned long ret = -EINVAL;
+	unsigned long charged = 0;
+
+	if (flags & ~(MREMAP_FIXED | MREMAP_MAYMOVE))
+		goto out;
+
+	if (addr & ~PAGE_MASK)
+		goto out;
+
+	old_len = PAGE_ALIGN(old_len);
+	new_len = PAGE_ALIGN(new_len);
+
+	/*
+	 * We allow a zero old-len as a special case
+	 * for DOS-emu "duplicate shm area" thing. But
+	 * a zero new-len is nonsensical.
+	 */
+	if (!new_len)
+		goto out;
+
+	/* new_addr is only valid if MREMAP_FIXED is specified */
+	if (flags & MREMAP_FIXED) {
+		if (new_addr & ~PAGE_MASK)
+			goto out;
+		if (!(flags & MREMAP_MAYMOVE))
+			goto out;
+
+		if (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)
+			goto out;
+
+		/* Check if the location we're moving into overlaps the
+		 * old location at all, and fail if it does.
+		 */
+		if ((new_addr <= addr) && (new_addr+new_len) > addr)
+			goto out;
+
+		if ((addr <= new_addr) && (addr+old_len) > new_addr)
+			goto out;
+
+		ret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);
+		if (ret)
+			goto out;
+
+		ret = do_munmap(mm, new_addr, new_len);
+		if (ret)
+			goto out;
+	}
+
+	/*
+	 * Always allow a shrinking remap: that just unmaps
+	 * the unnecessary pages..
+	 * do_munmap does all the needed commit accounting
+	 */
+	if (old_len >= new_len) {
+		ret = do_munmap(mm, addr+new_len, old_len - new_len);
+		if (ret && old_len != new_len)
+			goto out;
+		ret = addr;
+		if (!(flags & MREMAP_FIXED) || (new_addr == addr))
+			goto out;
+		old_len = new_len;
+	}
+
+	/*
+	 * Ok, we need to grow..  or relocate.
+	 */
+	ret = -EFAULT;
+	vma = find_vma(mm, addr);
+	if (!vma || vma->vm_start > addr)
+		goto out;
+	if (is_vm_hugetlb_page(vma)) {
+		ret = -EINVAL;
+		goto out;
+	}
+	/* We can't remap across vm area boundaries */
+	if (old_len > vma->vm_end - addr)
+		goto out;
+	if (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP)) {
+		if (new_len > old_len)
+			goto out;
+	}
+	if (vma->vm_flags & VM_LOCKED) {
+		unsigned long locked, lock_limit;
+		locked = mm->locked_vm << PAGE_SHIFT;
+		lock_limit = _lock_limit;
+		locked += new_len - old_len;
+		ret = -EAGAIN;
+		if (locked > lock_limit && !capable(CAP_IPC_LOCK))
+			goto out;
+	}
+	if (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT)) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	if (vma->vm_flags & VM_ACCOUNT) {
+		charged = (new_len - old_len) >> PAGE_SHIFT;
+		if (security_vm_enough_memory_mm(mm, charged))
+			goto out_nc;
+	}
+
+	/* old_len exactly to the end of the area..
+	 * And we're not relocating the area.
+	 */
+	if (old_len == vma->vm_end - addr &&
+	    !((flags & MREMAP_FIXED) && (addr != new_addr)) &&
+	    (old_len != new_len || !(flags & MREMAP_MAYMOVE))) {
+		unsigned long max_addr = TASK_SIZE;
+		if (vma->vm_next)
+			max_addr = vma->vm_next->vm_start;
+		/* can we just expand the current mapping? */
+		if (max_addr - addr >= new_len) {
+			int pages = (new_len - old_len) >> PAGE_SHIFT;
+
+			vma_adjust(vma, vma->vm_start,
+				addr + new_len, vma->vm_pgoff, NULL);
+
+			mm->total_vm += pages;
+			vm_stat_account(mm, vma->vm_flags, vma->vm_file, pages);
+			if (vma->vm_flags & VM_LOCKED) {
+				mm->locked_vm += pages;
+				mlock_vma_pages_range(vma, addr + old_len,
+						   addr + new_len);
+			}
+			ret = addr;
+			goto out;
+		}
+	}
+
+	/*
+	 * We weren't able to just expand or shrink the area,
+	 * we need to create a new one and move it..
+	 */
+	ret = -ENOMEM;
+	if (flags & MREMAP_MAYMOVE) {
+		if (!(flags & MREMAP_FIXED)) {
+			unsigned long map_flags = 0;
+			if (vma->vm_flags & VM_MAYSHARE)
+				map_flags |= MAP_SHARED;
+
+			if (*_new_addr == 0) {
+				new_addr = get_unmapped_area(vma->vm_file, 0,
+					     new_len, vma->vm_pgoff, map_flags);
+				*_new_addr = new_addr;
+			}
+			else
+				new_addr = *_new_addr;
+			if (new_addr & ~PAGE_MASK) {
+				ret = new_addr;
+				goto out;
+			}
+
+			ret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);
+			if (ret)
+				goto out;
+		}
+		ret = move_vma(vma, addr, old_len, new_len, new_addr);
+	}
+out:
+	if (ret & ~PAGE_MASK)
+		vm_unacct_memory(charged);
+out_nc:
+	return ret;
+}
+
+unsigned long do_mremap(unsigned long addr,
+			unsigned long old_len, unsigned long new_len,
+			unsigned long flags, unsigned long new_addr)
+{
+	unsigned long dummy = 0;
+
+	return __do_mremap(current->mm, addr, old_len, new_len, flags, new_addr,
+			   &dummy,
+			   current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur);
+}
+#endif
+
+unsigned long do_mremap(unsigned long addr,
+	unsigned long old_len, unsigned long new_len,
+	unsigned long flags, unsigned long new_addr)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma;
+	unsigned long ret = -EINVAL;
+	unsigned long charged = 0;
+
+	if (flags & ~(MREMAP_FIXED | MREMAP_MAYMOVE))
+		goto out;
+
+	if (addr & ~PAGE_MASK)
+		goto out;
+
+	old_len = PAGE_ALIGN(old_len);
+	new_len = PAGE_ALIGN(new_len);
+
+	/*
+	 * We allow a zero old-len as a special case
+	 * for DOS-emu "duplicate shm area" thing. But
+	 * a zero new-len is nonsensical.
+	 */
+	if (!new_len)
+		goto out;
+
+	/* new_addr is only valid if MREMAP_FIXED is specified */
+	if (flags & MREMAP_FIXED) {
+		if (new_addr & ~PAGE_MASK)
+			goto out;
+		if (!(flags & MREMAP_MAYMOVE))
+			goto out;
+
+		if (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)
+			goto out;
+
+		/* Check if the location we're moving into overlaps the
+		 * old location at all, and fail if it does.
+		 */
+		if ((new_addr <= addr) && (new_addr+new_len) > addr)
+			goto out;
+
+		if ((addr <= new_addr) && (addr+old_len) > new_addr)
+			goto out;
+
+		ret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);
+		if (ret)
+			goto out;
+
+		ret = do_munmap(mm, new_addr, new_len);
+		if (ret)
+			goto out;
+	}
+
+	/*
+	 * Always allow a shrinking remap: that just unmaps
+	 * the unnecessary pages..
+	 * do_munmap does all the needed commit accounting
+	 */
+	if (old_len >= new_len) {
+		ret = do_munmap(mm, addr+new_len, old_len - new_len);
+		if (ret && old_len != new_len)
+			goto out;
+		ret = addr;
+		if (!(flags & MREMAP_FIXED) || (new_addr == addr))
+			goto out;
+		old_len = new_len;
+	}
+
+	/*
+	 * Ok, we need to grow..  or relocate.
+	 */
+	ret = -EFAULT;
+	vma = find_vma(mm, addr);
+	if (!vma || vma->vm_start > addr)
+		goto out;
+	if (is_vm_hugetlb_page(vma)) {
+		ret = -EINVAL;
+		goto out;
+	}
+	/* We can't remap across vm area boundaries */
+	if (old_len > vma->vm_end - addr)
+		goto out;
+	if (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP)) {
+		if (new_len > old_len)
+			goto out;
+	}
+	if (vma->vm_flags & VM_LOCKED) {
+		unsigned long locked, lock_limit;
+		locked = mm->locked_vm << PAGE_SHIFT;
+		lock_limit = current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur;
+		locked += new_len - old_len;
+		ret = -EAGAIN;
+		if (locked > lock_limit && !capable(CAP_IPC_LOCK))
+			goto out;
+	}
+	if (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT)) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	if (vma->vm_flags & VM_ACCOUNT) {
+		charged = (new_len - old_len) >> PAGE_SHIFT;
+		if (security_vm_enough_memory(charged))
+			goto out_nc;
+	}
+
+	/* old_len exactly to the end of the area..
+	 * And we're not relocating the area.
+	 */
+	if (old_len == vma->vm_end - addr &&
+	    !((flags & MREMAP_FIXED) && (addr != new_addr)) &&
+	    (old_len != new_len || !(flags & MREMAP_MAYMOVE))) {
+		unsigned long max_addr = TASK_SIZE;
+		if (vma->vm_next)
+			max_addr = vma->vm_next->vm_start;
+		/* can we just expand the current mapping? */
+		if (max_addr - addr >= new_len) {
+			int pages = (new_len - old_len) >> PAGE_SHIFT;
+
+			vma_adjust(vma, vma->vm_start,
+				addr + new_len, vma->vm_pgoff, NULL);
+
+			mm->total_vm += pages;
+			vm_stat_account(mm, vma->vm_flags, vma->vm_file, pages);
+			if (vma->vm_flags & VM_LOCKED) {
+				mm->locked_vm += pages;
+				mlock_vma_pages_range(vma, addr + old_len,
+						   addr + new_len);
+			}
+			ret = addr;
+			goto out;
+		}
+	}
+
+	/*
+	 * We weren't able to just expand or shrink the area,
+	 * we need to create a new one and move it..
+	 */
+	ret = -ENOMEM;
+	if (flags & MREMAP_MAYMOVE) {
+		if (!(flags & MREMAP_FIXED)) {
+			unsigned long map_flags = 0;
+			if (vma->vm_flags & VM_MAYSHARE)
+				map_flags |= MAP_SHARED;
+
+			new_addr = get_unmapped_area(vma->vm_file, 0, new_len,
+						vma->vm_pgoff, map_flags);
+			if (new_addr & ~PAGE_MASK) {
+				ret = new_addr;
+				goto out;
+			}
+
+			ret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);
+			if (ret)
+				goto out;
+		}
+		ret = move_vma(vma, addr, old_len, new_len, new_addr);
+	}
+out:
+	if (ret & ~PAGE_MASK)
+		vm_unacct_memory(charged);
+out_nc:
+	return ret;
+}
+
+SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,
+		unsigned long, new_len, unsigned long, flags,
+		unsigned long, new_addr)
+{
+	unsigned long ret;
+	unsigned long _new_addr = 0;
+
+	down_write(&current->mm->mmap_sem);
+#ifdef CONFIG_KRG_MM
+	ret = __do_mremap(current->mm, addr, old_len, new_len, flags, new_addr,
+			  &_new_addr,
+			  current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur);
+#else
+	ret = do_mremap(addr, old_len, new_len, flags, new_addr);
+#endif
+	up_write(&current->mm->mmap_sem);
+
+#ifdef CONFIG_KRG_MM
+	if (!(ret & ~PAGE_MASK) && current->mm->anon_vma_kddm_set)
+		krg_do_mremap(current->mm, addr, old_len, new_len, flags,
+			      new_addr, _new_addr,
+			      current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur);
+#endif
+
+	return ret;
+}
diff -ruN linux-2.6.29/mm/page_alloc.c android_cluster/linux-2.6.29/mm/page_alloc.c
--- linux-2.6.29/mm/page_alloc.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/page_alloc.c	2014-06-09 18:19:41.984326841 -0700
@@ -249,10 +249,18 @@
 
 	printk(KERN_ALERT "BUG: Bad page state in process %s  pfn:%05lx\n",
 		current->comm, page_to_pfn(page));
+#ifdef CONFIG_KRG_MM
 	printk(KERN_ALERT
-		"page:%p flags:%p count:%d mapcount:%d mapping:%p index:%lx\n",
+	        "page:%p flags:%p count:%d mapcount:%d mapping:%p index:%lx kddm_count:%d obj_entry:%p\n",
+	       page, (void *)page->flags, page_count(page),
+	       page_mapcount(page), page->mapping, page->index,
+	       page_kddm_count(page), page->obj_entry);
+#else
+	printk(KERN_ALERT
+	        "page:%p flags:%p count:%d mapcount:%d mapping:%p index:%lx\n",
 		page, (void *)page->flags, page_count(page),
 		page_mapcount(page), page->mapping, page->index);
+#endif
 
 	dump_stack();
 out:
@@ -496,6 +504,10 @@
 	if (unlikely(page_mapcount(page) |
 		(page->mapping != NULL)  |
 		(page_count(page) != 0)  |
+#ifdef CONFIG_KRG_MM
+		(page->obj_entry != NULL) |
+		(page_kddm_count(page) != 0) |
+#endif
 		(page->flags & PAGE_FLAGS_CHECK_AT_FREE))) {
 		bad_page(page);
 		return 1;
@@ -1479,6 +1491,9 @@
 	unsigned long did_some_progress;
 	unsigned long pages_reclaimed = 0;
 
+#ifdef CONFIG_KRG_MM
+	krg_notify_mem(0);
+#endif
 	might_sleep_if(wait);
 
 	if (should_fail_alloc_page(gfp_mask, order))
@@ -1714,6 +1729,9 @@
 
 void __free_pages(struct page *page, unsigned int order)
 {
+#ifdef CONFIG_KRG_MM
+	krg_notify_mem(0);
+#endif
 	if (put_page_testzero(page)) {
 		if (order == 0)
 			free_hot_page(page);
@@ -2638,6 +2656,9 @@
 				continue;
 		}
 		page = pfn_to_page(pfn);
+#ifdef CONFIG_KRG_MM
+		atomic_set(&page->_kddm_count, 0);
+#endif
 		set_page_links(page, zone, nid, pfn);
 		mminit_verify_page_links(page, zone, nid, pfn);
 		init_page_count(page);
@@ -3550,6 +3571,10 @@
 		zone->reclaim_stat.recent_rotated[1] = 0;
 		zone->reclaim_stat.recent_scanned[0] = 0;
 		zone->reclaim_stat.recent_scanned[1] = 0;
+#ifdef CONFIG_KRG_MM
+		zone->reclaim_stat.recent_scanned[2] = 0;
+		zone->reclaim_stat.recent_rotated[2] = 0;
+#endif
 		zap_zone_vm_stats(zone);
 		zone->flags = 0;
 		if (!size)
diff -ruN linux-2.6.29/mm/rmap.c android_cluster/linux-2.6.29/mm/rmap.c
--- linux-2.6.29/mm/rmap.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/rmap.c	2014-05-27 23:04:10.498026428 -0700
@@ -52,10 +52,18 @@
 #include <linux/migrate.h>
 
 #include <asm/tlbflush.h>
+#ifdef CONFIG_KRG_MM
+#include <kerrighed/page_table_tree.h>
+#include <kddm/object.h>
+#include <kddm/kddm_types.h>
+#endif
 
 #include "internal.h"
 
-static struct kmem_cache *anon_vma_cachep;
+#ifndef CONFIG_KRG_MM
+static
+#endif
+struct kmem_cache *anon_vma_cachep;
 
 static inline struct anon_vma *anon_vma_alloc(void)
 {
@@ -191,7 +199,11 @@
  * Getting a lock on a stable anon_vma from a page off the LRU is
  * tricky: page_lock_anon_vma rely on RCU to guard against the races.
  */
+#ifdef CONFIG_KRG_MM
+struct anon_vma *page_lock_anon_vma(struct page *page)
+#else
 static struct anon_vma *page_lock_anon_vma(struct page *page)
+#endif
 {
 	struct anon_vma *anon_vma;
 	unsigned long anon_mapping;
@@ -211,7 +223,11 @@
 	return NULL;
 }
 
+#ifdef CONFIG_KRG_MM
+void page_unlock_anon_vma(struct anon_vma *anon_vma)
+#else
 static void page_unlock_anon_vma(struct anon_vma *anon_vma)
+#endif
 {
 	spin_unlock(&anon_vma->lock);
 	rcu_read_unlock();
@@ -675,7 +691,16 @@
 	atomic_set(&page->_mapcount, 0); /* increment count (starts at -1) */
 	__page_set_anon_rmap(page, vma, address);
 	if (page_evictable(page, vma))
+#ifdef CONFIG_KRG_MM
+	{
+		if (PageMigratable(page))
+			lru_cache_add_lru(page, LRU_ACTIVE_MIGR);
+		else
+			lru_cache_add_lru(page, LRU_ACTIVE_ANON);
+	}
+#else
 		lru_cache_add_lru(page, LRU_ACTIVE_ANON);
+#endif
 	else
 		add_page_to_unevictable_list(page);
 }
@@ -754,8 +779,13 @@
  * Subfunctions of try_to_unmap: try_to_unmap_one called
  * repeatedly from either try_to_unmap_anon or try_to_unmap_file.
  */
+#ifdef CONFIG_KRG_MM
+int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+		     int migration)
+#else
 static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
 				int migration)
+#endif
 {
 	struct mm_struct *mm = vma->vm_mm;
 	unsigned long address;
@@ -763,6 +793,9 @@
 	pte_t pteval;
 	spinlock_t *ptl;
 	int ret = SWAP_AGAIN;
+#ifdef CONFIG_KRG_MM
+	struct kddm_obj *obj_entry = NULL;
+#endif
 
 	address = vma_address(page, vma);
 	if (address == -EFAULT)
@@ -772,6 +805,35 @@
 	if (!pte)
 		goto out;
 
+#ifdef CONFIG_KRG_MM
+	if (PageToInvalidate(page)) {
+		if ((vma->vm_flags & (VM_LOCKED|VM_RESERVED))) {
+			ret = SWAP_FAIL;
+			goto out_unmap;
+		}
+
+		/* Nuke the page table entry. */
+		flush_cache_page(vma, address, page_to_pfn(page));
+		pteval = ptep_clear_flush(vma, address, pte);
+		update_hiwater_rss(mm);
+
+		if (PageAnon(page))
+			dec_mm_counter(mm, anon_rss);
+		else
+			dec_mm_counter(mm, file_rss);
+
+		page_remove_rmap(page);
+		page_cache_release(page);
+		goto out_unmap;
+	}
+
+	if (PageToSetReadOnly(page)) {
+		ptep_set_wrprotect(mm, address, pte);
+		flush_tlb_page(vma, address);
+		goto out_unmap;
+	}
+#endif // CONFIG_KRG_MM
+
 	/*
 	 * If the page is mlock()d, we cannot swap it out.
 	 * If it's recently referenced (perhaps page_referenced
@@ -786,6 +848,23 @@
 			ret = SWAP_FAIL;
 			goto out_unmap;
 		}
+#ifdef CONFIG_KRG_MM
+		/* Avoid unmap of a page in an address space being inserted in
+		 * a KDDM or in use in the KDDM layer */
+		obj_entry = page->obj_entry;
+		if (obj_entry) {
+			if ((mm->anon_vma_kddm_id && !mm->anon_vma_kddm_set) ||
+			    object_frozen(obj_entry, NULL)) {
+				ret = SWAP_FAIL;
+				goto out_unmap;
+			}
+
+			if (TEST_AND_SET_OBJECT_LOCKED(obj_entry)) {
+				ret = SWAP_FAIL;
+				goto out_unmap;
+			}
+		}
+#endif
   	}
 
 	/* Nuke the page table entry. */
@@ -826,6 +905,16 @@
 		}
 		set_pte_at(mm, address, pte, swp_entry_to_pte(entry));
 		BUG_ON(pte_file(*pte));
+#ifdef CONFIG_KRG_MM
+		wait_lock_kddm_page(page);
+		if (obj_entry && mm->anon_vma_kddm_id) {
+			obj_entry->object = (void*) mk_swap_pte_page(pte);
+			set_swap_pte_obj_entry(pte, obj_entry);
+			if (atomic_dec_and_test(&page->_kddm_count))
+				page->obj_entry = NULL;
+		}
+		unlock_kddm_page(page);
+#endif
 	} else if (PAGE_MIGRATION && migration) {
 		/* Establish migration entry for a file page */
 		swp_entry_t entry;
@@ -834,6 +923,10 @@
 	} else
 		dec_mm_counter(mm, file_rss);
 
+#ifdef CONFIG_KRG_MM
+	if (obj_entry)
+		CLEAR_OBJECT_LOCKED(obj_entry);
+#endif
 
 	page_remove_rmap(page);
 	page_cache_release(page);
diff -ruN linux-2.6.29/mm/shmem.c android_cluster/linux-2.6.29/mm/shmem.c
--- linux-2.6.29/mm/shmem.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/shmem.c	2014-06-09 18:19:42.020323239 -0700
@@ -161,6 +161,10 @@
 	return sb->s_fs_info;
 }
 
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/krgsyms.h>
+#endif
+
 /*
  * shmem_file_setup pre-accounts the whole fixed size of a VM object,
  * for shared memory and for shared anonymous (/dev/zero) mappings
@@ -199,7 +203,10 @@
 
 static const struct super_operations shmem_ops;
 static const struct address_space_operations shmem_aops;
-static const struct file_operations shmem_file_operations;
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+const struct file_operations shmem_file_operations;
 static const struct inode_operations shmem_inode_operations;
 static const struct inode_operations shmem_dir_inode_operations;
 static const struct inode_operations shmem_special_inode_operations;
@@ -2408,7 +2415,10 @@
 	.migratepage	= migrate_page,
 };
 
-static const struct file_operations shmem_file_operations = {
+#ifndef CONFIG_KRG_EPM
+static
+#endif
+const struct file_operations shmem_file_operations = {
 	.mmap		= shmem_mmap,
 #ifdef CONFIG_TMPFS
 	.llseek		= generic_file_llseek,
@@ -2522,6 +2532,14 @@
 		goto out2;
 	}
 
+#ifdef CONFIG_KRG_EPM
+	error = krgsyms_register(KRGSYMS_VM_OPS_SHMEM, &shmem_vm_ops);
+	if (error) {
+		printk(KERN_ERR "Could not register shmem_vm_ops\n");
+		goto out1_1;
+	}
+#endif
+
 	shm_mnt = vfs_kern_mount(&tmpfs_fs_type, MS_NOUSER,
 				tmpfs_fs_type.name, NULL);
 	if (IS_ERR(shm_mnt)) {
@@ -2532,6 +2550,10 @@
 	return 0;
 
 out1:
+#ifdef CONFIG_KRG_EPM
+	krgsyms_unregister(KRGSYMS_VM_OPS_SHMEM);
+out1_1:
+#endif
 	unregister_filesystem(&tmpfs_fs_type);
 out2:
 	destroy_inodecache();
diff -ruN linux-2.6.29/mm/swap.c android_cluster/linux-2.6.29/mm/swap.c
--- linux-2.6.29/mm/swap.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/swap.c	2014-05-27 23:04:10.502026345 -0700
@@ -152,10 +152,19 @@
 }
 
 static void update_page_reclaim_stat(struct zone *zone, struct page *page,
+#ifdef CONFIG_KRG_MM
+				     int file, int kddm, int rotated)
+#else
 				     int file, int rotated)
+#endif
 {
 	struct zone_reclaim_stat *reclaim_stat = &zone->reclaim_stat;
 	struct zone_reclaim_stat *memcg_reclaim_stat;
+#ifdef CONFIG_KRG_MM
+	/* Not clean but limit the patch on this function */
+	file = RECLAIM_STAT_INDEX(file, kddm);
+	BUG_ON ((file > 2) || (file < 0));
+#endif
 
 	memcg_reclaim_stat = mem_cgroup_get_reclaim_stat_from_page(page);
 
@@ -188,8 +197,12 @@
 		lru += LRU_ACTIVE;
 		add_page_to_lru_list(zone, page, lru);
 		__count_vm_event(PGACTIVATE);
-
+#ifdef CONFIG_KRG_MM
+		update_page_reclaim_stat(zone, page, !!file,
+					 page_is_migratable(page), 1);
+#else
 		update_page_reclaim_stat(zone, page, !!file, 1);
+#endif
 	}
 	spin_unlock_irq(&zone->lru_lock);
 }
@@ -344,6 +357,9 @@
 			continue;
 		}
 
+#if defined(CONFIG_KRG_MM) && defined(CONFIG_DEBUG_PAGEALLOC)
+		ClearPageInVec(page);
+#endif
 		if (!put_page_testzero(page))
 			continue;
 
@@ -422,12 +438,20 @@
 		VM_BUG_ON(PageActive(page));
 		VM_BUG_ON(PageUnevictable(page));
 		VM_BUG_ON(PageLRU(page));
+#if defined(CONFIG_KRG_MM) && defined(CONFIG_DEBUG_PAGEALLOC)
+		ClearPageInVec(page);
+#endif
 		SetPageLRU(page);
 		active = is_active_lru(lru);
 		file = is_file_lru(lru);
 		if (active)
 			SetPageActive(page);
+#ifdef CONFIG_KRG_MM
+		update_page_reclaim_stat(zone, page, file,
+					 is_kddm_lru(lru), active);
+#else
 		update_page_reclaim_stat(zone, page, file, active);
+#endif
 		add_page_to_lru_list(zone, page, lru);
 	}
 	if (zone)
diff -ruN linux-2.6.29/mm/swapfile.c android_cluster/linux-2.6.29/mm/swapfile.c
--- linux-2.6.29/mm/swapfile.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/swapfile.c	2014-05-27 23:04:10.502026345 -0700
@@ -512,7 +512,11 @@
 /*
  * How many references to page are currently swapped out?
  */
+#ifdef CONFIG_KRG_MM
+int page_swapcount(struct page *page)
+#else
 static inline int page_swapcount(struct page *page)
+#endif
 {
 	int count = 0;
 	struct swap_info_struct *p;
@@ -835,8 +839,13 @@
 	return 0;
 }
 
+#ifdef CONFIG_KRG_FAF
+static int __unuse_mm(struct mm_struct *mm,
+				swp_entry_t entry, struct page *page)
+#else
 static int unuse_mm(struct mm_struct *mm,
 				swp_entry_t entry, struct page *page)
+#endif
 {
 	struct vm_area_struct *vma;
 	int ret = 0;
@@ -999,7 +1008,11 @@
 			if (start_mm == &init_mm)
 				shmem = shmem_unuse(entry, page);
 			else
+#ifdef CONFIG_KRG_FAF
+				retval = __unuse_mm(start_mm, entry, page);
+#else
 				retval = unuse_mm(start_mm, entry, page);
+#endif
 		}
 		if (*swap_map > 1) {
 			int set_start_mm = (*swap_map >= swcount);
@@ -1029,7 +1042,11 @@
 					set_start_mm = 1;
 					shmem = shmem_unuse(entry, page);
 				} else
+#ifdef CONFIG_KRG_FAF
+					retval = __unuse_mm(mm, entry, page);
+#else
 					retval = unuse_mm(mm, entry, page);
+#endif
 				if (set_start_mm && *swap_map < swcount) {
 					mmput(new_start_mm);
 					atomic_inc(&mm->mm_users);
diff -ruN linux-2.6.29/mm/thrash.c android_cluster/linux-2.6.29/mm/thrash.c
--- linux-2.6.29/mm/thrash.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/thrash.c	2014-05-27 23:04:10.502026345 -0700
@@ -30,6 +30,11 @@
 {
 	int current_interval;
 
+#ifdef CONFIG_KRG_MM
+	if (!current->mm)
+		return;
+#endif
+
 	global_faults++;
 
 	current_interval = global_faults - current->mm->faultstamp;
diff -ruN linux-2.6.29/mm/vmscan.c android_cluster/linux-2.6.29/mm/vmscan.c
--- linux-2.6.29/mm/vmscan.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/vmscan.c	2014-05-27 23:04:10.502026345 -0700
@@ -48,6 +48,12 @@
 
 #include "internal.h"
 
+#ifdef CONFIG_KRG_MM
+#include <net/krgrpc/rpc.h>
+#endif
+
+#define RPC_MAX_PAGES 1700
+
 struct scan_control {
 	/* Incremented by the number of inactive pages that were scanned */
 	unsigned long nr_scanned;
@@ -82,7 +88,11 @@
 	unsigned long (*isolate_pages)(unsigned long nr, struct list_head *dst,
 			unsigned long *scanned, int order, int mode,
 			struct zone *z, struct mem_cgroup *mem_cont,
+#ifdef CONFIG_KRG_MM
+			int active, int file, int kddm);
+#else
 			int active, int file);
+#endif
 };
 
 #define lru_to_page(_head) (list_entry((_head)->prev, struct page, lru))
@@ -522,6 +532,10 @@
 		 * We know how to handle that.
 		 */
 		lru = active + page_is_file_cache(page);
+#ifdef CONFIG_KRG_MM
+		BUG_ON(page_is_migratable(page) && page_is_file_cache(page));
+		lru += page_is_migratable(page);
+#endif
 		lru_cache_add_lru(page, lru);
 	} else {
 		/*
@@ -564,11 +578,36 @@
 	VM_BUG_ON(PageLRU(page));
 
 	lru = !!TestClearPageActive(page) + page_is_file_cache(page);
+#ifdef CONFIG_KRG_MM
+	BUG_ON(page_is_migratable(page) && page_is_file_cache(page));
+	lru += page_is_migratable(page);
+#endif
 	lru_cache_add_lru(page, lru);
 	put_page(page);
 }
 #endif /* CONFIG_UNEVICTABLE_LRU */
 
+#ifdef CONFIG_KRG_MM
+static int check_injection_flow(void)
+{
+	long i = 0, limit = RPC_MAX_PAGES;
+
+	if ((rpc_consumed_bytes() / PAGE_SIZE) < limit)
+		return 0;
+
+	if (current_is_kswapd())
+		limit = limit / 2;
+	else
+		limit = 4 * limit / 5;
+
+	while ((rpc_consumed_bytes() / PAGE_SIZE) > limit) {
+		schedule();
+		i++;
+	}
+
+	return 0;
+}
+#endif
 
 /*
  * shrink_page_list() returns the number of reclaimed pages
@@ -596,6 +635,11 @@
 		page = lru_to_page(page_list);
 		list_del(&page->lru);
 
+#ifdef CONFIG_KRG_MM
+		if (PageMigratable(page))
+			check_injection_flow();
+#endif
+
 		if (!trylock_page(page))
 			goto keep;
 
@@ -637,6 +681,35 @@
 					referenced && page_mapping_inuse(page))
 			goto activate_locked;
 
+#ifdef CONFIG_KRG_MM
+		if (PageMigratable(page) && (page_mapped(page))) {
+			switch (try_to_flush_page(page)) {
+			case SWAP_FAIL:
+				goto activate_locked;
+                        case SWAP_AGAIN:
+                                goto keep_locked;
+			case SWAP_MLOCK:
+				goto cull_mlocked;
+			case SWAP_FLUSH_FAIL:
+				BUG(); /* TODO: Try a swap on disk */
+                        case SWAP_SUCCESS:
+                                ; /* try to free the page below */
+                        }
+
+			/*
+			 *  TODO: check this code. We can probably
+			 *  Reuse the code below in the
+			 *  page_has_private if section.
+			 */
+			unlock_page(page);
+			if (put_page_testzero(page))
+				goto free_it;
+			printk ("WARNING: page %p has count %d\n", page,
+				page_count(page));
+			nr_reclaimed++;
+			continue;
+		}
+#endif
 		/*
 		 * Anonymous process memory has backing store?
 		 * Try to allocate it some swap space here.
@@ -721,6 +794,9 @@
 		 * Otherwise, leave the page on the LRU so it is swappable.
 		 */
 		if (PagePrivate(page)) {
+#ifdef CONFIG_KRG_MM
+			BUG_ON (page->obj_entry);
+#endif
 			if (!try_to_release_page(page, sc->gfp_mask))
 				goto activate_locked;
 			if (!mapping && page_count(page) == 1) {
@@ -956,13 +1032,21 @@
 					unsigned long *scanned, int order,
 					int mode, struct zone *z,
 					struct mem_cgroup *mem_cont,
+#ifdef CONFIG_KRG_MM
+					int active, int file, int kddm)
+#else
 					int active, int file)
+#endif
 {
 	int lru = LRU_BASE;
 	if (active)
 		lru += LRU_ACTIVE;
 	if (file)
 		lru += LRU_FILE;
+#ifdef CONFIG_KRG_MM
+	if (kddm)
+		lru += LRU_MIGR;
+#endif
 	return isolate_lru_pages(nr, &z->lru[lru].list, dst, scanned, order,
 								mode, !!file);
 }
@@ -980,6 +1064,10 @@
 
 	list_for_each_entry(page, page_list, lru) {
 		lru = page_is_file_cache(page);
+#ifdef CONFIG_KRG_MM
+		BUG_ON(page_is_migratable(page) && page_is_file_cache(page));
+		lru += page_is_migratable(page);
+#endif
 		if (PageActive(page)) {
 			lru += LRU_ACTIVE;
 			ClearPageActive(page);
@@ -1042,7 +1130,11 @@
  */
 static unsigned long shrink_inactive_list(unsigned long max_scan,
 			struct zone *zone, struct scan_control *sc,
+#ifdef CONFIG_KRG_MM
+			int priority, int file, int kddm)
+#else
 			int priority, int file)
+#endif
 {
 	LIST_HEAD(page_list);
 	struct pagevec pvec;
@@ -1077,7 +1169,11 @@
 
 		nr_taken = sc->isolate_pages(sc->swap_cluster_max,
 			     &page_list, &nr_scan, sc->order, mode,
+#ifdef CONFIG_KRG_MM
+				zone, sc->mem_cgroup, 0, file, kddm);
+#else
 				zone, sc->mem_cgroup, 0, file);
+#endif
 		nr_active = clear_active_flags(&page_list, count);
 		__count_vm_events(PGDEACTIVATE, nr_active);
 
@@ -1089,6 +1185,12 @@
 						-count[LRU_ACTIVE_ANON]);
 		__mod_zone_page_state(zone, NR_INACTIVE_ANON,
 						-count[LRU_INACTIVE_ANON]);
+#ifdef CONFIG_KRG_MM
+		__mod_zone_page_state(zone, NR_ACTIVE_MIGR,
+						-count[LRU_ACTIVE_MIGR]);
+		__mod_zone_page_state(zone, NR_INACTIVE_MIGR,
+						-count[LRU_INACTIVE_MIGR]);
+#endif
 
 		if (scanning_global_lru(sc))
 			zone->pages_scanned += nr_scan;
@@ -1097,6 +1199,10 @@
 		reclaim_stat->recent_scanned[0] += count[LRU_ACTIVE_ANON];
 		reclaim_stat->recent_scanned[1] += count[LRU_INACTIVE_FILE];
 		reclaim_stat->recent_scanned[1] += count[LRU_ACTIVE_FILE];
+#ifdef CONFIG_KRG_MM
+		reclaim_stat->recent_scanned[2] += count[LRU_INACTIVE_MIGR];
+		reclaim_stat->recent_scanned[2] += count[LRU_ACTIVE_MIGR];
+#endif
 
 		spin_unlock_irq(&zone->lru_lock);
 
@@ -1156,7 +1262,11 @@
 			lru = page_lru(page);
 			add_page_to_lru_list(zone, page, lru);
 			if (PageActive(page)) {
+#ifdef CONFIG_KRG_MM
+				int file = reclaim_stat_index (page);
+#else
 				int file = !!page_is_file_cache(page);
+#endif
 				reclaim_stat->recent_rotated[file]++;
 			}
 			if (!pagevec_add(&pvec, page)) {
@@ -1207,7 +1317,11 @@
 
 
 static void shrink_active_list(unsigned long nr_pages, struct zone *zone,
+#ifdef CONFIG_KRG_MM
+		struct scan_control *sc, int priority, int file, int kddm)
+#else
 			struct scan_control *sc, int priority, int file)
+#endif
 {
 	unsigned long pgmoved;
 	int pgdeactivate = 0;
@@ -1223,7 +1337,11 @@
 	spin_lock_irq(&zone->lru_lock);
 	pgmoved = sc->isolate_pages(nr_pages, &l_hold, &pgscanned, sc->order,
 					ISOLATE_ACTIVE, zone,
+#ifdef CONFIG_KRG_MM
+					sc->mem_cgroup, 1, file, kddm);
+#else
 					sc->mem_cgroup, 1, file);
+#endif
 	/*
 	 * zone->pages_scanned is used for detect zone's oom
 	 * mem_cgroup remembers nr_scan by itself.
@@ -1231,14 +1349,25 @@
 	if (scanning_global_lru(sc)) {
 		zone->pages_scanned += pgscanned;
 	}
+#ifdef CONFIG_KRG_MM
+	reclaim_stat->recent_scanned[RECLAIM_STAT_INDEX(file, kddm)] += pgmoved;
+#else
 	reclaim_stat->recent_scanned[!!file] += pgmoved;
+#endif
 
 	if (file)
 		__mod_zone_page_state(zone, NR_ACTIVE_FILE, -pgmoved);
 	else
+#ifdef CONFIG_KRG_MM
+	if (kddm)
+		__mod_zone_page_state(zone, NR_ACTIVE_MIGR, -pgmoved);
+	else
+#endif
 		__mod_zone_page_state(zone, NR_ACTIVE_ANON, -pgmoved);
 	spin_unlock_irq(&zone->lru_lock);
 
+
+
 	pgmoved = 0;
 	while (!list_empty(&l_hold)) {
 		cond_resched();
@@ -1262,7 +1391,11 @@
 	 * Move the pages to the [file or anon] inactive list.
 	 */
 	pagevec_init(&pvec, 1);
+#ifdef CONFIG_KRG_MM
+	lru = BUILD_LRU_ID(0 /* inactive */, file, kddm);
+#else
 	lru = LRU_BASE + file * LRU_FILE;
+#endif
 
 	spin_lock_irq(&zone->lru_lock);
 	/*
@@ -1271,7 +1404,11 @@
 	 * This helps balance scan pressure between file and anonymous
 	 * pages in get_scan_ratio.
 	 */
+#ifdef CONFIG_KRG_MM
+	reclaim_stat->recent_rotated[RECLAIM_STAT_INDEX(file, kddm)] += pgmoved;
+#else
 	reclaim_stat->recent_rotated[!!file] += pgmoved;
+#endif
 
 	pgmoved = 0;
 	while (!list_empty(&l_inactive)) {
@@ -1344,21 +1481,65 @@
 	return low;
 }
 
+#ifdef CONFIG_KRG_MM
+static int inactive_kddm_is_low_global(struct zone *zone)
+{
+	unsigned long active, inactive;
+
+	active = zone_page_state(zone, NR_ACTIVE_MIGR);
+	inactive = zone_page_state(zone, NR_INACTIVE_MIGR);
+
+	if (inactive * zone->inactive_ratio < active)
+		return 1;
+
+	return 0;
+}
+
+static int inactive_kddm_is_low(struct zone *zone, struct scan_control *sc)
+{
+	int low;
+
+	if (scanning_global_lru(sc))
+		low = inactive_kddm_is_low_global(zone);
+	else
+		BUG();
+	return low;
+}
+#endif
+
 static unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan,
 	struct zone *zone, struct scan_control *sc, int priority)
 {
 	int file = is_file_lru(lru);
 
 	if (lru == LRU_ACTIVE_FILE) {
+#ifdef CONFIG_KRG_MM
+		shrink_active_list(nr_to_scan, zone, sc, priority, file, 0);
+#else
 		shrink_active_list(nr_to_scan, zone, sc, priority, file);
+#endif
 		return 0;
 	}
 
 	if (lru == LRU_ACTIVE_ANON && inactive_anon_is_low(zone, sc)) {
+#ifdef CONFIG_KRG_MM
+		shrink_active_list(nr_to_scan, zone, sc, priority, file, 0);
+#else
 		shrink_active_list(nr_to_scan, zone, sc, priority, file);
+#endif
+		return 0;
+	}
+#ifdef CONFIG_KRG_MM
+	if (lru == LRU_ACTIVE_MIGR && inactive_kddm_is_low(zone, sc)) {
+		shrink_active_list(nr_to_scan, zone, sc, priority, 0, 1);
 		return 0;
 	}
+
+	return shrink_inactive_list(nr_to_scan, zone, sc, priority, file,
+				    is_kddm_lru(lru));
+#else
 	return shrink_inactive_list(nr_to_scan, zone, sc, priority, file);
+#endif
 }
 
 /*
@@ -1373,23 +1554,32 @@
 static void get_scan_ratio(struct zone *zone, struct scan_control *sc,
 					unsigned long *percent)
 {
+#ifdef CONFIG_KRG_MM
+	unsigned long kddm, kddm_prio, kp;
+#endif
 	unsigned long anon, file, free;
 	unsigned long anon_prio, file_prio;
 	unsigned long ap, fp;
 	struct zone_reclaim_stat *reclaim_stat = get_reclaim_stat(zone, sc);
 
+#ifndef CONFIG_KRG_MM
 	/* If we have no swap space, do not bother scanning anon pages. */
 	if (nr_swap_pages <= 0) {
 		percent[0] = 0;
 		percent[1] = 100;
 		return;
 	}
+#endif
 
 	anon  = zone_nr_pages(zone, sc, LRU_ACTIVE_ANON) +
 		zone_nr_pages(zone, sc, LRU_INACTIVE_ANON);
 	file  = zone_nr_pages(zone, sc, LRU_ACTIVE_FILE) +
 		zone_nr_pages(zone, sc, LRU_INACTIVE_FILE);
 
+#ifdef CONFIG_KRG_MM
+	kddm  = zone_nr_pages(zone, sc, LRU_ACTIVE_MIGR) +
+		zone_nr_pages(zone, sc, LRU_INACTIVE_MIGR);
+#else
 	if (scanning_global_lru(sc)) {
 		free  = zone_page_state(zone, NR_FREE_PAGES);
 		/* If we have very few page cache pages,
@@ -1400,7 +1590,7 @@
 			return;
 		}
 	}
-
+#endif
 	/*
 	 * OK, so we have swap space and a fair amount of page cache
 	 * pages.  We use the recently rotated / recently scanned
@@ -1426,13 +1616,33 @@
 		spin_unlock_irq(&zone->lru_lock);
 	}
 
+#ifdef CONFIG_KRG_MM
+	if (unlikely(reclaim_stat->recent_scanned[2] > kddm / 4)) {
+		spin_lock_irq(&zone->lru_lock);
+		reclaim_stat->recent_scanned[2] /= 2;
+		reclaim_stat->recent_rotated[2] /= 2;
+		spin_unlock_irq(&zone->lru_lock);
+	}
+#endif
+
 	/*
 	 * With swappiness at 100, anonymous and file have the same priority.
 	 * This scanning priority is essentially the inverse of IO cost.
 	 */
 	anon_prio = sc->swappiness;
 	file_prio = 200 - sc->swappiness;
-
+#ifdef CONFIG_KRG_MM
+	if (!sc->may_swap || (nr_swap_pages <= 0))
+		anon_prio = 0;
+	if (scanning_global_lru(sc)) {
+		free  = zone_page_state(zone, NR_FREE_PAGES);
+		/* If we have very few page cache pages,
+		   force-scan anon pages. */
+		if (unlikely(file + free <= zone->pages_high))
+			file_prio = 0;
+	}
+	kddm_prio = 400 - anon_prio - file_prio;
+#endif
 	/*
 	 * The amount of pressure on anon vs file pages is inversely
 	 * proportional to the fraction of recently scanned pages on
@@ -1444,9 +1654,20 @@
 	fp = (file_prio + 1) * (reclaim_stat->recent_scanned[1] + 1);
 	fp /= reclaim_stat->recent_rotated[1] + 1;
 
+#ifdef CONFIG_KRG_MM
+	kp = (kddm_prio + 1) * (reclaim_stat->recent_scanned[2] + 1);
+	kp /= reclaim_stat->recent_rotated[2] + 1;
+
+	/* Normalize to percentages */
+	percent[0] = 100 * ap / (ap + fp + kp + 1);
+	percent[1] = 100 * fp / (ap + fp + kp + 1);
+	percent[2] = 100 - percent[0] - percent[1];
+#else
+
 	/* Normalize to percentages */
 	percent[0] = 100 * ap / (ap + fp + 1);
 	percent[1] = 100 - percent[0];
+#endif
 }
 
 
@@ -1458,7 +1679,11 @@
 {
 	unsigned long nr[NR_LRU_LISTS];
 	unsigned long nr_to_scan;
+#ifdef CONFIG_KRG_MM
+	unsigned long percent[3];	/* anon @ 0; file @ 1; kddm @ 2 */
+#else
 	unsigned long percent[2];	/* anon @ 0; file @ 1 */
+#endif
 	enum lru_list l;
 	unsigned long nr_reclaimed = sc->nr_reclaimed;
 	unsigned long swap_cluster_max = sc->swap_cluster_max;
@@ -1466,7 +1691,11 @@
 	get_scan_ratio(zone, sc, percent);
 
 	for_each_evictable_lru(l) {
+#ifdef CONFIG_KRG_MM
+		int file = RECLAIM_STAT_INDEX(is_file_lru(l), is_kddm_lru(l));
+#else
 		int file = is_file_lru(l);
+#endif
 		int scan;
 
 		scan = zone_nr_pages(zone, sc, l);
@@ -1486,7 +1715,11 @@
 	}
 
 	while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||
+#ifdef CONFIG_KRG_MM
+	       nr[LRU_INACTIVE_MIGR] || nr[LRU_INACTIVE_FILE]) {
+#else
 					nr[LRU_INACTIVE_FILE]) {
+#endif
 		for_each_evictable_lru(l) {
 			if (nr[l]) {
 				nr_to_scan = min(nr[l], swap_cluster_max);
@@ -1516,7 +1749,13 @@
 	 * rebalance the anon lru active/inactive ratio.
 	 */
 	if (inactive_anon_is_low(zone, sc))
+#ifdef CONFIG_KRG_MM
+		shrink_active_list(SWAP_CLUSTER_MAX, zone, sc, priority, 0, 0);
+	if (inactive_kddm_is_low(zone, sc))
+		shrink_active_list(SWAP_CLUSTER_MAX, zone, sc, priority, 0, 1);
+#else
 		shrink_active_list(SWAP_CLUSTER_MAX, zone, sc, priority, 0);
+#endif
 
 	throttle_vm_writeout(sc->gfp_mask);
 }
@@ -1814,7 +2053,15 @@
 			 */
 			if (inactive_anon_is_low(zone, &sc))
 				shrink_active_list(SWAP_CLUSTER_MAX, zone,
+#ifndef CONFIG_KRG_MM
 							&sc, priority, 0);
+#else
+							&sc, priority, 0, 0);
+			/* Do the same on kddm lru pages */
+			if (inactive_kddm_is_low(zone, &sc))
+				shrink_active_list(SWAP_CLUSTER_MAX, zone,
+						   &sc, priority, 0, 1);
+#endif
 
 			if (!zone_watermark_ok(zone, order, zone->pages_high,
 					       0, 0)) {
@@ -2041,6 +2288,10 @@
 {
 	return global_page_state(NR_ACTIVE_ANON)
 		+ global_page_state(NR_ACTIVE_FILE)
+#ifdef CONFIG_KRG_MM
+		+ global_page_state(NR_ACTIVE_MIGR)
+		+ global_page_state(NR_INACTIVE_MIGR)
+#endif
 		+ global_page_state(NR_INACTIVE_ANON)
 		+ global_page_state(NR_INACTIVE_FILE);
 }
@@ -2449,6 +2700,10 @@
 	if (page_evictable(page, NULL)) {
 		enum lru_list l = LRU_INACTIVE_ANON + page_is_file_cache(page);
 
+#ifdef CONFIG_KRG_MM
+		BUG_ON(page_is_migratable(page) && page_is_file_cache(page));
+		l += page_is_migratable(page);
+#endif
 		__dec_zone_state(zone, NR_UNEVICTABLE);
 		list_move(&page->lru, &zone->lru[l].list);
 		mem_cgroup_move_lists(page, LRU_UNEVICTABLE, l);
diff -ruN linux-2.6.29/mm/vmstat.c android_cluster/linux-2.6.29/mm/vmstat.c
--- linux-2.6.29/mm/vmstat.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/mm/vmstat.c	2014-05-27 23:04:10.506026262 -0700
@@ -728,7 +728,12 @@
 		   "\n        min      %lu"
 		   "\n        low      %lu"
 		   "\n        high     %lu"
+#ifdef CONFIG_KRG_MM
+		   "\n        scanned  %lu (aa: %lu ia: %lu af: %lu if: %lu "
+		   "ak: %lu ik: %lu)"
+#else
 		   "\n        scanned  %lu (aa: %lu ia: %lu af: %lu if: %lu)"
+#endif
 		   "\n        spanned  %lu"
 		   "\n        present  %lu",
 		   zone_page_state(zone, NR_FREE_PAGES),
@@ -740,6 +745,10 @@
 		   zone->lru[LRU_INACTIVE_ANON].nr_scan,
 		   zone->lru[LRU_ACTIVE_FILE].nr_scan,
 		   zone->lru[LRU_INACTIVE_FILE].nr_scan,
+#ifdef CONFIG_KRG_MM
+		   zone->lru[LRU_ACTIVE_MIGR].nr_scan,
+		   zone->lru[LRU_INACTIVE_MIGR].nr_scan,
+#endif
 		   zone->spanned_pages,
 		   zone->present_pages);
 
diff -ruN linux-2.6.29/net/ipv4/ipconfig.c android_cluster/linux-2.6.29/net/ipv4/ipconfig.c
--- linux-2.6.29/net/ipv4/ipconfig.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/ipv4/ipconfig.c	2014-05-27 23:04:10.542025514 -0700
@@ -59,6 +59,12 @@
 #include <net/ipconfig.h>
 #include <net/route.h>
 
+#ifdef CONFIG_KRGRPC
+#include <kerrighed/krginit.h>
+#include <kerrighed/hotplug.h>
+#include <kerrighed/krgnodemask.h>
+#endif
+
 #include <asm/uaccess.h>
 #include <net/checksum.h>
 #include <asm/processor.h>
@@ -1377,6 +1383,15 @@
 	ic_proto_used = ic_got_reply | (ic_proto_enabled & IC_USE_DHCP);
 #endif
 
+#ifdef CONFIG_KRG_AUTONODEID
+ 	if(ISSET_KRG_INIT_FLAGS(KRG_INITFLAGS_AUTONODEID)){
+		kerrighed_node_id = ((unsigned char *)&ic_myaddr)[3];
+		SET_KRG_INIT_FLAGS(KRG_INITFLAGS_NODEID);
+		printk("Automatic setting of kerrighed_node_id: %d\n",
+			kerrighed_node_id);
+	}
+#endif
+
 #ifndef IPCONFIG_SILENT
 	/*
 	 * Clue in the operator.
diff -ruN linux-2.6.29/net/Kconfig android_cluster/linux-2.6.29/net/Kconfig
--- linux-2.6.29/net/Kconfig	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/Kconfig	2014-06-09 18:19:42.040321240 -0700
@@ -172,6 +172,7 @@
 source "net/dccp/Kconfig"
 source "net/sctp/Kconfig"
 source "net/tipc/Kconfig"
+source "net/krgrpc/Kconfig"
 source "net/atm/Kconfig"
 source "net/802/Kconfig"
 source "net/bridge/Kconfig"
diff -ruN linux-2.6.29/net/krgrpc/comlayer.c android_cluster/linux-2.6.29/net/krgrpc/comlayer.c
--- linux-2.6.29/net/krgrpc/comlayer.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/krgrpc/comlayer.c	2014-05-27 23:04:10.578024766 -0700
@@ -0,0 +1,1409 @@
+/**
+ *
+ *  Copyright (C) 2007-2008 Pascal Gallard, Kerlabs <Pascal.Gallard@kerlabs.com>
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/tipc.h>
+#include <linux/tipc_config.h>
+#include <linux/irqflags.h>
+#include <linux/workqueue.h>
+#include <linux/spinlock.h>
+#include <linux/lockdep.h>
+#include <linux/sysrq.h>
+#include <linux/netdevice.h>
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <net/tipc/tipc.h>
+#include <net/tipc/tipc_plugin_port.h>
+#include <net/tipc/tipc_plugin_if.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+#include <linux/hashtable.h>
+
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+
+#include "rpc_internal.h"
+
+#define TIPC_KRG_SERVER_TYPE (1+TIPC_RESERVED_TYPES)
+
+#define ACK_CLEANUP_WINDOW_SIZE 100
+#define MAX_CONSECUTIVE_RECV 1000
+
+#define REJECT_BACKOFF (HZ / 2)
+
+#define ACK_CLEANUP_WINDOW_SIZE__LOWMEM_MODE 20
+#define MAX_CONSECUTIVE_RECV__LOWMEM_MODE 20
+
+struct tx_engine {
+	struct list_head delayed_tx_queue;
+	struct delayed_work delayed_tx_work; /* messages cannot be transmetted immediately */
+	struct list_head not_retx_queue; /* messages accepted by TIPC */
+	struct delayed_work cleanup_not_retx_work;
+	struct list_head retx_queue; /* messages refused by TIPC */
+	struct rpc_tx_elem *retx_iter;
+	struct delayed_work retx_work;
+	struct delayed_work unreachable_work;
+	struct delayed_work reachable_work;
+};
+
+static DEFINE_PER_CPU(struct tx_engine, tipc_tx_engine);
+static DEFINE_SPINLOCK(tipc_tx_queue_lock);
+static void tipc_send_ack_worker(struct work_struct *work);
+static DECLARE_DELAYED_WORK(tipc_ack_work, tipc_send_ack_worker);
+
+struct rx_engine {
+	kerrighed_node_t from;
+	struct sk_buff_head rx_queue;
+	struct delayed_work run_rx_queue_work;
+};
+
+struct rx_engine tipc_rx_engine[KERRIGHED_MAX_NODES];
+
+struct workqueue_struct *krgcom_wq;
+
+#ifdef CONFIG_64BIT
+
+static atomic64_t consumed_bytes;
+
+static inline void consumed_bytes_add(long load)
+{
+	atomic64_add(load, &consumed_bytes);
+}
+
+static inline void consumed_bytes_sub(long load)
+{
+	atomic64_sub(load, &consumed_bytes);
+}
+
+s64 rpc_consumed_bytes(void)
+{
+	return atomic64_read(&consumed_bytes);
+}
+
+#else /* !CONFIG_64BIT */
+
+static s64 consumed_bytes;
+static DEFINE_SPINLOCK(consumed_bytes_lock);
+
+static inline void consumed_bytes_add(long load)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&consumed_bytes_lock, flags);
+	consumed_bytes += load;
+	spin_unlock_irqrestore(&consumed_bytes_lock, flags);
+}
+
+static inline void consumed_bytes_sub(long load)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&consumed_bytes_lock, flags);
+	consumed_bytes -= load;
+	spin_unlock_irqrestore(&consumed_bytes_lock, flags);
+}
+
+s64 rpc_consumed_bytes(void)
+{
+	unsigned long flags;
+	s64 ret;
+
+	spin_lock_irqsave(&consumed_bytes_lock, flags);
+	ret = consumed_bytes;
+	spin_unlock_irqrestore(&consumed_bytes_lock, flags);
+
+	return ret;
+}
+
+#endif /* !CONFIG_64BIT */
+
+/*
+ * Local definition
+ */
+
+u32 tipc_user_ref = 0;
+u32 tipc_port_ref;
+DEFINE_PER_CPU(u32, tipc_send_ref);
+struct tipc_name_seq tipc_seq;
+
+krgnodemask_t nodes_requiring_ack;
+unsigned long last_cleanup_ack[KERRIGHED_MAX_NODES];
+static int ack_cleanup_window_size;
+static int consecutive_recv[KERRIGHED_MAX_NODES];
+static int max_consecutive_recv[KERRIGHED_MAX_NODES];
+
+void __rpc_put_raw_data(void *data){
+	kfree_skb((struct sk_buff*)data);
+}
+
+void __rpc_get_raw_data(void *data){
+	skb_get((struct sk_buff*)data);
+}
+
+static
+inline int __send_iovec(kerrighed_node_t node, int nr_iov, struct iovec *iov)
+{
+	struct tipc_name name = {
+		.type = TIPC_KRG_SERVER_TYPE,
+		.instance = node
+	};
+	struct __rpc_header *h = iov[0].iov_base;
+	int err;
+
+	h->link_ack_id = rpc_link_recv_seq_id[node] - 1;
+	lockdep_off();
+	err = tipc_send2name(per_cpu(tipc_send_ref, smp_processor_id()),
+			     &name, 0,
+			     nr_iov, iov);
+	lockdep_on();
+	if (!err)
+		consecutive_recv[node] = 0;
+
+	return err;
+}
+
+static
+inline int send_iovec(kerrighed_node_t node, int nr_iov, struct iovec *iov)
+{
+	int err;
+
+	local_bh_disable();
+	err = __send_iovec(node, nr_iov, iov);
+	local_bh_enable();
+
+	return err;
+}
+
+static struct rpc_tx_elem *__rpc_tx_elem_alloc(size_t size, int nr_dest)
+{
+	struct rpc_tx_elem *elem;
+
+	elem = kmem_cache_alloc(rpc_tx_elem_cachep, GFP_ATOMIC);
+	if (!elem)
+		goto oom;
+	consumed_bytes_add(size);
+	elem->data = kmalloc(size, GFP_ATOMIC);
+	if (!elem->data)
+		goto oom_free_elem;
+	elem->link_seq_id = kmalloc(sizeof(*elem->link_seq_id) * nr_dest,
+				    GFP_ATOMIC);
+	elem->iov[1].iov_len = size;
+	if (!elem->link_seq_id)
+		goto oom_free_data;
+
+	return elem;
+
+oom_free_data:
+	kfree(elem->data);
+oom_free_elem:
+	consumed_bytes_sub(size);
+	kmem_cache_free(rpc_tx_elem_cachep, elem);
+oom:
+	return NULL;
+}
+
+static void __rpc_tx_elem_free(struct rpc_tx_elem *elem)
+{
+	kfree(elem->link_seq_id);
+	kfree(elem->data);
+	consumed_bytes_sub(elem->iov[1].iov_len);
+	kmem_cache_free(rpc_tx_elem_cachep, elem);
+}
+
+static int __rpc_tx_elem_send(struct rpc_tx_elem *elem, int link_seq_index,
+			      kerrighed_node_t node)
+{
+	int err = 0;
+
+	elem->h.link_seq_id = elem->link_seq_id[link_seq_index];
+	if (elem->h.link_seq_id <= rpc_link_send_ack_id[node])
+		goto out;
+
+	/* try to send */
+	err = send_iovec(node, ARRAY_SIZE(elem->iov), elem->iov);
+
+out:
+	return err;
+}
+
+static
+void tipc_send_ack_worker(struct work_struct *work)
+{
+	struct iovec iov[1];
+	struct __rpc_header h;
+	kerrighed_node_t node;
+	int err;
+
+	if (next_krgnode(0, nodes_requiring_ack) > KERRIGHED_MAX_NODES)
+		return;
+
+	h.from = kerrighed_node_id;
+	h.rpcid = RPC_ACK;
+	h.flags = 0;
+
+	iov[0].iov_base = &h;
+	iov[0].iov_len = sizeof(h);
+
+	for_each_krgnode_mask(node, nodes_requiring_ack) {
+		err = send_iovec(node, ARRAY_SIZE(iov), iov);
+		if (!err)
+			krgnode_clear(node, nodes_requiring_ack);
+	}
+}
+
+static void tipc_delayed_tx_worker(struct work_struct *work)
+{
+	struct tx_engine *engine = container_of(work, struct tx_engine, delayed_tx_work.work);
+	LIST_HEAD(queue);
+	LIST_HEAD(not_retx_queue);
+	struct rpc_tx_elem *iter;
+	struct rpc_tx_elem *safe;
+
+	lockdep_off();
+
+	// get the waiting list
+	spin_lock_bh(&tipc_tx_queue_lock);
+	list_splice_init(&engine->delayed_tx_queue, &queue);
+	spin_unlock_bh(&tipc_tx_queue_lock);
+
+	if(list_empty(&queue))
+		goto exit_empty;
+
+	// browse the waiting list
+	list_for_each_entry_safe(iter, safe, &queue, tx_queue){
+		krgnodemask_t nodes;
+		kerrighed_node_t link_seq_index, node;
+
+		link_seq_index = iter->link_seq_index;
+		if (link_seq_index) {
+			/* Start with the first node to which we could not
+			 * transmit */
+			krgnodes_setall(nodes);
+			krgnodes_shift_left(nodes, nodes, iter->index);
+			krgnodes_and(nodes, nodes, iter->nodes);
+		} else {
+			/* Transmit to all nodes */
+			krgnodes_copy(nodes, iter->nodes);
+		}
+		for_each_krgnode_mask(node, nodes){
+			int err;
+
+			err = __rpc_tx_elem_send(iter, link_seq_index, node);
+			if (err < 0) {
+				iter->index = node;
+				iter->link_seq_index = link_seq_index;
+
+				goto exit;
+			}
+
+			link_seq_index++;
+		}
+		/* Reset the transmission cursor for future retransmissions */
+		iter->index = 0;
+		iter->link_seq_index = 0;
+
+		/* The message has been transmitted to all receivers. We should not have to
+		 * re-transmit it. So move it to not_retx_queue. */
+		list_move_tail(&iter->tx_queue, &not_retx_queue);
+	}
+
+ exit:
+	if (!list_empty(&queue)) {
+		// merge the two lists
+		spin_lock_bh(&tipc_tx_queue_lock);
+		list_splice(&queue, &engine->delayed_tx_queue);
+		list_splice(&not_retx_queue, engine->not_retx_queue.prev);
+		spin_unlock_bh(&tipc_tx_queue_lock);
+	} else {
+		if (likely(!list_empty(&not_retx_queue))) {
+			spin_lock_bh(&tipc_tx_queue_lock);
+			list_splice(&not_retx_queue, engine->not_retx_queue.prev);
+			spin_unlock_bh(&tipc_tx_queue_lock);
+		}
+	}
+
+exit_empty:
+	lockdep_on();
+}
+
+static void tipc_retx_worker(struct work_struct *work)
+{
+	struct tx_engine *engine = container_of(work, struct tx_engine, retx_work.work);
+	LIST_HEAD(queue);
+	LIST_HEAD(not_retx_queue);
+	struct rpc_tx_elem *iter;
+	struct rpc_tx_elem *safe;
+
+	lockdep_off();
+
+	// get the waiting list
+	spin_lock_bh(&tipc_tx_queue_lock);
+	list_splice_init(&engine->retx_queue, &queue);
+	iter = engine->retx_iter;
+	engine->retx_iter = NULL;
+	spin_unlock_bh(&tipc_tx_queue_lock);
+
+	if(list_empty(&queue))
+		goto exit_empty;
+
+	/* list_for_each_entry_safe_continue starts to iterate AFTER
+	   the current item. So current item can be anything as long as
+	   we are not trying to use it */
+	if(!iter) {
+		iter = list_entry(&queue,
+				  struct rpc_tx_elem,
+				  tx_queue);
+	} else {
+		/* iter points to an entry which failed to fully
+		 * retransmit. Start from it. */
+		iter = list_entry(iter->tx_queue.prev, struct rpc_tx_elem, tx_queue);
+	}
+
+	// browse the waiting list
+	list_for_each_entry_safe_continue(iter, safe, &queue, tx_queue){
+		krgnodemask_t nodes;
+		kerrighed_node_t link_seq_index, node;
+
+		link_seq_index = iter->link_seq_index;
+		if (link_seq_index) {
+			/* Start with the first node to which we could not
+			 * transmit */
+			krgnodes_setall(nodes);
+			krgnodes_shift_left(nodes, nodes, iter->index);
+			krgnodes_and(nodes, nodes, iter->nodes);
+		} else {
+			/* Transmit to all nodes */
+			krgnodes_copy(nodes, iter->nodes);
+		}
+		for_each_krgnode_mask(node, nodes){
+			int err;
+
+			err = __rpc_tx_elem_send(iter, link_seq_index, node);
+			if (err < 0) {
+				iter->index = node;
+				iter->link_seq_index = link_seq_index;
+
+				goto exit;
+			}
+
+			link_seq_index++;
+		}
+
+		/* Reset the transmission cursor for future retransmissions */
+		iter->index = 0;
+		iter->link_seq_index = 0;
+
+		list_move_tail(&iter->tx_queue, &not_retx_queue);
+	}
+
+	iter = NULL;
+
+ exit:
+ 	if (!list_empty(&not_retx_queue)){
+		spin_lock_bh(&tipc_tx_queue_lock);
+		list_splice(&not_retx_queue, &engine->not_retx_queue);
+		spin_unlock_bh(&tipc_tx_queue_lock);
+	}
+
+	if (!list_empty(&queue)) {
+		// merge the two lists
+		spin_lock_bh(&tipc_tx_queue_lock);
+		list_splice(&queue, &engine->retx_queue);
+		/* A concurrent run of the worker might already have set a
+		 * restart point later in the queue. Do not overwrite it unless
+		 * we set an earlier restart point. */
+		if (iter)
+			engine->retx_iter = iter;
+		spin_unlock_bh(&tipc_tx_queue_lock);
+	}
+
+exit_empty:
+	lockdep_on();
+}
+
+static void tipc_cleanup_not_retx_worker(struct work_struct *work)
+{
+	struct tx_engine *engine = container_of(work, struct tx_engine, cleanup_not_retx_work.work);
+	struct rpc_tx_elem *iter;
+	struct rpc_tx_elem *safe;
+	LIST_HEAD(queue);
+	int node;
+
+	spin_lock_bh(&tipc_tx_queue_lock);
+	list_splice_init(&engine->not_retx_queue, &queue);
+	spin_unlock_bh(&tipc_tx_queue_lock);
+
+	list_for_each_entry_safe(iter, safe, &queue, tx_queue){
+		int need_to_free, link_seq_index;
+
+		need_to_free = 0;
+		link_seq_index = 0;
+
+		for_each_krgnode_mask(node, iter->nodes){
+
+			iter->h.link_seq_id = iter->link_seq_id[link_seq_index];
+
+			if (iter->h.link_seq_id >
+			    rpc_link_send_ack_id[node])
+				goto next_iter;
+
+			link_seq_index++;
+		}
+		need_to_free = 1;
+
+	next_iter:
+		if(need_to_free){
+			list_del(&iter->tx_queue);
+			__rpc_tx_elem_free(iter);
+		}
+	}
+
+	if (!list_empty(&queue)) {
+		// merge the two lists
+		spin_lock_bh(&tipc_tx_queue_lock);
+		list_splice(&queue, &engine->not_retx_queue);
+		spin_unlock_bh(&tipc_tx_queue_lock);
+	}
+
+}
+
+static
+void tipc_unreachable_node_worker(struct work_struct *work){
+}
+
+static
+void tipc_reachable_node_worker(struct work_struct *work){
+	struct tx_engine *engine = container_of(work, struct tx_engine, reachable_work.work);
+
+	spin_lock_bh(&tipc_tx_queue_lock);
+	list_splice_init(&engine->not_retx_queue, &engine->retx_queue);
+	spin_unlock_bh(&tipc_tx_queue_lock);
+
+	queue_delayed_work_on(smp_processor_id(), krgcom_wq,
+			      &engine->retx_work, 0);
+}
+
+#define MAX_EMERGENCY_SEND 2
+
+int __rpc_emergency_send_buf_alloc(struct rpc_desc *desc, size_t size)
+{
+	struct rpc_tx_elem **elem;
+	int nr_dest;
+	int err = 0;
+	int i;
+
+	elem = kmalloc(sizeof(*elem) * MAX_EMERGENCY_SEND, GFP_ATOMIC);
+	if (!elem)
+		goto oom;
+	nr_dest = krgnodes_weight(desc->nodes);
+	for (i = 0; i < MAX_EMERGENCY_SEND; i++) {
+		elem[i] = __rpc_tx_elem_alloc(size, nr_dest);
+		if (!elem[i])
+			goto oom_free_elems;
+	}
+	desc->desc_send->emergency_send_buf = elem;
+
+out:
+	return err;
+
+oom_free_elems:
+	for (i--; i >= 0; i--)
+		__rpc_tx_elem_free(elem[i]);
+	kfree(elem);
+oom:
+	err = -ENOMEM;
+	goto out;
+}
+
+void __rpc_emergency_send_buf_free(struct rpc_desc *desc)
+{
+	struct rpc_tx_elem **elem = desc->desc_send->emergency_send_buf;
+	int i;
+
+	/* does not buy a lot, but still can help debug */
+	desc->desc_send->emergency_send_buf = NULL;
+	for (i = 0; i < MAX_EMERGENCY_SEND; i++)
+		if (elem[i])
+			/* emergency send buf was not used */
+			__rpc_tx_elem_free(elem[i]);
+	kfree(elem);
+}
+
+static struct rpc_tx_elem *next_emergency_send_buf(struct rpc_desc *desc)
+{
+	struct rpc_tx_elem **elems = desc->desc_send->emergency_send_buf;
+	struct rpc_tx_elem *buf = NULL;
+	int i;
+
+	for (i = 0; i < MAX_EMERGENCY_SEND; i++)
+		if (elems[i]) {
+			buf = elems[i];
+			elems[i] = NULL;
+			break;
+		}
+	return buf;
+}
+
+int __rpc_send_ll(struct rpc_desc* desc,
+			 krgnodemask_t *nodes,
+			 unsigned long seq_id,
+			 int __flags,
+			 const void* data, size_t size,
+			 int rpc_flags)
+{
+	struct rpc_tx_elem* elem;
+	struct tx_engine *engine;
+	kerrighed_node_t node;
+	int link_seq_index;
+
+	elem = __rpc_tx_elem_alloc(size, __krgnodes_weight(nodes));
+	if (!elem) {
+		if (rpc_flags & RPC_FLAGS_EMERGENCY_BUF)
+			elem = next_emergency_send_buf(desc);
+		if (!elem)
+			return -ENOMEM;
+	}
+
+	link_seq_index = 0;
+	__for_each_krgnode_mask(node, nodes) {
+		rpc_link_seq_id(elem->link_seq_id[link_seq_index], node);
+		link_seq_index++;
+	}
+	if (rpc_flags & RPC_FLAGS_NEW_DESC_ID)
+		rpc_new_desc_id_unlock();
+
+	elem->h.from = kerrighed_node_id;
+	elem->h.client = desc->client;
+	elem->h.desc_id = desc->desc_id;
+	elem->h.seq_id = seq_id;
+	
+	elem->h.flags = __flags;
+	if(desc->type == RPC_RQ_SRV)
+		elem->h.flags |= __RPC_HEADER_FLAGS_SRV_REPLY;
+
+	elem->h.rpcid = desc->rpcid;
+
+	elem->iov[0].iov_base = &elem->h;
+	elem->iov[0].iov_len = sizeof(elem->h);
+	
+	elem->iov[1].iov_base = (void *) data;
+	elem->iov[1].iov_len = size;
+
+	elem->index = 0;
+	elem->link_seq_index = 0;
+
+	memcpy(elem->data, data, size);
+	elem->iov[1].iov_base = elem->data;
+		
+	__krgnodes_copy(&elem->nodes, nodes);	
+
+	preempt_disable();
+	engine = &per_cpu(tipc_tx_engine, smp_processor_id());
+	if (irqs_disabled()) {
+		/* Add the packet in the tx_queue */
+		lockdep_off();
+		spin_lock(&tipc_tx_queue_lock);
+		list_add_tail(&elem->tx_queue, &engine->delayed_tx_queue);
+		spin_unlock(&tipc_tx_queue_lock);
+		lockdep_on();
+
+		/* Schedule the work ASAP */
+		queue_work(krgcom_wq, &engine->delayed_tx_work.work);
+
+	} else {
+		int err = 0;
+
+		link_seq_index = 0;
+		__for_each_krgnode_mask(node, nodes){
+
+			err = __rpc_tx_elem_send(elem, link_seq_index, node);
+			if(err<0){
+				spin_lock_bh(&tipc_tx_queue_lock);
+				list_add_tail(&elem->tx_queue,
+						&engine->retx_queue);
+				spin_unlock_bh(&tipc_tx_queue_lock);
+				break;
+			}
+
+			link_seq_index++;
+		}
+
+		if(err>=0){
+			/* Add the packet in the not_retx_queue */
+			spin_lock_bh(&tipc_tx_queue_lock);
+			list_add_tail(&elem->tx_queue, &engine->not_retx_queue);
+			spin_unlock_bh(&tipc_tx_queue_lock);
+		}
+	}
+	preempt_enable();
+	return 0;
+}
+
+inline
+void insert_in_seqid_order(struct rpc_desc_elem* desc_elem,
+			   struct rpc_desc_recv* desc_recv)
+{
+	struct rpc_desc_elem *iter;
+	struct list_head *at;
+
+	if (unlikely(desc_elem->flags & __RPC_HEADER_FLAGS_SIGNAL)) {
+		/* For a given seq_id, queue all received sigacks
+		 * before all signals, and try to preserve signals order
+		 */
+		int sigack = (desc_elem->flags & __RPC_HEADER_FLAGS_SIGACK);
+
+		at = &desc_recv->list_signal_head;
+		list_for_each_entry_reverse(iter, &desc_recv->list_signal_head,
+					    list_desc_elem)
+			if (iter->seq_id < desc_elem->seq_id
+			    || (iter->seq_id == desc_elem->seq_id && !sigack)) {
+				at = &iter->list_desc_elem;
+				break;
+			}
+	} else {
+		/* Data element
+		 * There can be only one single element per seq_id
+		 */
+		at = &desc_recv->list_desc_head;
+		list_for_each_entry_reverse(iter, &desc_recv->list_desc_head,
+					    list_desc_elem)
+			if (iter->seq_id < desc_elem->seq_id) {
+				at = &iter->list_desc_elem;
+				break;
+			}
+	}
+	list_add(&desc_elem->list_desc_elem, at);
+}
+
+/*
+ * do_action
+ * Process the received descriptor
+ *
+ * desc->desc_lock must be hold
+ */
+static
+inline
+int do_action(struct rpc_desc *desc, struct __rpc_header *h)
+{
+	switch (desc->state) {
+	case RPC_STATE_NEW:
+		spin_unlock(&desc->desc_lock);
+		return rpc_handle_new(desc);
+	case RPC_STATE_WAIT1:
+		if (desc->type == RPC_RQ_CLT
+		    && desc->wait_from != h->from) {
+			spin_unlock(&desc->desc_lock);
+			break;
+		}
+	case RPC_STATE_WAIT:
+		desc->state = RPC_STATE_RUN;
+		wake_up_process(desc->thread);
+		spin_unlock(&desc->desc_lock);
+		break;
+	default:
+		spin_unlock(&desc->desc_lock);			
+		break;
+	}
+	return 0;
+}
+
+void rpc_desc_elem_free(struct rpc_desc_elem *elem)
+{
+	kfree_skb(elem->raw);
+	kmem_cache_free(rpc_desc_elem_cachep, elem);
+}
+
+void rpc_do_signal(struct rpc_desc *desc,
+		   struct rpc_desc_elem *signal_elem)
+{
+	if (desc->thread)
+		send_sig(*(int*)signal_elem->data, desc->thread, 0);
+
+	__rpc_signalack(desc);
+
+	rpc_desc_elem_free(signal_elem);
+}
+
+/*
+ * handle_valid_desc
+ * We found the right descriptor, is-there a waiting buffer ?
+ */
+inline
+int handle_valid_desc(struct rpc_desc *desc,
+		      struct rpc_desc_recv *desc_recv,
+		      struct rpc_desc_elem* descelem,
+		      struct __rpc_header *h,
+		      struct sk_buff *buf){
+	int err;
+
+	// Update the received_packets map
+	if(descelem->seq_id<sizeof(desc_recv->received_packets)*8)
+		set_bit(descelem->seq_id-1, &desc_recv->received_packets);
+
+	// is there a waiting buffer ?
+	if (desc_recv->iter_provided) {
+
+		// there are some waiting buffer. is-there one for us ?
+		if (unlikely(h->flags & __RPC_HEADER_FLAGS_SIGNAL)
+		    && (!(h->flags & __RPC_HEADER_FLAGS_SIGACK))) {
+			struct rpc_desc_elem *provided;
+
+			provided = list_entry(desc_recv->list_provided_head.prev,
+					      struct rpc_desc_elem, list_desc_elem);
+			
+			if (descelem->seq_id <= provided->seq_id) {
+
+				rpc_do_signal(desc, descelem);
+
+				spin_unlock(&desc->desc_lock);
+				return 0;
+				
+			} else {
+				insert_in_seqid_order(descelem, desc_recv);
+			}
+
+		} else {
+			
+			if (desc_recv->iter_provided->seq_id == descelem->seq_id) {
+				//printk("%d tipc_handler_ordered: found a waiting buffer (%lu)\n",
+				//       current->pid, descelem->seq_id);
+			} else {
+				insert_in_seqid_order(descelem, desc_recv);
+			}
+		}
+		
+		goto do_action;
+		
+	}
+	
+	// unexpected message
+	if (unlikely(h->flags & __RPC_HEADER_FLAGS_SIGNAL)
+	    && (!(h->flags & __RPC_HEADER_FLAGS_SIGACK))
+	    && (h->seq_id <= atomic_read(&desc_recv->seq_id))
+	    && ((desc->service->flags & RPC_FLAGS_NOBLOCK) || desc->thread)) {
+
+		rpc_do_signal(desc, descelem);
+
+		spin_unlock(&desc->desc_lock);
+		return 0;
+	}
+	
+	insert_in_seqid_order(descelem, desc_recv);
+	atomic_inc(&desc_recv->nbunexpected);
+	
+ do_action:
+	err = do_action(desc, h);
+	if (err) {
+		/*
+		 * Keeping this packet at this layer would need to reschedule
+		 * its handling, which lower layers already do without needing
+		 * more metadata. So dropping that packet at this layer is
+		 * simpler and safe.
+		 */
+		spin_lock(&desc->desc_lock);
+		/*
+		 * We expect that only the first packet of a new transaction may
+		 * fail to be handled by do_action(). Otherwise the packet might
+		 * have been unpacked by the handler, or the handler may have
+		 * played with nbunexpected.
+		 */
+		BUG_ON(desc->state != RPC_STATE_NEW || descelem->seq_id > 1);
+		atomic_dec(&desc_recv->nbunexpected);
+		list_del(&descelem->list_desc_elem);
+		spin_unlock(&desc->desc_lock);
+	}
+	return err;
+}
+
+static struct rpc_desc *server_rpc_desc_setup(const struct __rpc_header *h)
+{
+	struct rpc_desc *desc;
+
+	desc = rpc_desc_alloc();
+	if (!desc)
+		goto out;
+
+	desc->desc_send = rpc_desc_send_alloc();
+	if (!desc->desc_send)
+		goto err_desc_send;
+
+	desc->desc_recv[0] = rpc_desc_recv_alloc();
+	if (!desc->desc_recv[0])
+		goto err_desc_recv;
+
+	// Since a RPC_RQ_CLT can only be received from one node:
+	// by choice, we decide to use 0 as the corresponding id
+	krgnode_set(0, desc->nodes);
+
+	desc->desc_id = h->desc_id;
+	desc->type = RPC_RQ_SRV;
+	desc->client = h->client;
+	desc->rpcid = h->rpcid;
+	desc->service = rpc_services[h->rpcid];
+	desc->thread = NULL;
+
+	if (__rpc_emergency_send_buf_alloc(desc, 0))
+		goto err_emergency_send;
+
+	desc->state = RPC_STATE_NEW;
+
+	rpc_desc_get(desc);
+
+	BUG_ON(h->desc_id != desc->desc_id);
+	if (__hashtable_add(desc_srv[h->client], h->desc_id, desc))
+		goto err_hashtable;
+
+out:
+	return desc;
+
+err_hashtable:
+	rpc_desc_put(desc);
+	__rpc_emergency_send_buf_free(desc);
+err_emergency_send:
+	kmem_cache_free(rpc_desc_recv_cachep, desc->desc_recv[0]);
+err_desc_recv:
+	kmem_cache_free(rpc_desc_send_cachep, desc->desc_send);
+err_desc_send:
+	rpc_desc_put(desc);
+	return NULL;
+}
+
+/*
+ * tipc_handler_ordered
+ * Packets are in the right order, so we have to find the corresponding
+ * descriptor (if any).
+ */
+static int tipc_handler_ordered(struct sk_buff *buf,
+				unsigned const char* data,
+				unsigned int size)
+{
+	unsigned char const* iter;
+	struct __rpc_header *h;
+	struct rpc_desc *desc;
+	struct rpc_desc_elem* descelem;
+	struct rpc_desc_recv* desc_recv;
+	struct hashtable_t* desc_ht;
+	int err = 0;
+
+	iter = data;
+	h = (struct __rpc_header*)iter;
+	iter += sizeof(struct __rpc_header);
+
+	/* select the right array regarding the type of request:
+	   __RPC_HEADER_FLAGS_SRV_REPLY: we are the client side -> desc_clt
+	   else: we are the server side -> desc_srv[]
+	*/
+	desc_ht = (h->flags & __RPC_HEADER_FLAGS_SRV_REPLY) ? desc_clt : desc_srv[h->client];
+
+	hashtable_lock(desc_ht);
+	desc = __hashtable_find(desc_ht, h->desc_id);
+
+	if (desc) {
+		BUG_ON(desc->desc_id != h->desc_id);
+		rpc_desc_get(desc);
+
+	} else {
+		
+		spin_lock(&rpc_desc_done_lock[h->client]);
+		if (unlikely(h->desc_id <= rpc_desc_done_id[h->client])) {
+			
+			spin_unlock(&rpc_desc_done_lock[h->client]);
+			hashtable_unlock(desc_ht);
+			goto out;
+
+		}
+
+		rpc_desc_done_id[h->client] = h->desc_id;
+		spin_unlock(&rpc_desc_done_lock[h->client]);
+
+		if(h->flags & __RPC_HEADER_FLAGS_SRV_REPLY){
+
+			// requesting desc is already closed (most probably an async request
+			// just discard this packet
+			hashtable_unlock(desc_ht);
+			goto out;
+
+		}else{
+
+			desc = server_rpc_desc_setup(h);
+			if (!desc) {
+				/*
+				 * Drop the packet, but not silently.
+				 * tipc_handler() may decide to drop more pending
+				 * packets to decrease memory pressure, or keep
+				 * the packet and retry handling it later.
+				 */
+				hashtable_unlock(desc_ht);
+				err = -ENOMEM;
+				goto out;
+			}
+
+		}
+
+	}
+
+	BUG_ON(desc->desc_id != h->desc_id);
+
+	/* Optimization: do not allocate memory if we already know that it is
+	 * useless to.
+	 * If desc is valid after double check, desc_recv retrieved below will
+	 * be valid too, since hashtable's lock acts as a memory barrier between
+	 * the processor having allocated desc (and inserted it in the table)
+	 * and us.
+	 * If desc has a valid state here, as long as we do not release
+	 * hashtable's lock desc_recv retrieved below is valid too (see
+	 * rpc_end()).
+	 */
+	switch (desc->type) {
+	case RPC_RQ_CLT:
+		// we are in the client side (just received a msg from server)
+		desc_recv = desc->desc_recv[h->from];
+		break;
+
+	case RPC_RQ_SRV:
+		// we are in the server side (just received a msg from client)
+		desc_recv = desc->desc_recv[0];
+		break;
+
+	case RPC_RQ_FWD:
+		printk("tipc_handler_ordered: todo\n");
+		BUG();
+		break;
+
+	default:
+		printk("unexpected case %d\n", desc->type);
+		BUG();
+	}
+	/* Is the transaction still accepting packets? */
+	if (!(desc->state & RPC_STATE_MASK_VALID) ||
+	    (desc_recv->flags & RPC_FLAGS_CLOSED)) {
+		hashtable_unlock(desc_ht);
+		goto out_put;
+	}
+
+	hashtable_unlock(desc_ht);
+
+	descelem = kmem_cache_alloc(rpc_desc_elem_cachep, GFP_ATOMIC);
+	if (!descelem) {
+		/*
+		 * Same OOM handling as for new rpc_desc above, except that we
+		 * keep the rpc_desc, even if we just created it, because it is
+		 * now visible in the hashtable and it would just add
+		 * complexity to try to free it.
+		 */
+		err = -ENOMEM;
+		goto out_put;
+	}
+
+	skb_get(buf);
+	descelem->raw = buf;
+	descelem->data = (void*) iter;
+	descelem->seq_id = h->seq_id;
+	descelem->size = size - (iter - data);
+	descelem->flags = h->flags;
+		
+	spin_lock(&desc->desc_lock);
+
+	/* Double-check withe desc->desc_lock held */
+	if (!(desc->state & RPC_STATE_MASK_VALID) ||
+	    (desc_recv->flags & RPC_FLAGS_CLOSED)) {
+		// This side is closed. Discard the packet
+		spin_unlock(&desc->desc_lock);
+		rpc_desc_elem_free(descelem);
+		goto out_put;
+	}
+
+	/* Releases desc->desc_lock */
+	err = handle_valid_desc(desc, desc_recv, descelem, h, buf);
+	if (err)
+		/* Same OOM handling as above */
+		rpc_desc_elem_free(descelem);
+
+out_put:
+	rpc_desc_put(desc);
+out:
+	return err;
+}
+
+static inline int handle_one_packet(kerrighed_node_t node,
+				    struct sk_buff *buf,
+				    unsigned char const *data,
+				    unsigned int size)
+{
+	int err;
+
+	err = tipc_handler_ordered(buf, data, size);
+	if (!err) {
+		if (node == kerrighed_node_id)
+			rpc_link_send_ack_id[node] = rpc_link_recv_seq_id[node];
+		rpc_link_recv_seq_id[node]++;
+	}
+	return err;
+}
+
+static void schedule_run_rx_queue(struct rx_engine *engine);
+
+static void run_rx_queue(struct rx_engine *engine)
+{
+	struct sk_buff_head *queue;
+	kerrighed_node_t node;
+	struct sk_buff *buf;
+	struct __rpc_header *h;
+
+	node = engine->from;
+	queue = &engine->rx_queue;
+	while ((buf = skb_peek(queue))) {
+		h = (struct __rpc_header *)buf->data;
+
+		BUG_ON(h->link_seq_id < rpc_link_recv_seq_id[node]);
+		if (h->link_seq_id > rpc_link_recv_seq_id[node])
+			break;
+
+		if (handle_one_packet(node, buf, buf->data, buf->len)) {
+			schedule_run_rx_queue(engine);
+			break;
+		}
+
+		__skb_unlink(buf, queue);
+		kfree_skb(buf);
+	}
+}
+
+static void run_rx_queue_worker(struct work_struct *work)
+{
+	struct rx_engine *engine =
+		container_of(work, struct rx_engine, run_rx_queue_work.work);
+	spin_lock_bh(&engine->rx_queue.lock);
+	run_rx_queue(engine);
+	spin_unlock_bh(&engine->rx_queue.lock);
+}
+
+static void schedule_run_rx_queue(struct rx_engine *engine)
+{
+	queue_delayed_work(krgcom_wq, &engine->run_rx_queue_work, HZ / 2);
+}
+
+/*
+ * tipc_handler
+ * receives packets from TIPC and orders them
+ */
+static void tipc_handler(void *usr_handle,
+			 u32 port_ref,
+			 struct sk_buff **buf,
+			 unsigned char const *data,
+			 unsigned int size,
+			 unsigned int importance,
+			 struct tipc_portid const *orig,
+			 struct tipc_name_seq const *dest)
+{
+	struct sk_buff_head *queue;
+	struct sk_buff *__buf;
+	struct __rpc_header *h;
+
+	__buf = *buf;
+	h = (struct __rpc_header*)data;
+	BUG_ON(size != __buf->len);
+
+	queue = &tipc_rx_engine[h->from].rx_queue;
+	spin_lock(&queue->lock);
+
+	// Update the ack value sent by the other node
+	if (h->link_ack_id > rpc_link_send_ack_id[h->from]){
+		rpc_link_send_ack_id[h->from] = h->link_ack_id;
+		if(rpc_link_send_ack_id[h->from] - last_cleanup_ack[h->from]
+			> ack_cleanup_window_size){
+			int cpuid;
+			last_cleanup_ack[h->from] = h->link_ack_id;
+			for_each_online_cpu(cpuid){
+				struct tx_engine *engine = &per_cpu(tipc_tx_engine,
+									cpuid);
+				queue_delayed_work_on(cpuid, krgcom_wq,
+							&engine->cleanup_not_retx_work,0);
+
+			}
+		}
+
+	}
+
+	if (h->rpcid == RPC_ACK)
+		goto exit;
+
+	// Check if we are not receiving an already received packet
+	if (h->link_seq_id < rpc_link_recv_seq_id[h->from]) {
+		krgnode_set(h->from, nodes_requiring_ack);
+		queue_delayed_work(krgcom_wq, &tipc_ack_work, 0);
+		goto exit;
+	}
+
+	// Check if we are receiving lot of packets but sending none
+	if (consecutive_recv[h->from] >= max_consecutive_recv[h->from]){
+		krgnode_set(h->from, nodes_requiring_ack);
+		queue_delayed_work(krgcom_wq, &tipc_ack_work, 0);
+	}
+	consecutive_recv[h->from]++;
+
+	// Is-it the next ordered message ?
+	if (h->link_seq_id > rpc_link_recv_seq_id[h->from]) {
+		struct sk_buff *at;
+		unsigned long seq_id = h->link_seq_id;
+
+		/*
+		 * Insert in the ordered list.
+		 * Optimized for in-order reception.
+		 */
+		skb_queue_reverse_walk(queue, at) {
+			struct __rpc_header *ath;
+
+			ath = (struct __rpc_header *)at->data;
+			if (ath->link_seq_id < seq_id)
+				break;
+			else if (ath->link_seq_id == seq_id)
+				/* Duplicate */
+				goto exit;
+		}
+		skb_get(__buf);
+		__skb_queue_after(queue, at, __buf);
+		goto exit;
+	}
+
+	if (handle_one_packet(h->from, __buf, data, size)) {
+		skb_get(__buf);
+		__skb_queue_head(queue, __buf);
+		schedule_run_rx_queue(&tipc_rx_engine[h->from]);
+	} else {
+		run_rx_queue(&tipc_rx_engine[h->from]);
+	}
+
+ exit:
+	spin_unlock(&queue->lock);
+}
+
+static
+u32 port_dispatcher(struct tipc_port *p_ptr, struct sk_buff *buf)
+{
+	struct tipc_msg *msg = (struct tipc_msg *)buf->data;
+	long cpuid = (long)p_ptr->usr_handle;
+	struct tx_engine *engine = &per_cpu(tipc_tx_engine, cpuid);
+
+	/*
+	 * We might have sent something while TIPC is still setting up the
+	 * connection to the peer. Retransmit after a small delay, unless the peer
+	 * disconnects, in which case port_wakeup() will retransmit when
+	 * possible.
+	 */
+	if (msg_errcode(msg) == TIPC_ERR_NO_NAME
+	    && krgnode_present(msg_nameinst(msg))) {
+		queue_delayed_work(krgcom_wq, &tipc_ack_work, REJECT_BACKOFF);
+		queue_delayed_work_on(cpuid, krgcom_wq,
+				      &engine->reachable_work, REJECT_BACKOFF);
+	}
+
+	kfree_skb(buf);
+	return TIPC_OK;
+}
+
+static
+void port_wakeup(struct tipc_port *p_ptr){
+	long cpuid = (long)p_ptr->usr_handle;
+	struct tx_engine *engine = &per_cpu(tipc_tx_engine, cpuid);
+
+	/*
+	 * Schedule the work ASAP
+	 * To help the other side freeing memory, we try to favor acks and delay
+	 * retx by 1 jiffy.
+	 */
+
+	queue_delayed_work(krgcom_wq, &tipc_ack_work, 0);
+
+	queue_delayed_work_on(cpuid, krgcom_wq, &engine->retx_work, 1);
+	queue_delayed_work_on(cpuid, krgcom_wq, &engine->delayed_tx_work, 1);
+}
+
+int comlayer_enable_dev(const char *name)
+{
+	char buf[256];
+	int res;
+
+	printk("Try to enable bearer on %s:", name);
+
+	snprintf(buf, sizeof(buf), "eth:%s", name);
+
+	res = tipc_enable_bearer(buf, tipc_addr(1, 1, 0), TIPC_MEDIA_LINK_PRI);
+	if (res)
+		printk("failed\n");
+	else
+		printk("ok\n");
+
+	return res;
+}
+
+void comlayer_enable(void)
+{
+	struct net_device *netdev;
+
+	read_lock(&dev_base_lock);
+	for_each_netdev(&init_net, netdev)
+		comlayer_enable_dev(netdev->name);
+	read_unlock(&dev_base_lock);
+}
+
+int comlayer_disable_dev(const char *name)
+{
+	int res;
+
+	printk("Try to disable bearer on %s:", name);
+
+	res = tipc_disable_bearer(name);
+	if (res)
+		printk("failed\n");
+	else
+		printk("ok\n");
+
+	return res;
+}
+
+void comlayer_disable(void)
+{
+	struct net_device *netdev;
+
+	read_lock(&dev_base_lock);
+	for_each_netdev(&init_net, netdev)
+		comlayer_disable_dev(netdev->name);
+	read_unlock(&dev_base_lock);
+}
+
+void krg_node_reachable(kerrighed_node_t nodeid){
+	int cpuid;
+
+	queue_delayed_work(krgcom_wq, &tipc_ack_work, 0);
+	for_each_online_cpu(cpuid){
+		struct tx_engine *engine = &per_cpu(tipc_tx_engine, cpuid);
+
+		queue_delayed_work_on(cpuid, krgcom_wq,
+				      &engine->reachable_work, 0);
+	}
+}
+
+void krg_node_unreachable(kerrighed_node_t nodeid){
+}
+
+void rpc_enable_lowmem_mode(kerrighed_node_t nodeid){
+	max_consecutive_recv[nodeid] = MAX_CONSECUTIVE_RECV__LOWMEM_MODE;
+
+	krgnode_set(nodeid, nodes_requiring_ack);
+	queue_delayed_work(krgcom_wq, &tipc_ack_work, 0);
+}
+
+void rpc_disable_lowmem_mode(kerrighed_node_t nodeid){
+	max_consecutive_recv[nodeid] = MAX_CONSECUTIVE_RECV;
+}
+
+void rpc_enable_local_lowmem_mode(void){
+	int cpuid;
+
+	ack_cleanup_window_size = ACK_CLEANUP_WINDOW_SIZE__LOWMEM_MODE;
+
+	for_each_online_cpu(cpuid){
+		struct tx_engine *engine = &per_cpu(tipc_tx_engine, cpuid);
+		queue_delayed_work_on(cpuid, krgcom_wq,
+			&engine->cleanup_not_retx_work, 0);
+	}
+}
+
+void rpc_disable_local_lowmem_mode(void){
+	ack_cleanup_window_size = ACK_CLEANUP_WINDOW_SIZE;
+}
+
+int comlayer_init(void)
+{
+	int res = 0;
+	long i;
+
+	krgnodes_clear(nodes_requiring_ack);	
+
+	for_each_possible_cpu(i) {
+		struct tx_engine *engine = &per_cpu(tipc_tx_engine, i);
+		INIT_LIST_HEAD(&engine->delayed_tx_queue);
+		INIT_DELAYED_WORK(&engine->delayed_tx_work,
+					tipc_delayed_tx_worker);
+		INIT_LIST_HEAD(&engine->not_retx_queue);
+		INIT_DELAYED_WORK(&engine->cleanup_not_retx_work,
+					tipc_cleanup_not_retx_worker);
+		INIT_LIST_HEAD(&engine->retx_queue);
+		engine->retx_iter = NULL;
+		INIT_DELAYED_WORK(&engine->retx_work, tipc_retx_worker);
+
+		INIT_DELAYED_WORK(&engine->reachable_work, tipc_reachable_node_worker);
+		INIT_DELAYED_WORK(&engine->unreachable_work, tipc_unreachable_node_worker);
+	}
+
+	krgcom_wq = create_workqueue("krgcom");
+
+	ack_cleanup_window_size = ACK_CLEANUP_WINDOW_SIZE;
+
+	for (i = 0; i < KERRIGHED_MAX_NODES; i++) {
+		tipc_rx_engine[i].from = i;
+		skb_queue_head_init(&tipc_rx_engine[i].rx_queue);
+		INIT_DELAYED_WORK(&tipc_rx_engine[i].run_rx_queue_work,
+				  run_rx_queue_worker);
+		last_cleanup_ack[i] = 0;
+		consecutive_recv[i] = 0;
+		max_consecutive_recv[i] = MAX_CONSECUTIVE_RECV;
+	}
+
+	tipc_net_id = kerrighed_session_id;
+
+	lockdep_off();
+
+	tipc_core_start_net(tipc_addr(1, 1, kerrighed_node_id+1));
+
+	res = tipc_attach(&tipc_user_ref, NULL, NULL);
+	if (res)
+		goto exit_error;
+
+	res = tipc_createport(tipc_user_ref, NULL, TIPC_LOW_IMPORTANCE,
+			      NULL, NULL, NULL,
+			      NULL, tipc_handler, NULL,
+			      NULL, &tipc_port_ref);
+	if (res)
+		return res;
+
+        tipc_seq.type = TIPC_KRG_SERVER_TYPE;
+        tipc_seq.lower = tipc_seq.upper = kerrighed_node_id;
+        res = tipc_publish(tipc_port_ref, TIPC_CLUSTER_SCOPE, &tipc_seq);
+
+	for_each_possible_cpu(i){
+		u32* send_ref = &per_cpu(tipc_send_ref, i);
+		struct tipc_port* p;
+
+		/* since TIPC do strange assumption regarding this field
+		   we need to initialise it. But this field is dedicated
+		   to the plugins of TIPC. ie: only our code use this field. So
+		   we can set it to any value we want.
+		*/
+		p = tipc_createport_raw((void*)i,
+					port_dispatcher, port_wakeup,
+					TIPC_LOW_IMPORTANCE,
+					(void*)0x1111);
+		if(p){
+			*send_ref = p->ref;
+			spin_unlock_bh(p->lock);
+		} else {
+			spin_unlock_bh(p->lock);
+			goto exit_error;
+		}
+	};
+
+	lockdep_on();
+
+	return 0;
+	
+ exit_error:
+	printk("Error while trying to init TIPC (%d)\n", res);
+        return res;
+}
diff -ruN linux-2.6.29/net/krgrpc/Kconfig android_cluster/linux-2.6.29/net/krgrpc/Kconfig
--- linux-2.6.29/net/krgrpc/Kconfig	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/krgrpc/Kconfig	2014-05-27 23:04:10.578024766 -0700
@@ -0,0 +1,16 @@
+#
+# Kerrighed RPC Configuration
+#
+
+config KRGRPC
+	bool "Kerrighed RPC Protocol"
+	depends on TIPC
+	---help---
+	  Cluster-aware Remote Procedure Call (RPC) framework.
+
+	  This protocol is dedicated to distributed operating system services.
+	  User-space application are not supposed to use this protocol.
+
+	  For more informations about KRPC, see http://www.kerrighed.org
+
+	  Currently, if you are not planning to run Kerrighed, say N.
diff -ruN linux-2.6.29/net/krgrpc/Makefile android_cluster/linux-2.6.29/net/krgrpc/Makefile
--- linux-2.6.29/net/krgrpc/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/krgrpc/Makefile	2014-05-27 23:04:10.578024766 -0700
@@ -0,0 +1,11 @@
+#
+# Makefile for the Kerrighed Remote Procedure Call layer
+#
+
+obj-$(CONFIG_KRGRPC) := krgrpc.o
+
+krgrpc-y := rpc.o comlayer.o thread_pool.o rpclayer.o monitor.o rpc_hotplug.o synchro.o
+
+EXTRA_CFLAGS += -I$(M) -Wall -Werror
+
+# end of file
diff -ruN linux-2.6.29/net/krgrpc/monitor.c android_cluster/linux-2.6.29/net/krgrpc/monitor.c
--- linux-2.6.29/net/krgrpc/monitor.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/krgrpc/monitor.c	2014-05-27 23:04:10.578024766 -0700
@@ -0,0 +1,72 @@
+/**
+ *
+ *  Copyright (C) 2007 Pascal Gallard, Kerlabs <Pascal.Gallard@kerlabs.com>
+ *
+ */
+
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <kerrighed/krgnodemask.h>
+#include <kerrighed/krginit.h>
+
+#include <kerrighed/workqueue.h>
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+
+#include "rpc_internal.h"
+
+static struct timer_list rpc_timer;
+struct work_struct rpc_work;
+struct rpc_service pingpong_service;
+
+static void rpc_pingpong_handler (struct rpc_desc *rpc_desc,
+				  void *data,
+				  size_t size){
+	unsigned long l = *(unsigned long*)data;
+
+	l++;
+	
+	rpc_pack(rpc_desc, 0, &l, sizeof(l));
+};
+
+static void rpc_worker(struct work_struct *data)
+{
+	static unsigned long l = 0;
+	krgnodemask_t n;
+	int r;
+
+	r = 0;
+	l++;
+	
+	krgnodes_clear(n);
+	krgnode_set(0, n);
+
+	r = rpc_async(RPC_PINGPONG, 0, &l, sizeof(l));
+	if(r<0)
+		return;
+	
+}
+
+static void rpc_timer_cb(unsigned long _arg)
+{
+	return;
+	queue_work(krg_wq, &rpc_work);
+	mod_timer(&rpc_timer, jiffies + 2*HZ);
+}
+
+int rpc_monitor_init(void){
+	rpc_register_void(RPC_PINGPONG,
+			  rpc_pingpong_handler, 0);
+	
+	init_timer(&rpc_timer);
+	rpc_timer.function = rpc_timer_cb;
+	rpc_timer.data = 0;
+	if(kerrighed_node_id != 0)
+		mod_timer(&rpc_timer, jiffies + 10*HZ);
+	INIT_WORK(&rpc_work, rpc_worker);
+
+	return 0;
+}
+
+void rpc_monitor_cleanup(void){
+}
diff -ruN linux-2.6.29/net/krgrpc/rpc.c android_cluster/linux-2.6.29/net/krgrpc/rpc.c
--- linux-2.6.29/net/krgrpc/rpc.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/krgrpc/rpc.c	2014-05-27 23:04:10.578024766 -0700
@@ -0,0 +1,343 @@
+/**
+ *
+ *  Copyright (C) 2007 Pascal Gallard, Kerlabs <Pascal.Gallard@kerlabs.com>
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/smp.h>
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <linux/irqflags.h>
+#include <linux/spinlock.h>
+#include <linux/lockdep.h>
+#include <linux/string.h>
+#include <kerrighed/krgnodemask.h>
+#include <linux/hashtable.h>
+
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+
+#include "rpc_internal.h"
+
+struct rpc_service** rpc_services;
+unsigned long rpc_desc_id;
+hashtable_t* desc_srv[KERRIGHED_MAX_NODES];
+hashtable_t* desc_clt;
+spinlock_t rpc_desc_done_lock[KERRIGHED_MAX_NODES];
+unsigned long rpc_desc_done_id[KERRIGHED_MAX_NODES];
+
+unsigned long rpc_link_send_seq_id[KERRIGHED_MAX_NODES];
+unsigned long rpc_link_send_ack_id[KERRIGHED_MAX_NODES];
+unsigned long rpc_link_recv_seq_id[KERRIGHED_MAX_NODES];
+
+DEFINE_PER_CPU(struct list_head, rpc_desc_trash);
+
+struct kmem_cache* rpc_desc_cachep;
+struct kmem_cache* rpc_desc_send_cachep;
+struct kmem_cache* rpc_desc_recv_cachep;
+struct kmem_cache* rpc_desc_elem_cachep;
+struct kmem_cache* rpc_tx_elem_cachep;
+struct kmem_cache* __rpc_synchro_cachep;
+
+static struct lock_class_key rpc_desc_srv_lock_key;
+static struct lock_class_key rpc_desc_clt_lock_key;
+
+unsigned long rpc_mask[RPCID_MAX/(sizeof(unsigned long)*8)+1];
+
+/*
+ * RPC management
+ */
+inline
+struct rpc_service* rpc_service_init(enum rpcid rpcid,
+				     enum rpc_target rpc_target,
+				     enum rpc_handler rpc_handler,
+				     struct rpc_synchro *rpc_synchro,
+				     rpc_handler_t h,
+				     unsigned long flags){
+	struct rpc_service* service;
+
+	service = kmalloc(sizeof(*service), GFP_KERNEL);
+	if(!service){
+		printk("OOM in rpc_service_init\n");
+		return NULL;
+	};
+	
+	service->id = rpcid;
+	service->target = rpc_target;
+	service->handler = rpc_handler;
+	service->h = h;
+	service->synchro = rpc_synchro;
+	service->flags = flags;
+
+	return service;
+};
+
+int __rpc_register(enum rpcid rpcid,
+		   enum rpc_target rpc_target,
+		   enum rpc_handler rpc_handler,
+		   struct rpc_synchro *rpc_synchro,
+		   void* _h, unsigned long flags){
+	rpc_handler_t h = (rpc_handler_t)_h;
+	rpc_services[rpcid] = rpc_service_init(rpcid, rpc_target, rpc_handler,
+					       rpc_synchro, h, flags);
+
+	rpc_disable(rpcid);
+	return 0;
+};
+
+struct rpc_desc* rpc_desc_alloc(void){
+	struct rpc_desc* desc;
+	int in_interrupt;
+	int cpu = smp_processor_id();
+	
+	in_interrupt = 0;
+	if(list_empty(&per_cpu(rpc_desc_trash, cpu))){
+		desc = kmem_cache_alloc(rpc_desc_cachep, GFP_ATOMIC);
+		if(!desc)
+			return NULL;
+		
+		in_interrupt = 1;
+	}else{
+		desc = container_of(per_cpu(rpc_desc_trash, cpu).next,
+				    struct rpc_desc,
+				    list);
+		list_del(&desc->list);
+	};
+
+	memset(desc, 0, sizeof(*desc));
+	spin_lock_init(&desc->desc_lock);
+	desc->in_interrupt = in_interrupt;
+	atomic_set(&desc->usage, 1);
+	desc->__synchro = NULL;
+
+	return desc;
+};
+
+void rpc_desc_get(struct rpc_desc* desc){
+	BUG_ON(atomic_read(&desc->usage)==0);
+	atomic_inc(&desc->usage);
+};
+
+void rpc_desc_put(struct rpc_desc* desc){
+	BUG_ON(atomic_read(&desc->usage)==0);
+	if(!atomic_dec_and_test(&desc->usage))
+		return;
+	
+	kmem_cache_free(rpc_desc_cachep, desc);
+};
+
+struct rpc_desc_send* rpc_desc_send_alloc(void){
+	struct rpc_desc_send* desc_send;
+
+	desc_send = kmem_cache_alloc(rpc_desc_send_cachep, GFP_ATOMIC);
+	if(!desc_send)
+		return NULL;
+
+	atomic_set(&desc_send->seq_id, 0);
+	spin_lock_init(&desc_send->lock);
+	INIT_LIST_HEAD(&desc_send->list_desc_head);
+	desc_send->flags = 0;
+
+	return desc_send;
+};
+
+struct rpc_desc_recv* rpc_desc_recv_alloc(void){
+	struct rpc_desc_recv* desc_recv;
+
+	desc_recv = kmem_cache_alloc(rpc_desc_recv_cachep, GFP_ATOMIC);
+	if(!desc_recv)
+		return NULL;
+
+	atomic_set(&desc_recv->seq_id, 0);
+	atomic_set(&desc_recv->nbunexpected, 0);
+	INIT_LIST_HEAD(&desc_recv->list_desc_head);
+	INIT_LIST_HEAD(&desc_recv->list_provided_head);
+	INIT_LIST_HEAD(&desc_recv->list_signal_head);
+	desc_recv->iter = NULL;
+	desc_recv->iter_provided = NULL;
+	desc_recv->received_packets = 0;
+	desc_recv->flags = 0;
+	
+	return desc_recv;
+};
+
+
+void test(void){
+}
+
+/*
+ *
+ * Enable a registered RPC
+ * We must take the waiting_desc_lock.
+ * After each rpc handle, the krgrpc go through the waiting_desc
+ * list, in order to find another desc to process. We must avoid
+ * to enable an RPC when such iteration is happened
+ *
+ */
+void rpc_enable(enum rpcid rpcid){
+	spin_lock_bh(&waiting_desc_lock);
+	if(rpc_services[rpcid]->id == rpcid)
+		clear_bit(rpcid, rpc_mask);
+
+	spin_unlock_bh(&waiting_desc_lock);
+};
+
+void rpc_enable_all(void){
+	int i;
+
+	for(i=0;i<RPCID_MAX;i++)
+		rpc_enable(i);
+	
+	if(!list_empty(&waiting_desc))
+		rpc_wake_up_thread(NULL);
+};
+
+void rpc_disable(enum rpcid rpcid){
+	if(rpc_services[rpcid]->id == rpcid)
+		set_bit(rpcid, rpc_mask);
+};
+
+
+/** Initialisation of the rpc module.
+ *  @author Pascal Gallard
+ */
+
+void rpc_undef_handler (struct rpc_desc *desc){
+	printk("service %d not registered\n", desc->rpcid);
+};
+
+void rpc_enable_alldev(void)
+{
+	comlayer_enable();
+}
+
+int rpc_enable_dev(const char *name)
+{
+	return comlayer_enable_dev(name);
+}
+
+void rpc_disable_alldev(void)
+{
+	comlayer_disable();
+}
+
+int rpc_disable_dev(const char *name)
+{
+	return comlayer_disable_dev(name);
+}
+
+int init_rpc(void)
+{
+	int i, res;
+	struct rpc_service *rpc_undef_service;
+
+	rpc_desc_cachep = kmem_cache_create("rpc_desc",
+					    sizeof(struct rpc_desc),
+					    0, 0, NULL);
+	if(!rpc_desc_cachep)
+		return -ENOMEM;
+	
+	rpc_desc_send_cachep = kmem_cache_create("rpc_desc_send",
+						 sizeof(struct rpc_desc_send),
+						 0, 0, NULL);
+	if(!rpc_desc_send_cachep)
+		return -ENOMEM;
+
+	rpc_desc_recv_cachep = kmem_cache_create("rpc_desc_recv",
+						 sizeof(struct rpc_desc_recv),
+						 0, 0, NULL);
+	if(!rpc_desc_recv_cachep)
+		return -ENOMEM;
+
+	rpc_tx_elem_cachep = kmem_cache_create("rpc_tx_elem",
+					       sizeof(struct rpc_tx_elem),
+					       0, 0, NULL);
+	if(!rpc_tx_elem_cachep)
+		return -ENOMEM;
+
+	rpc_desc_elem_cachep = kmem_cache_create("rpc_desc_elem",
+						 sizeof(struct rpc_desc_elem),
+						 0, 0, NULL);
+	if(!rpc_desc_elem_cachep)
+		return -ENOMEM;
+
+	__rpc_synchro_cachep = kmem_cache_create("__rpc_synchro",
+						 sizeof(struct __rpc_synchro),
+						 0, 0, NULL);
+	if(!__rpc_synchro_cachep)
+		return -ENOMEM;
+	
+	memset(rpc_mask, 0, sizeof(rpc_mask));
+	
+	rpc_services = kmalloc(sizeof(*rpc_services)*(RPCID_MAX+1),
+			       GFP_KERNEL);
+	if(!rpc_services)
+		return -ENOMEM;
+
+	rpc_undef_service = rpc_service_init(RPC_UNDEF,
+					     RPC_TARGET_NODE,
+					     RPC_HANDLER_KTHREAD_VOID,
+					     NULL,
+					     rpc_undef_handler, 0);
+
+	for(i=0;i<RPCID_MAX;i++)
+		rpc_services[i] = rpc_undef_service;
+	
+	for_each_possible_cpu(i){
+		INIT_LIST_HEAD(&per_cpu(rpc_desc_trash, i));
+	};
+		
+	rpc_desc_id = 1;
+
+	for(i=0;i<KERRIGHED_MAX_NODES;i++){
+		desc_srv[i] = hashtable_new(32);
+		if(!desc_srv[i])
+			return -ENOMEM;
+
+		lockdep_set_class(&desc_srv[i]->lock, &rpc_desc_srv_lock_key);
+
+		rpc_desc_done_id[i] = 0;
+		spin_lock_init(&rpc_desc_done_lock[i]);
+
+	};
+	desc_clt = hashtable_new(32);
+	if(!desc_clt)
+		return -ENOMEM;
+
+	lockdep_set_class(&desc_clt->lock, &rpc_desc_clt_lock_key);
+
+	for (i = 0; i < KERRIGHED_MAX_NODES; i++) {
+		rpc_link_send_seq_id[i] = 1;
+		rpc_link_send_ack_id[i] = 0;
+		rpc_link_recv_seq_id[i] = 1;
+	}
+		
+	res = thread_pool_init();
+	if(res)
+		return res;
+	
+	res = comlayer_init();
+	if(res)
+		return res;
+
+	res = rpclayer_init();
+	if(res)
+		return res;
+
+	res = rpc_monitor_init();
+	if(res)
+		return res;
+	
+	printk("RPC initialisation done\n");
+	
+	return 0;
+}
+
+/** Cleanup of the Nazgul module.
+ *  @author Pascal Gallard
+ */
+void cleanup_rpc(void)
+{
+}
diff -ruN linux-2.6.29/net/krgrpc/rpc_hotplug.c android_cluster/linux-2.6.29/net/krgrpc/rpc_hotplug.c
--- linux-2.6.29/net/krgrpc/rpc_hotplug.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/krgrpc/rpc_hotplug.c	2014-05-27 23:04:10.578024766 -0700
@@ -0,0 +1,51 @@
+/*
+ *  Copyright (C) 2006-2007, Pascal Gallard, Kerlabs.
+ */
+
+#include <linux/notifier.h>
+#include <linux/kernel.h>
+#include <kerrighed/krgnodemask.h>
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+#include <kerrighed/hotplug.h>
+
+#include "rpc_internal.h"
+
+static void rpc_remove(krgnodemask_t * vector)
+{
+	printk("Have to send all the tx_queue before stopping the node\n");
+};
+
+
+/**
+ *
+ * Notifier related part
+ *
+ */
+
+#ifdef CONFIG_KERRIGHED
+static int rpc_notification(struct notifier_block *nb, hotplug_event_t event,
+			    void *data){
+	struct hotplug_node_set *node_set = data;
+	
+	switch(event){
+	case HOTPLUG_NOTIFY_REMOVE:
+		rpc_remove(&node_set->v);
+		break;
+	default:
+		break;
+	}
+	
+	return NOTIFY_OK;
+};
+#endif
+
+int rpc_hotplug_init(void){
+#ifdef CONFIG_KERRIGHED
+	register_hotplug_notifier(rpc_notification, HOTPLUG_PRIO_RPC);
+#endif
+	return 0;
+};
+
+void rpc_hotplug_cleanup(void){
+};
diff -ruN linux-2.6.29/net/krgrpc/rpc_internal.h android_cluster/linux-2.6.29/net/krgrpc/rpc_internal.h
--- linux-2.6.29/net/krgrpc/rpc_internal.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/krgrpc/rpc_internal.h	2014-05-27 23:04:10.578024766 -0700
@@ -0,0 +1,233 @@
+#ifndef __RPC_INTERNAL__
+#define __RPC_INTERNAL__
+
+#include <linux/uio.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/radix-tree.h>
+#include <linux/slab.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krgnodemask.h>
+#include <net/krgrpc/rpc.h>
+
+#define __RPC_HEADER_FLAGS_SIGNAL    (1<<0)
+#define __RPC_HEADER_FLAGS_SIGACK    (1<<1)
+#define __RPC_HEADER_FLAGS_SRV_REPLY (1<<3)
+#define __RPC_HEADER_FLAGS_CANCEL_PACK (1<<4)
+
+enum {
+	__RPC_FLAGS_EMERGENCY_BUF = __RPC_FLAGS_MAX,
+	__RPC_FLAGS_NEW_DESC_ID,
+	__RPC_FLAGS_CLOSED,
+};
+
+#define RPC_FLAGS_EMERGENCY_BUF	(1<<__RPC_FLAGS_EMERGENCY_BUF)
+#define RPC_FLAGS_NEW_DESC_ID	(1<<__RPC_FLAGS_NEW_DESC_ID)
+#define RPC_FLAGS_CLOSED	(1<<__RPC_FLAGS_CLOSED)
+
+struct rpc_desc_send {
+	atomic_t seq_id;
+	spinlock_t lock;
+	struct list_head list_desc_head;
+	void *emergency_send_buf;
+	int flags;
+};
+
+struct rpc_desc_recv {
+	atomic_t seq_id;
+	atomic_t nbunexpected;
+	unsigned long received_packets;      // bitfield
+	struct list_head list_desc_head;
+	struct list_head list_provided_head;
+	struct list_head list_signal_head;
+	struct rpc_desc_elem *iter;
+	struct rpc_desc_elem *iter_provided;
+	int flags;
+};
+
+struct __rpc_synchro_tree {
+	spinlock_t lock;
+	struct radix_tree_root rt;
+};
+
+enum ____rpc_synchro_flags {
+	/* __rpc_synchro has been removed from its radix tree */
+	____RPC_SYNCHRO_DEAD,
+};
+
+#define __RPC_SYNCHRO_DEAD (1<<____RPC_SYNCHRO_DEAD)
+
+struct __rpc_synchro {
+	atomic_t usage;
+	atomic_t v;
+	struct list_head list_waiting_head;
+	spinlock_t lock;
+	unsigned long key;
+	struct __rpc_synchro_tree *tree;
+	int flags;
+};
+
+struct rpc_synchro {
+	int max;
+	int order;
+	unsigned long mask_packets;          // bitfield
+	union {
+		struct __rpc_synchro tab;
+		struct __rpc_synchro_tree tree;
+	} nodes[KERRIGHED_MAX_NODES];
+	struct list_head list_synchro;
+	char label[16];
+};
+
+struct rpc_service {
+	enum rpc_target target;
+	enum rpc_handler handler;
+	rpc_handler_t h;
+	struct rpc_synchro *synchro;
+	enum rpcid id;
+	unsigned long flags;
+};
+
+struct __rpc_header {
+	kerrighed_node_t from;
+	kerrighed_node_t client;
+	unsigned long desc_id;
+	unsigned long seq_id;
+	unsigned long link_seq_id;
+	unsigned long link_ack_id;
+	enum rpcid rpcid;
+	int flags;
+};
+
+struct rpc_desc_elem {
+	unsigned long seq_id;
+	void* raw;
+	void* data;
+	size_t size;
+	struct list_head list_desc_elem;
+	int flags;
+};
+
+struct rpc_tx_elem {
+	krgnodemask_t nodes;
+	kerrighed_node_t index;
+	kerrighed_node_t link_seq_index;
+	void *data;
+	struct iovec iov[2];
+	struct __rpc_header h;
+	unsigned long *link_seq_id;
+	struct list_head tx_queue;
+};
+
+extern struct rpc_service** rpc_services;
+
+struct hashtable_t;
+extern struct hashtable_t* desc_srv[KERRIGHED_MAX_NODES];
+extern struct hashtable_t* desc_clt;
+extern unsigned long rpc_desc_id;
+extern unsigned long rpc_desc_done_id[KERRIGHED_MAX_NODES];
+extern spinlock_t rpc_desc_done_lock[KERRIGHED_MAX_NODES];
+
+extern struct kmem_cache* rpc_desc_cachep;
+extern struct kmem_cache* rpc_desc_send_cachep;
+extern struct kmem_cache* rpc_desc_recv_cachep;
+extern struct kmem_cache* rpc_desc_elem_cachep;
+extern struct kmem_cache* rpc_tx_elem_cachep;
+extern struct kmem_cache* __rpc_synchro_cachep;
+
+extern unsigned long rpc_mask[RPCID_MAX/(sizeof(unsigned long)*8)+1];
+extern spinlock_t waiting_desc_lock;
+extern struct list_head waiting_desc;
+
+extern struct list_head list_synchro_head;
+
+extern unsigned long rpc_link_send_seq_id[KERRIGHED_MAX_NODES];
+extern unsigned long rpc_link_send_ack_id[KERRIGHED_MAX_NODES];
+extern unsigned long rpc_link_recv_seq_id[KERRIGHED_MAX_NODES];
+
+struct rpc_desc* rpc_desc_alloc(void);
+struct rpc_desc_send* rpc_desc_send_alloc(void);
+struct rpc_desc_recv* rpc_desc_recv_alloc(void);
+void rpc_desc_elem_free(struct rpc_desc_elem *elem);
+
+void rpc_desc_get(struct rpc_desc* desc);
+void rpc_desc_put(struct rpc_desc* desc);
+
+void rpc_do_signal(struct rpc_desc *desc,
+		   struct rpc_desc_elem *signal_elem);
+void rpc_signal_deliver_pending(struct rpc_desc *desc,
+				struct rpc_desc_recv *desc_recv);
+int __rpc_signalack(struct rpc_desc* desc);
+
+int rpc_handle_new(struct rpc_desc* desc);
+void rpc_wake_up_thread(struct rpc_desc *desc);
+
+void rpc_new_desc_id_lock(void);
+void rpc_new_desc_id_unlock(void);
+int __rpc_emergency_send_buf_alloc(struct rpc_desc *desc, size_t size);
+void __rpc_emergency_send_buf_free(struct rpc_desc *desc);
+int __rpc_send_ll(struct rpc_desc* desc,
+		  krgnodemask_t *nodes,
+		  unsigned long seq_id,
+		  int __flags,
+		  const void* data, size_t size,
+		  int rpc_flags);
+
+void __rpc_put_raw_data(void *raw);
+void __rpc_get_raw_data(void *raw);
+
+void __rpc_synchro_free(struct rpc_desc *desc);
+int rpc_synchro_lookup(struct rpc_desc* desc);
+
+int comlayer_init(void);
+void comlayer_enable(void);
+int comlayer_enable_dev(const char *name);
+void comlayer_disable(void);
+int comlayer_disable_dev(const char *name);
+int thread_pool_init(void);
+int rpclayer_init(void);
+int rpc_monitor_init(void);
+
+#define rpc_link_seq_id(p, node) \
+  __asm__ __volatile__( \
+    "lock xadd %%eax, %1" \
+    :"=a" (p), "=m" (rpc_link_send_seq_id[node]) \
+    :"a" (1) : "memory")
+
+#define rpc_desc_set_id(p) \
+  __asm__ __volatile__( \
+    "lock xadd %%eax, %1" \
+    :"=a" (p), "=m" (rpc_desc_id) \
+    :"a" (1) : "memory")
+
+#endif
+
+static inline
+int __rpc_synchro_get(struct __rpc_synchro *__rpc_synchro){
+	return !atomic_inc_not_zero(&__rpc_synchro->usage);
+}
+
+static inline
+void __rpc_synchro_put(struct __rpc_synchro *__rpc_synchro){
+
+	if(!atomic_dec_and_test(&__rpc_synchro->usage))
+		return;
+
+	// Check if we are in a tree
+	// If we are, we need to free the data
+	if(__rpc_synchro->tree){
+		spin_lock_bh(&__rpc_synchro->tree->lock);
+
+		/* Maybe another CPU or a softIRQ had to replace __rpc_synchro
+		 * in the radix tree (see rpc_synchro_lookup_order1())
+		 */
+		if (likely(!(__rpc_synchro->flags & __RPC_SYNCHRO_DEAD)))
+			radix_tree_delete(&__rpc_synchro->tree->rt,
+					  __rpc_synchro->key);
+
+		spin_unlock_bh(&__rpc_synchro->tree->lock);
+
+		kmem_cache_free(__rpc_synchro_cachep,
+				__rpc_synchro);
+	}
+}
diff -ruN linux-2.6.29/net/krgrpc/rpclayer.c android_cluster/linux-2.6.29/net/krgrpc/rpclayer.c
--- linux-2.6.29/net/krgrpc/rpclayer.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/krgrpc/rpclayer.c	2014-05-27 23:04:10.578024766 -0700
@@ -0,0 +1,817 @@
+/**
+ *
+ *  Copyright (C) 2007 Pascal Gallard, Kerlabs <Pascal.Gallard@kerlabs.com>
+ *
+ */
+
+#include <linux/skbuff.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/irqflags.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/lockdep.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/krgnodemask.h>
+#include <linux/hashtable.h>
+
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+
+#include "rpc_internal.h"
+
+/* In __rpc_send, unsure atomicity of rpc_link_seq_id and rpc_desc_set_id */
+static spinlock_t lock_id;
+
+kerrighed_node_t rpc_desc_get_client(struct rpc_desc *desc){
+	BUG_ON(!desc);
+	return desc->client;
+}
+
+void rpc_new_desc_id_lock(void)
+{
+	if (!irqs_disabled())
+		local_bh_disable();
+	spin_lock(&lock_id);
+	hashtable_lock(desc_clt);
+}
+
+void rpc_new_desc_id_unlock(void)
+{
+	hashtable_unlock(desc_clt);
+	spin_unlock(&lock_id);
+	if (!irqs_disabled())
+		local_bh_enable();
+}
+
+inline
+int __rpc_send(struct rpc_desc* desc,
+		      unsigned long seq_id, int __flags,
+		      const void* data, size_t size,
+		      int rpc_flags)
+{
+	int err = 0;
+
+	switch (desc->type) {
+	case RPC_RQ_CLT:
+		if (desc->desc_id == 0) {
+
+			rpc_new_desc_id_lock();
+
+			rpc_desc_set_id(desc->desc_id);
+
+			if (__hashtable_add(desc_clt, desc->desc_id, desc)) {
+				rpc_new_desc_id_unlock();
+
+				desc->desc_id = 0;
+
+				return -ENOMEM;
+			}
+
+			/* Calls rpc_new_desc_id_unlock() on success */
+			err = __rpc_send_ll(desc, &desc->nodes,
+					    seq_id,
+					    __flags, data, size,
+					    rpc_flags | RPC_FLAGS_NEW_DESC_ID);
+			if (err) {
+				__hashtable_remove(desc_clt, desc->desc_id);
+				rpc_new_desc_id_unlock();
+
+				desc->desc_id = 0;
+			}
+
+		} else
+			err = __rpc_send_ll(desc, &desc->nodes,
+					    seq_id,
+					    __flags, data, size,
+					    rpc_flags);
+		break;
+
+	case RPC_RQ_SRV: {
+		krgnodemask_t nodes;
+
+		krgnodes_clear(nodes);
+		krgnode_set(desc->client, nodes);
+
+		err = __rpc_send_ll(desc, &nodes, seq_id,
+				    __flags, data, size,
+				    rpc_flags);
+		break;
+	}
+
+	default:
+		printk("unexpected case %d\n", desc->type);
+		BUG();
+	}
+
+	return err;
+}
+
+struct rpc_desc* rpc_begin_m(enum rpcid rpcid,
+			     krgnodemask_t* nodes)
+{
+	struct rpc_desc* desc;
+	int i;
+
+	desc = rpc_desc_alloc();
+	if(!desc)
+		goto oom;
+
+	__krgnodes_copy(&desc->nodes, nodes);
+	desc->type = RPC_RQ_CLT;
+	desc->client = kerrighed_node_id;
+	
+	desc->desc_send = rpc_desc_send_alloc();
+	if(!desc->desc_send)
+		goto oom_free_desc;
+
+	for_each_krgnode_mask(i, desc->nodes){
+		desc->desc_recv[i] = rpc_desc_recv_alloc();
+		if(!desc->desc_recv[i])
+			goto oom_free_desc_recv;
+	}
+
+	desc->rpcid = rpcid;
+	desc->service = rpc_services[rpcid];
+	desc->client = kerrighed_node_id;
+
+	if (__rpc_emergency_send_buf_alloc(desc, 0))
+		goto oom_free_desc_recv;
+
+	desc->state = RPC_STATE_RUN;
+
+	return desc;
+
+oom_free_desc_recv:
+	for_each_krgnode_mask(i, desc->nodes)
+		if (desc->desc_recv[i])
+			kmem_cache_free(rpc_desc_recv_cachep,
+					desc->desc_recv[i]);
+	kmem_cache_free(rpc_desc_send_cachep, desc->desc_send);
+oom_free_desc:
+	rpc_desc_put(desc);
+oom:
+	return NULL;
+}
+
+inline
+int __rpc_end_pack(struct rpc_desc* desc)
+{
+	struct rpc_desc_elem *descelem, *safe;
+	int err = 0;
+
+	list_for_each_entry_safe(descelem, safe,
+				 &desc->desc_send->list_desc_head,
+				 list_desc_elem) {
+		/*
+		 * After first error, just discard remaining packets as
+		 * receivers may not be able to unpack them because of the
+		 * missing ones.
+		 */
+		if (!err) {
+			err = -EPIPE;
+			if (!(desc->desc_send->flags & RPC_FLAGS_CLOSED)) {
+				err = __rpc_send(desc, descelem->seq_id, 0,
+						 descelem->data, descelem->size,
+						 0);
+				if (err)
+					rpc_cancel_pack(desc);
+			}
+		}
+		list_del(&descelem->list_desc_elem);
+		kmem_cache_free(rpc_desc_elem_cachep, descelem);
+	}
+	return err;
+}
+
+inline
+int __rpc_end_unpack(struct rpc_desc_recv* desc_recv)
+{
+	while (!list_empty(&desc_recv->list_provided_head)) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+	}
+	return 0;
+}
+
+static void __rpc_end_unpack_clean_queue(struct list_head *elem_head)
+{
+	struct rpc_desc_elem *iter, *safe;
+
+	list_for_each_entry_safe(iter, safe, elem_head, list_desc_elem) {
+		list_del(&iter->list_desc_elem);
+		rpc_desc_elem_free(iter);
+	}
+}
+
+inline
+int __rpc_end_unpack_clean(struct rpc_desc* desc)
+{
+	int i;
+
+	for_each_krgnode_mask(i, desc->nodes){
+		struct rpc_desc_recv* desc_recv = desc->desc_recv[i];
+
+		desc->desc_recv[i] = NULL;
+
+		if (unlikely(!list_empty(&desc_recv->list_desc_head)))
+			__rpc_end_unpack_clean_queue(&desc_recv->list_desc_head);
+		if (unlikely(!list_empty(&desc_recv->list_signal_head)))
+			__rpc_end_unpack_clean_queue(&desc_recv->list_signal_head);
+
+		kmem_cache_free(rpc_desc_recv_cachep, desc_recv);
+	}
+	
+	return 0;
+}
+
+int rpc_end(struct rpc_desc* desc, int flags)
+{
+	struct rpc_desc_send *rpc_desc_send;
+	struct hashtable_t* desc_ht;
+	int err;
+
+	lockdep_off();
+	
+	err = __rpc_end_pack(desc);
+
+	switch(desc->type){
+	case RPC_RQ_CLT:{
+		int i;
+
+		for_each_krgnode_mask(i, desc->nodes){
+			__rpc_end_unpack(desc->desc_recv[i]);
+		}
+
+		desc_ht = desc_clt;
+		break;
+	}
+	case RPC_RQ_SRV:
+		
+		__rpc_end_unpack(desc->desc_recv[0]);
+
+		desc_ht = desc_srv[desc->client];
+		break;
+	default:
+		printk("unexpected case\n");
+		BUG();
+	}
+
+	spin_lock_bh(&desc->desc_lock);
+	hashtable_lock(desc_ht);
+
+	desc->state = RPC_STATE_END;
+
+	__hashtable_remove(desc_ht, desc->desc_id);
+	BUG_ON(__hashtable_find(desc_ht, desc->desc_id));
+
+	hashtable_unlock(desc_ht);
+	spin_unlock_bh(&desc->desc_lock);
+
+	__rpc_emergency_send_buf_free(desc);
+
+	rpc_desc_send = desc->desc_send;
+	desc->desc_send = NULL;
+	kmem_cache_free(rpc_desc_send_cachep, rpc_desc_send);
+
+	__rpc_end_unpack_clean(desc);
+
+	if(desc->__synchro)
+		__rpc_synchro_put(desc->__synchro);
+
+	rpc_desc_put(desc);
+
+	lockdep_on();
+	return err;
+}
+
+int rpc_cancel_pack(struct rpc_desc* desc)
+{
+	int last_pack;
+	unsigned long seq_id;
+	int err = 0;
+
+	if (desc->desc_send->flags & RPC_FLAGS_CLOSED)
+		goto out;
+
+	last_pack = list_empty(&desc->desc_send->list_desc_head);
+	if (last_pack) {
+		seq_id = atomic_inc_return(&desc->desc_send->seq_id);
+	} else {
+		struct rpc_desc_elem *next;
+
+		next = list_entry(desc->desc_send->list_desc_head.next,
+				  struct rpc_desc_elem, list_desc_elem);
+		seq_id = next->seq_id;
+	}
+
+	err = __rpc_send(desc, seq_id,
+			 __RPC_HEADER_FLAGS_CANCEL_PACK,
+			 0, 0,
+			 RPC_FLAGS_EMERGENCY_BUF);
+
+	/*
+	 * if RPC_FLAGS_EMERGENCY_BUF was used too many times, then
+	 * either MAX_EMERGENCY_SEND should be increased or the caller fixed.
+	 */
+	WARN_ON(err);
+	if (!err)
+		desc->desc_send->flags |= RPC_FLAGS_CLOSED;
+	else if (last_pack)
+		/* Allow caller to retry */
+		atomic_dec(&desc->desc_send->seq_id);
+
+out:
+	return err;
+}
+
+void rpc_cancel_unpack_from(struct rpc_desc *desc, kerrighed_node_t node)
+{
+	struct rpc_desc_recv *desc_recv = desc->desc_recv[node];
+
+	desc_recv->flags |= RPC_FLAGS_CLOSED;
+	/* TODO: send a notification to the sender so that it stops sending */
+}
+
+void rpc_cancel_unpack(struct rpc_desc* desc)
+{
+	kerrighed_node_t node;
+
+	for_each_krgnode_mask(node, desc->nodes)
+		rpc_cancel_unpack_from(desc, node);
+}
+
+int rpc_cancel(struct rpc_desc* desc){
+	int err;
+
+	err = rpc_cancel_pack(desc);
+	rpc_cancel_unpack(desc);
+
+	return err;
+}
+
+int rpc_forward(struct rpc_desc* desc, kerrighed_node_t node){
+	return 0;
+}
+
+int rpc_pack(struct rpc_desc* desc, int flags, const void* data, size_t size)
+{
+	int err = -EPIPE;
+
+	if (desc->desc_send->flags & RPC_FLAGS_CLOSED)
+		goto out;
+
+	if (flags & RPC_FLAGS_LATER) {
+		struct rpc_desc_elem *descelem;
+
+		err = -ENOMEM;
+		descelem = kmem_cache_alloc(rpc_desc_elem_cachep, GFP_ATOMIC);
+		if (!descelem)
+			goto out;
+
+		descelem->data = (void *) data;
+		descelem->size = size;
+		descelem->seq_id = atomic_inc_return(&desc->desc_send->seq_id);
+
+		list_add_tail(&descelem->list_desc_elem,
+			      &desc->desc_send->list_desc_head); 
+		return descelem->seq_id;
+	}
+
+	err = __rpc_send(desc, atomic_inc_return(&desc->desc_send->seq_id), 0,
+			 data, size,
+			 0);
+	if (err)
+		/* Allow caller to retry or cancel */
+		atomic_dec(&desc->desc_send->seq_id);
+
+out:
+	return err;
+}
+
+int rpc_wait_pack(struct rpc_desc* desc, int seq_id)
+{
+	struct rpc_desc_elem *descelem, *safe;
+	int err;
+	int last_seq_id = 0;
+
+	if (!list_empty(&desc->desc_send->list_desc_head)) {
+		list_for_each_entry_safe(descelem, safe,
+					 &desc->desc_send->list_desc_head,
+					 list_desc_elem) {
+			if (descelem->seq_id > seq_id)
+				break;
+
+			err = -EPIPE;
+			if (!(desc->desc_send->flags & RPC_FLAGS_CLOSED))
+				err = __rpc_send(desc, descelem->seq_id, 0,
+						 descelem->data, descelem->size,
+						 0);
+			if (err) {
+				seq_id = last_seq_id;
+				break;
+			}
+			last_seq_id = descelem->seq_id;
+			list_del(&descelem->list_desc_elem);
+			kmem_cache_free(rpc_desc_elem_cachep, descelem);
+		}
+	}
+
+	return seq_id;
+}
+
+static void __rpc_signal_dequeue_pending(struct rpc_desc *desc,
+					 struct rpc_desc_recv *desc_recv,
+					 struct list_head *head)
+{
+	struct rpc_desc_elem *descelem, *tmp_elem;
+	unsigned long seq_id;
+
+	seq_id = desc_recv->iter ? desc_recv->iter->seq_id : 0;
+	list_for_each_entry_safe(descelem, tmp_elem,
+				 &desc_recv->list_signal_head, list_desc_elem) {
+		if (descelem->seq_id > seq_id
+		    || (descelem->flags & __RPC_HEADER_FLAGS_SIGACK))
+			break;
+		list_move_tail(&descelem->list_desc_elem, head);
+	}
+}
+
+static void __rpc_signal_deliver_pending(struct rpc_desc *desc,
+					 struct list_head *head)
+{
+	struct rpc_desc_elem *descelem, *tmp_elem;
+
+	list_for_each_entry_safe(descelem, tmp_elem, head, list_desc_elem) {
+		list_del(&descelem->list_desc_elem);
+		rpc_do_signal(desc, descelem);
+	}
+}
+
+void rpc_signal_deliver_pending(struct rpc_desc *desc,
+				struct rpc_desc_recv *desc_recv)
+{
+	LIST_HEAD(signals_head);
+
+	spin_lock_bh(&desc->desc_lock);
+	if (unlikely(!list_empty(&desc_recv->list_signal_head)))
+		__rpc_signal_dequeue_pending(desc, desc_recv, &signals_head);
+	spin_unlock_bh(&desc->desc_lock);
+	if (unlikely(!list_empty(&signals_head)))
+		__rpc_signal_deliver_pending(desc, &signals_head);
+}
+
+/* Dequeue sigacks up to ones sent after the next data to unpack */
+static
+struct rpc_desc_elem *
+__rpc_signal_dequeue_sigack(struct rpc_desc *desc,
+			    struct rpc_desc_recv *desc_recv)
+{
+	struct rpc_desc_elem *ret = NULL;
+
+	if (unlikely(!list_empty(&desc_recv->list_signal_head))) {
+		struct rpc_desc_elem *sig;
+		unsigned long seq_id;
+
+		seq_id = desc_recv->iter ? desc_recv->iter->seq_id : 0;
+		sig = list_entry(desc_recv->list_signal_head.next,
+				 struct rpc_desc_elem, list_desc_elem);
+		if ((sig->flags & __RPC_HEADER_FLAGS_SIGACK)
+		    && sig->seq_id <= seq_id + 1) {
+			list_del(&sig->list_desc_elem);
+			ret = sig;
+		}
+	}
+
+	return ret;
+}
+
+inline
+int __rpc_unpack_from_node(struct rpc_desc* desc, kerrighed_node_t node,
+			   int flags, void* data, size_t size)
+{
+	struct rpc_desc_elem *descelem;
+	struct rpc_desc_recv* desc_recv = desc->desc_recv[node];
+	LIST_HEAD(signals_head);
+	LIST_HEAD(sigacks_head);
+	atomic_t seq_id;
+
+	BUG_ON(!desc);
+	BUG_ON(!data);
+
+	if (desc_recv->flags & RPC_FLAGS_CLOSED)
+		return RPC_EPIPE;
+
+	if (unlikely(desc_recv->flags & RPC_FLAGS_REPOST))
+		atomic_set(&seq_id, atomic_read(&desc_recv->seq_id));
+	else
+		atomic_set(&seq_id, atomic_inc_return(&desc_recv->seq_id));
+
+ restart:
+	spin_lock_bh(&desc->desc_lock);
+
+	/* Return rpc_signalacks ASAP */
+	if (unlikely(!list_empty(&desc_recv->list_signal_head))) {
+		for (;;) {
+			descelem = __rpc_signal_dequeue_sigack(desc, desc_recv);
+			if (!descelem)
+				break;
+			if (flags & RPC_FLAGS_SIGACK) {
+				spin_unlock_bh(&desc->desc_lock);
+				rpc_desc_elem_free(descelem);
+				desc_recv->flags |= RPC_FLAGS_REPOST;
+				return RPC_ESIGACK;
+			}
+			/* Store discarded sigacks in a list to free them with
+			 * desc_lock released */
+			list_add(&descelem->list_desc_elem, &sigacks_head);
+		}
+	}
+
+	if (desc_recv->iter == NULL) {
+
+		if (list_empty(&desc_recv->list_desc_head)) {
+			goto __restart;
+		} else {
+
+			descelem = container_of(desc_recv->list_desc_head.next,
+						struct rpc_desc_elem,
+						list_desc_elem);
+
+			if (descelem->seq_id != 1) {
+				goto __restart;
+			}
+			desc_recv->iter = descelem;
+		}
+	} else {
+		struct rpc_desc_elem *next_desc_recv;
+
+		if (list_is_last(&desc_recv->iter->list_desc_elem,
+				 &desc_recv->list_desc_head)) {
+			goto __restart;
+		}
+
+		next_desc_recv = container_of(desc_recv->iter->list_desc_elem.next,
+					      struct rpc_desc_elem,
+					      list_desc_elem);
+
+		if (desc_recv->iter->seq_id+1 != next_desc_recv->seq_id) {
+			goto __restart;
+		}
+		desc_recv->iter = next_desc_recv;
+	}
+	atomic_dec(&desc_recv->nbunexpected);
+
+	/* Signals sent right after the matching pack() must be delivered
+	 * now (actually with desc_recv's lock released). */
+	if (unlikely(!list_empty(&desc_recv->list_signal_head)))
+		__rpc_signal_dequeue_pending(desc, desc_recv, &signals_head);
+
+	spin_unlock_bh(&desc->desc_lock);
+
+	if (unlikely(!list_empty(&signals_head)))
+		__rpc_signal_deliver_pending(desc, &signals_head);
+	if (unlikely(!list_empty(&sigacks_head)))
+		__rpc_end_unpack_clean_queue(&sigacks_head);
+
+	if (desc_recv->iter->flags & __RPC_HEADER_FLAGS_CANCEL_PACK) {
+		desc_recv->flags |= RPC_FLAGS_CLOSED;
+		return RPC_EPIPE;
+	}
+
+	if (flags & RPC_FLAGS_NOCOPY) {
+		struct rpc_data *rpc_data = data;
+
+		__rpc_get_raw_data(desc_recv->iter->raw);
+
+		rpc_data->raw = desc_recv->iter->raw;
+		rpc_data->data = desc_recv->iter->data;
+		rpc_data->size = size;
+
+	} else if (desc_recv->iter->size <= size) {
+		memcpy(data, desc_recv->iter->data, desc_recv->iter->size);
+	} else {
+		printk("unsufficient room for received packet (%d  %lu-%lu)!\n",
+		       desc->rpcid,
+		       desc->desc_id, desc_recv->iter->seq_id);
+		BUG();
+	}
+
+	desc_recv->flags &= ~RPC_FLAGS_REPOST;
+	return RPC_EOK;
+
+ __restart:
+	if (flags&RPC_FLAGS_NOBLOCK) {
+		struct rpc_desc_elem *descelem;
+
+		descelem = kmem_cache_alloc(rpc_desc_elem_cachep, GFP_ATOMIC);
+		if (!descelem) {
+			printk("OOM in __rpc_unpack_from_node\n");
+			BUG();
+		}
+		
+		descelem->data = data;
+		descelem->size = size;
+		descelem->seq_id = atomic_read(&seq_id);
+		
+		list_add_tail(&descelem->list_desc_elem,
+			      &desc_recv->list_provided_head); 
+
+		if (!desc_recv->iter_provided)
+			desc_recv->iter_provided = descelem;
+		
+		spin_unlock_bh(&desc->desc_lock);
+		desc_recv->flags &= ~RPC_FLAGS_REPOST;
+		return RPC_EOK;
+	}
+
+	desc->thread = current;
+	desc->wait_from = node;
+	desc->state = RPC_STATE_WAIT1;
+	set_current_state(flags & RPC_FLAGS_INTR ? TASK_INTERRUPTIBLE : TASK_UNINTERRUPTIBLE);
+	spin_unlock_bh(&desc->desc_lock);
+
+	schedule();
+	if (signal_pending(current) && (flags & RPC_FLAGS_INTR)) {
+		desc_recv->flags |= RPC_FLAGS_REPOST;
+		return RPC_EINTR;
+	}
+
+	goto restart;
+}
+
+enum rpc_error
+rpc_unpack(struct rpc_desc* desc, int flags, void* data, size_t size){
+	switch(desc->type){
+	case RPC_RQ_CLT:{
+		kerrighed_node_t node;
+		// ASSUME that only one node is set in desc->nodes
+		// If it's not a single request, the result of this function (in this case)
+		// is UNDEFINED
+
+		BUG_ON(krgnodes_weight(desc->nodes)!=1);
+		
+		node = first_krgnode(desc->nodes);
+		
+		BUG_ON(node >= KERRIGHED_MAX_NODES);
+		
+		return __rpc_unpack_from_node(desc, node, flags, data, size);
+	}
+	case RPC_RQ_SRV:
+		return __rpc_unpack_from_node(desc, 0, flags, data, size);
+	default:
+		printk("unexpected case\n");
+		BUG();
+	}
+	
+	return 0;
+}
+
+enum rpc_error
+rpc_unpack_from(struct rpc_desc* desc, kerrighed_node_t node,
+		int flags, void* data, size_t size)
+{
+	switch(desc->type){
+	case RPC_RQ_CLT:
+		return __rpc_unpack_from_node(desc, node, flags, data, size);
+	case RPC_RQ_SRV:
+		if(node == desc->client)
+			return __rpc_unpack_from_node(desc, node, flags, data, size);
+		return 0;
+	default:
+		printk("unexpected case\n");
+		BUG();
+	}
+
+	return 0;
+}
+
+kerrighed_node_t rpc_wait_return(struct rpc_desc* desc, int* value)
+{
+	kerrighed_node_t node;
+
+	if (desc->type != RPC_RQ_CLT)
+		return -1;
+
+
+ __restart:
+	
+	spin_lock_bh(&desc->desc_lock);
+	for(node=0;node<KERRIGHED_MAX_NODES;node++){
+		if(desc->desc_recv[node]
+		   && atomic_read(&desc->desc_recv[node]->nbunexpected)){
+
+			spin_unlock_bh(&desc->desc_lock);
+
+			if(value)
+				rpc_unpack_from(desc, node,
+						0, value, sizeof(*value));
+
+			return node;
+		}
+	}
+
+	desc->state = RPC_STATE_WAIT;
+	desc->thread = current;
+	set_current_state(TASK_INTERRUPTIBLE);
+	spin_unlock_bh(&desc->desc_lock);
+
+	schedule();
+
+	goto __restart;
+}
+
+int rpc_wait_return_from(struct rpc_desc* desc, kerrighed_node_t node)
+{
+
+	if(desc->type != RPC_RQ_CLT)
+		return -1;
+
+ __restart:
+	
+	spin_lock_bh(&desc->desc_lock);
+	if(atomic_read(&desc->desc_recv[node]->nbunexpected)){
+		int value;
+
+		spin_unlock_bh(&desc->desc_lock);
+		rpc_unpack_type_from(desc, node, value);
+		return value;
+	}
+	
+	desc->state = RPC_STATE_WAIT1;
+	desc->wait_from = node;
+	desc->thread = current;
+	set_current_state(TASK_INTERRUPTIBLE);
+	spin_unlock_bh(&desc->desc_lock);
+
+	schedule();
+	
+	goto __restart;
+}
+
+int rpc_wait_all(struct rpc_desc *desc)
+{
+	int i;
+	
+	if(desc->type != RPC_RQ_CLT)
+		return -1;
+
+	// on doit tester si tous les retours sont effectuee
+	// (comment definir qu'un retour est acheve ? variable d'etat dans desc_recv ?)
+	// tant qu'il reste des retours a effectuer, on attend et on boucle
+
+	for_each_krgnode_mask(i, desc->nodes){
+
+		if(list_empty(&desc->desc_recv[i]->list_provided_head))
+			continue;
+		
+		spin_lock_bh(&desc->desc_lock);
+		desc->state = RPC_STATE_WAIT;
+		desc->thread = current;
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_unlock_bh(&desc->desc_lock);
+
+		schedule();
+
+	}
+	
+	return 0;
+}
+
+int rpc_signal(struct rpc_desc* desc, int sigid)
+{
+	if (desc->desc_send->flags & RPC_FLAGS_CLOSED)
+		return -EPIPE;
+	return __rpc_send(desc, atomic_read(&desc->desc_send->seq_id),
+			  __RPC_HEADER_FLAGS_SIGNAL,
+			  &sigid, sizeof(sigid),
+			  0);
+}
+
+int __rpc_signalack(struct rpc_desc* desc)
+{
+	int v;
+
+	if (desc->desc_send->flags & RPC_FLAGS_CLOSED)
+		return -EPIPE;
+	return __rpc_send(desc, atomic_read(&desc->desc_send->seq_id),
+			  __RPC_HEADER_FLAGS_SIGNAL | __RPC_HEADER_FLAGS_SIGACK,
+			  &v, sizeof(v),
+			  0);
+}
+
+void rpc_free_buffer(struct rpc_data *rpc_data)
+{
+	__rpc_put_raw_data(rpc_data->raw);
+}
+
+int rpclayer_init(void)
+{
+	spin_lock_init(&lock_id);
+	return 0;
+}
+
+void rpclayer_cleanup(void){
+}
diff -ruN linux-2.6.29/net/krgrpc/synchro.c android_cluster/linux-2.6.29/net/krgrpc/synchro.c
--- linux-2.6.29/net/krgrpc/synchro.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/krgrpc/synchro.c	2014-05-27 23:04:10.578024766 -0700
@@ -0,0 +1,165 @@
+/**
+ *
+ *  Copyright (C) 2007 Pascal Gallard, Kerlabs <Pascal.Gallard@kerlabs.com>
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/smp.h>
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <linux/irqflags.h>
+#include <linux/spinlock.h>
+#include <linux/lockdep.h>
+#include <linux/string.h>
+#include <kerrighed/krgnodemask.h>
+
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+
+#include "rpc_internal.h"
+
+LIST_HEAD(list_synchro_head);
+
+inline
+void __rpc_synchro_init(struct __rpc_synchro *__rpc_synchro,
+			int max){
+	__rpc_synchro->key = 0;
+	atomic_set(&__rpc_synchro->usage, 1);
+	atomic_set(&__rpc_synchro->v, max);
+	INIT_LIST_HEAD(&__rpc_synchro->list_waiting_head);
+	spin_lock_init(&__rpc_synchro->lock);
+	__rpc_synchro->tree = NULL;
+	__rpc_synchro->flags = 0;
+}
+
+/*
+ * RPC synchro
+ */
+struct rpc_synchro* rpc_synchro_new(int max,
+				    char *label,
+				    int order){
+	struct rpc_synchro *ret;
+	kerrighed_node_t i;
+
+	ret = kmalloc(sizeof(*ret), GFP_KERNEL);
+	BUG_ON(!ret);
+
+	if(label)
+		snprintf(ret->label, sizeof(ret->label), "%s", label);
+	else
+		snprintf(ret->label, sizeof(ret->label), "no name");
+
+	ret->max = max;
+	ret->order = order;
+
+	ret->mask_packets = 0;
+	for(i=0;i<ret->order;i++)
+		set_bit(i, &ret->mask_packets);
+
+	if(order){
+		for(i=0;i<KERRIGHED_MAX_NODES;i++){
+			INIT_RADIX_TREE(&ret->nodes[i].tree.rt, GFP_ATOMIC);
+			spin_lock_init(&ret->nodes[i].tree.lock);
+		}
+	}else{
+		for(i=0;i<KERRIGHED_MAX_NODES;i++)
+			__rpc_synchro_init(&ret->nodes[i].tab, max);
+	}
+
+	list_add_tail(&ret->list_synchro, &list_synchro_head);
+	return ret;
+}
+
+inline
+int rpc_synchro_lookup_order0(struct rpc_desc *desc){
+	__rpc_synchro_get(&desc->service->synchro->nodes[desc->client].tab);
+	desc->__synchro = &desc->service->synchro->nodes[desc->client].tab;
+	return 0;
+
+};
+
+inline
+int rpc_synchro_lookup_order1(struct rpc_desc *desc){
+	struct rpc_desc_elem *descelem;
+	unsigned long key;
+	struct rpc_synchro *synchro;
+	struct __rpc_synchro *__synchro;
+	struct __rpc_synchro_tree *__rpc_synchro_tree;
+
+	synchro = desc->service->synchro;
+
+	descelem = list_entry(desc->desc_recv[0]->list_desc_head.next,
+			      struct rpc_desc_elem, list_desc_elem);
+
+	key = *((unsigned long*)descelem->data);
+
+	__rpc_synchro_tree = &synchro->nodes[desc->client].tree;
+
+	spin_lock(&__rpc_synchro_tree->lock);
+	__synchro = radix_tree_lookup(&__rpc_synchro_tree->rt, key);
+
+	if(__synchro && __rpc_synchro_get(__synchro)) {
+		/* __synchro is beeing freed. Just remove it from the tree and
+		 * replace it with a clean new one. */
+		radix_tree_delete(&__rpc_synchro_tree->rt, __synchro->key);
+		__synchro->flags &= __RPC_SYNCHRO_DEAD;
+		__synchro = NULL;
+	}
+	if (!__synchro){
+		__synchro = kmem_cache_alloc(__rpc_synchro_cachep, GFP_ATOMIC);
+		if(!__synchro){
+			spin_unlock(&__rpc_synchro_tree->lock);
+			return -ENOMEM;
+		}
+
+		__rpc_synchro_init(__synchro, synchro->max);
+
+		__synchro->key = key;
+		__synchro->tree = __rpc_synchro_tree;
+
+		radix_tree_insert(&__rpc_synchro_tree->rt, key, __synchro);
+	}
+	spin_unlock(&__rpc_synchro_tree->lock);
+
+	desc->__synchro = __synchro;
+
+	return 0;
+}
+
+inline
+int rpc_synchro_lookup_order_generic(struct rpc_desc *desc){
+#if 0
+	if((desc->desc_recv[0]->received_packets & desc->service->synchro->mask_packets)
+	   == desc->service->synchro->mask_packets){
+		desc->__synchro = &desc->service->synchro->nodes[desc->client];
+		return 0;
+	}
+
+	return -ENOENT;
+#endif
+
+	printk("rpc_synchro_lookup: order > 1 => TODO\n");
+	BUG();
+
+	return 0;
+}
+
+int rpc_synchro_lookup(struct rpc_desc *desc){
+
+	int order;
+
+	if(!desc->service->synchro)
+		return 0;
+
+	order = desc->service->synchro->order;
+
+	if(likely(order==0)){
+		return rpc_synchro_lookup_order0(desc);
+	}else if (likely(order==1)){
+		return rpc_synchro_lookup_order1(desc);
+	}else{
+		return rpc_synchro_lookup_order_generic(desc);
+	}
+}
diff -ruN linux-2.6.29/net/krgrpc/thread_pool.c android_cluster/linux-2.6.29/net/krgrpc/thread_pool.c
--- linux-2.6.29/net/krgrpc/thread_pool.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/krgrpc/thread_pool.c	2014-05-27 23:04:10.578024766 -0700
@@ -0,0 +1,577 @@
+/**
+ *
+ *  Copyright (C) 2007 Pascal Gallard, Kerlabs <Pascal.Gallard@kerlabs.com>
+ *
+ */
+
+#include <linux/smp.h>
+#include <linux/sched.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/types.h>
+#include <linux/init_task.h>
+#include <linux/fs_struct.h>
+#include <linux/file.h>
+#include <linux/fdtable.h>
+#include <kerrighed/sys/types.h>
+#include <kerrighed/krginit.h>
+#include <kerrighed/workqueue.h>
+#include <net/krgrpc/rpcid.h>
+#include <net/krgrpc/rpc.h>
+
+#include "rpc_internal.h"
+
+typedef unsigned long threads_vector_t;
+#define THREADS_VECTOR_WIDTH (sizeof(threads_vector_t)*8)
+
+struct threads_pool {
+	threads_vector_t threads_vector;
+	struct task_struct* threads[THREADS_VECTOR_WIDTH];
+	struct rpc_desc* desc[THREADS_VECTOR_WIDTH];
+	int nbthreads;
+};
+
+DEFINE_PER_CPU(struct threads_pool, threads_pool);
+
+struct waiting_desc {
+	struct list_head list_waiting_desc;
+	struct rpc_desc *desc;
+};
+
+struct list_head waiting_desc;
+spinlock_t waiting_desc_lock;
+
+struct {
+	atomic_t request[NR_CPUS];
+	struct delayed_work dwork;
+} new_thread_data;
+
+void (*rpc_handlers[RPC_HANDLER_MAX])(struct rpc_desc* desc);
+
+#define KRGRPC_INIT_FDTABLE \
+{                                                       \
+        .max_fds        = NR_OPEN_DEFAULT,              \
+        .fd             = &krgrpc_files.fd_array[0],    \
+        .close_on_exec  = (fd_set *)&krgrpc_files.close_on_exec_init, \
+        .open_fds       = (fd_set *)&krgrpc_files.open_fds_init,  \
+        .rcu            = RCU_HEAD_INIT,                \
+        .next           = NULL,                         \
+}
+
+#define KRGRPC_INIT_FILES \
+{                                                       \
+        .count          = ATOMIC_INIT(1),               \
+        .fdt            = &krgrpc_files.fdtab,          \
+        .fdtab          = KRGRPC_INIT_FDTABLE,          \
+	.file_lock	= __SPIN_LOCK_UNLOCKED(krgrpc_files.file_lock), \
+        .next_fd        = 0,                            \
+        .close_on_exec_init = { { 0, } },               \
+        .open_fds_init  = { { 0, } },                   \
+        .fd_array       = { NULL, }                     \
+}
+
+static struct files_struct krgrpc_files = KRGRPC_INIT_FILES;
+struct task_struct *first_krgrpc = NULL;
+
+static struct completion init_complete;
+
+void rpc_handler_kthread(struct rpc_desc* desc){
+	((rpc_handler_t)desc->service->h)(desc);
+};
+
+void rpc_handler_kthread_void(struct rpc_desc* desc){
+	int err;
+	struct rpc_data rpc_data;
+
+	BUG_ON(!desc);
+	
+	err = rpc_unpack(desc, RPC_FLAGS_NOCOPY,
+			 &rpc_data, 0);
+	
+	if(!err){
+		BUG_ON(!desc);
+		BUG_ON(!desc->service);
+		BUG_ON(!desc->service->h);
+		
+		((rpc_handler_void_t)desc->service->h)(desc, rpc_data.data,
+						       rpc_data.size);
+
+		rpc_free_buffer(&rpc_data);
+		
+	}else{
+		printk("unexpected event\n");
+		BUG();
+	};
+
+};
+
+void rpc_handler_kthread_int(struct rpc_desc* desc){
+	int res;
+	int id;
+	int err;
+	struct rpc_data rpc_data;
+
+	err = rpc_unpack(desc, RPC_FLAGS_NOCOPY,
+			 &rpc_data, 0);
+	
+	if(!err){
+	
+		id = rpc_pack(desc, RPC_FLAGS_LATER,
+				&res, sizeof(res));
+
+		res = ((rpc_handler_int_t)desc->service->h)(desc,
+							    rpc_data.data,
+							    rpc_data.size);
+
+		rpc_free_buffer(&rpc_data);
+		rpc_wait_pack(desc, id);
+
+	}else{
+		printk("unexpected event\n");
+		BUG();
+	};
+};
+
+inline
+void do_krgrpc_handler(struct rpc_desc* desc,
+		       int thread_pool_id){
+	struct __rpc_synchro* __synchro;
+	kerrighed_node_t client;
+	struct waiting_desc *wd;
+
+	BUG_ON(desc->type != RPC_RQ_SRV);
+
+	__synchro = desc->__synchro;
+        if(__synchro)
+                __rpc_synchro_get(__synchro);
+			
+ continue_in_synchro:
+	client = desc->client;
+	BUG_ON(!desc->desc_recv[0]);
+
+	if(test_bit(desc->rpcid, rpc_mask)){
+		printk("need to move current desc in the waiting_desc queue\n");
+		BUG();
+	};
+
+	/* Deliver immediately rpc_signals sent before first client's pack() */
+	rpc_signal_deliver_pending(desc, desc->desc_recv[0]);
+	rpc_handlers[desc->service->handler](desc);
+	BUG_ON(signal_pending(current));
+
+	rpc_end(desc, 0);
+
+	if(__synchro){
+		// check pending_work in the synchro
+		spin_lock_bh(&__synchro->lock);
+
+		if(!list_empty(&__synchro->list_waiting_head)){
+
+			wd = list_entry(__synchro->list_waiting_head.next,
+					struct waiting_desc,
+					list_waiting_desc);
+
+			list_del(&wd->list_waiting_desc);
+
+			spin_unlock_bh(&__synchro->lock);
+
+			rpc_desc_put(wd->desc);
+
+			desc = wd->desc;
+			desc->thread = current;
+			desc->state = RPC_STATE_RUN;
+
+			kfree(wd);
+
+			goto continue_in_synchro;
+		}else{
+			atomic_inc(&__synchro->v);
+			spin_unlock_bh(&__synchro->lock);
+		}
+
+		__rpc_synchro_put(__synchro);
+	}
+}
+
+static int thread_pool_init_fs(void)
+{
+	struct fs_struct *new_fs;
+
+	new_fs = copy_fs_struct(current->fs);
+	if (!new_fs)
+		return -ENOMEM;
+	exit_fs(current);
+	task_lock(current);
+	current->fs = new_fs;
+	task_unlock(current);
+	return 0;
+}
+
+int thread_pool_run(void* _data){
+	struct threads_pool* thread_pool;
+	struct rpc_desc *desc;
+	int j;
+
+	/*
+	 * Unlike the files_struct, we want each RPC handler to have an
+	 * independent fs_struct, so that they can chroot() at will.
+	 * Each RPC handler is responsible for correctly resetting its root
+	 * whenever it does chroot()
+	 */
+	if (thread_pool_init_fs()) {
+		if(atomic_inc_return(&new_thread_data.request[smp_processor_id()]) == 1)
+			queue_delayed_work(krg_nb_wq, &new_thread_data.dwork, 0);
+		return -EAGAIN;
+	}
+
+	/* We don't want to share the init_task.files struct.
+	   We want that krgrpc share their own files struct. */
+	atomic_inc(&krgrpc_files.count);
+	reset_files_struct(&krgrpc_files);
+
+	thread_pool = &per_cpu(threads_pool, smp_processor_id());
+
+	j = find_next_zero_bit(&thread_pool->threads_vector,
+			       THREADS_VECTOR_WIDTH,
+			       thread_pool->nbthreads);
+
+	if(j < THREADS_VECTOR_WIDTH){
+		BUG_ON(j < thread_pool->nbthreads);
+
+		set_bit(j, &thread_pool->threads_vector);
+		mb();
+
+		thread_pool->threads[j] = current;
+		thread_pool->nbthreads++;
+		desc = thread_pool->desc[j];
+		thread_pool->desc[j] = NULL;
+
+		/* Here we just want to have a pointer on one
+		   krgrpc. We dont care about the first or the second one */
+		if(!first_krgrpc){
+			first_krgrpc = current;
+			complete(&init_complete);
+		}
+	}else{
+		desc = NULL;
+	}
+
+	while (!kthread_should_stop()) {
+		struct waiting_desc *wd, *wd_safe;
+
+	continue_in_waiting_desc:
+		
+		if(desc)
+			do_krgrpc_handler(desc, j);
+
+		spin_lock_bh(&waiting_desc_lock);
+		list_for_each_entry_safe(wd, wd_safe,
+					 &waiting_desc,
+					 list_waiting_desc){
+
+			if(test_bit(wd->desc->rpcid, rpc_mask))
+				continue;
+			
+			list_del(&wd->list_waiting_desc);
+			spin_unlock_bh(&waiting_desc_lock);
+
+			//put: remove from the list
+			rpc_desc_put(wd->desc);
+			
+			desc = wd->desc;
+			desc->thread = current;
+			desc->state = RPC_STATE_RUN;
+			kfree(wd);
+
+			BUG_ON(!desc->desc_recv[0]);
+			goto continue_in_waiting_desc;
+		};
+
+		if(j<THREADS_VECTOR_WIDTH){
+			set_current_state(TASK_INTERRUPTIBLE);
+			clear_bit(j, &thread_pool->threads_vector);
+			spin_unlock_bh(&waiting_desc_lock);
+
+			schedule();
+
+			// prepare the next work
+			desc = thread_pool->desc[j];
+			thread_pool->desc[j] = NULL;
+		}else{
+			spin_unlock_bh(&waiting_desc_lock);
+			return 0;
+		}
+
+		BUG_ON(signal_pending(current));
+
+	};
+	return 0;
+};
+
+static
+void new_thread_worker(struct work_struct *data){
+	int i;
+
+	for_each_online_cpu(i){
+		while(atomic_add_unless(&new_thread_data.request[i],
+					 -1, 0)){
+			struct task_struct *tsk;
+
+			tsk = kthread_create(thread_pool_run, NULL, "krgrpc");
+			if (IS_ERR(tsk)) {
+				atomic_inc(&new_thread_data.request[i]);
+				/* Backoff,
+				 * hope it will be possible next time */
+				queue_delayed_work(krg_nb_wq,
+						   &new_thread_data.dwork,
+						   HZ);
+				break;
+			}
+			kthread_bind(tsk, i);
+			wake_up_process(tsk);
+
+		};
+	};
+
+};
+
+inline
+void list_waiting_ordered_add(struct list_head *head,
+			      struct waiting_desc *wd){
+	//get: going to add to a list
+	rpc_desc_get(wd->desc);
+
+	if(list_empty(head)){
+		list_add(&wd->list_waiting_desc, head);
+	}else{
+		struct waiting_desc *iter;
+		list_for_each_entry_reverse(iter, head,
+					    list_waiting_desc){
+			if(iter->desc->desc_id < wd->desc->desc_id){
+				list_add(&wd->list_waiting_desc,
+					 &iter->list_waiting_desc);
+				return;
+			};
+		};
+		list_add(&wd->list_waiting_desc, head);
+	};
+};
+
+inline
+int queue_waiting_desc(struct rpc_desc* desc){
+	struct waiting_desc* wd;
+	int r = 0;
+
+	wd = kmalloc(sizeof(struct waiting_desc), GFP_ATOMIC);
+	if(!wd){
+		r = -ENOMEM;
+		goto out;
+	};
+
+	rpc_desc_get(desc);
+	wd->desc = desc;
+	desc->state = RPC_STATE_HANDLE;
+
+	spin_lock(&waiting_desc_lock);
+	list_add_tail(&wd->list_waiting_desc, &waiting_desc);
+	spin_unlock(&waiting_desc_lock);
+
+out:
+	return r;
+}
+
+inline
+struct rpc_desc* handle_in_interrupt(struct rpc_desc* desc){
+	struct __rpc_synchro *__synchro;
+	struct waiting_desc *wd;
+
+	__synchro = desc->__synchro;
+
+	if(__synchro)
+		__rpc_synchro_get(__synchro);
+
+ continue_in_synchro:
+
+	rpc_handlers[desc->service->handler](desc);
+
+	rpc_end(desc, 0);
+
+	if(__synchro){
+		spin_lock_bh(&__synchro->lock);
+
+		if(!list_empty(&__synchro->list_waiting_head)){
+
+			wd = list_entry(__synchro->list_waiting_head.next,
+					struct waiting_desc,
+					list_waiting_desc);
+
+			list_del(&wd->list_waiting_desc);
+
+			spin_unlock_bh(&__synchro->lock);
+
+			rpc_desc_put(wd->desc);
+
+			desc = wd->desc;
+			desc->thread = NULL;
+			desc->state = RPC_STATE_RUN;
+
+			kfree(wd);
+
+			if(desc->service->flags & RPC_FLAGS_NOBLOCK)
+				goto continue_in_synchro;
+		}else{
+			atomic_inc(&__synchro->v);
+			spin_unlock_bh(&__synchro->lock);
+		}
+
+		__rpc_synchro_put(__synchro);
+
+	}
+
+	return desc;
+}
+
+int rpc_handle_new(struct rpc_desc* desc){
+	struct threads_pool* thread_pool = &per_cpu(threads_pool, smp_processor_id());
+	struct __rpc_synchro *__synchro;
+	int i, r=0;
+
+	if (!desc->__synchro) {
+		r = rpc_synchro_lookup(desc);
+		if (r)
+			return r;
+	}
+
+	__synchro = desc->__synchro;
+	if(__synchro){
+		spin_lock(&__synchro->lock);
+		
+		if(atomic_read(&__synchro->v)){
+			
+			atomic_dec(&__synchro->v);
+			spin_unlock(&__synchro->lock);
+
+		}else{
+			struct waiting_desc *wd;
+
+			wd = kmalloc(sizeof(struct waiting_desc),
+				     GFP_ATOMIC);
+			if(!wd) {
+				spin_unlock(&__synchro->lock);
+				return -ENOMEM;
+			}
+
+			wd->desc = desc;
+			desc->state = RPC_STATE_HANDLE;
+
+			list_waiting_ordered_add(&__synchro->list_waiting_head,
+						 wd);
+
+			spin_unlock(&__synchro->lock);
+			return 0;
+		}
+	}
+
+	// Is it a disabled rpc ?
+	if(unlikely(test_bit(desc->rpcid, rpc_mask))){
+		if (queue_waiting_desc(desc))
+			r = -ENOMEM;
+		return r;
+	};
+
+	// Is it an interruption-ready handler ?
+	if(likely(desc->service->flags & RPC_FLAGS_NOBLOCK)
+	   && !(desc = handle_in_interrupt(desc)))
+		return r;
+	
+	// Is-there any available handler ?
+	i = find_first_zero_bit(&thread_pool->threads_vector,
+				thread_pool->nbthreads);
+	
+	if(i < thread_pool->nbthreads){
+		// Found an available handler
+
+		set_bit(i, &thread_pool->threads_vector);
+
+		thread_pool->desc[i] = desc;
+		desc->thread = thread_pool->threads[i];
+		desc->state = RPC_STATE_RUN;
+
+		wake_up_process(desc->thread);
+	}else{
+
+		// No available handler
+		if (queue_waiting_desc(desc))
+			return -ENOMEM;
+
+		if(atomic_inc_return(&new_thread_data.request[smp_processor_id()]) == 1)
+			queue_delayed_work(krg_nb_wq, &new_thread_data.dwork, 0);
+
+	};
+	
+	return r;
+};
+
+void rpc_wake_up_thread(struct rpc_desc *desc){
+	struct threads_pool* thread_pool = &per_cpu(threads_pool, smp_processor_id());
+	int i;
+
+	// Is-there any available handler ?
+	i = find_first_zero_bit(&thread_pool->threads_vector,
+				thread_pool->nbthreads);
+	
+	if(i < thread_pool->nbthreads){
+		set_bit(i, &thread_pool->threads_vector);
+
+		thread_pool->desc[i] = desc;
+
+		if(desc){
+			desc->thread = thread_pool->threads[i];
+			desc->state = RPC_STATE_RUN;
+		};
+
+		wake_up_process(thread_pool->threads[i]);
+	}else{
+
+		if(atomic_inc_return(&new_thread_data.request[smp_processor_id()]) == 1)
+			queue_delayed_work(krg_nb_wq, &new_thread_data.dwork, 0);
+
+	}
+};
+
+int thread_pool_init(void){
+	int i;
+
+	for_each_possible_cpu(i){
+		struct threads_pool* thread_pool = &per_cpu(threads_pool, i);
+		int j;
+
+		thread_pool->threads_vector = 0;
+		thread_pool->nbthreads = 0;
+		
+		for(j = 0; j<THREADS_VECTOR_WIDTH; j++){
+			thread_pool->threads[j] = NULL;
+			thread_pool->desc[j] = NULL;
+		};
+
+		atomic_set(&new_thread_data.request[i], 0);
+	};
+
+	INIT_DELAYED_WORK(&new_thread_data.dwork, new_thread_worker);
+	
+	INIT_LIST_HEAD(&waiting_desc);
+	spin_lock_init(&waiting_desc_lock);
+
+	rpc_handlers[RPC_HANDLER_KTHREAD] = rpc_handler_kthread;
+	rpc_handlers[RPC_HANDLER_KTHREAD_VOID] = rpc_handler_kthread_void;
+	rpc_handlers[RPC_HANDLER_KTHREAD_INT] = rpc_handler_kthread_int;
+
+	init_completion(&init_complete);
+	atomic_inc(&new_thread_data.request[smp_processor_id()]);
+	queue_delayed_work(krg_nb_wq, &new_thread_data.dwork, 0);
+	wait_for_completion(&init_complete);
+	
+	return 0;
+};
diff -ruN linux-2.6.29/net/Makefile android_cluster/linux-2.6.29/net/Makefile
--- linux-2.6.29/net/Makefile	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/Makefile	2014-05-27 23:04:10.506026262 -0700
@@ -52,6 +52,7 @@
 obj-y				+= wireless/
 obj-$(CONFIG_MAC80211)		+= mac80211/
 obj-$(CONFIG_TIPC)		+= tipc/
+obj-$(CONFIG_KRGRPC)		+= krgrpc/
 obj-$(CONFIG_NETLABEL)		+= netlabel/
 obj-$(CONFIG_IUCV)		+= iucv/
 obj-$(CONFIG_RFKILL)		+= rfkill/
diff -ruN linux-2.6.29/net/socket.c android_cluster/linux-2.6.29/net/socket.c
--- linux-2.6.29/net/socket.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/socket.c	2014-06-09 18:19:42.436281632 -0700
@@ -95,6 +95,9 @@
 
 #include <net/sock.h>
 #include <linux/netfilter.h>
+#ifdef CONFIG_KRG_FAF
+#include <kerrighed/faf.h>
+#endif
 
 static int sock_no_open(struct inode *irrelevant, struct file *dontcare);
 static ssize_t sock_aio_read(struct kiocb *iocb, const struct iovec *iov,
@@ -454,14 +457,33 @@
 	return sock;
 }
 
+#ifdef CONFIG_KRG_FAF
+static struct socket *sockfd_lookup_light(int fd, int *err, int *fput_needed,
+					  struct file **faf_file)
+#else
 static struct socket *sockfd_lookup_light(int fd, int *err, int *fput_needed)
+#endif
 {
 	struct file *file;
 	struct socket *sock;
 
 	*err = -EBADF;
+#ifdef CONFIG_KRG_FAF
+	*faf_file = NULL;
+#endif
 	file = fget_light(fd, fput_needed);
 	if (file) {
+#ifdef CONFIG_KRG_FAF
+		if (file->f_flags & O_FAF_CLT) {
+			if (!*fput_needed) {
+				/* TODO: really needed? */
+				get_file(file);
+				*fput_needed = 1;
+			}
+			*faf_file = file;
+			return NULL;
+		}
+#endif
 		sock = sock_from_file(file, err);
 		if (sock)
 			return sock;
@@ -1361,8 +1383,19 @@
 	struct socket *sock;
 	struct sockaddr_storage address;
 	int err, fput_needed;
+#ifdef CONFIG_KRG_FAF
+	struct file *faf_file;
 
+	sock = sockfd_lookup_light(fd, &err, &fput_needed, &faf_file);
+	if (faf_file) {
+		err = krg_faf_bind(faf_file,
+				  (struct sockaddr __user *)umyaddr, addrlen);
+		fput_light(faf_file, fput_needed);
+		return err;
+	}
+#else
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
+#endif
 	if (sock) {
 		err = move_addr_to_kernel(umyaddr, addrlen, (struct sockaddr *)&address);
 		if (err >= 0) {
@@ -1390,8 +1423,18 @@
 	struct socket *sock;
 	int err, fput_needed;
 	int somaxconn;
+#ifdef CONFIG_KRG_FAF
+	struct file *faf_file;
 
+	sock = sockfd_lookup_light(fd, &err, &fput_needed, &faf_file);
+	if (faf_file) {
+		err = krg_faf_listen(faf_file, backlog);
+		fput_light(faf_file, fput_needed);
+		return err;
+	}
+#else
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
+#endif
 	if (sock) {
 		somaxconn = sock_net(sock->sk)->core.sysctl_somaxconn;
 		if ((unsigned)backlog > somaxconn)
@@ -1425,6 +1468,9 @@
 	struct file *newfile;
 	int err, len, newfd, fput_needed;
 	struct sockaddr_storage address;
+#ifdef CONFIG_KRG_FAF
+	struct file *faf_file;
+#endif
 
 	if (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK))
 		return -EINVAL;
@@ -1432,7 +1478,16 @@
 	if (SOCK_NONBLOCK != O_NONBLOCK && (flags & SOCK_NONBLOCK))
 		flags = (flags & ~SOCK_NONBLOCK) | O_NONBLOCK;
 
+#ifdef CONFIG_KRG_FAF
+	sock = sockfd_lookup_light(fd, &err, &fput_needed, &faf_file);
+	if (faf_file) {
+		err = krg_faf_accept(faf_file, upeer_sockaddr, upeer_addrlen);
+		fput_light(faf_file, fput_needed);
+		goto out;
+	}
+#else
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
+#endif
 	if (!sock)
 		goto out;
 
@@ -1526,8 +1581,18 @@
 	struct socket *sock;
 	struct sockaddr_storage address;
 	int err, fput_needed;
+#ifdef CONFIG_KRG_FAF
+	struct file *faf_file;
 
+	sock = sockfd_lookup_light(fd, &err, &fput_needed, &faf_file);
+	if (faf_file) {
+		err = krg_faf_connect(faf_file, uservaddr, addrlen);
+		fput_light(faf_file, fput_needed);
+		goto out;
+	}
+#else
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
+#endif
 	if (!sock)
 		goto out;
 	err = move_addr_to_kernel(uservaddr, addrlen, (struct sockaddr *)&address);
@@ -1558,8 +1623,18 @@
 	struct socket *sock;
 	struct sockaddr_storage address;
 	int len, err, fput_needed;
+#ifdef CONFIG_KRG_FAF
+	struct file *faf_file;
 
+	sock = sockfd_lookup_light(fd, &err, &fput_needed, &faf_file);
+	if (faf_file) {
+		err = krg_faf_getsockname(faf_file, usockaddr, usockaddr_len);
+		fput_light(faf_file, fput_needed);
+		goto out;
+	}
+#else
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
+#endif
 	if (!sock)
 		goto out;
 
@@ -1589,8 +1664,18 @@
 	struct socket *sock;
 	struct sockaddr_storage address;
 	int len, err, fput_needed;
+#ifdef CONFIG_KRG_FAF
+	struct file *faf_file;
 
+	sock = sockfd_lookup_light(fd, &err, &fput_needed, &faf_file);
+	if (faf_file) {
+		err = krg_faf_getpeername(faf_file, usockaddr, usockaddr_len);
+		fput_light(faf_file, fput_needed);
+		return err;
+	}
+#else
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
+#endif
 	if (sock != NULL) {
 		err = security_socket_getpeername(sock);
 		if (err) {
@@ -1625,11 +1710,17 @@
 	struct msghdr msg;
 	struct iovec iov;
 	int fput_needed;
+#ifdef CONFIG_KRG_FAF
+	struct file *faf_file;
 
+	sock = sockfd_lookup_light(fd, &err, &fput_needed, &faf_file);
+	if (!sock && !faf_file)
+		goto out;
+#else
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
 	if (!sock)
 		goto out;
-
+#endif
 	iov.iov_base = buff;
 	iov.iov_len = len;
 	msg.msg_name = NULL;
@@ -1645,12 +1736,24 @@
 		msg.msg_name = (struct sockaddr *)&address;
 		msg.msg_namelen = addr_len;
 	}
+#ifdef CONFIG_KRG_FAF
+	if (faf_file) {
+		msg.msg_flags = flags;
+		err = krg_faf_sendmsg(faf_file, &msg, len);
+		goto out_put;
+	}
+#endif
 	if (sock->file->f_flags & O_NONBLOCK)
 		flags |= MSG_DONTWAIT;
 	msg.msg_flags = flags;
 	err = sock_sendmsg(sock, &msg, len);
 
 out_put:
+#ifdef CONFIG_KRG_FAF
+	if (faf_file)
+		fput_light(faf_file, fput_needed);
+	else
+#endif
 	fput_light(sock->file, fput_needed);
 out:
 	return err;
@@ -1682,10 +1785,17 @@
 	struct sockaddr_storage address;
 	int err, err2;
 	int fput_needed;
+#ifdef CONFIG_KRG_FAF
+	struct file * faf_file;
 
+	sock = sockfd_lookup_light(fd, &err, &fput_needed, &faf_file);
+	if (!sock && !faf_file)
+		goto out;
+#else
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
 	if (!sock)
 		goto out;
+#endif
 
 	msg.msg_control = NULL;
 	msg.msg_controllen = 0;
@@ -1695,10 +1805,19 @@
 	iov.iov_base = ubuf;
 	msg.msg_name = (struct sockaddr *)&address;
 	msg.msg_namelen = sizeof(address);
+#ifdef CONFIG_KRG_FAF
+	if (faf_file) {
+		err = krg_faf_recvmsg(faf_file, &msg, size, flags);
+		goto check_err;
+	}
+#endif
 	if (sock->file->f_flags & O_NONBLOCK)
 		flags |= MSG_DONTWAIT;
 	err = sock_recvmsg(sock, &msg, size, flags);
 
+#ifdef CONFIG_KRG_FAF
+check_err:
+#endif
 	if (err >= 0 && addr != NULL) {
 		err2 = move_addr_to_user((struct sockaddr *)&address,
 					 msg.msg_namelen, addr, addr_len);
@@ -1706,6 +1825,11 @@
 			err = err2;
 	}
 
+#ifdef CONFIG_KRG_FAF
+	if (faf_file)
+		fput_light(faf_file, fput_needed);
+	else
+#endif
 	fput_light(sock->file, fput_needed);
 out:
 	return err;
@@ -1731,11 +1855,24 @@
 {
 	int err, fput_needed;
 	struct socket *sock;
+#ifdef CONFIG_KRG_FAF
+	struct file *faf_file;
+#endif
 
 	if (optlen < 0)
 		return -EINVAL;
 
+#ifdef CONFIG_KRG_FAF
+	sock = sockfd_lookup_light(fd, &err, &fput_needed, &faf_file);
+	if (faf_file) {
+		err = krg_faf_setsockopt(faf_file, level, optname,
+					 optval, optlen);
+		fput_light(faf_file, fput_needed);
+		return err;
+	}
+#else
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
+#endif
 	if (sock != NULL) {
 		err = security_socket_setsockopt(sock, level, optname);
 		if (err)
@@ -1765,8 +1902,19 @@
 {
 	int err, fput_needed;
 	struct socket *sock;
+#ifdef CONFIG_KRG_FAF
+	struct file *faf_file;
 
+	sock = sockfd_lookup_light(fd, &err, &fput_needed, &faf_file);
+	if (faf_file) {
+		err = krg_faf_getsockopt(faf_file, level, optname,
+					 optval, optlen);
+		fput_light(faf_file, fput_needed);
+		return err;
+	}
+#else
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
+#endif
 	if (sock != NULL) {
 		err = security_socket_getsockopt(sock, level, optname);
 		if (err)
@@ -1794,8 +1942,18 @@
 {
 	int err, fput_needed;
 	struct socket *sock;
+#ifdef CONFIG_KRG_FAF
+	struct file *faf_file;
 
+	sock = sockfd_lookup_light(fd, &err, &fput_needed, &faf_file);
+	if (faf_file) {
+		err = krg_faf_shutdown(faf_file, how);
+		fput_light(faf_file, fput_needed);
+		return err;
+	}
+#else
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
+#endif
 	if (sock != NULL) {
 		err = security_socket_shutdown(sock, how);
 		if (!err)
@@ -1812,6 +1970,31 @@
 #define COMPAT_NAMELEN(msg)	COMPAT_MSG(msg, msg_namelen)
 #define COMPAT_FLAGS(msg)	COMPAT_MSG(msg, msg_flags)
 
+#ifdef CONFIG_KRG_FAF
+static inline
+void *
+faf_sock_kmalloc(struct sock *sk, int size, gfp_t flags, struct file *faf_file)
+{
+	void *mem;
+
+	if (faf_file)
+		mem = kmalloc(size, flags);
+	else
+		mem = sock_kmalloc(sk, size, flags);
+	return mem;
+}
+
+static inline
+void
+faf_sock_kfree_s(struct sock *sk, void *mem, int size, struct file *faf_file)
+{
+	if (faf_file)
+		kfree(mem);
+	else
+		sock_kfree_s(sk, mem, size);
+}
+#endif /* CONFIG_KRG_FAF */
+
 /*
  *	BSD sendmsg interface
  */
@@ -1830,6 +2013,9 @@
 	struct msghdr msg_sys;
 	int err, ctl_len, iov_size, total_len;
 	int fput_needed;
+#ifdef CONFIG_KRG_FAF
+	struct file *faf_file;
+#endif
 
 	err = -EFAULT;
 	if (MSG_CMSG_COMPAT & flags) {
@@ -1839,9 +2025,18 @@
 	else if (copy_from_user(&msg_sys, msg, sizeof(struct msghdr)))
 		return -EFAULT;
 
+#ifdef CONFIG_KRG_FAF
+	sock = sockfd_lookup_light(fd, &err, &fput_needed, &faf_file);
+	if (!sock && !faf_file)
+		goto out;
+	err = -ENOSYS;
+	if (faf_file && (flags & MSG_CMSG_COMPAT))
+		goto out_put;
+#else
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
 	if (!sock)
 		goto out;
+#endif
 
 	/* do not move before msg_sys is valid */
 	err = -EMSGSIZE;
@@ -1852,7 +2047,11 @@
 	err = -ENOMEM;
 	iov_size = msg_sys.msg_iovlen * sizeof(struct iovec);
 	if (msg_sys.msg_iovlen > UIO_FASTIOV) {
+#ifdef CONFIG_KRG_FAF
+		iov = faf_sock_kmalloc(sock->sk, iov_size, GFP_KERNEL, faf_file);
+#else
 		iov = sock_kmalloc(sock->sk, iov_size, GFP_KERNEL);
+#endif
 		if (!iov)
 			goto out_put;
 	}
@@ -1885,7 +2084,12 @@
 		ctl_len = msg_sys.msg_controllen;
 	} else if (ctl_len) {
 		if (ctl_len > sizeof(ctl)) {
+#ifdef CONFIG_KRG_FAF
+			ctl_buf = faf_sock_kmalloc(sock->sk, ctl_len, GFP_KERNEL,
+						   faf_file);
+#else
 			ctl_buf = sock_kmalloc(sock->sk, ctl_len, GFP_KERNEL);
+#endif
 			if (ctl_buf == NULL)
 				goto out_freeiov;
 		}
@@ -1902,17 +2106,36 @@
 	}
 	msg_sys.msg_flags = flags;
 
+#ifdef CONFIG_KRG_FAF
+	if (faf_file) {
+		err = krg_faf_sendmsg(faf_file, &msg_sys, total_len);
+		goto out_freectl;
+	}
+#endif
 	if (sock->file->f_flags & O_NONBLOCK)
 		msg_sys.msg_flags |= MSG_DONTWAIT;
 	err = sock_sendmsg(sock, &msg_sys, total_len);
 
 out_freectl:
 	if (ctl_buf != ctl)
+#ifdef CONFIG_KRG_FAF
+		faf_sock_kfree_s(sock->sk, ctl_buf, ctl_len, faf_file);
+#else
 		sock_kfree_s(sock->sk, ctl_buf, ctl_len);
+#endif
 out_freeiov:
 	if (iov != iovstack)
+#ifdef CONFIG_KRG_FAF
+		faf_sock_kfree_s(sock->sk, iov, iov_size, faf_file);
+#else
 		sock_kfree_s(sock->sk, iov, iov_size);
+#endif
 out_put:
+#ifdef CONFIG_KRG_FAF
+	if (faf_file)
+		fput_light(faf_file, fput_needed);
+	else
+#endif
 	fput_light(sock->file, fput_needed);
 out:
 	return err;
@@ -1941,6 +2164,9 @@
 	/* user mode address pointers */
 	struct sockaddr __user *uaddr;
 	int __user *uaddr_len;
+#ifdef CONFIG_KRG_FAF
+	struct file *faf_file;
+#endif
 
 	if (MSG_CMSG_COMPAT & flags) {
 		if (get_compat_msghdr(&msg_sys, msg_compat))
@@ -1949,9 +2175,18 @@
 	else if (copy_from_user(&msg_sys, msg, sizeof(struct msghdr)))
 		return -EFAULT;
 
+#ifdef CONFIG_KRG_FAF
+	sock = sockfd_lookup_light(fd, &err, &fput_needed, &faf_file);
+	if (!sock && !faf_file)
+		goto out;
+	err = -ENOSYS;
+	if (faf_file && (flags & MSG_CMSG_COMPAT))
+		goto out_put;
+#else
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
 	if (!sock)
 		goto out;
+#endif
 
 	err = -EMSGSIZE;
 	if (msg_sys.msg_iovlen > UIO_MAXIOV)
@@ -1961,7 +2196,11 @@
 	err = -ENOMEM;
 	iov_size = msg_sys.msg_iovlen * sizeof(struct iovec);
 	if (msg_sys.msg_iovlen > UIO_FASTIOV) {
+#ifdef CONFIG_KRG_FAF
+		iov = faf_sock_kmalloc(sock->sk, iov_size, GFP_KERNEL, faf_file);
+#else
 		iov = sock_kmalloc(sock->sk, iov_size, GFP_KERNEL);
+#endif
 		if (!iov)
 			goto out_put;
 	}
@@ -1988,9 +2227,18 @@
 	cmsg_ptr = (unsigned long)msg_sys.msg_control;
 	msg_sys.msg_flags = flags & (MSG_CMSG_CLOEXEC|MSG_CMSG_COMPAT);
 
+#ifdef CONFIG_KRG_FAF
+	if (faf_file) {
+		err = krg_faf_recvmsg(faf_file, &msg_sys, total_len, flags);
+		goto check_err;
+	}
+#endif
 	if (sock->file->f_flags & O_NONBLOCK)
 		flags |= MSG_DONTWAIT;
 	err = sock_recvmsg(sock, &msg_sys, total_len, flags);
+#ifdef CONFIG_KRG_FAF
+check_err:
+#endif
 	if (err < 0)
 		goto out_freeiov;
 	len = err;
@@ -2018,8 +2266,17 @@
 
 out_freeiov:
 	if (iov != iovstack)
+#ifdef CONFIG_KRG_FAF
+		faf_sock_kfree_s(sock->sk, iov, iov_size, faf_file);
+#else
 		sock_kfree_s(sock->sk, iov, iov_size);
+#endif
 out_put:
+#ifdef CONFIG_KRG_FAF
+	if (faf_file)
+		fput_light(faf_file, fput_needed);
+	else
+#endif
 	fput_light(sock->file, fput_needed);
 out:
 	return err;
diff -ruN linux-2.6.29/net/tipc/addr.c android_cluster/linux-2.6.29/net/tipc/addr.c
--- linux-2.6.29/net/tipc/addr.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/addr.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,94 +0,0 @@
-/*
- * net/tipc/addr.c: TIPC address utility routines
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2004-2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "dbg.h"
-#include "addr.h"
-#include "zone.h"
-#include "cluster.h"
-#include "net.h"
-
-u32 tipc_get_addr(void)
-{
-	return tipc_own_addr;
-}
-
-/**
- * tipc_addr_domain_valid - validates a network domain address
- *
- * Accepts <Z.C.N>, <Z.C.0>, <Z.0.0>, and <0.0.0>,
- * where Z, C, and N are non-zero and do not exceed the configured limits.
- *
- * Returns 1 if domain address is valid, otherwise 0
- */
-
-int tipc_addr_domain_valid(u32 addr)
-{
-	u32 n = tipc_node(addr);
-	u32 c = tipc_cluster(addr);
-	u32 z = tipc_zone(addr);
-	u32 max_nodes = tipc_max_nodes;
-
-	if (is_slave(addr))
-		max_nodes = LOWEST_SLAVE + tipc_max_slaves;
-	if (n > max_nodes)
-		return 0;
-	if (c > tipc_max_clusters)
-		return 0;
-	if (z > tipc_max_zones)
-		return 0;
-
-	if (n && (!z || !c))
-		return 0;
-	if (c && !z)
-		return 0;
-	return 1;
-}
-
-/**
- * tipc_addr_node_valid - validates a proposed network address for this node
- *
- * Accepts <Z.C.N>, where Z, C, and N are non-zero and do not exceed
- * the configured limits.
- *
- * Returns 1 if address can be used, otherwise 0
- */
-
-int tipc_addr_node_valid(u32 addr)
-{
-	return (tipc_addr_domain_valid(addr) && tipc_node(addr));
-}
-
diff -ruN linux-2.6.29/net/tipc/addr.h android_cluster/linux-2.6.29/net/tipc/addr.h
--- linux-2.6.29/net/tipc/addr.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/addr.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,123 +0,0 @@
-/*
- * net/tipc/addr.h: Include file for TIPC address utility routines
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2004-2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_ADDR_H
-#define _TIPC_ADDR_H
-
-static inline u32 own_node(void)
-{
-	return tipc_node(tipc_own_addr);
-}
-
-static inline u32 own_cluster(void)
-{
-	return tipc_cluster(tipc_own_addr);
-}
-
-static inline u32 own_zone(void)
-{
-	return tipc_zone(tipc_own_addr);
-}
-
-static inline int in_own_cluster(u32 addr)
-{
-	return !((addr ^ tipc_own_addr) >> 12);
-}
-
-static inline int is_slave(u32 addr)
-{
-	return addr & 0x800;
-}
-
-static inline int may_route(u32 addr)
-{
-	return(addr ^ tipc_own_addr) >> 11;
-}
-
-static inline int in_scope(u32 domain, u32 addr)
-{
-	if (!domain || (domain == addr))
-		return 1;
-	if (domain == (addr & 0xfffff000u)) /* domain <Z.C.0> */
-		return 1;
-	if (domain == (addr & 0xff000000u)) /* domain <Z.0.0> */
-		return 1;
-	return 0;
-}
-
-/**
- * addr_scope - convert message lookup domain to equivalent 2-bit scope value
- */
-
-static inline int addr_scope(u32 domain)
-{
-	if (likely(!domain))
-		return TIPC_ZONE_SCOPE;
-	if (tipc_node(domain))
-		return TIPC_NODE_SCOPE;
-	if (tipc_cluster(domain))
-		return TIPC_CLUSTER_SCOPE;
-	return TIPC_ZONE_SCOPE;
-}
-
-/**
- * addr_domain - convert 2-bit scope value to equivalent message lookup domain
- *
- * Needed when address of a named message must be looked up a second time
- * after a network hop.
- */
-
-static inline int addr_domain(int sc)
-{
-	if (likely(sc == TIPC_NODE_SCOPE))
-		return tipc_own_addr;
-	if (sc == TIPC_CLUSTER_SCOPE)
-		return tipc_addr(tipc_zone(tipc_own_addr),
-				 tipc_cluster(tipc_own_addr), 0);
-	return tipc_addr(tipc_zone(tipc_own_addr), 0, 0);
-}
-
-static inline char *addr_string_fill(char *string, u32 addr)
-{
-	snprintf(string, 16, "<%u.%u.%u>",
-		 tipc_zone(addr), tipc_cluster(addr), tipc_node(addr));
-	return string;
-}
-
-int tipc_addr_domain_valid(u32);
-int tipc_addr_node_valid(u32 addr);
-
-#endif
diff -ruN linux-2.6.29/net/tipc/bcast.c android_cluster/linux-2.6.29/net/tipc/bcast.c
--- linux-2.6.29/net/tipc/bcast.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/bcast.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,831 +0,0 @@
-/*
- * net/tipc/bcast.c: TIPC broadcast code
- *
- * Copyright (c) 2004-2006, Ericsson AB
- * Copyright (c) 2004, Intel Corporation.
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "msg.h"
-#include "dbg.h"
-#include "link.h"
-#include "net.h"
-#include "node.h"
-#include "port.h"
-#include "addr.h"
-#include "node_subscr.h"
-#include "name_distr.h"
-#include "bearer.h"
-#include "name_table.h"
-#include "bcast.h"
-
-#define MAX_PKT_DEFAULT_MCAST 1500	/* bcast link max packet size (fixed) */
-
-#define BCLINK_WIN_DEFAULT 20		/* bcast link window size (default) */
-
-#define BCLINK_LOG_BUF_SIZE 0
-
-/*
- * Loss rate for incoming broadcast frames; used to test retransmission code.
- * Set to N to cause every N'th frame to be discarded; 0 => don't discard any.
- */
-
-#define TIPC_BCAST_LOSS_RATE 0
-
-/**
- * struct bcbearer_pair - a pair of bearers used by broadcast link
- * @primary: pointer to primary bearer
- * @secondary: pointer to secondary bearer
- *
- * Bearers must have same priority and same set of reachable destinations
- * to be paired.
- */
-
-struct bcbearer_pair {
-	struct bearer *primary;
-	struct bearer *secondary;
-};
-
-/**
- * struct bcbearer - bearer used by broadcast link
- * @bearer: (non-standard) broadcast bearer structure
- * @media: (non-standard) broadcast media structure
- * @bpairs: array of bearer pairs
- * @bpairs_temp: temporary array of bearer pairs used by tipc_bcbearer_sort()
- * @remains: temporary node map used by tipc_bcbearer_send()
- * @remains_new: temporary node map used tipc_bcbearer_send()
- *
- * Note: The fields labelled "temporary" are incorporated into the bearer
- * to avoid consuming potentially limited stack space through the use of
- * large local variables within multicast routines.  Concurrent access is
- * prevented through use of the spinlock "bc_lock".
- */
-
-struct bcbearer {
-	struct bearer bearer;
-	struct media media;
-	struct bcbearer_pair bpairs[MAX_BEARERS];
-	struct bcbearer_pair bpairs_temp[TIPC_MAX_LINK_PRI + 1];
-	struct tipc_node_map remains;
-	struct tipc_node_map remains_new;
-};
-
-/**
- * struct bclink - link used for broadcast messages
- * @link: (non-standard) broadcast link structure
- * @node: (non-standard) node structure representing b'cast link's peer node
- *
- * Handles sequence numbering, fragmentation, bundling, etc.
- */
-
-struct bclink {
-	struct link link;
-	struct tipc_node node;
-};
-
-
-static struct bcbearer *bcbearer = NULL;
-static struct bclink *bclink = NULL;
-static struct link *bcl = NULL;
-static DEFINE_SPINLOCK(bc_lock);
-
-char tipc_bclink_name[] = "multicast-link";
-
-
-static u32 buf_seqno(struct sk_buff *buf)
-{
-	return msg_seqno(buf_msg(buf));
-}
-
-static u32 bcbuf_acks(struct sk_buff *buf)
-{
-	return (u32)(unsigned long)TIPC_SKB_CB(buf)->handle;
-}
-
-static void bcbuf_set_acks(struct sk_buff *buf, u32 acks)
-{
-	TIPC_SKB_CB(buf)->handle = (void *)(unsigned long)acks;
-}
-
-static void bcbuf_decr_acks(struct sk_buff *buf)
-{
-	bcbuf_set_acks(buf, bcbuf_acks(buf) - 1);
-}
-
-
-/**
- * bclink_set_gap - set gap according to contents of current deferred pkt queue
- *
- * Called with 'node' locked, bc_lock unlocked
- */
-
-static void bclink_set_gap(struct tipc_node *n_ptr)
-{
-	struct sk_buff *buf = n_ptr->bclink.deferred_head;
-
-	n_ptr->bclink.gap_after = n_ptr->bclink.gap_to =
-		mod(n_ptr->bclink.last_in);
-	if (unlikely(buf != NULL))
-		n_ptr->bclink.gap_to = mod(buf_seqno(buf) - 1);
-}
-
-/**
- * bclink_ack_allowed - test if ACK or NACK message can be sent at this moment
- *
- * This mechanism endeavours to prevent all nodes in network from trying
- * to ACK or NACK at the same time.
- *
- * Note: TIPC uses a different trigger to distribute ACKs than it does to
- *       distribute NACKs, but tries to use the same spacing (divide by 16).
- */
-
-static int bclink_ack_allowed(u32 n)
-{
-	return((n % TIPC_MIN_LINK_WIN) == tipc_own_tag);
-}
-
-
-/**
- * bclink_retransmit_pkt - retransmit broadcast packets
- * @after: sequence number of last packet to *not* retransmit
- * @to: sequence number of last packet to retransmit
- *
- * Called with bc_lock locked
- */
-
-static void bclink_retransmit_pkt(u32 after, u32 to)
-{
-	struct sk_buff *buf;
-
-	buf = bcl->first_out;
-	while (buf && less_eq(buf_seqno(buf), after)) {
-		buf = buf->next;
-	}
-	tipc_link_retransmit(bcl, buf, mod(to - after));
-}
-
-/**
- * tipc_bclink_acknowledge - handle acknowledgement of broadcast packets
- * @n_ptr: node that sent acknowledgement info
- * @acked: broadcast sequence # that has been acknowledged
- *
- * Node is locked, bc_lock unlocked.
- */
-
-void tipc_bclink_acknowledge(struct tipc_node *n_ptr, u32 acked)
-{
-	struct sk_buff *crs;
-	struct sk_buff *next;
-	unsigned int released = 0;
-
-	if (less_eq(acked, n_ptr->bclink.acked))
-		return;
-
-	spin_lock_bh(&bc_lock);
-
-	/* Skip over packets that node has previously acknowledged */
-
-	crs = bcl->first_out;
-	while (crs && less_eq(buf_seqno(crs), n_ptr->bclink.acked)) {
-		crs = crs->next;
-	}
-
-	/* Update packets that node is now acknowledging */
-
-	while (crs && less_eq(buf_seqno(crs), acked)) {
-		next = crs->next;
-		bcbuf_decr_acks(crs);
-		if (bcbuf_acks(crs) == 0) {
-			bcl->first_out = next;
-			bcl->out_queue_size--;
-			buf_discard(crs);
-			released = 1;
-		}
-		crs = next;
-	}
-	n_ptr->bclink.acked = acked;
-
-	/* Try resolving broadcast link congestion, if necessary */
-
-	if (unlikely(bcl->next_out))
-		tipc_link_push_queue(bcl);
-	if (unlikely(released && !list_empty(&bcl->waiting_ports)))
-		tipc_link_wakeup_ports(bcl, 0);
-	spin_unlock_bh(&bc_lock);
-}
-
-/**
- * bclink_send_ack - unicast an ACK msg
- *
- * tipc_net_lock and node lock set
- */
-
-static void bclink_send_ack(struct tipc_node *n_ptr)
-{
-	struct link *l_ptr = n_ptr->active_links[n_ptr->addr & 1];
-
-	if (l_ptr != NULL)
-		tipc_link_send_proto_msg(l_ptr, STATE_MSG, 0, 0, 0, 0, 0);
-}
-
-/**
- * bclink_send_nack- broadcast a NACK msg
- *
- * tipc_net_lock and node lock set
- */
-
-static void bclink_send_nack(struct tipc_node *n_ptr)
-{
-	struct sk_buff *buf;
-	struct tipc_msg *msg;
-
-	if (!less(n_ptr->bclink.gap_after, n_ptr->bclink.gap_to))
-		return;
-
-	buf = buf_acquire(INT_H_SIZE);
-	if (buf) {
-		msg = buf_msg(buf);
-		msg_init(msg, BCAST_PROTOCOL, STATE_MSG,
-			 INT_H_SIZE, n_ptr->addr);
-		msg_set_mc_netid(msg, tipc_net_id);
-		msg_set_bcast_ack(msg, mod(n_ptr->bclink.last_in));
-		msg_set_bcgap_after(msg, n_ptr->bclink.gap_after);
-		msg_set_bcgap_to(msg, n_ptr->bclink.gap_to);
-		msg_set_bcast_tag(msg, tipc_own_tag);
-
-		if (tipc_bearer_send(&bcbearer->bearer, buf, NULL)) {
-			bcl->stats.sent_nacks++;
-			buf_discard(buf);
-		} else {
-			tipc_bearer_schedule(bcl->b_ptr, bcl);
-			bcl->proto_msg_queue = buf;
-			bcl->stats.bearer_congs++;
-		}
-
-		/*
-		 * Ensure we doesn't send another NACK msg to the node
-		 * until 16 more deferred messages arrive from it
-		 * (i.e. helps prevent all nodes from NACK'ing at same time)
-		 */
-
-		n_ptr->bclink.nack_sync = tipc_own_tag;
-	}
-}
-
-/**
- * tipc_bclink_check_gap - send a NACK if a sequence gap exists
- *
- * tipc_net_lock and node lock set
- */
-
-void tipc_bclink_check_gap(struct tipc_node *n_ptr, u32 last_sent)
-{
-	if (!n_ptr->bclink.supported ||
-	    less_eq(last_sent, mod(n_ptr->bclink.last_in)))
-		return;
-
-	bclink_set_gap(n_ptr);
-	if (n_ptr->bclink.gap_after == n_ptr->bclink.gap_to)
-		n_ptr->bclink.gap_to = last_sent;
-	bclink_send_nack(n_ptr);
-}
-
-/**
- * tipc_bclink_peek_nack - process a NACK msg meant for another node
- *
- * Only tipc_net_lock set.
- */
-
-static void tipc_bclink_peek_nack(u32 dest, u32 sender_tag, u32 gap_after, u32 gap_to)
-{
-	struct tipc_node *n_ptr = tipc_node_find(dest);
-	u32 my_after, my_to;
-
-	if (unlikely(!n_ptr || !tipc_node_is_up(n_ptr)))
-		return;
-	tipc_node_lock(n_ptr);
-	/*
-	 * Modify gap to suppress unnecessary NACKs from this node
-	 */
-	my_after = n_ptr->bclink.gap_after;
-	my_to = n_ptr->bclink.gap_to;
-
-	if (less_eq(gap_after, my_after)) {
-		if (less(my_after, gap_to) && less(gap_to, my_to))
-			n_ptr->bclink.gap_after = gap_to;
-		else if (less_eq(my_to, gap_to))
-			n_ptr->bclink.gap_to = n_ptr->bclink.gap_after;
-	} else if (less_eq(gap_after, my_to)) {
-		if (less_eq(my_to, gap_to))
-			n_ptr->bclink.gap_to = gap_after;
-	} else {
-		/*
-		 * Expand gap if missing bufs not in deferred queue:
-		 */
-		struct sk_buff *buf = n_ptr->bclink.deferred_head;
-		u32 prev = n_ptr->bclink.gap_to;
-
-		for (; buf; buf = buf->next) {
-			u32 seqno = buf_seqno(buf);
-
-			if (mod(seqno - prev) != 1) {
-				buf = NULL;
-				break;
-			}
-			if (seqno == gap_after)
-				break;
-			prev = seqno;
-		}
-		if (buf == NULL)
-			n_ptr->bclink.gap_to = gap_after;
-	}
-	/*
-	 * Some nodes may send a complementary NACK now:
-	 */
-	if (bclink_ack_allowed(sender_tag + 1)) {
-		if (n_ptr->bclink.gap_to != n_ptr->bclink.gap_after) {
-			bclink_send_nack(n_ptr);
-			bclink_set_gap(n_ptr);
-		}
-	}
-	tipc_node_unlock(n_ptr);
-}
-
-/**
- * tipc_bclink_send_msg - broadcast a packet to all nodes in cluster
- */
-
-int tipc_bclink_send_msg(struct sk_buff *buf)
-{
-	int res;
-
-	spin_lock_bh(&bc_lock);
-
-	res = tipc_link_send_buf(bcl, buf);
-	if (unlikely(res == -ELINKCONG))
-		buf_discard(buf);
-	else
-		bcl->stats.sent_info++;
-
-	if (bcl->out_queue_size > bcl->stats.max_queue_sz)
-		bcl->stats.max_queue_sz = bcl->out_queue_size;
-	bcl->stats.queue_sz_counts++;
-	bcl->stats.accu_queue_sz += bcl->out_queue_size;
-
-	spin_unlock_bh(&bc_lock);
-	return res;
-}
-
-/**
- * tipc_bclink_recv_pkt - receive a broadcast packet, and deliver upwards
- *
- * tipc_net_lock is read_locked, no other locks set
- */
-
-void tipc_bclink_recv_pkt(struct sk_buff *buf)
-{
-#if (TIPC_BCAST_LOSS_RATE)
-	static int rx_count = 0;
-#endif
-	struct tipc_msg *msg = buf_msg(buf);
-	struct tipc_node* node = tipc_node_find(msg_prevnode(msg));
-	u32 next_in;
-	u32 seqno;
-	struct sk_buff *deferred;
-
-	msg_dbg(msg, "<BC<<<");
-
-	if (unlikely(!node || !tipc_node_is_up(node) || !node->bclink.supported ||
-		     (msg_mc_netid(msg) != tipc_net_id))) {
-		buf_discard(buf);
-		return;
-	}
-
-	if (unlikely(msg_user(msg) == BCAST_PROTOCOL)) {
-		msg_dbg(msg, "<BCNACK<<<");
-		if (msg_destnode(msg) == tipc_own_addr) {
-			tipc_node_lock(node);
-			tipc_bclink_acknowledge(node, msg_bcast_ack(msg));
-			tipc_node_unlock(node);
-			spin_lock_bh(&bc_lock);
-			bcl->stats.recv_nacks++;
-			bcl->owner->next = node;   /* remember requestor */
-			bclink_retransmit_pkt(msg_bcgap_after(msg),
-					      msg_bcgap_to(msg));
-			bcl->owner->next = NULL;
-			spin_unlock_bh(&bc_lock);
-		} else {
-			tipc_bclink_peek_nack(msg_destnode(msg),
-					      msg_bcast_tag(msg),
-					      msg_bcgap_after(msg),
-					      msg_bcgap_to(msg));
-		}
-		buf_discard(buf);
-		return;
-	}
-
-#if (TIPC_BCAST_LOSS_RATE)
-	if (++rx_count == TIPC_BCAST_LOSS_RATE) {
-		rx_count = 0;
-		buf_discard(buf);
-		return;
-	}
-#endif
-
-	tipc_node_lock(node);
-receive:
-	deferred = node->bclink.deferred_head;
-	next_in = mod(node->bclink.last_in + 1);
-	seqno = msg_seqno(msg);
-
-	if (likely(seqno == next_in)) {
-		bcl->stats.recv_info++;
-		node->bclink.last_in++;
-		bclink_set_gap(node);
-		if (unlikely(bclink_ack_allowed(seqno))) {
-			bclink_send_ack(node);
-			bcl->stats.sent_acks++;
-		}
-		if (likely(msg_isdata(msg))) {
-			tipc_node_unlock(node);
-			tipc_port_recv_mcast(buf, NULL);
-		} else if (msg_user(msg) == MSG_BUNDLER) {
-			bcl->stats.recv_bundles++;
-			bcl->stats.recv_bundled += msg_msgcnt(msg);
-			tipc_node_unlock(node);
-			tipc_link_recv_bundle(buf);
-		} else if (msg_user(msg) == MSG_FRAGMENTER) {
-			bcl->stats.recv_fragments++;
-			if (tipc_link_recv_fragment(&node->bclink.defragm,
-						    &buf, &msg))
-				bcl->stats.recv_fragmented++;
-			tipc_node_unlock(node);
-			tipc_net_route_msg(buf);
-		} else {
-			tipc_node_unlock(node);
-			tipc_net_route_msg(buf);
-		}
-		if (deferred && (buf_seqno(deferred) == mod(next_in + 1))) {
-			tipc_node_lock(node);
-			buf = deferred;
-			msg = buf_msg(buf);
-			node->bclink.deferred_head = deferred->next;
-			goto receive;
-		}
-		return;
-	} else if (less(next_in, seqno)) {
-		u32 gap_after = node->bclink.gap_after;
-		u32 gap_to = node->bclink.gap_to;
-
-		if (tipc_link_defer_pkt(&node->bclink.deferred_head,
-					&node->bclink.deferred_tail,
-					buf)) {
-			node->bclink.nack_sync++;
-			bcl->stats.deferred_recv++;
-			if (seqno == mod(gap_after + 1))
-				node->bclink.gap_after = seqno;
-			else if (less(gap_after, seqno) && less(seqno, gap_to))
-				node->bclink.gap_to = seqno;
-		}
-		if (bclink_ack_allowed(node->bclink.nack_sync)) {
-			if (gap_to != gap_after)
-				bclink_send_nack(node);
-			bclink_set_gap(node);
-		}
-	} else {
-		bcl->stats.duplicates++;
-		buf_discard(buf);
-	}
-	tipc_node_unlock(node);
-}
-
-u32 tipc_bclink_get_last_sent(void)
-{
-	u32 last_sent = mod(bcl->next_out_no - 1);
-
-	if (bcl->next_out)
-		last_sent = mod(buf_seqno(bcl->next_out) - 1);
-	return last_sent;
-}
-
-u32 tipc_bclink_acks_missing(struct tipc_node *n_ptr)
-{
-	return (n_ptr->bclink.supported &&
-		(tipc_bclink_get_last_sent() != n_ptr->bclink.acked));
-}
-
-
-/**
- * tipc_bcbearer_send - send a packet through the broadcast pseudo-bearer
- *
- * Send through as many bearers as necessary to reach all nodes
- * that support TIPC multicasting.
- *
- * Returns 0 if packet sent successfully, non-zero if not
- */
-
-static int tipc_bcbearer_send(struct sk_buff *buf,
-			      struct tipc_bearer *unused1,
-			      struct tipc_media_addr *unused2)
-{
-	static int send_count = 0;
-
-	int bp_index;
-	int swap_time;
-
-	/* Prepare buffer for broadcasting (if first time trying to send it) */
-
-	if (likely(!msg_non_seq(buf_msg(buf)))) {
-		struct tipc_msg *msg;
-
-		assert(tipc_cltr_bcast_nodes.count != 0);
-		bcbuf_set_acks(buf, tipc_cltr_bcast_nodes.count);
-		msg = buf_msg(buf);
-		msg_set_non_seq(msg, 1);
-		msg_set_mc_netid(msg, tipc_net_id);
-	}
-
-	/* Determine if bearer pairs should be swapped following this attempt */
-
-	if ((swap_time = (++send_count >= 10)))
-		send_count = 0;
-
-	/* Send buffer over bearers until all targets reached */
-
-	bcbearer->remains = tipc_cltr_bcast_nodes;
-
-	for (bp_index = 0; bp_index < MAX_BEARERS; bp_index++) {
-		struct bearer *p = bcbearer->bpairs[bp_index].primary;
-		struct bearer *s = bcbearer->bpairs[bp_index].secondary;
-
-		if (!p)
-			break;	/* no more bearers to try */
-
-		tipc_nmap_diff(&bcbearer->remains, &p->nodes, &bcbearer->remains_new);
-		if (bcbearer->remains_new.count == bcbearer->remains.count)
-			continue;	/* bearer pair doesn't add anything */
-
-		if (!p->publ.blocked &&
-		    !p->media->send_msg(buf, &p->publ, &p->media->bcast_addr)) {
-			if (swap_time && s && !s->publ.blocked)
-				goto swap;
-			else
-				goto update;
-		}
-
-		if (!s || s->publ.blocked ||
-		    s->media->send_msg(buf, &s->publ, &s->media->bcast_addr))
-			continue;	/* unable to send using bearer pair */
-swap:
-		bcbearer->bpairs[bp_index].primary = s;
-		bcbearer->bpairs[bp_index].secondary = p;
-update:
-		if (bcbearer->remains_new.count == 0)
-			return 0;
-
-		bcbearer->remains = bcbearer->remains_new;
-	}
-
-	/* Unable to reach all targets */
-
-	bcbearer->bearer.publ.blocked = 1;
-	bcl->stats.bearer_congs++;
-	return 1;
-}
-
-/**
- * tipc_bcbearer_sort - create sets of bearer pairs used by broadcast bearer
- */
-
-void tipc_bcbearer_sort(void)
-{
-	struct bcbearer_pair *bp_temp = bcbearer->bpairs_temp;
-	struct bcbearer_pair *bp_curr;
-	int b_index;
-	int pri;
-
-	spin_lock_bh(&bc_lock);
-
-	/* Group bearers by priority (can assume max of two per priority) */
-
-	memset(bp_temp, 0, sizeof(bcbearer->bpairs_temp));
-
-	for (b_index = 0; b_index < MAX_BEARERS; b_index++) {
-		struct bearer *b = &tipc_bearers[b_index];
-
-		if (!b->active || !b->nodes.count)
-			continue;
-
-		if (!bp_temp[b->priority].primary)
-			bp_temp[b->priority].primary = b;
-		else
-			bp_temp[b->priority].secondary = b;
-	}
-
-	/* Create array of bearer pairs for broadcasting */
-
-	bp_curr = bcbearer->bpairs;
-	memset(bcbearer->bpairs, 0, sizeof(bcbearer->bpairs));
-
-	for (pri = TIPC_MAX_LINK_PRI; pri >= 0; pri--) {
-
-		if (!bp_temp[pri].primary)
-			continue;
-
-		bp_curr->primary = bp_temp[pri].primary;
-
-		if (bp_temp[pri].secondary) {
-			if (tipc_nmap_equal(&bp_temp[pri].primary->nodes,
-					    &bp_temp[pri].secondary->nodes)) {
-				bp_curr->secondary = bp_temp[pri].secondary;
-			} else {
-				bp_curr++;
-				bp_curr->primary = bp_temp[pri].secondary;
-			}
-		}
-
-		bp_curr++;
-	}
-
-	spin_unlock_bh(&bc_lock);
-}
-
-/**
- * tipc_bcbearer_push - resolve bearer congestion
- *
- * Forces bclink to push out any unsent packets, until all packets are gone
- * or congestion reoccurs.
- * No locks set when function called
- */
-
-void tipc_bcbearer_push(void)
-{
-	struct bearer *b_ptr;
-
-	spin_lock_bh(&bc_lock);
-	b_ptr = &bcbearer->bearer;
-	if (b_ptr->publ.blocked) {
-		b_ptr->publ.blocked = 0;
-		tipc_bearer_lock_push(b_ptr);
-	}
-	spin_unlock_bh(&bc_lock);
-}
-
-
-int tipc_bclink_stats(char *buf, const u32 buf_size)
-{
-	struct print_buf pb;
-
-	if (!bcl)
-		return 0;
-
-	tipc_printbuf_init(&pb, buf, buf_size);
-
-	spin_lock_bh(&bc_lock);
-
-	tipc_printf(&pb, "Link <%s>\n"
-			 "  Window:%u packets\n",
-		    bcl->name, bcl->queue_limit[0]);
-	tipc_printf(&pb, "  RX packets:%u fragments:%u/%u bundles:%u/%u\n",
-		    bcl->stats.recv_info,
-		    bcl->stats.recv_fragments,
-		    bcl->stats.recv_fragmented,
-		    bcl->stats.recv_bundles,
-		    bcl->stats.recv_bundled);
-	tipc_printf(&pb, "  TX packets:%u fragments:%u/%u bundles:%u/%u\n",
-		    bcl->stats.sent_info,
-		    bcl->stats.sent_fragments,
-		    bcl->stats.sent_fragmented,
-		    bcl->stats.sent_bundles,
-		    bcl->stats.sent_bundled);
-	tipc_printf(&pb, "  RX naks:%u defs:%u dups:%u\n",
-		    bcl->stats.recv_nacks,
-		    bcl->stats.deferred_recv,
-		    bcl->stats.duplicates);
-	tipc_printf(&pb, "  TX naks:%u acks:%u dups:%u\n",
-		    bcl->stats.sent_nacks,
-		    bcl->stats.sent_acks,
-		    bcl->stats.retransmitted);
-	tipc_printf(&pb, "  Congestion bearer:%u link:%u  Send queue max:%u avg:%u\n",
-		    bcl->stats.bearer_congs,
-		    bcl->stats.link_congs,
-		    bcl->stats.max_queue_sz,
-		    bcl->stats.queue_sz_counts
-		    ? (bcl->stats.accu_queue_sz / bcl->stats.queue_sz_counts)
-		    : 0);
-
-	spin_unlock_bh(&bc_lock);
-	return tipc_printbuf_validate(&pb);
-}
-
-int tipc_bclink_reset_stats(void)
-{
-	if (!bcl)
-		return -ENOPROTOOPT;
-
-	spin_lock_bh(&bc_lock);
-	memset(&bcl->stats, 0, sizeof(bcl->stats));
-	spin_unlock_bh(&bc_lock);
-	return 0;
-}
-
-int tipc_bclink_set_queue_limits(u32 limit)
-{
-	if (!bcl)
-		return -ENOPROTOOPT;
-	if ((limit < TIPC_MIN_LINK_WIN) || (limit > TIPC_MAX_LINK_WIN))
-		return -EINVAL;
-
-	spin_lock_bh(&bc_lock);
-	tipc_link_set_queue_limits(bcl, limit);
-	spin_unlock_bh(&bc_lock);
-	return 0;
-}
-
-int tipc_bclink_init(void)
-{
-	bcbearer = kzalloc(sizeof(*bcbearer), GFP_ATOMIC);
-	bclink = kzalloc(sizeof(*bclink), GFP_ATOMIC);
-	if (!bcbearer || !bclink) {
- nomem:
-		warn("Multicast link creation failed, no memory\n");
-		kfree(bcbearer);
-		bcbearer = NULL;
-		kfree(bclink);
-		bclink = NULL;
-		return -ENOMEM;
-	}
-
-	INIT_LIST_HEAD(&bcbearer->bearer.cong_links);
-	bcbearer->bearer.media = &bcbearer->media;
-	bcbearer->media.send_msg = tipc_bcbearer_send;
-	sprintf(bcbearer->media.name, "tipc-multicast");
-
-	bcl = &bclink->link;
-	INIT_LIST_HEAD(&bcl->waiting_ports);
-	bcl->next_out_no = 1;
-	spin_lock_init(&bclink->node.lock);
-	bcl->owner = &bclink->node;
-	bcl->max_pkt = MAX_PKT_DEFAULT_MCAST;
-	tipc_link_set_queue_limits(bcl, BCLINK_WIN_DEFAULT);
-	bcl->b_ptr = &bcbearer->bearer;
-	bcl->state = WORKING_WORKING;
-	sprintf(bcl->name, tipc_bclink_name);
-
-	if (BCLINK_LOG_BUF_SIZE) {
-		char *pb = kmalloc(BCLINK_LOG_BUF_SIZE, GFP_ATOMIC);
-
-		if (!pb)
-			goto nomem;
-		tipc_printbuf_init(&bcl->print_buf, pb, BCLINK_LOG_BUF_SIZE);
-	}
-
-	return 0;
-}
-
-void tipc_bclink_stop(void)
-{
-	spin_lock_bh(&bc_lock);
-	if (bcbearer) {
-		tipc_link_stop(bcl);
-		if (BCLINK_LOG_BUF_SIZE)
-			kfree(bcl->print_buf.buf);
-		bcl = NULL;
-		kfree(bclink);
-		bclink = NULL;
-		kfree(bcbearer);
-		bcbearer = NULL;
-	}
-	spin_unlock_bh(&bc_lock);
-}
-
diff -ruN linux-2.6.29/net/tipc/bcast.h android_cluster/linux-2.6.29/net/tipc/bcast.h
--- linux-2.6.29/net/tipc/bcast.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/bcast.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,210 +0,0 @@
-/*
- * net/tipc/bcast.h: Include file for TIPC broadcast code
- *
- * Copyright (c) 2003-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_BCAST_H
-#define _TIPC_BCAST_H
-
-#define MAX_NODES 4096
-#define WSIZE 32
-
-/**
- * struct tipc_node_map - set of node identifiers
- * @count: # of nodes in set
- * @map: bitmap of node identifiers that are in the set
- */
-
-struct tipc_node_map {
-	u32 count;
-	u32 map[MAX_NODES / WSIZE];
-};
-
-
-#define PLSIZE 32
-
-/**
- * struct port_list - set of node local destination ports
- * @count: # of ports in set (only valid for first entry in list)
- * @next: pointer to next entry in list
- * @ports: array of port references
- */
-
-struct port_list {
-	int count;
-	struct port_list *next;
-	u32 ports[PLSIZE];
-};
-
-
-struct tipc_node;
-
-extern char tipc_bclink_name[];
-
-
-/**
- * nmap_add - add a node to a node map
- */
-
-static inline void tipc_nmap_add(struct tipc_node_map *nm_ptr, u32 node)
-{
-	int n = tipc_node(node);
-	int w = n / WSIZE;
-	u32 mask = (1 << (n % WSIZE));
-
-	if ((nm_ptr->map[w] & mask) == 0) {
-		nm_ptr->count++;
-		nm_ptr->map[w] |= mask;
-	}
-}
-
-/**
- * nmap_remove - remove a node from a node map
- */
-
-static inline void tipc_nmap_remove(struct tipc_node_map *nm_ptr, u32 node)
-{
-	int n = tipc_node(node);
-	int w = n / WSIZE;
-	u32 mask = (1 << (n % WSIZE));
-
-	if ((nm_ptr->map[w] & mask) != 0) {
-		nm_ptr->map[w] &= ~mask;
-		nm_ptr->count--;
-	}
-}
-
-/**
- * nmap_equal - test for equality of node maps
- */
-
-static inline int tipc_nmap_equal(struct tipc_node_map *nm_a, struct tipc_node_map *nm_b)
-{
-	return !memcmp(nm_a, nm_b, sizeof(*nm_a));
-}
-
-/**
- * nmap_diff - find differences between node maps
- * @nm_a: input node map A
- * @nm_b: input node map B
- * @nm_diff: output node map A-B (i.e. nodes of A that are not in B)
- */
-
-static inline void tipc_nmap_diff(struct tipc_node_map *nm_a, struct tipc_node_map *nm_b,
-				  struct tipc_node_map *nm_diff)
-{
-	int stop = ARRAY_SIZE(nm_a->map);
-	int w;
-	int b;
-	u32 map;
-
-	memset(nm_diff, 0, sizeof(*nm_diff));
-	for (w = 0; w < stop; w++) {
-		map = nm_a->map[w] ^ (nm_a->map[w] & nm_b->map[w]);
-		nm_diff->map[w] = map;
-		if (map != 0) {
-			for (b = 0 ; b < WSIZE; b++) {
-				if (map & (1 << b))
-					nm_diff->count++;
-			}
-		}
-	}
-}
-
-/**
- * port_list_add - add a port to a port list, ensuring no duplicates
- */
-
-static inline void tipc_port_list_add(struct port_list *pl_ptr, u32 port)
-{
-	struct port_list *item = pl_ptr;
-	int i;
-	int item_sz = PLSIZE;
-	int cnt = pl_ptr->count;
-
-	for (; ; cnt -= item_sz, item = item->next) {
-		if (cnt < PLSIZE)
-			item_sz = cnt;
-		for (i = 0; i < item_sz; i++)
-			if (item->ports[i] == port)
-				return;
-		if (i < PLSIZE) {
-			item->ports[i] = port;
-			pl_ptr->count++;
-			return;
-		}
-		if (!item->next) {
-			item->next = kmalloc(sizeof(*item), GFP_ATOMIC);
-			if (!item->next) {
-				warn("Incomplete multicast delivery, no memory\n");
-				return;
-			}
-			item->next->next = NULL;
-		}
-	}
-}
-
-/**
- * port_list_free - free dynamically created entries in port_list chain
- *
- * Note: First item is on stack, so it doesn't need to be released
- */
-
-static inline void tipc_port_list_free(struct port_list *pl_ptr)
-{
-	struct port_list *item;
-	struct port_list *next;
-
-	for (item = pl_ptr->next; item; item = next) {
-		next = item->next;
-		kfree(item);
-	}
-}
-
-
-int  tipc_bclink_init(void);
-void tipc_bclink_stop(void);
-void tipc_bclink_acknowledge(struct tipc_node *n_ptr, u32 acked);
-int  tipc_bclink_send_msg(struct sk_buff *buf);
-void tipc_bclink_recv_pkt(struct sk_buff *buf);
-u32  tipc_bclink_get_last_sent(void);
-u32  tipc_bclink_acks_missing(struct tipc_node *n_ptr);
-void tipc_bclink_check_gap(struct tipc_node *n_ptr, u32 seqno);
-int  tipc_bclink_stats(char *stats_buf, const u32 buf_size);
-int  tipc_bclink_reset_stats(void);
-int  tipc_bclink_set_queue_limits(u32 limit);
-void tipc_bcbearer_sort(void);
-void tipc_bcbearer_push(void);
-
-#endif
diff -ruN linux-2.6.29/net/tipc/bearer.c android_cluster/linux-2.6.29/net/tipc/bearer.c
--- linux-2.6.29/net/tipc/bearer.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/bearer.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,705 +0,0 @@
-/*
- * net/tipc/bearer.c: TIPC bearer code
- *
- * Copyright (c) 1996-2006, Ericsson AB
- * Copyright (c) 2004-2006, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "config.h"
-#include "dbg.h"
-#include "bearer.h"
-#include "link.h"
-#include "port.h"
-#include "discover.h"
-#include "bcast.h"
-
-#define MAX_ADDR_STR 32
-
-static struct media *media_list = NULL;
-static u32 media_count = 0;
-
-struct bearer *tipc_bearers = NULL;
-
-/**
- * media_name_valid - validate media name
- *
- * Returns 1 if media name is valid, otherwise 0.
- */
-
-static int media_name_valid(const char *name)
-{
-	u32 len;
-
-	len = strlen(name);
-	if ((len + 1) > TIPC_MAX_MEDIA_NAME)
-		return 0;
-	return (strspn(name, tipc_alphabet) == len);
-}
-
-/**
- * media_find - locates specified media object by name
- */
-
-static struct media *media_find(const char *name)
-{
-	struct media *m_ptr;
-	u32 i;
-
-	for (i = 0, m_ptr = media_list; i < media_count; i++, m_ptr++) {
-		if (!strcmp(m_ptr->name, name))
-			return m_ptr;
-	}
-	return NULL;
-}
-
-/**
- * tipc_register_media - register a media type
- *
- * Bearers for this media type must be activated separately at a later stage.
- */
-
-int  tipc_register_media(u32 media_type,
-			 char *name,
-			 int (*enable)(struct tipc_bearer *),
-			 void (*disable)(struct tipc_bearer *),
-			 int (*send_msg)(struct sk_buff *,
-					 struct tipc_bearer *,
-					 struct tipc_media_addr *),
-			 char *(*addr2str)(struct tipc_media_addr *a,
-					   char *str_buf, int str_size),
-			 struct tipc_media_addr *bcast_addr,
-			 const u32 bearer_priority,
-			 const u32 link_tolerance,  /* [ms] */
-			 const u32 send_window_limit)
-{
-	struct media *m_ptr;
-	u32 media_id;
-	u32 i;
-	int res = -EINVAL;
-
-	write_lock_bh(&tipc_net_lock);
-	if (!media_list)
-		goto exit;
-
-	if (!media_name_valid(name)) {
-		warn("Media <%s> rejected, illegal name\n", name);
-		goto exit;
-	}
-	if (!bcast_addr) {
-		warn("Media <%s> rejected, no broadcast address\n", name);
-		goto exit;
-	}
-	if ((bearer_priority < TIPC_MIN_LINK_PRI) &&
-	    (bearer_priority > TIPC_MAX_LINK_PRI)) {
-		warn("Media <%s> rejected, illegal priority (%u)\n", name,
-		     bearer_priority);
-		goto exit;
-	}
-	if ((link_tolerance < TIPC_MIN_LINK_TOL) ||
-	    (link_tolerance > TIPC_MAX_LINK_TOL)) {
-		warn("Media <%s> rejected, illegal tolerance (%u)\n", name,
-		     link_tolerance);
-		goto exit;
-	}
-
-	media_id = media_count++;
-	if (media_id >= MAX_MEDIA) {
-		warn("Media <%s> rejected, media limit reached (%u)\n", name,
-		     MAX_MEDIA);
-		media_count--;
-		goto exit;
-	}
-	for (i = 0; i < media_id; i++) {
-		if (media_list[i].type_id == media_type) {
-			warn("Media <%s> rejected, duplicate type (%u)\n", name,
-			     media_type);
-			media_count--;
-			goto exit;
-		}
-		if (!strcmp(name, media_list[i].name)) {
-			warn("Media <%s> rejected, duplicate name\n", name);
-			media_count--;
-			goto exit;
-		}
-	}
-
-	m_ptr = &media_list[media_id];
-	m_ptr->type_id = media_type;
-	m_ptr->send_msg = send_msg;
-	m_ptr->enable_bearer = enable;
-	m_ptr->disable_bearer = disable;
-	m_ptr->addr2str = addr2str;
-	memcpy(&m_ptr->bcast_addr, bcast_addr, sizeof(*bcast_addr));
-	m_ptr->bcast = 1;
-	strcpy(m_ptr->name, name);
-	m_ptr->priority = bearer_priority;
-	m_ptr->tolerance = link_tolerance;
-	m_ptr->window = send_window_limit;
-	dbg("Media <%s> registered\n", name);
-	res = 0;
-exit:
-	write_unlock_bh(&tipc_net_lock);
-	return res;
-}
-
-/**
- * tipc_media_addr_printf - record media address in print buffer
- */
-
-void tipc_media_addr_printf(struct print_buf *pb, struct tipc_media_addr *a)
-{
-	struct media *m_ptr;
-	u32 media_type;
-	u32 i;
-
-	media_type = ntohl(a->type);
-	for (i = 0, m_ptr = media_list; i < media_count; i++, m_ptr++) {
-		if (m_ptr->type_id == media_type)
-			break;
-	}
-
-	if ((i < media_count) && (m_ptr->addr2str != NULL)) {
-		char addr_str[MAX_ADDR_STR];
-
-		tipc_printf(pb, "%s(%s)", m_ptr->name,
-			    m_ptr->addr2str(a, addr_str, sizeof(addr_str)));
-	} else {
-		unchar *addr = (unchar *)&a->dev_addr;
-
-		tipc_printf(pb, "UNKNOWN(%u)", media_type);
-		for (i = 0; i < (sizeof(*a) - sizeof(a->type)); i++) {
-			tipc_printf(pb, "-%02x", addr[i]);
-		}
-	}
-}
-
-/**
- * tipc_media_get_names - record names of registered media in buffer
- */
-
-struct sk_buff *tipc_media_get_names(void)
-{
-	struct sk_buff *buf;
-	struct media *m_ptr;
-	int i;
-
-	buf = tipc_cfg_reply_alloc(MAX_MEDIA * TLV_SPACE(TIPC_MAX_MEDIA_NAME));
-	if (!buf)
-		return NULL;
-
-	read_lock_bh(&tipc_net_lock);
-	for (i = 0, m_ptr = media_list; i < media_count; i++, m_ptr++) {
-		tipc_cfg_append_tlv(buf, TIPC_TLV_MEDIA_NAME, m_ptr->name,
-				    strlen(m_ptr->name) + 1);
-	}
-	read_unlock_bh(&tipc_net_lock);
-	return buf;
-}
-
-/**
- * bearer_name_validate - validate & (optionally) deconstruct bearer name
- * @name - ptr to bearer name string
- * @name_parts - ptr to area for bearer name components (or NULL if not needed)
- *
- * Returns 1 if bearer name is valid, otherwise 0.
- */
-
-static int bearer_name_validate(const char *name,
-				struct bearer_name *name_parts)
-{
-	char name_copy[TIPC_MAX_BEARER_NAME];
-	char *media_name;
-	char *if_name;
-	u32 media_len;
-	u32 if_len;
-
-	/* copy bearer name & ensure length is OK */
-
-	name_copy[TIPC_MAX_BEARER_NAME - 1] = 0;
-	/* need above in case non-Posix strncpy() doesn't pad with nulls */
-	strncpy(name_copy, name, TIPC_MAX_BEARER_NAME);
-	if (name_copy[TIPC_MAX_BEARER_NAME - 1] != 0)
-		return 0;
-
-	/* ensure all component parts of bearer name are present */
-
-	media_name = name_copy;
-	if ((if_name = strchr(media_name, ':')) == NULL)
-		return 0;
-	*(if_name++) = 0;
-	media_len = if_name - media_name;
-	if_len = strlen(if_name) + 1;
-
-	/* validate component parts of bearer name */
-
-	if ((media_len <= 1) || (media_len > TIPC_MAX_MEDIA_NAME) ||
-	    (if_len <= 1) || (if_len > TIPC_MAX_IF_NAME) ||
-	    (strspn(media_name, tipc_alphabet) != (media_len - 1)) ||
-	    (strspn(if_name, tipc_alphabet) != (if_len - 1)))
-		return 0;
-
-	/* return bearer name components, if necessary */
-
-	if (name_parts) {
-		strcpy(name_parts->media_name, media_name);
-		strcpy(name_parts->if_name, if_name);
-	}
-	return 1;
-}
-
-/**
- * bearer_find - locates bearer object with matching bearer name
- */
-
-static struct bearer *bearer_find(const char *name)
-{
-	struct bearer *b_ptr;
-	u32 i;
-
-	if (tipc_mode != TIPC_NET_MODE)
-		return NULL;
-
-	for (i = 0, b_ptr = tipc_bearers; i < MAX_BEARERS; i++, b_ptr++) {
-		if (b_ptr->active && (!strcmp(b_ptr->publ.name, name)))
-			return b_ptr;
-	}
-	return NULL;
-}
-
-/**
- * tipc_bearer_find_interface - locates bearer object with matching interface name
- */
-
-struct bearer *tipc_bearer_find_interface(const char *if_name)
-{
-	struct bearer *b_ptr;
-	char *b_if_name;
-	u32 i;
-
-	for (i = 0, b_ptr = tipc_bearers; i < MAX_BEARERS; i++, b_ptr++) {
-		if (!b_ptr->active)
-			continue;
-		b_if_name = strchr(b_ptr->publ.name, ':') + 1;
-		if (!strcmp(b_if_name, if_name))
-			return b_ptr;
-	}
-	return NULL;
-}
-
-/**
- * tipc_bearer_get_names - record names of bearers in buffer
- */
-
-struct sk_buff *tipc_bearer_get_names(void)
-{
-	struct sk_buff *buf;
-	struct media *m_ptr;
-	struct bearer *b_ptr;
-	int i, j;
-
-	buf = tipc_cfg_reply_alloc(MAX_BEARERS * TLV_SPACE(TIPC_MAX_BEARER_NAME));
-	if (!buf)
-		return NULL;
-
-	read_lock_bh(&tipc_net_lock);
-	for (i = 0, m_ptr = media_list; i < media_count; i++, m_ptr++) {
-		for (j = 0; j < MAX_BEARERS; j++) {
-			b_ptr = &tipc_bearers[j];
-			if (b_ptr->active && (b_ptr->media == m_ptr)) {
-				tipc_cfg_append_tlv(buf, TIPC_TLV_BEARER_NAME,
-						    b_ptr->publ.name,
-						    strlen(b_ptr->publ.name) + 1);
-			}
-		}
-	}
-	read_unlock_bh(&tipc_net_lock);
-	return buf;
-}
-
-void tipc_bearer_add_dest(struct bearer *b_ptr, u32 dest)
-{
-	tipc_nmap_add(&b_ptr->nodes, dest);
-	tipc_disc_update_link_req(b_ptr->link_req);
-	tipc_bcbearer_sort();
-}
-
-void tipc_bearer_remove_dest(struct bearer *b_ptr, u32 dest)
-{
-	tipc_nmap_remove(&b_ptr->nodes, dest);
-	tipc_disc_update_link_req(b_ptr->link_req);
-	tipc_bcbearer_sort();
-}
-
-/*
- * bearer_push(): Resolve bearer congestion. Force the waiting
- * links to push out their unsent packets, one packet per link
- * per iteration, until all packets are gone or congestion reoccurs.
- * 'tipc_net_lock' is read_locked when this function is called
- * bearer.lock must be taken before calling
- * Returns binary true(1) ore false(0)
- */
-static int bearer_push(struct bearer *b_ptr)
-{
-	u32 res = 0;
-	struct link *ln, *tln;
-
-	if (b_ptr->publ.blocked)
-		return 0;
-
-	while (!list_empty(&b_ptr->cong_links) && (res != PUSH_FAILED)) {
-		list_for_each_entry_safe(ln, tln, &b_ptr->cong_links, link_list) {
-			res = tipc_link_push_packet(ln);
-			if (res == PUSH_FAILED)
-				break;
-			if (res == PUSH_FINISHED)
-				list_move_tail(&ln->link_list, &b_ptr->links);
-		}
-	}
-	return list_empty(&b_ptr->cong_links);
-}
-
-void tipc_bearer_lock_push(struct bearer *b_ptr)
-{
-	int res;
-
-	spin_lock_bh(&b_ptr->publ.lock);
-	res = bearer_push(b_ptr);
-	spin_unlock_bh(&b_ptr->publ.lock);
-	if (res)
-		tipc_bcbearer_push();
-}
-
-
-/*
- * Interrupt enabling new requests after bearer congestion or blocking:
- * See bearer_send().
- */
-void tipc_continue(struct tipc_bearer *tb_ptr)
-{
-	struct bearer *b_ptr = (struct bearer *)tb_ptr;
-
-	spin_lock_bh(&b_ptr->publ.lock);
-	b_ptr->continue_count++;
-	if (!list_empty(&b_ptr->cong_links))
-		tipc_k_signal((Handler)tipc_bearer_lock_push, (unsigned long)b_ptr);
-	b_ptr->publ.blocked = 0;
-	spin_unlock_bh(&b_ptr->publ.lock);
-}
-
-/*
- * Schedule link for sending of messages after the bearer
- * has been deblocked by 'continue()'. This method is called
- * when somebody tries to send a message via this link while
- * the bearer is congested. 'tipc_net_lock' is in read_lock here
- * bearer.lock is busy
- */
-
-static void tipc_bearer_schedule_unlocked(struct bearer *b_ptr, struct link *l_ptr)
-{
-	list_move_tail(&l_ptr->link_list, &b_ptr->cong_links);
-}
-
-/*
- * Schedule link for sending of messages after the bearer
- * has been deblocked by 'continue()'. This method is called
- * when somebody tries to send a message via this link while
- * the bearer is congested. 'tipc_net_lock' is in read_lock here,
- * bearer.lock is free
- */
-
-void tipc_bearer_schedule(struct bearer *b_ptr, struct link *l_ptr)
-{
-	spin_lock_bh(&b_ptr->publ.lock);
-	tipc_bearer_schedule_unlocked(b_ptr, l_ptr);
-	spin_unlock_bh(&b_ptr->publ.lock);
-}
-
-
-/*
- * tipc_bearer_resolve_congestion(): Check if there is bearer congestion,
- * and if there is, try to resolve it before returning.
- * 'tipc_net_lock' is read_locked when this function is called
- */
-int tipc_bearer_resolve_congestion(struct bearer *b_ptr, struct link *l_ptr)
-{
-	int res = 1;
-
-	if (list_empty(&b_ptr->cong_links))
-		return 1;
-	spin_lock_bh(&b_ptr->publ.lock);
-	if (!bearer_push(b_ptr)) {
-		tipc_bearer_schedule_unlocked(b_ptr, l_ptr);
-		res = 0;
-	}
-	spin_unlock_bh(&b_ptr->publ.lock);
-	return res;
-}
-
-
-/**
- * tipc_enable_bearer - enable bearer with the given name
- */
-
-int tipc_enable_bearer(const char *name, u32 bcast_scope, u32 priority)
-{
-	struct bearer *b_ptr;
-	struct media *m_ptr;
-	struct bearer_name b_name;
-	char addr_string[16];
-	u32 bearer_id;
-	u32 with_this_prio;
-	u32 i;
-	int res = -EINVAL;
-
-	if (tipc_mode != TIPC_NET_MODE) {
-		warn("Bearer <%s> rejected, not supported in standalone mode\n",
-		     name);
-		return -ENOPROTOOPT;
-	}
-	if (!bearer_name_validate(name, &b_name)) {
-		warn("Bearer <%s> rejected, illegal name\n", name);
-		return -EINVAL;
-	}
-	if (!tipc_addr_domain_valid(bcast_scope) ||
-	    !in_scope(bcast_scope, tipc_own_addr)) {
-		warn("Bearer <%s> rejected, illegal broadcast scope\n", name);
-		return -EINVAL;
-	}
-	if ((priority < TIPC_MIN_LINK_PRI ||
-	     priority > TIPC_MAX_LINK_PRI) &&
-	    (priority != TIPC_MEDIA_LINK_PRI)) {
-		warn("Bearer <%s> rejected, illegal priority\n", name);
-		return -EINVAL;
-	}
-
-	write_lock_bh(&tipc_net_lock);
-
-	m_ptr = media_find(b_name.media_name);
-	if (!m_ptr) {
-		warn("Bearer <%s> rejected, media <%s> not registered\n", name,
-		     b_name.media_name);
-		goto failed;
-	}
-
-	if (priority == TIPC_MEDIA_LINK_PRI)
-		priority = m_ptr->priority;
-
-restart:
-	bearer_id = MAX_BEARERS;
-	with_this_prio = 1;
-	for (i = MAX_BEARERS; i-- != 0; ) {
-		if (!tipc_bearers[i].active) {
-			bearer_id = i;
-			continue;
-		}
-		if (!strcmp(name, tipc_bearers[i].publ.name)) {
-			warn("Bearer <%s> rejected, already enabled\n", name);
-			goto failed;
-		}
-		if ((tipc_bearers[i].priority == priority) &&
-		    (++with_this_prio > 2)) {
-			if (priority-- == 0) {
-				warn("Bearer <%s> rejected, duplicate priority\n",
-				     name);
-				goto failed;
-			}
-			warn("Bearer <%s> priority adjustment required %u->%u\n",
-			     name, priority + 1, priority);
-			goto restart;
-		}
-	}
-	if (bearer_id >= MAX_BEARERS) {
-		warn("Bearer <%s> rejected, bearer limit reached (%u)\n",
-		     name, MAX_BEARERS);
-		goto failed;
-	}
-
-	b_ptr = &tipc_bearers[bearer_id];
-	memset(b_ptr, 0, sizeof(struct bearer));
-
-	strcpy(b_ptr->publ.name, name);
-	res = m_ptr->enable_bearer(&b_ptr->publ);
-	if (res) {
-		warn("Bearer <%s> rejected, enable failure (%d)\n", name, -res);
-		goto failed;
-	}
-
-	b_ptr->identity = bearer_id;
-	b_ptr->media = m_ptr;
-	b_ptr->net_plane = bearer_id + 'A';
-	b_ptr->active = 1;
-	b_ptr->detect_scope = bcast_scope;
-	b_ptr->priority = priority;
-	INIT_LIST_HEAD(&b_ptr->cong_links);
-	INIT_LIST_HEAD(&b_ptr->links);
-	if (m_ptr->bcast) {
-		b_ptr->link_req = tipc_disc_init_link_req(b_ptr, &m_ptr->bcast_addr,
-							  bcast_scope, 2);
-	}
-	spin_lock_init(&b_ptr->publ.lock);
-	write_unlock_bh(&tipc_net_lock);
-	info("Enabled bearer <%s>, discovery domain %s, priority %u\n",
-	     name, addr_string_fill(addr_string, bcast_scope), priority);
-	return 0;
-failed:
-	write_unlock_bh(&tipc_net_lock);
-	return res;
-}
-
-/**
- * tipc_block_bearer(): Block the bearer with the given name,
- *                      and reset all its links
- */
-
-int tipc_block_bearer(const char *name)
-{
-	struct bearer *b_ptr = NULL;
-	struct link *l_ptr;
-	struct link *temp_l_ptr;
-
-	read_lock_bh(&tipc_net_lock);
-	b_ptr = bearer_find(name);
-	if (!b_ptr) {
-		warn("Attempt to block unknown bearer <%s>\n", name);
-		read_unlock_bh(&tipc_net_lock);
-		return -EINVAL;
-	}
-
-	info("Blocking bearer <%s>\n", name);
-	spin_lock_bh(&b_ptr->publ.lock);
-	b_ptr->publ.blocked = 1;
-	list_for_each_entry_safe(l_ptr, temp_l_ptr, &b_ptr->links, link_list) {
-		struct tipc_node *n_ptr = l_ptr->owner;
-
-		spin_lock_bh(&n_ptr->lock);
-		tipc_link_reset(l_ptr);
-		spin_unlock_bh(&n_ptr->lock);
-	}
-	spin_unlock_bh(&b_ptr->publ.lock);
-	read_unlock_bh(&tipc_net_lock);
-	return 0;
-}
-
-/**
- * bearer_disable -
- *
- * Note: This routine assumes caller holds tipc_net_lock.
- */
-
-static int bearer_disable(const char *name)
-{
-	struct bearer *b_ptr;
-	struct link *l_ptr;
-	struct link *temp_l_ptr;
-
-	b_ptr = bearer_find(name);
-	if (!b_ptr) {
-		warn("Attempt to disable unknown bearer <%s>\n", name);
-		return -EINVAL;
-	}
-
-	info("Disabling bearer <%s>\n", name);
-	tipc_disc_stop_link_req(b_ptr->link_req);
-	spin_lock_bh(&b_ptr->publ.lock);
-	b_ptr->link_req = NULL;
-	b_ptr->publ.blocked = 1;
-	if (b_ptr->media->disable_bearer) {
-		spin_unlock_bh(&b_ptr->publ.lock);
-		write_unlock_bh(&tipc_net_lock);
-		b_ptr->media->disable_bearer(&b_ptr->publ);
-		write_lock_bh(&tipc_net_lock);
-		spin_lock_bh(&b_ptr->publ.lock);
-	}
-	list_for_each_entry_safe(l_ptr, temp_l_ptr, &b_ptr->links, link_list) {
-		tipc_link_delete(l_ptr);
-	}
-	spin_unlock_bh(&b_ptr->publ.lock);
-	memset(b_ptr, 0, sizeof(struct bearer));
-	return 0;
-}
-
-int tipc_disable_bearer(const char *name)
-{
-	int res;
-
-	write_lock_bh(&tipc_net_lock);
-	res = bearer_disable(name);
-	write_unlock_bh(&tipc_net_lock);
-	return res;
-}
-
-
-
-int tipc_bearer_init(void)
-{
-	int res;
-
-	write_lock_bh(&tipc_net_lock);
-	tipc_bearers = kcalloc(MAX_BEARERS, sizeof(struct bearer), GFP_ATOMIC);
-	media_list = kcalloc(MAX_MEDIA, sizeof(struct media), GFP_ATOMIC);
-	if (tipc_bearers && media_list) {
-		res = 0;
-	} else {
-		kfree(tipc_bearers);
-		kfree(media_list);
-		tipc_bearers = NULL;
-		media_list = NULL;
-		res = -ENOMEM;
-	}
-	write_unlock_bh(&tipc_net_lock);
-	return res;
-}
-
-void tipc_bearer_stop(void)
-{
-	u32 i;
-
-	if (!tipc_bearers)
-		return;
-
-	for (i = 0; i < MAX_BEARERS; i++) {
-		if (tipc_bearers[i].active)
-			tipc_bearers[i].publ.blocked = 1;
-	}
-	for (i = 0; i < MAX_BEARERS; i++) {
-		if (tipc_bearers[i].active)
-			bearer_disable(tipc_bearers[i].publ.name);
-	}
-	kfree(tipc_bearers);
-	kfree(media_list);
-	tipc_bearers = NULL;
-	media_list = NULL;
-	media_count = 0;
-}
-
-
diff -ruN linux-2.6.29/net/tipc/bearer.h android_cluster/linux-2.6.29/net/tipc/bearer.h
--- linux-2.6.29/net/tipc/bearer.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/bearer.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,170 +0,0 @@
-/*
- * net/tipc/bearer.h: Include file for TIPC bearer code
- *
- * Copyright (c) 1996-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_BEARER_H
-#define _TIPC_BEARER_H
-
-#include "core.h"
-#include "bcast.h"
-
-#define MAX_BEARERS 8
-#define MAX_MEDIA 4
-
-
-/**
- * struct media - TIPC media information available to internal users
- * @send_msg: routine which handles buffer transmission
- * @enable_bearer: routine which enables a bearer
- * @disable_bearer: routine which disables a bearer
- * @addr2str: routine which converts bearer's address to string form
- * @bcast_addr: media address used in broadcasting
- * @bcast: non-zero if media supports broadcasting [currently mandatory]
- * @priority: default link (and bearer) priority
- * @tolerance: default time (in ms) before declaring link failure
- * @window: default window (in packets) before declaring link congestion
- * @type_id: TIPC media identifier [defined in tipc_bearer.h]
- * @name: media name
- */
-
-struct media {
-	int (*send_msg)(struct sk_buff *buf,
-			struct tipc_bearer *b_ptr,
-			struct tipc_media_addr *dest);
-	int (*enable_bearer)(struct tipc_bearer *b_ptr);
-	void (*disable_bearer)(struct tipc_bearer *b_ptr);
-	char *(*addr2str)(struct tipc_media_addr *a,
-			  char *str_buf, int str_size);
-	struct tipc_media_addr bcast_addr;
-	int bcast;
-	u32 priority;
-	u32 tolerance;
-	u32 window;
-	u32 type_id;
-	char name[TIPC_MAX_MEDIA_NAME];
-};
-
-/**
- * struct bearer - TIPC bearer information available to internal users
- * @publ: bearer information available to privileged users
- * @media: ptr to media structure associated with bearer
- * @priority: default link priority for bearer
- * @detect_scope: network address mask used during automatic link creation
- * @identity: array index of this bearer within TIPC bearer array
- * @link_req: ptr to (optional) structure making periodic link setup requests
- * @links: list of non-congested links associated with bearer
- * @cong_links: list of congested links associated with bearer
- * @continue_count: # of times bearer has resumed after congestion or blocking
- * @active: non-zero if bearer structure is represents a bearer
- * @net_plane: network plane ('A' through 'H') currently associated with bearer
- * @nodes: indicates which nodes in cluster can be reached through bearer
- */
-
-struct bearer {
-	struct tipc_bearer publ;
-	struct media *media;
-	u32 priority;
-	u32 detect_scope;
-	u32 identity;
-	struct link_req *link_req;
-	struct list_head links;
-	struct list_head cong_links;
-	u32 continue_count;
-	int active;
-	char net_plane;
-	struct tipc_node_map nodes;
-};
-
-struct bearer_name {
-	char media_name[TIPC_MAX_MEDIA_NAME];
-	char if_name[TIPC_MAX_IF_NAME];
-};
-
-struct link;
-
-extern struct bearer *tipc_bearers;
-
-void tipc_media_addr_printf(struct print_buf *pb, struct tipc_media_addr *a);
-struct sk_buff *tipc_media_get_names(void);
-
-struct sk_buff *tipc_bearer_get_names(void);
-void tipc_bearer_add_dest(struct bearer *b_ptr, u32 dest);
-void tipc_bearer_remove_dest(struct bearer *b_ptr, u32 dest);
-void tipc_bearer_schedule(struct bearer *b_ptr, struct link *l_ptr);
-struct bearer *tipc_bearer_find_interface(const char *if_name);
-int tipc_bearer_resolve_congestion(struct bearer *b_ptr, struct link *l_ptr);
-int tipc_bearer_init(void);
-void tipc_bearer_stop(void);
-void tipc_bearer_lock_push(struct bearer *b_ptr);
-
-
-/**
- * tipc_bearer_send- sends buffer to destination over bearer
- *
- * Returns true (1) if successful, or false (0) if unable to send
- *
- * IMPORTANT:
- * The media send routine must not alter the buffer being passed in
- * as it may be needed for later retransmission!
- *
- * If the media send routine returns a non-zero value (indicating that
- * it was unable to send the buffer), it must:
- *   1) mark the bearer as blocked,
- *   2) call tipc_continue() once the bearer is able to send again.
- * Media types that are unable to meet these two critera must ensure their
- * send routine always returns success -- even if the buffer was not sent --
- * and let TIPC's link code deal with the undelivered message.
- */
-
-static inline int tipc_bearer_send(struct bearer *b_ptr, struct sk_buff *buf,
-				   struct tipc_media_addr *dest)
-{
-	return !b_ptr->media->send_msg(buf, &b_ptr->publ, dest);
-}
-
-/**
- * tipc_bearer_congested - determines if bearer is currently congested
- */
-
-static inline int tipc_bearer_congested(struct bearer *b_ptr, struct link *l_ptr)
-{
-	if (unlikely(b_ptr->publ.blocked))
-		return 1;
-	if (likely(list_empty(&b_ptr->cong_links)))
-		return 0;
-	return !tipc_bearer_resolve_congestion(b_ptr, l_ptr);
-}
-
-#endif
diff -ruN linux-2.6.29/net/tipc/cluster.c android_cluster/linux-2.6.29/net/tipc/cluster.c
--- linux-2.6.29/net/tipc/cluster.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/cluster.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,576 +0,0 @@
-/*
- * net/tipc/cluster.c: TIPC cluster management routines
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "cluster.h"
-#include "addr.h"
-#include "node_subscr.h"
-#include "link.h"
-#include "node.h"
-#include "net.h"
-#include "msg.h"
-#include "bearer.h"
-
-static void tipc_cltr_multicast(struct cluster *c_ptr, struct sk_buff *buf,
-				u32 lower, u32 upper);
-static struct sk_buff *tipc_cltr_prepare_routing_msg(u32 data_size, u32 dest);
-
-struct tipc_node **tipc_local_nodes = NULL;
-struct tipc_node_map tipc_cltr_bcast_nodes = {0,{0,}};
-u32 tipc_highest_allowed_slave = 0;
-
-struct cluster *tipc_cltr_create(u32 addr)
-{
-	struct _zone *z_ptr;
-	struct cluster *c_ptr;
-	int max_nodes;
-
-	c_ptr = kzalloc(sizeof(*c_ptr), GFP_ATOMIC);
-	if (c_ptr == NULL) {
-		warn("Cluster creation failure, no memory\n");
-		return NULL;
-	}
-
-	c_ptr->addr = tipc_addr(tipc_zone(addr), tipc_cluster(addr), 0);
-	if (in_own_cluster(addr))
-		max_nodes = LOWEST_SLAVE + tipc_max_slaves;
-	else
-		max_nodes = tipc_max_nodes + 1;
-
-	c_ptr->nodes = kcalloc(max_nodes + 1, sizeof(void*), GFP_ATOMIC);
-	if (c_ptr->nodes == NULL) {
-		warn("Cluster creation failure, no memory for node area\n");
-		kfree(c_ptr);
-		return NULL;
-	}
-
-	if (in_own_cluster(addr))
-		tipc_local_nodes = c_ptr->nodes;
-	c_ptr->highest_slave = LOWEST_SLAVE - 1;
-	c_ptr->highest_node = 0;
-
-	z_ptr = tipc_zone_find(tipc_zone(addr));
-	if (!z_ptr) {
-		z_ptr = tipc_zone_create(addr);
-	}
-	if (!z_ptr) {
-		kfree(c_ptr->nodes);
-		kfree(c_ptr);
-		return NULL;
-	}
-
-	tipc_zone_attach_cluster(z_ptr, c_ptr);
-	c_ptr->owner = z_ptr;
-	return c_ptr;
-}
-
-void tipc_cltr_delete(struct cluster *c_ptr)
-{
-	u32 n_num;
-
-	if (!c_ptr)
-		return;
-	for (n_num = 1; n_num <= c_ptr->highest_node; n_num++) {
-		tipc_node_delete(c_ptr->nodes[n_num]);
-	}
-	for (n_num = LOWEST_SLAVE; n_num <= c_ptr->highest_slave; n_num++) {
-		tipc_node_delete(c_ptr->nodes[n_num]);
-	}
-	kfree(c_ptr->nodes);
-	kfree(c_ptr);
-}
-
-u32 tipc_cltr_next_node(struct cluster *c_ptr, u32 addr)
-{
-	struct tipc_node *n_ptr;
-	u32 n_num = tipc_node(addr) + 1;
-
-	if (!c_ptr)
-		return addr;
-	for (; n_num <= c_ptr->highest_node; n_num++) {
-		n_ptr = c_ptr->nodes[n_num];
-		if (n_ptr && tipc_node_has_active_links(n_ptr))
-			return n_ptr->addr;
-	}
-	for (n_num = 1; n_num < tipc_node(addr); n_num++) {
-		n_ptr = c_ptr->nodes[n_num];
-		if (n_ptr && tipc_node_has_active_links(n_ptr))
-			return n_ptr->addr;
-	}
-	return 0;
-}
-
-void tipc_cltr_attach_node(struct cluster *c_ptr, struct tipc_node *n_ptr)
-{
-	u32 n_num = tipc_node(n_ptr->addr);
-	u32 max_n_num = tipc_max_nodes;
-
-	if (in_own_cluster(n_ptr->addr))
-		max_n_num = tipc_highest_allowed_slave;
-	assert(n_num > 0);
-	assert(n_num <= max_n_num);
-	assert(c_ptr->nodes[n_num] == NULL);
-	c_ptr->nodes[n_num] = n_ptr;
-	if (n_num > c_ptr->highest_node)
-		c_ptr->highest_node = n_num;
-}
-
-/**
- * tipc_cltr_select_router - select router to a cluster
- *
- * Uses deterministic and fair algorithm.
- */
-
-u32 tipc_cltr_select_router(struct cluster *c_ptr, u32 ref)
-{
-	u32 n_num;
-	u32 ulim = c_ptr->highest_node;
-	u32 mask;
-	u32 tstart;
-
-	assert(!in_own_cluster(c_ptr->addr));
-	if (!ulim)
-		return 0;
-
-	/* Start entry must be random */
-	mask = tipc_max_nodes;
-	while (mask > ulim)
-		mask >>= 1;
-	tstart = ref & mask;
-	n_num = tstart;
-
-	/* Lookup upwards with wrap-around */
-	do {
-		if (tipc_node_is_up(c_ptr->nodes[n_num]))
-			break;
-	} while (++n_num <= ulim);
-	if (n_num > ulim) {
-		n_num = 1;
-		do {
-			if (tipc_node_is_up(c_ptr->nodes[n_num]))
-				break;
-		} while (++n_num < tstart);
-		if (n_num == tstart)
-			return 0;
-	}
-	assert(n_num <= ulim);
-	return tipc_node_select_router(c_ptr->nodes[n_num], ref);
-}
-
-/**
- * tipc_cltr_select_node - select destination node within a remote cluster
- *
- * Uses deterministic and fair algorithm.
- */
-
-struct tipc_node *tipc_cltr_select_node(struct cluster *c_ptr, u32 selector)
-{
-	u32 n_num;
-	u32 mask = tipc_max_nodes;
-	u32 start_entry;
-
-	assert(!in_own_cluster(c_ptr->addr));
-	if (!c_ptr->highest_node)
-		return NULL;
-
-	/* Start entry must be random */
-	while (mask > c_ptr->highest_node) {
-		mask >>= 1;
-	}
-	start_entry = (selector & mask) ? selector & mask : 1u;
-	assert(start_entry <= c_ptr->highest_node);
-
-	/* Lookup upwards with wrap-around */
-	for (n_num = start_entry; n_num <= c_ptr->highest_node; n_num++) {
-		if (tipc_node_has_active_links(c_ptr->nodes[n_num]))
-			return c_ptr->nodes[n_num];
-	}
-	for (n_num = 1; n_num < start_entry; n_num++) {
-		if (tipc_node_has_active_links(c_ptr->nodes[n_num]))
-			return c_ptr->nodes[n_num];
-	}
-	return NULL;
-}
-
-/*
- *    Routing table management: See description in node.c
- */
-
-static struct sk_buff *tipc_cltr_prepare_routing_msg(u32 data_size, u32 dest)
-{
-	u32 size = INT_H_SIZE + data_size;
-	struct sk_buff *buf = buf_acquire(size);
-	struct tipc_msg *msg;
-
-	if (buf) {
-		msg = buf_msg(buf);
-		memset((char *)msg, 0, size);
-		msg_init(msg, ROUTE_DISTRIBUTOR, 0, INT_H_SIZE, dest);
-	}
-	return buf;
-}
-
-void tipc_cltr_bcast_new_route(struct cluster *c_ptr, u32 dest,
-			     u32 lower, u32 upper)
-{
-	struct sk_buff *buf = tipc_cltr_prepare_routing_msg(0, c_ptr->addr);
-	struct tipc_msg *msg;
-
-	if (buf) {
-		msg = buf_msg(buf);
-		msg_set_remote_node(msg, dest);
-		msg_set_type(msg, ROUTE_ADDITION);
-		tipc_cltr_multicast(c_ptr, buf, lower, upper);
-	} else {
-		warn("Memory squeeze: broadcast of new route failed\n");
-	}
-}
-
-void tipc_cltr_bcast_lost_route(struct cluster *c_ptr, u32 dest,
-				u32 lower, u32 upper)
-{
-	struct sk_buff *buf = tipc_cltr_prepare_routing_msg(0, c_ptr->addr);
-	struct tipc_msg *msg;
-
-	if (buf) {
-		msg = buf_msg(buf);
-		msg_set_remote_node(msg, dest);
-		msg_set_type(msg, ROUTE_REMOVAL);
-		tipc_cltr_multicast(c_ptr, buf, lower, upper);
-	} else {
-		warn("Memory squeeze: broadcast of lost route failed\n");
-	}
-}
-
-void tipc_cltr_send_slave_routes(struct cluster *c_ptr, u32 dest)
-{
-	struct sk_buff *buf;
-	struct tipc_msg *msg;
-	u32 highest = c_ptr->highest_slave;
-	u32 n_num;
-	int send = 0;
-
-	assert(!is_slave(dest));
-	assert(in_own_cluster(dest));
-	assert(in_own_cluster(c_ptr->addr));
-	if (highest <= LOWEST_SLAVE)
-		return;
-	buf = tipc_cltr_prepare_routing_msg(highest - LOWEST_SLAVE + 1,
-					    c_ptr->addr);
-	if (buf) {
-		msg = buf_msg(buf);
-		msg_set_remote_node(msg, c_ptr->addr);
-		msg_set_type(msg, SLAVE_ROUTING_TABLE);
-		for (n_num = LOWEST_SLAVE; n_num <= highest; n_num++) {
-			if (c_ptr->nodes[n_num] &&
-			    tipc_node_has_active_links(c_ptr->nodes[n_num])) {
-				send = 1;
-				msg_set_dataoctet(msg, n_num);
-			}
-		}
-		if (send)
-			tipc_link_send(buf, dest, dest);
-		else
-			buf_discard(buf);
-	} else {
-		warn("Memory squeeze: broadcast of lost route failed\n");
-	}
-}
-
-void tipc_cltr_send_ext_routes(struct cluster *c_ptr, u32 dest)
-{
-	struct sk_buff *buf;
-	struct tipc_msg *msg;
-	u32 highest = c_ptr->highest_node;
-	u32 n_num;
-	int send = 0;
-
-	if (in_own_cluster(c_ptr->addr))
-		return;
-	assert(!is_slave(dest));
-	assert(in_own_cluster(dest));
-	highest = c_ptr->highest_node;
-	buf = tipc_cltr_prepare_routing_msg(highest + 1, c_ptr->addr);
-	if (buf) {
-		msg = buf_msg(buf);
-		msg_set_remote_node(msg, c_ptr->addr);
-		msg_set_type(msg, EXT_ROUTING_TABLE);
-		for (n_num = 1; n_num <= highest; n_num++) {
-			if (c_ptr->nodes[n_num] &&
-			    tipc_node_has_active_links(c_ptr->nodes[n_num])) {
-				send = 1;
-				msg_set_dataoctet(msg, n_num);
-			}
-		}
-		if (send)
-			tipc_link_send(buf, dest, dest);
-		else
-			buf_discard(buf);
-	} else {
-		warn("Memory squeeze: broadcast of external route failed\n");
-	}
-}
-
-void tipc_cltr_send_local_routes(struct cluster *c_ptr, u32 dest)
-{
-	struct sk_buff *buf;
-	struct tipc_msg *msg;
-	u32 highest = c_ptr->highest_node;
-	u32 n_num;
-	int send = 0;
-
-	assert(is_slave(dest));
-	assert(in_own_cluster(c_ptr->addr));
-	buf = tipc_cltr_prepare_routing_msg(highest, c_ptr->addr);
-	if (buf) {
-		msg = buf_msg(buf);
-		msg_set_remote_node(msg, c_ptr->addr);
-		msg_set_type(msg, LOCAL_ROUTING_TABLE);
-		for (n_num = 1; n_num <= highest; n_num++) {
-			if (c_ptr->nodes[n_num] &&
-			    tipc_node_has_active_links(c_ptr->nodes[n_num])) {
-				send = 1;
-				msg_set_dataoctet(msg, n_num);
-			}
-		}
-		if (send)
-			tipc_link_send(buf, dest, dest);
-		else
-			buf_discard(buf);
-	} else {
-		warn("Memory squeeze: broadcast of local route failed\n");
-	}
-}
-
-void tipc_cltr_recv_routing_table(struct sk_buff *buf)
-{
-	struct tipc_msg *msg = buf_msg(buf);
-	struct cluster *c_ptr;
-	struct tipc_node *n_ptr;
-	unchar *node_table;
-	u32 table_size;
-	u32 router;
-	u32 rem_node = msg_remote_node(msg);
-	u32 z_num;
-	u32 c_num;
-	u32 n_num;
-
-	c_ptr = tipc_cltr_find(rem_node);
-	if (!c_ptr) {
-		c_ptr = tipc_cltr_create(rem_node);
-		if (!c_ptr) {
-			buf_discard(buf);
-			return;
-		}
-	}
-
-	node_table = buf->data + msg_hdr_sz(msg);
-	table_size = msg_size(msg) - msg_hdr_sz(msg);
-	router = msg_prevnode(msg);
-	z_num = tipc_zone(rem_node);
-	c_num = tipc_cluster(rem_node);
-
-	switch (msg_type(msg)) {
-	case LOCAL_ROUTING_TABLE:
-		assert(is_slave(tipc_own_addr));
-	case EXT_ROUTING_TABLE:
-		for (n_num = 1; n_num < table_size; n_num++) {
-			if (node_table[n_num]) {
-				u32 addr = tipc_addr(z_num, c_num, n_num);
-				n_ptr = c_ptr->nodes[n_num];
-				if (!n_ptr) {
-					n_ptr = tipc_node_create(addr);
-				}
-				if (n_ptr)
-					tipc_node_add_router(n_ptr, router);
-			}
-		}
-		break;
-	case SLAVE_ROUTING_TABLE:
-		assert(!is_slave(tipc_own_addr));
-		assert(in_own_cluster(c_ptr->addr));
-		for (n_num = 1; n_num < table_size; n_num++) {
-			if (node_table[n_num]) {
-				u32 slave_num = n_num + LOWEST_SLAVE;
-				u32 addr = tipc_addr(z_num, c_num, slave_num);
-				n_ptr = c_ptr->nodes[slave_num];
-				if (!n_ptr) {
-					n_ptr = tipc_node_create(addr);
-				}
-				if (n_ptr)
-					tipc_node_add_router(n_ptr, router);
-			}
-		}
-		break;
-	case ROUTE_ADDITION:
-		if (!is_slave(tipc_own_addr)) {
-			assert(!in_own_cluster(c_ptr->addr)
-			       || is_slave(rem_node));
-		} else {
-			assert(in_own_cluster(c_ptr->addr)
-			       && !is_slave(rem_node));
-		}
-		n_ptr = c_ptr->nodes[tipc_node(rem_node)];
-		if (!n_ptr)
-			n_ptr = tipc_node_create(rem_node);
-		if (n_ptr)
-			tipc_node_add_router(n_ptr, router);
-		break;
-	case ROUTE_REMOVAL:
-		if (!is_slave(tipc_own_addr)) {
-			assert(!in_own_cluster(c_ptr->addr)
-			       || is_slave(rem_node));
-		} else {
-			assert(in_own_cluster(c_ptr->addr)
-			       && !is_slave(rem_node));
-		}
-		n_ptr = c_ptr->nodes[tipc_node(rem_node)];
-		if (n_ptr)
-			tipc_node_remove_router(n_ptr, router);
-		break;
-	default:
-		assert(!"Illegal routing manager message received\n");
-	}
-	buf_discard(buf);
-}
-
-void tipc_cltr_remove_as_router(struct cluster *c_ptr, u32 router)
-{
-	u32 start_entry;
-	u32 tstop;
-	u32 n_num;
-
-	if (is_slave(router))
-		return;	/* Slave nodes can not be routers */
-
-	if (in_own_cluster(c_ptr->addr)) {
-		start_entry = LOWEST_SLAVE;
-		tstop = c_ptr->highest_slave;
-	} else {
-		start_entry = 1;
-		tstop = c_ptr->highest_node;
-	}
-
-	for (n_num = start_entry; n_num <= tstop; n_num++) {
-		if (c_ptr->nodes[n_num]) {
-			tipc_node_remove_router(c_ptr->nodes[n_num], router);
-		}
-	}
-}
-
-/**
- * tipc_cltr_multicast - multicast message to local nodes
- */
-
-static void tipc_cltr_multicast(struct cluster *c_ptr, struct sk_buff *buf,
-			 u32 lower, u32 upper)
-{
-	struct sk_buff *buf_copy;
-	struct tipc_node *n_ptr;
-	u32 n_num;
-	u32 tstop;
-
-	assert(lower <= upper);
-	assert(((lower >= 1) && (lower <= tipc_max_nodes)) ||
-	       ((lower >= LOWEST_SLAVE) && (lower <= tipc_highest_allowed_slave)));
-	assert(((upper >= 1) && (upper <= tipc_max_nodes)) ||
-	       ((upper >= LOWEST_SLAVE) && (upper <= tipc_highest_allowed_slave)));
-	assert(in_own_cluster(c_ptr->addr));
-
-	tstop = is_slave(upper) ? c_ptr->highest_slave : c_ptr->highest_node;
-	if (tstop > upper)
-		tstop = upper;
-	for (n_num = lower; n_num <= tstop; n_num++) {
-		n_ptr = c_ptr->nodes[n_num];
-		if (n_ptr && tipc_node_has_active_links(n_ptr)) {
-			buf_copy = skb_copy(buf, GFP_ATOMIC);
-			if (buf_copy == NULL)
-				break;
-			msg_set_destnode(buf_msg(buf_copy), n_ptr->addr);
-			tipc_link_send(buf_copy, n_ptr->addr, n_ptr->addr);
-		}
-	}
-	buf_discard(buf);
-}
-
-/**
- * tipc_cltr_broadcast - broadcast message to all nodes within cluster
- */
-
-void tipc_cltr_broadcast(struct sk_buff *buf)
-{
-	struct sk_buff *buf_copy;
-	struct cluster *c_ptr;
-	struct tipc_node *n_ptr;
-	u32 n_num;
-	u32 tstart;
-	u32 tstop;
-	u32 node_type;
-
-	if (tipc_mode == TIPC_NET_MODE) {
-		c_ptr = tipc_cltr_find(tipc_own_addr);
-		assert(in_own_cluster(c_ptr->addr));	/* For now */
-
-		/* Send to standard nodes, then repeat loop sending to slaves */
-		tstart = 1;
-		tstop = c_ptr->highest_node;
-		for (node_type = 1; node_type <= 2; node_type++) {
-			for (n_num = tstart; n_num <= tstop; n_num++) {
-				n_ptr = c_ptr->nodes[n_num];
-				if (n_ptr && tipc_node_has_active_links(n_ptr)) {
-					buf_copy = skb_copy(buf, GFP_ATOMIC);
-					if (buf_copy == NULL)
-						goto exit;
-					msg_set_destnode(buf_msg(buf_copy),
-							 n_ptr->addr);
-					tipc_link_send(buf_copy, n_ptr->addr,
-						       n_ptr->addr);
-				}
-			}
-			tstart = LOWEST_SLAVE;
-			tstop = c_ptr->highest_slave;
-		}
-	}
-exit:
-	buf_discard(buf);
-}
-
-int tipc_cltr_init(void)
-{
-	tipc_highest_allowed_slave = LOWEST_SLAVE + tipc_max_slaves;
-	return tipc_cltr_create(tipc_own_addr) ? 0 : -ENOMEM;
-}
-
diff -ruN linux-2.6.29/net/tipc/cluster.h android_cluster/linux-2.6.29/net/tipc/cluster.h
--- linux-2.6.29/net/tipc/cluster.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/cluster.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,92 +0,0 @@
-/*
- * net/tipc/cluster.h: Include file for TIPC cluster management routines
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_CLUSTER_H
-#define _TIPC_CLUSTER_H
-
-#include "addr.h"
-#include "zone.h"
-
-#define LOWEST_SLAVE  2048u
-
-/**
- * struct cluster - TIPC cluster structure
- * @addr: network address of cluster
- * @owner: pointer to zone that cluster belongs to
- * @nodes: array of pointers to all nodes within cluster
- * @highest_node: id of highest numbered node within cluster
- * @highest_slave: (used for secondary node support)
- */
-
-struct cluster {
-	u32 addr;
-	struct _zone *owner;
-	struct tipc_node **nodes;
-	u32 highest_node;
-	u32 highest_slave;
-};
-
-
-extern struct tipc_node **tipc_local_nodes;
-extern u32 tipc_highest_allowed_slave;
-extern struct tipc_node_map tipc_cltr_bcast_nodes;
-
-void tipc_cltr_remove_as_router(struct cluster *c_ptr, u32 router);
-void tipc_cltr_send_ext_routes(struct cluster *c_ptr, u32 dest);
-struct tipc_node *tipc_cltr_select_node(struct cluster *c_ptr, u32 selector);
-u32 tipc_cltr_select_router(struct cluster *c_ptr, u32 ref);
-void tipc_cltr_recv_routing_table(struct sk_buff *buf);
-struct cluster *tipc_cltr_create(u32 addr);
-void tipc_cltr_delete(struct cluster *c_ptr);
-void tipc_cltr_attach_node(struct cluster *c_ptr, struct tipc_node *n_ptr);
-void tipc_cltr_send_slave_routes(struct cluster *c_ptr, u32 dest);
-void tipc_cltr_broadcast(struct sk_buff *buf);
-int tipc_cltr_init(void);
-u32 tipc_cltr_next_node(struct cluster *c_ptr, u32 addr);
-void tipc_cltr_bcast_new_route(struct cluster *c_ptr, u32 dest, u32 lo, u32 hi);
-void tipc_cltr_send_local_routes(struct cluster *c_ptr, u32 dest);
-void tipc_cltr_bcast_lost_route(struct cluster *c_ptr, u32 dest, u32 lo, u32 hi);
-
-static inline struct cluster *tipc_cltr_find(u32 addr)
-{
-	struct _zone *z_ptr = tipc_zone_find(addr);
-
-	if (z_ptr)
-		return z_ptr->clusters[1];
-	return NULL;
-}
-
-#endif
diff -ruN linux-2.6.29/net/tipc/config.c android_cluster/linux-2.6.29/net/tipc/config.c
--- linux-2.6.29/net/tipc/config.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/config.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,705 +0,0 @@
-/*
- * net/tipc/config.c: TIPC configuration management code
- *
- * Copyright (c) 2002-2006, Ericsson AB
- * Copyright (c) 2004-2007, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "dbg.h"
-#include "bearer.h"
-#include "port.h"
-#include "link.h"
-#include "zone.h"
-#include "addr.h"
-#include "name_table.h"
-#include "node.h"
-#include "config.h"
-#include "discover.h"
-
-struct subscr_data {
-	char usr_handle[8];
-	u32 domain;
-	u32 port_ref;
-	struct list_head subd_list;
-};
-
-struct manager {
-	u32 user_ref;
-	u32 port_ref;
-	u32 subscr_ref;
-	u32 link_subscriptions;
-	struct list_head link_subscribers;
-};
-
-static struct manager mng = { 0};
-
-static DEFINE_SPINLOCK(config_lock);
-
-static const void *req_tlv_area;	/* request message TLV area */
-static int req_tlv_space;		/* request message TLV area size */
-static int rep_headroom;		/* reply message headroom to use */
-
-
-void tipc_cfg_link_event(u32 addr, char *name, int up)
-{
-	/* TIPC DOESN'T HANDLE LINK EVENT SUBSCRIPTIONS AT THE MOMENT */
-}
-
-
-struct sk_buff *tipc_cfg_reply_alloc(int payload_size)
-{
-	struct sk_buff *buf;
-
-	buf = alloc_skb(rep_headroom + payload_size, GFP_ATOMIC);
-	if (buf)
-		skb_reserve(buf, rep_headroom);
-	return buf;
-}
-
-int tipc_cfg_append_tlv(struct sk_buff *buf, int tlv_type,
-			void *tlv_data, int tlv_data_size)
-{
-	struct tlv_desc *tlv = (struct tlv_desc *)skb_tail_pointer(buf);
-	int new_tlv_space = TLV_SPACE(tlv_data_size);
-
-	if (skb_tailroom(buf) < new_tlv_space) {
-		dbg("tipc_cfg_append_tlv unable to append TLV\n");
-		return 0;
-	}
-	skb_put(buf, new_tlv_space);
-	tlv->tlv_type = htons(tlv_type);
-	tlv->tlv_len  = htons(TLV_LENGTH(tlv_data_size));
-	if (tlv_data_size && tlv_data)
-		memcpy(TLV_DATA(tlv), tlv_data, tlv_data_size);
-	return 1;
-}
-
-struct sk_buff *tipc_cfg_reply_unsigned_type(u16 tlv_type, u32 value)
-{
-	struct sk_buff *buf;
-	__be32 value_net;
-
-	buf = tipc_cfg_reply_alloc(TLV_SPACE(sizeof(value)));
-	if (buf) {
-		value_net = htonl(value);
-		tipc_cfg_append_tlv(buf, tlv_type, &value_net,
-				    sizeof(value_net));
-	}
-	return buf;
-}
-
-struct sk_buff *tipc_cfg_reply_string_type(u16 tlv_type, char *string)
-{
-	struct sk_buff *buf;
-	int string_len = strlen(string) + 1;
-
-	buf = tipc_cfg_reply_alloc(TLV_SPACE(string_len));
-	if (buf)
-		tipc_cfg_append_tlv(buf, tlv_type, string, string_len);
-	return buf;
-}
-
-
-
-
-#if 0
-
-/* Now obsolete code for handling commands not yet implemented the new way */
-
-int tipc_cfg_cmd(const struct tipc_cmd_msg * msg,
-		 char *data,
-		 u32 sz,
-		 u32 *ret_size,
-		 struct tipc_portid *orig)
-{
-	int rv = -EINVAL;
-	u32 cmd = msg->cmd;
-
-	*ret_size = 0;
-	switch (cmd) {
-	case TIPC_REMOVE_LINK:
-	case TIPC_CMD_BLOCK_LINK:
-	case TIPC_CMD_UNBLOCK_LINK:
-		if (!cfg_check_connection(orig))
-			rv = link_control(msg->argv.link_name, msg->cmd, 0);
-		break;
-	case TIPC_ESTABLISH:
-		{
-			int connected;
-
-			tipc_isconnected(mng.conn_port_ref, &connected);
-			if (connected || !orig) {
-				rv = TIPC_FAILURE;
-				break;
-			}
-			rv = tipc_connect2port(mng.conn_port_ref, orig);
-			if (rv == TIPC_OK)
-				orig = 0;
-			break;
-		}
-	case TIPC_GET_PEER_ADDRESS:
-		*ret_size = link_peer_addr(msg->argv.link_name, data, sz);
-		break;
-	case TIPC_GET_ROUTES:
-		rv = TIPC_OK;
-		break;
-	default: {}
-	}
-	if (*ret_size)
-		rv = TIPC_OK;
-	return rv;
-}
-
-static void cfg_cmd_event(struct tipc_cmd_msg *msg,
-			  char *data,
-			  u32 sz,
-			  struct tipc_portid const *orig)
-{
-	int rv = -EINVAL;
-	struct tipc_cmd_result_msg rmsg;
-	struct iovec msg_sect[2];
-	int *arg;
-
-	msg->cmd = ntohl(msg->cmd);
-
-	cfg_prepare_res_msg(msg->cmd, msg->usr_handle, rv, &rmsg, msg_sect,
-			    data, 0);
-	if (ntohl(msg->magic) != TIPC_MAGIC)
-		goto exit;
-
-	switch (msg->cmd) {
-	case TIPC_CREATE_LINK:
-		if (!cfg_check_connection(orig))
-			rv = disc_create_link(&msg->argv.create_link);
-		break;
-	case TIPC_LINK_SUBSCRIBE:
-		{
-			struct subscr_data *sub;
-
-			if (mng.link_subscriptions > 64)
-				break;
-			sub = kmalloc(sizeof(*sub),
-							    GFP_ATOMIC);
-			if (sub == NULL) {
-				warn("Memory squeeze; dropped remote link subscription\n");
-				break;
-			}
-			INIT_LIST_HEAD(&sub->subd_list);
-			tipc_createport(mng.user_ref,
-					(void *)sub,
-					TIPC_HIGH_IMPORTANCE,
-					0,
-					0,
-					(tipc_conn_shutdown_event)cfg_linksubscr_cancel,
-					0,
-					0,
-					(tipc_conn_msg_event)cfg_linksubscr_cancel,
-					0,
-					&sub->port_ref);
-			if (!sub->port_ref) {
-				kfree(sub);
-				break;
-			}
-			memcpy(sub->usr_handle,msg->usr_handle,
-			       sizeof(sub->usr_handle));
-			sub->domain = msg->argv.domain;
-			list_add_tail(&sub->subd_list, &mng.link_subscribers);
-			tipc_connect2port(sub->port_ref, orig);
-			rmsg.retval = TIPC_OK;
-			tipc_send(sub->port_ref, 2u, msg_sect);
-			mng.link_subscriptions++;
-			return;
-		}
-	default:
-		rv = tipc_cfg_cmd(msg, data, sz, (u32 *)&msg_sect[1].iov_len, orig);
-	}
-	exit:
-	rmsg.result_len = htonl(msg_sect[1].iov_len);
-	rmsg.retval = htonl(rv);
-	tipc_cfg_respond(msg_sect, 2u, orig);
-}
-#endif
-
-static struct sk_buff *cfg_enable_bearer(void)
-{
-	struct tipc_bearer_config *args;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_BEARER_CONFIG))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	args = (struct tipc_bearer_config *)TLV_DATA(req_tlv_area);
-	if (tipc_enable_bearer(args->name,
-			       ntohl(args->detect_scope),
-			       ntohl(args->priority)))
-		return tipc_cfg_reply_error_string("unable to enable bearer");
-
-	return tipc_cfg_reply_none();
-}
-
-static struct sk_buff *cfg_disable_bearer(void)
-{
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_BEARER_NAME))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	if (tipc_disable_bearer((char *)TLV_DATA(req_tlv_area)))
-		return tipc_cfg_reply_error_string("unable to disable bearer");
-
-	return tipc_cfg_reply_none();
-}
-
-static struct sk_buff *cfg_set_own_addr(void)
-{
-	u32 addr;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_NET_ADDR))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	addr = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
-	if (addr == tipc_own_addr)
-		return tipc_cfg_reply_none();
-	if (!tipc_addr_node_valid(addr))
-		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
-						   " (node address)");
-	if (tipc_mode == TIPC_NET_MODE)
-		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
-						   " (cannot change node address once assigned)");
-
-	/*
-	 * Must release all spinlocks before calling start_net() because
-	 * Linux version of TIPC calls eth_media_start() which calls
-	 * register_netdevice_notifier() which may block!
-	 *
-	 * Temporarily releasing the lock should be harmless for non-Linux TIPC,
-	 * but Linux version of eth_media_start() should really be reworked
-	 * so that it can be called with spinlocks held.
-	 */
-
-	spin_unlock_bh(&config_lock);
-	tipc_core_start_net(addr);
-	spin_lock_bh(&config_lock);
-	return tipc_cfg_reply_none();
-}
-
-static struct sk_buff *cfg_set_remote_mng(void)
-{
-	u32 value;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
-	tipc_remote_management = (value != 0);
-	return tipc_cfg_reply_none();
-}
-
-static struct sk_buff *cfg_set_max_publications(void)
-{
-	u32 value;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
-	if (value != delimit(value, 1, 65535))
-		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
-						   " (max publications must be 1-65535)");
-	tipc_max_publications = value;
-	return tipc_cfg_reply_none();
-}
-
-static struct sk_buff *cfg_set_max_subscriptions(void)
-{
-	u32 value;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
-	if (value != delimit(value, 1, 65535))
-		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
-						   " (max subscriptions must be 1-65535");
-	tipc_max_subscriptions = value;
-	return tipc_cfg_reply_none();
-}
-
-static struct sk_buff *cfg_set_max_ports(void)
-{
-	u32 value;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
-	if (value == tipc_max_ports)
-		return tipc_cfg_reply_none();
-	if (value != delimit(value, 127, 65535))
-		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
-						   " (max ports must be 127-65535)");
-	if (tipc_mode != TIPC_NOT_RUNNING)
-		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
-			" (cannot change max ports while TIPC is active)");
-	tipc_max_ports = value;
-	return tipc_cfg_reply_none();
-}
-
-static struct sk_buff *cfg_set_max_zones(void)
-{
-	u32 value;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
-	if (value == tipc_max_zones)
-		return tipc_cfg_reply_none();
-	if (value != delimit(value, 1, 255))
-		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
-						   " (max zones must be 1-255)");
-	if (tipc_mode == TIPC_NET_MODE)
-		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
-			" (cannot change max zones once TIPC has joined a network)");
-	tipc_max_zones = value;
-	return tipc_cfg_reply_none();
-}
-
-static struct sk_buff *cfg_set_max_clusters(void)
-{
-	u32 value;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
-	if (value != delimit(value, 1, 1))
-		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
-						   " (max clusters fixed at 1)");
-	return tipc_cfg_reply_none();
-}
-
-static struct sk_buff *cfg_set_max_nodes(void)
-{
-	u32 value;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
-	if (value == tipc_max_nodes)
-		return tipc_cfg_reply_none();
-	if (value != delimit(value, 8, 2047))
-		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
-						   " (max nodes must be 8-2047)");
-	if (tipc_mode == TIPC_NET_MODE)
-		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
-			" (cannot change max nodes once TIPC has joined a network)");
-	tipc_max_nodes = value;
-	return tipc_cfg_reply_none();
-}
-
-static struct sk_buff *cfg_set_max_slaves(void)
-{
-	u32 value;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
-	if (value != 0)
-		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
-						   " (max secondary nodes fixed at 0)");
-	return tipc_cfg_reply_none();
-}
-
-static struct sk_buff *cfg_set_netid(void)
-{
-	u32 value;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
-	if (value == tipc_net_id)
-		return tipc_cfg_reply_none();
-	if (value != delimit(value, 1, 9999))
-		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
-						   " (network id must be 1-9999)");
-	if (tipc_mode == TIPC_NET_MODE)
-		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
-			" (cannot change network id once TIPC has joined a network)");
-	tipc_net_id = value;
-	return tipc_cfg_reply_none();
-}
-
-struct sk_buff *tipc_cfg_do_cmd(u32 orig_node, u16 cmd, const void *request_area,
-				int request_space, int reply_headroom)
-{
-	struct sk_buff *rep_tlv_buf;
-
-	spin_lock_bh(&config_lock);
-
-	/* Save request and reply details in a well-known location */
-
-	req_tlv_area = request_area;
-	req_tlv_space = request_space;
-	rep_headroom = reply_headroom;
-
-	/* Check command authorization */
-
-	if (likely(orig_node == tipc_own_addr)) {
-		/* command is permitted */
-	} else if (cmd >= 0x8000) {
-		rep_tlv_buf = tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
-							  " (cannot be done remotely)");
-		goto exit;
-	} else if (!tipc_remote_management) {
-		rep_tlv_buf = tipc_cfg_reply_error_string(TIPC_CFG_NO_REMOTE);
-		goto exit;
-	}
-	else if (cmd >= 0x4000) {
-		u32 domain = 0;
-
-		if ((tipc_nametbl_translate(TIPC_ZM_SRV, 0, &domain) == 0) ||
-		    (domain != orig_node)) {
-			rep_tlv_buf = tipc_cfg_reply_error_string(TIPC_CFG_NOT_ZONE_MSTR);
-			goto exit;
-		}
-	}
-
-	/* Call appropriate processing routine */
-
-	switch (cmd) {
-	case TIPC_CMD_NOOP:
-		rep_tlv_buf = tipc_cfg_reply_none();
-		break;
-	case TIPC_CMD_GET_NODES:
-		rep_tlv_buf = tipc_node_get_nodes(req_tlv_area, req_tlv_space);
-		break;
-	case TIPC_CMD_GET_LINKS:
-		rep_tlv_buf = tipc_node_get_links(req_tlv_area, req_tlv_space);
-		break;
-	case TIPC_CMD_SHOW_LINK_STATS:
-		rep_tlv_buf = tipc_link_cmd_show_stats(req_tlv_area, req_tlv_space);
-		break;
-	case TIPC_CMD_RESET_LINK_STATS:
-		rep_tlv_buf = tipc_link_cmd_reset_stats(req_tlv_area, req_tlv_space);
-		break;
-	case TIPC_CMD_SHOW_NAME_TABLE:
-		rep_tlv_buf = tipc_nametbl_get(req_tlv_area, req_tlv_space);
-		break;
-	case TIPC_CMD_GET_BEARER_NAMES:
-		rep_tlv_buf = tipc_bearer_get_names();
-		break;
-	case TIPC_CMD_GET_MEDIA_NAMES:
-		rep_tlv_buf = tipc_media_get_names();
-		break;
-	case TIPC_CMD_SHOW_PORTS:
-		rep_tlv_buf = tipc_port_get_ports();
-		break;
-#if 0
-	case TIPC_CMD_SHOW_PORT_STATS:
-		rep_tlv_buf = port_show_stats(req_tlv_area, req_tlv_space);
-		break;
-	case TIPC_CMD_RESET_PORT_STATS:
-		rep_tlv_buf = tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED);
-		break;
-#endif
-	case TIPC_CMD_SET_LOG_SIZE:
-		rep_tlv_buf = tipc_log_resize_cmd(req_tlv_area, req_tlv_space);
-		break;
-	case TIPC_CMD_DUMP_LOG:
-		rep_tlv_buf = tipc_log_dump();
-		break;
-	case TIPC_CMD_SET_LINK_TOL:
-	case TIPC_CMD_SET_LINK_PRI:
-	case TIPC_CMD_SET_LINK_WINDOW:
-		rep_tlv_buf = tipc_link_cmd_config(req_tlv_area, req_tlv_space, cmd);
-		break;
-	case TIPC_CMD_ENABLE_BEARER:
-		rep_tlv_buf = cfg_enable_bearer();
-		break;
-	case TIPC_CMD_DISABLE_BEARER:
-		rep_tlv_buf = cfg_disable_bearer();
-		break;
-	case TIPC_CMD_SET_NODE_ADDR:
-		rep_tlv_buf = cfg_set_own_addr();
-		break;
-	case TIPC_CMD_SET_REMOTE_MNG:
-		rep_tlv_buf = cfg_set_remote_mng();
-		break;
-	case TIPC_CMD_SET_MAX_PORTS:
-		rep_tlv_buf = cfg_set_max_ports();
-		break;
-	case TIPC_CMD_SET_MAX_PUBL:
-		rep_tlv_buf = cfg_set_max_publications();
-		break;
-	case TIPC_CMD_SET_MAX_SUBSCR:
-		rep_tlv_buf = cfg_set_max_subscriptions();
-		break;
-	case TIPC_CMD_SET_MAX_ZONES:
-		rep_tlv_buf = cfg_set_max_zones();
-		break;
-	case TIPC_CMD_SET_MAX_CLUSTERS:
-		rep_tlv_buf = cfg_set_max_clusters();
-		break;
-	case TIPC_CMD_SET_MAX_NODES:
-		rep_tlv_buf = cfg_set_max_nodes();
-		break;
-	case TIPC_CMD_SET_MAX_SLAVES:
-		rep_tlv_buf = cfg_set_max_slaves();
-		break;
-	case TIPC_CMD_SET_NETID:
-		rep_tlv_buf = cfg_set_netid();
-		break;
-	case TIPC_CMD_GET_REMOTE_MNG:
-		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_remote_management);
-		break;
-	case TIPC_CMD_GET_MAX_PORTS:
-		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_ports);
-		break;
-	case TIPC_CMD_GET_MAX_PUBL:
-		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_publications);
-		break;
-	case TIPC_CMD_GET_MAX_SUBSCR:
-		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_subscriptions);
-		break;
-	case TIPC_CMD_GET_MAX_ZONES:
-		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_zones);
-		break;
-	case TIPC_CMD_GET_MAX_CLUSTERS:
-		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_clusters);
-		break;
-	case TIPC_CMD_GET_MAX_NODES:
-		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_nodes);
-		break;
-	case TIPC_CMD_GET_MAX_SLAVES:
-		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_slaves);
-		break;
-	case TIPC_CMD_GET_NETID:
-		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_net_id);
-		break;
-	case TIPC_CMD_NOT_NET_ADMIN:
-		rep_tlv_buf =
-			tipc_cfg_reply_error_string(TIPC_CFG_NOT_NET_ADMIN);
-		break;
-	default:
-		rep_tlv_buf = tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
-							  " (unknown command)");
-		break;
-	}
-
-	/* Return reply buffer */
-exit:
-	spin_unlock_bh(&config_lock);
-	return rep_tlv_buf;
-}
-
-static void cfg_named_msg_event(void *userdata,
-				u32 port_ref,
-				struct sk_buff **buf,
-				const unchar *msg,
-				u32 size,
-				u32 importance,
-				struct tipc_portid const *orig,
-				struct tipc_name_seq const *dest)
-{
-	struct tipc_cfg_msg_hdr *req_hdr;
-	struct tipc_cfg_msg_hdr *rep_hdr;
-	struct sk_buff *rep_buf;
-
-	/* Validate configuration message header (ignore invalid message) */
-
-	req_hdr = (struct tipc_cfg_msg_hdr *)msg;
-	if ((size < sizeof(*req_hdr)) ||
-	    (size != TCM_ALIGN(ntohl(req_hdr->tcm_len))) ||
-	    (ntohs(req_hdr->tcm_flags) != TCM_F_REQUEST)) {
-		warn("Invalid configuration message discarded\n");
-		return;
-	}
-
-	/* Generate reply for request (if can't, return request) */
-
-	rep_buf = tipc_cfg_do_cmd(orig->node,
-				  ntohs(req_hdr->tcm_type),
-				  msg + sizeof(*req_hdr),
-				  size - sizeof(*req_hdr),
-				  BUF_HEADROOM + MAX_H_SIZE + sizeof(*rep_hdr));
-	if (rep_buf) {
-		skb_push(rep_buf, sizeof(*rep_hdr));
-		rep_hdr = (struct tipc_cfg_msg_hdr *)rep_buf->data;
-		memcpy(rep_hdr, req_hdr, sizeof(*rep_hdr));
-		rep_hdr->tcm_len = htonl(rep_buf->len);
-		rep_hdr->tcm_flags &= htons(~TCM_F_REQUEST);
-	} else {
-		rep_buf = *buf;
-		*buf = NULL;
-	}
-
-	/* NEED TO ADD CODE TO HANDLE FAILED SEND (SUCH AS CONGESTION) */
-	tipc_send_buf2port(port_ref, orig, rep_buf, rep_buf->len);
-}
-
-int tipc_cfg_init(void)
-{
-	struct tipc_name_seq seq;
-	int res;
-
-	memset(&mng, 0, sizeof(mng));
-	INIT_LIST_HEAD(&mng.link_subscribers);
-
-	res = tipc_attach(&mng.user_ref, NULL, NULL);
-	if (res)
-		goto failed;
-
-	res = tipc_createport(mng.user_ref, NULL, TIPC_CRITICAL_IMPORTANCE,
-			      NULL, NULL, NULL,
-			      NULL, cfg_named_msg_event, NULL,
-			      NULL, &mng.port_ref);
-	if (res)
-		goto failed;
-
-	seq.type = TIPC_CFG_SRV;
-	seq.lower = seq.upper = tipc_own_addr;
-	res = tipc_nametbl_publish_rsv(mng.port_ref, TIPC_ZONE_SCOPE, &seq);
-	if (res)
-		goto failed;
-
-	return 0;
-
-failed:
-	err("Unable to create configuration service\n");
-	tipc_detach(mng.user_ref);
-	mng.user_ref = 0;
-	return res;
-}
-
-void tipc_cfg_stop(void)
-{
-	if (mng.user_ref) {
-		tipc_detach(mng.user_ref);
-		mng.user_ref = 0;
-	}
-}
diff -ruN linux-2.6.29/net/tipc/config.h android_cluster/linux-2.6.29/net/tipc/config.h
--- linux-2.6.29/net/tipc/config.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/config.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,79 +0,0 @@
-/*
- * net/tipc/config.h: Include file for TIPC configuration service code
- *
- * Copyright (c) 2003-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_CONFIG_H
-#define _TIPC_CONFIG_H
-
-/* ---------------------------------------------------------------------- */
-
-#include "core.h"
-#include "link.h"
-
-struct sk_buff *tipc_cfg_reply_alloc(int payload_size);
-int tipc_cfg_append_tlv(struct sk_buff *buf, int tlv_type,
-			void *tlv_data, int tlv_data_size);
-struct sk_buff *tipc_cfg_reply_unsigned_type(u16 tlv_type, u32 value);
-struct sk_buff *tipc_cfg_reply_string_type(u16 tlv_type, char *string);
-
-static inline struct sk_buff *tipc_cfg_reply_none(void)
-{
-	return tipc_cfg_reply_alloc(0);
-}
-
-static inline struct sk_buff *tipc_cfg_reply_unsigned(u32 value)
-{
-	return tipc_cfg_reply_unsigned_type(TIPC_TLV_UNSIGNED, value);
-}
-
-static inline struct sk_buff *tipc_cfg_reply_error_string(char *string)
-{
-	return tipc_cfg_reply_string_type(TIPC_TLV_ERROR_STRING, string);
-}
-
-static inline struct sk_buff *tipc_cfg_reply_ultra_string(char *string)
-{
-	return tipc_cfg_reply_string_type(TIPC_TLV_ULTRA_STRING, string);
-}
-
-struct sk_buff *tipc_cfg_do_cmd(u32 orig_node, u16 cmd,
-				const void *req_tlv_area, int req_tlv_space,
-				int headroom);
-
-void tipc_cfg_link_event(u32 addr, char *name, int up);
-int  tipc_cfg_init(void);
-void tipc_cfg_stop(void);
-
-#endif
diff -ruN linux-2.6.29/net/tipc/core.c android_cluster/linux-2.6.29/net/tipc/core.c
--- linux-2.6.29/net/tipc/core.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/core.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,276 +0,0 @@
-/*
- * net/tipc/core.c: TIPC module code
- *
- * Copyright (c) 2003-2006, Ericsson AB
- * Copyright (c) 2005-2006, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include <linux/init.h>
-#include <linux/module.h>
-#include <linux/kernel.h>
-#include <linux/random.h>
-
-#include "core.h"
-#include "dbg.h"
-#include "ref.h"
-#include "net.h"
-#include "user_reg.h"
-#include "name_table.h"
-#include "subscr.h"
-#include "config.h"
-
-
-#define TIPC_MOD_VER "1.6.4"
-
-#ifndef CONFIG_TIPC_ZONES
-#define CONFIG_TIPC_ZONES 3
-#endif
-
-#ifndef CONFIG_TIPC_CLUSTERS
-#define CONFIG_TIPC_CLUSTERS 1
-#endif
-
-#ifndef CONFIG_TIPC_NODES
-#define CONFIG_TIPC_NODES 255
-#endif
-
-#ifndef CONFIG_TIPC_SLAVE_NODES
-#define CONFIG_TIPC_SLAVE_NODES 0
-#endif
-
-#ifndef CONFIG_TIPC_PORTS
-#define CONFIG_TIPC_PORTS 8191
-#endif
-
-#ifndef CONFIG_TIPC_LOG
-#define CONFIG_TIPC_LOG 0
-#endif
-
-/* global variables used by multiple sub-systems within TIPC */
-
-int tipc_mode = TIPC_NOT_RUNNING;
-int tipc_random;
-atomic_t tipc_user_count = ATOMIC_INIT(0);
-
-const char tipc_alphabet[] =
-	"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_.";
-
-/* configurable TIPC parameters */
-
-u32 tipc_own_addr;
-int tipc_max_zones;
-int tipc_max_clusters;
-int tipc_max_nodes;
-int tipc_max_slaves;
-int tipc_max_ports;
-int tipc_max_subscriptions;
-int tipc_max_publications;
-int tipc_net_id;
-int tipc_remote_management;
-
-
-int tipc_get_mode(void)
-{
-	return tipc_mode;
-}
-
-/**
- * tipc_core_stop_net - shut down TIPC networking sub-systems
- */
-
-void tipc_core_stop_net(void)
-{
-	tipc_eth_media_stop();
-	tipc_net_stop();
-}
-
-/**
- * start_net - start TIPC networking sub-systems
- */
-
-int tipc_core_start_net(unsigned long addr)
-{
-	int res;
-
-	if ((res = tipc_net_start(addr)) ||
-	    (res = tipc_eth_media_start())) {
-		tipc_core_stop_net();
-	}
-	return res;
-}
-
-/**
- * tipc_core_stop - switch TIPC from SINGLE NODE to NOT RUNNING mode
- */
-
-void tipc_core_stop(void)
-{
-	if (tipc_mode != TIPC_NODE_MODE)
-		return;
-
-	tipc_mode = TIPC_NOT_RUNNING;
-
-	tipc_netlink_stop();
-	tipc_handler_stop();
-	tipc_cfg_stop();
-	tipc_subscr_stop();
-	tipc_reg_stop();
-	tipc_nametbl_stop();
-	tipc_ref_table_stop();
-	tipc_socket_stop();
-}
-
-/**
- * tipc_core_start - switch TIPC from NOT RUNNING to SINGLE NODE mode
- */
-
-int tipc_core_start(void)
-{
-	int res;
-
-	if (tipc_mode != TIPC_NOT_RUNNING)
-		return -ENOPROTOOPT;
-
-	get_random_bytes(&tipc_random, sizeof(tipc_random));
-	tipc_mode = TIPC_NODE_MODE;
-
-	if ((res = tipc_handler_start()) ||
-	    (res = tipc_ref_table_init(tipc_max_ports, tipc_random)) ||
-	    (res = tipc_reg_start()) ||
-	    (res = tipc_nametbl_init()) ||
-	    (res = tipc_k_signal((Handler)tipc_subscr_start, 0)) ||
-	    (res = tipc_k_signal((Handler)tipc_cfg_init, 0)) ||
-	    (res = tipc_netlink_start()) ||
-	    (res = tipc_socket_init())) {
-		tipc_core_stop();
-	}
-	return res;
-}
-
-
-static int __init tipc_init(void)
-{
-	int res;
-
-	tipc_log_resize(CONFIG_TIPC_LOG);
-	info("Activated (version " TIPC_MOD_VER
-	     " compiled " __DATE__ " " __TIME__ ")\n");
-
-	tipc_own_addr = 0;
-	tipc_remote_management = 1;
-	tipc_max_publications = 10000;
-	tipc_max_subscriptions = 2000;
-	tipc_max_ports = delimit(CONFIG_TIPC_PORTS, 127, 65536);
-	tipc_max_zones = delimit(CONFIG_TIPC_ZONES, 1, 255);
-	tipc_max_clusters = delimit(CONFIG_TIPC_CLUSTERS, 1, 1);
-	tipc_max_nodes = delimit(CONFIG_TIPC_NODES, 8, 2047);
-	tipc_max_slaves = delimit(CONFIG_TIPC_SLAVE_NODES, 0, 2047);
-	tipc_net_id = 4711;
-
-	if ((res = tipc_core_start()))
-		err("Unable to start in single node mode\n");
-	else
-		info("Started in single node mode\n");
-	return res;
-}
-
-static void __exit tipc_exit(void)
-{
-	tipc_core_stop_net();
-	tipc_core_stop();
-	info("Deactivated\n");
-	tipc_log_resize(0);
-}
-
-module_init(tipc_init);
-module_exit(tipc_exit);
-
-MODULE_DESCRIPTION("TIPC: Transparent Inter Process Communication");
-MODULE_LICENSE("Dual BSD/GPL");
-MODULE_VERSION(TIPC_MOD_VER);
-
-/* Native TIPC API for kernel-space applications (see tipc.h) */
-
-EXPORT_SYMBOL(tipc_attach);
-EXPORT_SYMBOL(tipc_detach);
-EXPORT_SYMBOL(tipc_get_addr);
-EXPORT_SYMBOL(tipc_get_mode);
-EXPORT_SYMBOL(tipc_createport);
-EXPORT_SYMBOL(tipc_deleteport);
-EXPORT_SYMBOL(tipc_ownidentity);
-EXPORT_SYMBOL(tipc_portimportance);
-EXPORT_SYMBOL(tipc_set_portimportance);
-EXPORT_SYMBOL(tipc_portunreliable);
-EXPORT_SYMBOL(tipc_set_portunreliable);
-EXPORT_SYMBOL(tipc_portunreturnable);
-EXPORT_SYMBOL(tipc_set_portunreturnable);
-EXPORT_SYMBOL(tipc_publish);
-EXPORT_SYMBOL(tipc_withdraw);
-EXPORT_SYMBOL(tipc_connect2port);
-EXPORT_SYMBOL(tipc_disconnect);
-EXPORT_SYMBOL(tipc_shutdown);
-EXPORT_SYMBOL(tipc_isconnected);
-EXPORT_SYMBOL(tipc_peer);
-EXPORT_SYMBOL(tipc_ref_valid);
-EXPORT_SYMBOL(tipc_send);
-EXPORT_SYMBOL(tipc_send_buf);
-EXPORT_SYMBOL(tipc_send2name);
-EXPORT_SYMBOL(tipc_forward2name);
-EXPORT_SYMBOL(tipc_send_buf2name);
-EXPORT_SYMBOL(tipc_forward_buf2name);
-EXPORT_SYMBOL(tipc_send2port);
-EXPORT_SYMBOL(tipc_forward2port);
-EXPORT_SYMBOL(tipc_send_buf2port);
-EXPORT_SYMBOL(tipc_forward_buf2port);
-EXPORT_SYMBOL(tipc_multicast);
-/* EXPORT_SYMBOL(tipc_multicast_buf); not available yet */
-EXPORT_SYMBOL(tipc_ispublished);
-EXPORT_SYMBOL(tipc_available_nodes);
-
-/* TIPC API for external bearers (see tipc_bearer.h) */
-
-EXPORT_SYMBOL(tipc_block_bearer);
-EXPORT_SYMBOL(tipc_continue);
-EXPORT_SYMBOL(tipc_disable_bearer);
-EXPORT_SYMBOL(tipc_enable_bearer);
-EXPORT_SYMBOL(tipc_recv_msg);
-EXPORT_SYMBOL(tipc_register_media);
-
-/* TIPC API for external APIs (see tipc_port.h) */
-
-EXPORT_SYMBOL(tipc_createport_raw);
-EXPORT_SYMBOL(tipc_reject_msg);
-EXPORT_SYMBOL(tipc_send_buf_fast);
-EXPORT_SYMBOL(tipc_acknowledge);
-EXPORT_SYMBOL(tipc_get_port);
-EXPORT_SYMBOL(tipc_get_handle);
-
diff -ruN linux-2.6.29/net/tipc/core.h android_cluster/linux-2.6.29/net/tipc/core.h
--- linux-2.6.29/net/tipc/core.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/core.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,375 +0,0 @@
-/*
- * net/tipc/core.h: Include file for TIPC global declarations
- *
- * Copyright (c) 2005-2006, Ericsson AB
- * Copyright (c) 2005-2007, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_CORE_H
-#define _TIPC_CORE_H
-
-#include <linux/tipc.h>
-#include <linux/tipc_config.h>
-#include <net/tipc/tipc_msg.h>
-#include <net/tipc/tipc_port.h>
-#include <net/tipc/tipc_bearer.h>
-#include <net/tipc/tipc.h>
-#include <linux/types.h>
-#include <linux/kernel.h>
-#include <linux/errno.h>
-#include <linux/mm.h>
-#include <linux/timer.h>
-#include <linux/string.h>
-#include <asm/uaccess.h>
-#include <linux/interrupt.h>
-#include <asm/atomic.h>
-#include <asm/hardirq.h>
-#include <linux/netdevice.h>
-#include <linux/in.h>
-#include <linux/list.h>
-#include <linux/vmalloc.h>
-
-/*
- * TIPC sanity test macros
- */
-
-#define assert(i)  BUG_ON(!(i))
-
-/*
- * TIPC system monitoring code
- */
-
-/*
- * TIPC's print buffer subsystem supports the following print buffers:
- *
- * TIPC_NULL : null buffer (i.e. print nowhere)
- * TIPC_CONS : system console
- * TIPC_LOG  : TIPC log buffer
- * &buf	     : user-defined buffer (struct print_buf *)
- *
- * Note: TIPC_LOG is configured to echo its output to the system console;
- *       user-defined buffers can be configured to do the same thing.
- */
-
-extern struct print_buf *const TIPC_NULL;
-extern struct print_buf *const TIPC_CONS;
-extern struct print_buf *const TIPC_LOG;
-
-void tipc_printf(struct print_buf *, const char *fmt, ...);
-
-/*
- * TIPC_OUTPUT is the destination print buffer for system messages.
- */
-
-#ifndef TIPC_OUTPUT
-#define TIPC_OUTPUT TIPC_LOG
-#endif
-
-/*
- * TIPC can be configured to send system messages to TIPC_OUTPUT
- * or to the system console only.
- */
-
-#ifdef CONFIG_TIPC_DEBUG
-
-#define err(fmt, arg...)  tipc_printf(TIPC_OUTPUT, \
-					KERN_ERR "TIPC: " fmt, ## arg)
-#define warn(fmt, arg...) tipc_printf(TIPC_OUTPUT, \
-					KERN_WARNING "TIPC: " fmt, ## arg)
-#define info(fmt, arg...) tipc_printf(TIPC_OUTPUT, \
-					KERN_NOTICE "TIPC: " fmt, ## arg)
-
-#else
-
-#define err(fmt, arg...)  printk(KERN_ERR "TIPC: " fmt , ## arg)
-#define info(fmt, arg...) printk(KERN_INFO "TIPC: " fmt , ## arg)
-#define warn(fmt, arg...) printk(KERN_WARNING "TIPC: " fmt , ## arg)
-
-#endif
-
-/*
- * DBG_OUTPUT is the destination print buffer for debug messages.
- * It defaults to the the null print buffer, but can be redefined
- * (typically in the individual .c files being debugged) to allow
- * selected debug messages to be generated where needed.
- */
-
-#ifndef DBG_OUTPUT
-#define DBG_OUTPUT TIPC_NULL
-#endif
-
-/*
- * TIPC can be configured to send debug messages to the specified print buffer
- * (typically DBG_OUTPUT) or to suppress them entirely.
- */
-
-#ifdef CONFIG_TIPC_DEBUG
-
-#define dbg(fmt, arg...)  \
-	do { \
-		if (DBG_OUTPUT != TIPC_NULL) \
-			tipc_printf(DBG_OUTPUT, fmt, ## arg); \
-	} while (0)
-#define msg_dbg(msg, txt) \
-	do { \
-		if (DBG_OUTPUT != TIPC_NULL) \
-			tipc_msg_dbg(DBG_OUTPUT, msg, txt); \
-	} while (0)
-#define dump(fmt, arg...) \
-	do { \
-		if (DBG_OUTPUT != TIPC_NULL) \
-			tipc_dump_dbg(DBG_OUTPUT, fmt, ##arg); \
-	} while (0)
-
-void tipc_msg_dbg(struct print_buf *, struct tipc_msg *, const char *);
-void tipc_dump_dbg(struct print_buf *, const char *fmt, ...);
-
-#else
-
-#define dbg(fmt, arg...)	do {} while (0)
-#define msg_dbg(msg, txt)	do {} while (0)
-#define dump(fmt, arg...)	do {} while (0)
-
-#define tipc_msg_dbg(...)	do {} while (0)
-#define tipc_dump_dbg(...)	do {} while (0)
-
-#endif
-
-
-/*
- * TIPC-specific error codes
- */
-
-#define ELINKCONG EAGAIN	/* link congestion <=> resource unavailable */
-
-/*
- * Global configuration variables
- */
-
-extern u32 tipc_own_addr;
-extern int tipc_max_zones;
-extern int tipc_max_clusters;
-extern int tipc_max_nodes;
-extern int tipc_max_slaves;
-extern int tipc_max_ports;
-extern int tipc_max_subscriptions;
-extern int tipc_max_publications;
-extern int tipc_net_id;
-extern int tipc_remote_management;
-
-/*
- * Other global variables
- */
-
-extern int tipc_mode;
-extern int tipc_random;
-extern const char tipc_alphabet[];
-extern atomic_t tipc_user_count;
-
-
-/*
- * Routines available to privileged subsystems
- */
-
-extern int  tipc_core_start(void);
-extern void tipc_core_stop(void);
-extern int  tipc_core_start_net(unsigned long addr);
-extern void tipc_core_stop_net(void);
-extern int  tipc_handler_start(void);
-extern void tipc_handler_stop(void);
-extern int  tipc_netlink_start(void);
-extern void tipc_netlink_stop(void);
-extern int  tipc_socket_init(void);
-extern void tipc_socket_stop(void);
-
-static inline int delimit(int val, int min, int max)
-{
-	if (val > max)
-		return max;
-	if (val < min)
-		return min;
-	return val;
-}
-
-
-/*
- * TIPC timer and signal code
- */
-
-typedef void (*Handler) (unsigned long);
-
-u32 tipc_k_signal(Handler routine, unsigned long argument);
-
-/**
- * k_init_timer - initialize a timer
- * @timer: pointer to timer structure
- * @routine: pointer to routine to invoke when timer expires
- * @argument: value to pass to routine when timer expires
- *
- * Timer must be initialized before use (and terminated when no longer needed).
- */
-
-static inline void k_init_timer(struct timer_list *timer, Handler routine,
-				unsigned long argument)
-{
-	dbg("initializing timer %p\n", timer);
-	setup_timer(timer, routine, argument);
-}
-
-/**
- * k_start_timer - start a timer
- * @timer: pointer to timer structure
- * @msec: time to delay (in ms)
- *
- * Schedules a previously initialized timer for later execution.
- * If timer is already running, the new timeout overrides the previous request.
- *
- * To ensure the timer doesn't expire before the specified delay elapses,
- * the amount of delay is rounded up when converting to the jiffies
- * then an additional jiffy is added to account for the fact that
- * the starting time may be in the middle of the current jiffy.
- */
-
-static inline void k_start_timer(struct timer_list *timer, unsigned long msec)
-{
-	dbg("starting timer %p for %u\n", timer, msec);
-	mod_timer(timer, jiffies + msecs_to_jiffies(msec) + 1);
-}
-
-/**
- * k_cancel_timer - cancel a timer
- * @timer: pointer to timer structure
- *
- * Cancels a previously initialized timer.
- * Can be called safely even if the timer is already inactive.
- *
- * WARNING: Must not be called when holding locks required by the timer's
- *          timeout routine, otherwise deadlock can occur on SMP systems!
- */
-
-static inline void k_cancel_timer(struct timer_list *timer)
-{
-	dbg("cancelling timer %p\n", timer);
-	del_timer_sync(timer);
-}
-
-/**
- * k_term_timer - terminate a timer
- * @timer: pointer to timer structure
- *
- * Prevents further use of a previously initialized timer.
- *
- * WARNING: Caller must ensure timer isn't currently running.
- *
- * (Do not "enhance" this routine to automatically cancel an active timer,
- * otherwise deadlock can arise when a timeout routine calls k_term_timer.)
- */
-
-static inline void k_term_timer(struct timer_list *timer)
-{
-	dbg("terminating timer %p\n", timer);
-}
-
-
-/*
- * TIPC message buffer code
- *
- * TIPC message buffer headroom reserves space for the worst-case
- * link-level device header (in case the message is sent off-node).
- *
- * Note: Headroom should be a multiple of 4 to ensure the TIPC header fields
- *       are word aligned for quicker access
- */
-
-#define BUF_HEADROOM LL_MAX_HEADER
-
-struct tipc_skb_cb {
-	void *handle;
-};
-
-#define TIPC_SKB_CB(__skb) ((struct tipc_skb_cb *)&((__skb)->cb[0]))
-
-
-static inline struct tipc_msg *buf_msg(struct sk_buff *skb)
-{
-	return (struct tipc_msg *)skb->data;
-}
-
-/**
- * buf_acquire - creates a TIPC message buffer
- * @size: message size (including TIPC header)
- *
- * Returns a new buffer with data pointers set to the specified size.
- *
- * NOTE: Headroom is reserved to allow prepending of a data link header.
- *       There may also be unrequested tailroom present at the buffer's end.
- */
-
-static inline struct sk_buff *buf_acquire(u32 size)
-{
-	struct sk_buff *skb;
-	unsigned int buf_size = (BUF_HEADROOM + size + 3) & ~3u;
-
-	skb = alloc_skb_fclone(buf_size, GFP_ATOMIC);
-	if (skb) {
-		skb_reserve(skb, BUF_HEADROOM);
-		skb_put(skb, size);
-		skb->next = NULL;
-	}
-	return skb;
-}
-
-/**
- * buf_discard - frees a TIPC message buffer
- * @skb: message buffer
- *
- * Frees a message buffer.  If passed NULL, just returns.
- */
-
-static inline void buf_discard(struct sk_buff *skb)
-{
-	kfree_skb(skb);
-}
-
-/**
- * buf_linearize - convert a TIPC message buffer into a single contiguous piece
- * @skb: message buffer
- *
- * Returns 0 on success.
- */
-
-static inline int buf_linearize(struct sk_buff *skb)
-{
-	return skb_linearize(skb);
-}
-
-#endif
diff -ruN linux-2.6.29/net/tipc/dbg.c android_cluster/linux-2.6.29/net/tipc/dbg.c
--- linux-2.6.29/net/tipc/dbg.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/dbg.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,427 +0,0 @@
-/*
- * net/tipc/dbg.c: TIPC print buffer routines for debugging
- *
- * Copyright (c) 1996-2006, Ericsson AB
- * Copyright (c) 2005-2007, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "config.h"
-#include "dbg.h"
-
-/*
- * TIPC pre-defines the following print buffers:
- *
- * TIPC_NULL : null buffer (i.e. print nowhere)
- * TIPC_CONS : system console
- * TIPC_LOG  : TIPC log buffer
- *
- * Additional user-defined print buffers are also permitted.
- */
-
-static struct print_buf null_buf = { NULL, 0, NULL, 0 };
-struct print_buf *const TIPC_NULL = &null_buf;
-
-static struct print_buf cons_buf = { NULL, 0, NULL, 1 };
-struct print_buf *const TIPC_CONS = &cons_buf;
-
-static struct print_buf log_buf = { NULL, 0, NULL, 1 };
-struct print_buf *const TIPC_LOG = &log_buf;
-
-/*
- * Locking policy when using print buffers.
- *
- * 1) tipc_printf() uses 'print_lock' to protect against concurrent access to
- * 'print_string' when writing to a print buffer. This also protects against
- * concurrent writes to the print buffer being written to.
- *
- * 2) tipc_dump() and tipc_log_XXX() leverage the aforementioned
- * use of 'print_lock' to protect against all types of concurrent operations
- * on their associated print buffer (not just write operations).
- *
- * Note: All routines of the form tipc_printbuf_XXX() are lock-free, and rely
- * on the caller to prevent simultaneous use of the print buffer(s) being
- * manipulated.
- */
-
-static char print_string[TIPC_PB_MAX_STR];
-static DEFINE_SPINLOCK(print_lock);
-
-
-#define FORMAT(PTR,LEN,FMT) \
-{\
-       va_list args;\
-       va_start(args, FMT);\
-       LEN = vsprintf(PTR, FMT, args);\
-       va_end(args);\
-       *(PTR + LEN) = '\0';\
-}
-
-/**
- * tipc_printbuf_init - initialize print buffer to empty
- * @pb: pointer to print buffer structure
- * @raw: pointer to character array used by print buffer
- * @size: size of character array
- *
- * Note: If the character array is too small (or absent), the print buffer
- * becomes a null device that discards anything written to it.
- */
-
-void tipc_printbuf_init(struct print_buf *pb, char *raw, u32 size)
-{
-	pb->buf = raw;
-	pb->crs = raw;
-	pb->size = size;
-	pb->echo = 0;
-
-	if (size < TIPC_PB_MIN_SIZE) {
-		pb->buf = NULL;
-	} else if (raw) {
-		pb->buf[0] = 0;
-		pb->buf[size - 1] = ~0;
-	}
-}
-
-/**
- * tipc_printbuf_reset - reinitialize print buffer to empty state
- * @pb: pointer to print buffer structure
- */
-
-void tipc_printbuf_reset(struct print_buf *pb)
-{
-	if (pb->buf) {
-		pb->crs = pb->buf;
-		pb->buf[0] = 0;
-		pb->buf[pb->size - 1] = ~0;
-	}
-}
-
-/**
- * tipc_printbuf_empty - test if print buffer is in empty state
- * @pb: pointer to print buffer structure
- *
- * Returns non-zero if print buffer is empty.
- */
-
-int tipc_printbuf_empty(struct print_buf *pb)
-{
-	return (!pb->buf || (pb->crs == pb->buf));
-}
-
-/**
- * tipc_printbuf_validate - check for print buffer overflow
- * @pb: pointer to print buffer structure
- *
- * Verifies that a print buffer has captured all data written to it.
- * If data has been lost, linearize buffer and prepend an error message
- *
- * Returns length of print buffer data string (including trailing NUL)
- */
-
-int tipc_printbuf_validate(struct print_buf *pb)
-{
-	char *err = "\n\n*** PRINT BUFFER OVERFLOW ***\n\n";
-	char *cp_buf;
-	struct print_buf cb;
-
-	if (!pb->buf)
-		return 0;
-
-	if (pb->buf[pb->size - 1] == 0) {
-		cp_buf = kmalloc(pb->size, GFP_ATOMIC);
-		if (cp_buf) {
-			tipc_printbuf_init(&cb, cp_buf, pb->size);
-			tipc_printbuf_move(&cb, pb);
-			tipc_printbuf_move(pb, &cb);
-			kfree(cp_buf);
-			memcpy(pb->buf, err, strlen(err));
-		} else {
-			tipc_printbuf_reset(pb);
-			tipc_printf(pb, err);
-		}
-	}
-	return (pb->crs - pb->buf + 1);
-}
-
-/**
- * tipc_printbuf_move - move print buffer contents to another print buffer
- * @pb_to: pointer to destination print buffer structure
- * @pb_from: pointer to source print buffer structure
- *
- * Current contents of destination print buffer (if any) are discarded.
- * Source print buffer becomes empty if a successful move occurs.
- */
-
-void tipc_printbuf_move(struct print_buf *pb_to, struct print_buf *pb_from)
-{
-	int len;
-
-	/* Handle the cases where contents can't be moved */
-
-	if (!pb_to->buf)
-		return;
-
-	if (!pb_from->buf) {
-		tipc_printbuf_reset(pb_to);
-		return;
-	}
-
-	if (pb_to->size < pb_from->size) {
-		strcpy(pb_to->buf, "*** PRINT BUFFER MOVE ERROR ***");
-		pb_to->buf[pb_to->size - 1] = ~0;
-		pb_to->crs = strchr(pb_to->buf, 0);
-		return;
-	}
-
-	/* Copy data from char after cursor to end (if used) */
-
-	len = pb_from->buf + pb_from->size - pb_from->crs - 2;
-	if ((pb_from->buf[pb_from->size - 1] == 0) && (len > 0)) {
-		strcpy(pb_to->buf, pb_from->crs + 1);
-		pb_to->crs = pb_to->buf + len;
-	} else
-		pb_to->crs = pb_to->buf;
-
-	/* Copy data from start to cursor (always) */
-
-	len = pb_from->crs - pb_from->buf;
-	strcpy(pb_to->crs, pb_from->buf);
-	pb_to->crs += len;
-
-	tipc_printbuf_reset(pb_from);
-}
-
-/**
- * tipc_printf - append formatted output to print buffer
- * @pb: pointer to print buffer
- * @fmt: formatted info to be printed
- */
-
-void tipc_printf(struct print_buf *pb, const char *fmt, ...)
-{
-	int chars_to_add;
-	int chars_left;
-	char save_char;
-
-	spin_lock_bh(&print_lock);
-
-	FORMAT(print_string, chars_to_add, fmt);
-	if (chars_to_add >= TIPC_PB_MAX_STR)
-		strcpy(print_string, "*** PRINT BUFFER STRING TOO LONG ***");
-
-	if (pb->buf) {
-		chars_left = pb->buf + pb->size - pb->crs - 1;
-		if (chars_to_add <= chars_left) {
-			strcpy(pb->crs, print_string);
-			pb->crs += chars_to_add;
-		} else if (chars_to_add >= (pb->size - 1)) {
-			strcpy(pb->buf, print_string + chars_to_add + 1
-			       - pb->size);
-			pb->crs = pb->buf + pb->size - 1;
-		} else {
-			strcpy(pb->buf, print_string + chars_left);
-			save_char = print_string[chars_left];
-			print_string[chars_left] = 0;
-			strcpy(pb->crs, print_string);
-			print_string[chars_left] = save_char;
-			pb->crs = pb->buf + chars_to_add - chars_left;
-		}
-	}
-
-	if (pb->echo)
-		printk(print_string);
-
-	spin_unlock_bh(&print_lock);
-}
-
-#ifdef CONFIG_TIPC_DEBUG
-
-/**
- * print_to_console - write string of bytes to console in multiple chunks
- */
-
-static void print_to_console(char *crs, int len)
-{
-	int rest = len;
-
-	while (rest > 0) {
-		int sz = rest < TIPC_PB_MAX_STR ? rest : TIPC_PB_MAX_STR;
-		char c = crs[sz];
-
-		crs[sz] = 0;
-		printk((const char *)crs);
-		crs[sz] = c;
-		rest -= sz;
-		crs += sz;
-	}
-}
-
-/**
- * printbuf_dump - write print buffer contents to console
- */
-
-static void printbuf_dump(struct print_buf *pb)
-{
-	int len;
-
-	if (!pb->buf) {
-		printk("*** PRINT BUFFER NOT ALLOCATED ***");
-		return;
-	}
-
-	/* Dump print buffer from char after cursor to end (if used) */
-
-	len = pb->buf + pb->size - pb->crs - 2;
-	if ((pb->buf[pb->size - 1] == 0) && (len > 0))
-		print_to_console(pb->crs + 1, len);
-
-	/* Dump print buffer from start to cursor (always) */
-
-	len = pb->crs - pb->buf;
-	print_to_console(pb->buf, len);
-}
-
-/**
- * tipc_dump_dbg - dump (non-console) print buffer to console
- * @pb: pointer to print buffer
- */
-
-void tipc_dump_dbg(struct print_buf *pb, const char *fmt, ...)
-{
-	int len;
-
-	if (pb == TIPC_CONS)
-		return;
-
-	spin_lock_bh(&print_lock);
-
-	FORMAT(print_string, len, fmt);
-	printk(print_string);
-
-	printk("\n---- Start of %s log dump ----\n\n",
-	       (pb == TIPC_LOG) ? "global" : "local");
-	printbuf_dump(pb);
-	tipc_printbuf_reset(pb);
-	printk("\n---- End of dump ----\n");
-
-	spin_unlock_bh(&print_lock);
-}
-
-#endif
-
-/**
- * tipc_log_resize - change the size of the TIPC log buffer
- * @log_size: print buffer size to use
- */
-
-int tipc_log_resize(int log_size)
-{
-	int res = 0;
-
-	spin_lock_bh(&print_lock);
-	if (TIPC_LOG->buf) {
-		kfree(TIPC_LOG->buf);
-		TIPC_LOG->buf = NULL;
-	}
-	if (log_size) {
-		if (log_size < TIPC_PB_MIN_SIZE)
-			log_size = TIPC_PB_MIN_SIZE;
-		res = TIPC_LOG->echo;
-		tipc_printbuf_init(TIPC_LOG, kmalloc(log_size, GFP_ATOMIC),
-				   log_size);
-		TIPC_LOG->echo = res;
-		res = !TIPC_LOG->buf;
-	}
-	spin_unlock_bh(&print_lock);
-
-	return res;
-}
-
-/**
- * tipc_log_resize_cmd - reconfigure size of TIPC log buffer
- */
-
-struct sk_buff *tipc_log_resize_cmd(const void *req_tlv_area, int req_tlv_space)
-{
-	u32 value;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
-	if (value != delimit(value, 0, 32768))
-		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
-						   " (log size must be 0-32768)");
-	if (tipc_log_resize(value))
-		return tipc_cfg_reply_error_string(
-			"unable to create specified log (log size is now 0)");
-	return tipc_cfg_reply_none();
-}
-
-/**
- * tipc_log_dump - capture TIPC log buffer contents in configuration message
- */
-
-struct sk_buff *tipc_log_dump(void)
-{
-	struct sk_buff *reply;
-
-	spin_lock_bh(&print_lock);
-	if (!TIPC_LOG->buf) {
-		spin_unlock_bh(&print_lock);
-		reply = tipc_cfg_reply_ultra_string("log not activated\n");
-	} else if (tipc_printbuf_empty(TIPC_LOG)) {
-		spin_unlock_bh(&print_lock);
-		reply = tipc_cfg_reply_ultra_string("log is empty\n");
-	}
-	else {
-		struct tlv_desc *rep_tlv;
-		struct print_buf pb;
-		int str_len;
-
-		str_len = min(TIPC_LOG->size, 32768u);
-		spin_unlock_bh(&print_lock);
-		reply = tipc_cfg_reply_alloc(TLV_SPACE(str_len));
-		if (reply) {
-			rep_tlv = (struct tlv_desc *)reply->data;
-			tipc_printbuf_init(&pb, TLV_DATA(rep_tlv), str_len);
-			spin_lock_bh(&print_lock);
-			tipc_printbuf_move(&pb, TIPC_LOG);
-			spin_unlock_bh(&print_lock);
-			str_len = strlen(TLV_DATA(rep_tlv)) + 1;
-			skb_put(reply, TLV_SPACE(str_len));
-			TLV_SET(rep_tlv, TIPC_TLV_ULTRA_STRING, NULL, str_len);
-		}
-	}
-	return reply;
-}
-
diff -ruN linux-2.6.29/net/tipc/dbg.h android_cluster/linux-2.6.29/net/tipc/dbg.h
--- linux-2.6.29/net/tipc/dbg.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/dbg.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,70 +0,0 @@
-/*
- * net/tipc/dbg.h: Include file for TIPC print buffer routines
- *
- * Copyright (c) 1997-2006, Ericsson AB
- * Copyright (c) 2005-2007, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_DBG_H
-#define _TIPC_DBG_H
-
-/**
- * struct print_buf - TIPC print buffer structure
- * @buf: pointer to character array containing print buffer contents
- * @size: size of character array
- * @crs: pointer to first unused space in character array (i.e. final NUL)
- * @echo: echo output to system console if non-zero
- */
-
-struct print_buf {
-	char *buf;
-	u32 size;
-	char *crs;
-	int echo;
-};
-
-#define TIPC_PB_MIN_SIZE 64	/* minimum size for a print buffer's array */
-#define TIPC_PB_MAX_STR 512	/* max printable string (with trailing NUL) */
-
-void tipc_printbuf_init(struct print_buf *pb, char *buf, u32 size);
-void tipc_printbuf_reset(struct print_buf *pb);
-int  tipc_printbuf_empty(struct print_buf *pb);
-int  tipc_printbuf_validate(struct print_buf *pb);
-void tipc_printbuf_move(struct print_buf *pb_to, struct print_buf *pb_from);
-
-int tipc_log_resize(int log_size);
-
-struct sk_buff *tipc_log_resize_cmd(const void *req_tlv_area,
-				    int req_tlv_space);
-struct sk_buff *tipc_log_dump(void);
-
-#endif
diff -ruN linux-2.6.29/net/tipc/discover.c android_cluster/linux-2.6.29/net/tipc/discover.c
--- linux-2.6.29/net/tipc/discover.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/discover.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,344 +0,0 @@
-/*
- * net/tipc/discover.c
- *
- * Copyright (c) 2003-2006, Ericsson AB
- * Copyright (c) 2005-2006, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "dbg.h"
-#include "link.h"
-#include "zone.h"
-#include "discover.h"
-#include "port.h"
-#include "name_table.h"
-
-#define TIPC_LINK_REQ_INIT	125	/* min delay during bearer start up */
-#define TIPC_LINK_REQ_FAST	2000	/* normal delay if bearer has no links */
-#define TIPC_LINK_REQ_SLOW	600000	/* normal delay if bearer has links */
-
-#if 0
-#define  GET_NODE_INFO         300
-#define  GET_NODE_INFO_RESULT  301
-#define  FORWARD_LINK_PROBE    302
-#define  LINK_REQUEST_REJECTED 303
-#define  LINK_REQUEST_ACCEPTED 304
-#define  DROP_LINK_REQUEST     305
-#define  CHECK_LINK_COUNT      306
-#endif
-
-/*
- * TODO: Most of the inter-cluster setup stuff should be
- * rewritten, and be made conformant with specification.
- */
-
-
-/**
- * struct link_req - information about an ongoing link setup request
- * @bearer: bearer issuing requests
- * @dest: destination address for request messages
- * @buf: request message to be (repeatedly) sent
- * @timer: timer governing period between requests
- * @timer_intv: current interval between requests (in ms)
- */
-struct link_req {
-	struct bearer *bearer;
-	struct tipc_media_addr dest;
-	struct sk_buff *buf;
-	struct timer_list timer;
-	unsigned int timer_intv;
-};
-
-
-#if 0
-int disc_create_link(const struct tipc_link_create *argv)
-{
-	/*
-	 * Code for inter cluster link setup here
-	 */
-	return TIPC_OK;
-}
-#endif
-
-/*
- * disc_lost_link(): A link has lost contact
- */
-
-void tipc_disc_link_event(u32 addr, char *name, int up)
-{
-	if (in_own_cluster(addr))
-		return;
-	/*
-	 * Code for inter cluster link setup here
-	 */
-}
-
-/**
- * tipc_disc_init_msg - initialize a link setup message
- * @type: message type (request or response)
- * @req_links: number of links associated with message
- * @dest_domain: network domain of node(s) which should respond to message
- * @b_ptr: ptr to bearer issuing message
- */
-
-static struct sk_buff *tipc_disc_init_msg(u32 type,
-					  u32 req_links,
-					  u32 dest_domain,
-					  struct bearer *b_ptr)
-{
-	struct sk_buff *buf = buf_acquire(DSC_H_SIZE);
-	struct tipc_msg *msg;
-
-	if (buf) {
-		msg = buf_msg(buf);
-		msg_init(msg, LINK_CONFIG, type, DSC_H_SIZE, dest_domain);
-		msg_set_non_seq(msg, 1);
-		msg_set_req_links(msg, req_links);
-		msg_set_dest_domain(msg, dest_domain);
-		msg_set_bc_netid(msg, tipc_net_id);
-		msg_set_media_addr(msg, &b_ptr->publ.addr);
-	}
-	return buf;
-}
-
-/**
- * disc_dupl_alert - issue node address duplication alert
- * @b_ptr: pointer to bearer detecting duplication
- * @node_addr: duplicated node address
- * @media_addr: media address advertised by duplicated node
- */
-
-static void disc_dupl_alert(struct bearer *b_ptr, u32 node_addr,
-			    struct tipc_media_addr *media_addr)
-{
-	char node_addr_str[16];
-	char media_addr_str[64];
-	struct print_buf pb;
-
-	addr_string_fill(node_addr_str, node_addr);
-	tipc_printbuf_init(&pb, media_addr_str, sizeof(media_addr_str));
-	tipc_media_addr_printf(&pb, media_addr);
-	tipc_printbuf_validate(&pb);
-	warn("Duplicate %s using %s seen on <%s>\n",
-	     node_addr_str, media_addr_str, b_ptr->publ.name);
-}
-
-/**
- * tipc_disc_recv_msg - handle incoming link setup message (request or response)
- * @buf: buffer containing message
- * @b_ptr: bearer that message arrived on
- */
-
-void tipc_disc_recv_msg(struct sk_buff *buf, struct bearer *b_ptr)
-{
-	struct link *link;
-	struct tipc_media_addr media_addr;
-	struct tipc_msg *msg = buf_msg(buf);
-	u32 dest = msg_dest_domain(msg);
-	u32 orig = msg_prevnode(msg);
-	u32 net_id = msg_bc_netid(msg);
-	u32 type = msg_type(msg);
-
-	msg_get_media_addr(msg,&media_addr);
-	msg_dbg(msg, "RECV:");
-	buf_discard(buf);
-
-	if (net_id != tipc_net_id)
-		return;
-	if (!tipc_addr_domain_valid(dest))
-		return;
-	if (!tipc_addr_node_valid(orig))
-		return;
-	if (orig == tipc_own_addr) {
-		if (memcmp(&media_addr, &b_ptr->publ.addr, sizeof(media_addr)))
-			disc_dupl_alert(b_ptr, tipc_own_addr, &media_addr);
-		return;
-	}
-	if (!in_scope(dest, tipc_own_addr))
-		return;
-	if (is_slave(tipc_own_addr) && is_slave(orig))
-		return;
-	if (is_slave(orig) && !in_own_cluster(orig))
-		return;
-	if (in_own_cluster(orig)) {
-		/* Always accept link here */
-		struct sk_buff *rbuf;
-		struct tipc_media_addr *addr;
-		struct tipc_node *n_ptr = tipc_node_find(orig);
-		int link_fully_up;
-
-		dbg(" in own cluster\n");
-		if (n_ptr == NULL) {
-			n_ptr = tipc_node_create(orig);
-			if (!n_ptr)
-				return;
-		}
-		spin_lock_bh(&n_ptr->lock);
-		link = n_ptr->links[b_ptr->identity];
-		if (!link) {
-			dbg("creating link\n");
-			link = tipc_link_create(b_ptr, orig, &media_addr);
-			if (!link) {
-				spin_unlock_bh(&n_ptr->lock);
-				return;
-			}
-		}
-		addr = &link->media_addr;
-		if (memcmp(addr, &media_addr, sizeof(*addr))) {
-			if (tipc_link_is_up(link) || (!link->started)) {
-				disc_dupl_alert(b_ptr, orig, &media_addr);
-				spin_unlock_bh(&n_ptr->lock);
-				return;
-			}
-			warn("Resetting link <%s>, peer interface address changed\n",
-			     link->name);
-			memcpy(addr, &media_addr, sizeof(*addr));
-			tipc_link_reset(link);
-		}
-		link_fully_up = (link->state == WORKING_WORKING);
-		spin_unlock_bh(&n_ptr->lock);
-		if ((type == DSC_RESP_MSG) || link_fully_up)
-			return;
-		rbuf = tipc_disc_init_msg(DSC_RESP_MSG, 1, orig, b_ptr);
-		if (rbuf != NULL) {
-			msg_dbg(buf_msg(rbuf),"SEND:");
-			b_ptr->media->send_msg(rbuf, &b_ptr->publ, &media_addr);
-			buf_discard(rbuf);
-		}
-	}
-}
-
-/**
- * tipc_disc_stop_link_req - stop sending periodic link setup requests
- * @req: ptr to link request structure
- */
-
-void tipc_disc_stop_link_req(struct link_req *req)
-{
-	if (!req)
-		return;
-
-	k_cancel_timer(&req->timer);
-	k_term_timer(&req->timer);
-	buf_discard(req->buf);
-	kfree(req);
-}
-
-/**
- * tipc_disc_update_link_req - update frequency of periodic link setup requests
- * @req: ptr to link request structure
- */
-
-void tipc_disc_update_link_req(struct link_req *req)
-{
-	if (!req)
-		return;
-
-	if (req->timer_intv == TIPC_LINK_REQ_SLOW) {
-		if (!req->bearer->nodes.count) {
-			req->timer_intv = TIPC_LINK_REQ_FAST;
-			k_start_timer(&req->timer, req->timer_intv);
-		}
-	} else if (req->timer_intv == TIPC_LINK_REQ_FAST) {
-		if (req->bearer->nodes.count) {
-			req->timer_intv = TIPC_LINK_REQ_SLOW;
-			k_start_timer(&req->timer, req->timer_intv);
-		}
-	} else {
-		/* leave timer "as is" if haven't yet reached a "normal" rate */
-	}
-}
-
-/**
- * disc_timeout - send a periodic link setup request
- * @req: ptr to link request structure
- *
- * Called whenever a link setup request timer associated with a bearer expires.
- */
-
-static void disc_timeout(struct link_req *req)
-{
-	spin_lock_bh(&req->bearer->publ.lock);
-
-	req->bearer->media->send_msg(req->buf, &req->bearer->publ, &req->dest);
-
-	if ((req->timer_intv == TIPC_LINK_REQ_SLOW) ||
-	    (req->timer_intv == TIPC_LINK_REQ_FAST)) {
-		/* leave timer interval "as is" if already at a "normal" rate */
-	} else {
-		req->timer_intv *= 2;
-		if (req->timer_intv > TIPC_LINK_REQ_FAST)
-			req->timer_intv = TIPC_LINK_REQ_FAST;
-		if ((req->timer_intv == TIPC_LINK_REQ_FAST) &&
-		    (req->bearer->nodes.count))
-			req->timer_intv = TIPC_LINK_REQ_SLOW;
-	}
-	k_start_timer(&req->timer, req->timer_intv);
-
-	spin_unlock_bh(&req->bearer->publ.lock);
-}
-
-/**
- * tipc_disc_init_link_req - start sending periodic link setup requests
- * @b_ptr: ptr to bearer issuing requests
- * @dest: destination address for request messages
- * @dest_domain: network domain of node(s) which should respond to message
- * @req_links: max number of desired links
- *
- * Returns pointer to link request structure, or NULL if unable to create.
- */
-
-struct link_req *tipc_disc_init_link_req(struct bearer *b_ptr,
-					 const struct tipc_media_addr *dest,
-					 u32 dest_domain,
-					 u32 req_links)
-{
-	struct link_req *req;
-
-	req = kmalloc(sizeof(*req), GFP_ATOMIC);
-	if (!req)
-		return NULL;
-
-	req->buf = tipc_disc_init_msg(DSC_REQ_MSG, req_links, dest_domain, b_ptr);
-	if (!req->buf) {
-		kfree(req);
-		return NULL;
-	}
-
-	memcpy(&req->dest, dest, sizeof(*dest));
-	req->bearer = b_ptr;
-	req->timer_intv = TIPC_LINK_REQ_INIT;
-	k_init_timer(&req->timer, (Handler)disc_timeout, (unsigned long)req);
-	k_start_timer(&req->timer, req->timer_intv);
-	return req;
-}
-
diff -ruN linux-2.6.29/net/tipc/discover.h android_cluster/linux-2.6.29/net/tipc/discover.h
--- linux-2.6.29/net/tipc/discover.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/discover.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,58 +0,0 @@
-/*
- * net/tipc/discover.h
- *
- * Copyright (c) 2003-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_DISCOVER_H
-#define _TIPC_DISCOVER_H
-
-#include "core.h"
-
-struct link_req;
-
-struct link_req *tipc_disc_init_link_req(struct bearer *b_ptr,
-					 const struct tipc_media_addr *dest,
-					 u32 dest_domain,
-					 u32 req_links);
-void tipc_disc_update_link_req(struct link_req *req);
-void tipc_disc_stop_link_req(struct link_req *req);
-
-void tipc_disc_recv_msg(struct sk_buff *buf, struct bearer *b_ptr);
-
-void tipc_disc_link_event(u32 addr, char *name, int up);
-#if 0
-int  disc_create_link(const struct tipc_link_create *argv);
-#endif
-
-#endif
diff -ruN linux-2.6.29/net/tipc/eth_media.c android_cluster/linux-2.6.29/net/tipc/eth_media.c
--- linux-2.6.29/net/tipc/eth_media.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/eth_media.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,313 +0,0 @@
-/*
- * net/tipc/eth_media.c: Ethernet bearer support for TIPC
- *
- * Copyright (c) 2001-2007, Ericsson AB
- * Copyright (c) 2005-2007, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include <net/tipc/tipc.h>
-#include <net/tipc/tipc_bearer.h>
-#include <net/tipc/tipc_msg.h>
-#include <linux/netdevice.h>
-#include <net/net_namespace.h>
-
-#define MAX_ETH_BEARERS		2
-#define ETH_LINK_PRIORITY	TIPC_DEF_LINK_PRI
-#define ETH_LINK_TOLERANCE	TIPC_DEF_LINK_TOL
-#define ETH_LINK_WINDOW		TIPC_DEF_LINK_WIN
-
-/**
- * struct eth_bearer - Ethernet bearer data structure
- * @bearer: ptr to associated "generic" bearer structure
- * @dev: ptr to associated Ethernet network device
- * @tipc_packet_type: used in binding TIPC to Ethernet driver
- */
-
-struct eth_bearer {
-	struct tipc_bearer *bearer;
-	struct net_device *dev;
-	struct packet_type tipc_packet_type;
-};
-
-static struct eth_bearer eth_bearers[MAX_ETH_BEARERS];
-static int eth_started = 0;
-static struct notifier_block notifier;
-
-/**
- * send_msg - send a TIPC message out over an Ethernet interface
- */
-
-static int send_msg(struct sk_buff *buf, struct tipc_bearer *tb_ptr,
-		    struct tipc_media_addr *dest)
-{
-	struct sk_buff *clone;
-	struct net_device *dev;
-
-	clone = skb_clone(buf, GFP_ATOMIC);
-	if (clone) {
-		skb_reset_network_header(clone);
-		dev = ((struct eth_bearer *)(tb_ptr->usr_handle))->dev;
-		clone->dev = dev;
-		dev_hard_header(clone, dev, ETH_P_TIPC,
-				 &dest->dev_addr.eth_addr,
-				 dev->dev_addr, clone->len);
-		dev_queue_xmit(clone);
-	}
-	return 0;
-}
-
-/**
- * recv_msg - handle incoming TIPC message from an Ethernet interface
- *
- * Accept only packets explicitly sent to this node, or broadcast packets;
- * ignores packets sent using Ethernet multicast, and traffic sent to other
- * nodes (which can happen if interface is running in promiscuous mode).
- * Routine truncates any Ethernet padding/CRC appended to the message,
- * and ensures message size matches actual length
- */
-
-static int recv_msg(struct sk_buff *buf, struct net_device *dev,
-		    struct packet_type *pt, struct net_device *orig_dev)
-{
-	struct eth_bearer *eb_ptr = (struct eth_bearer *)pt->af_packet_priv;
-	u32 size;
-
-	if (!net_eq(dev_net(dev), &init_net)) {
-		kfree_skb(buf);
-		return 0;
-	}
-
-	if (likely(eb_ptr->bearer)) {
-		if (likely(buf->pkt_type <= PACKET_BROADCAST)) {
-			size = msg_size((struct tipc_msg *)buf->data);
-			skb_trim(buf, size);
-			if (likely(buf->len == size)) {
-				buf->next = NULL;
-				tipc_recv_msg(buf, eb_ptr->bearer);
-				return 0;
-			}
-		}
-	}
-	kfree_skb(buf);
-	return 0;
-}
-
-/**
- * enable_bearer - attach TIPC bearer to an Ethernet interface
- */
-
-static int enable_bearer(struct tipc_bearer *tb_ptr)
-{
-	struct net_device *dev = NULL;
-	struct net_device *pdev = NULL;
-	struct eth_bearer *eb_ptr = &eth_bearers[0];
-	struct eth_bearer *stop = &eth_bearers[MAX_ETH_BEARERS];
-	char *driver_name = strchr((const char *)tb_ptr->name, ':') + 1;
-
-	/* Find device with specified name */
-
-	for_each_netdev(&init_net, pdev){
-		if (!strncmp(pdev->name, driver_name, IFNAMSIZ)) {
-			dev = pdev;
-			break;
-		}
-	}
-	if (!dev)
-		return -ENODEV;
-
-	/* Find Ethernet bearer for device (or create one) */
-
-	for (;(eb_ptr != stop) && eb_ptr->dev && (eb_ptr->dev != dev); eb_ptr++);
-	if (eb_ptr == stop)
-		return -EDQUOT;
-	if (!eb_ptr->dev) {
-		eb_ptr->dev = dev;
-		eb_ptr->tipc_packet_type.type = htons(ETH_P_TIPC);
-		eb_ptr->tipc_packet_type.dev = dev;
-		eb_ptr->tipc_packet_type.func = recv_msg;
-		eb_ptr->tipc_packet_type.af_packet_priv = eb_ptr;
-		INIT_LIST_HEAD(&(eb_ptr->tipc_packet_type.list));
-		dev_hold(dev);
-		dev_add_pack(&eb_ptr->tipc_packet_type);
-	}
-
-	/* Associate TIPC bearer with Ethernet bearer */
-
-	eb_ptr->bearer = tb_ptr;
-	tb_ptr->usr_handle = (void *)eb_ptr;
-	tb_ptr->mtu = dev->mtu;
-	tb_ptr->blocked = 0;
-	tb_ptr->addr.type = htonl(TIPC_MEDIA_TYPE_ETH);
-	memcpy(&tb_ptr->addr.dev_addr, &dev->dev_addr, ETH_ALEN);
-	return 0;
-}
-
-/**
- * disable_bearer - detach TIPC bearer from an Ethernet interface
- *
- * We really should do dev_remove_pack() here, but this function can not be
- * called at tasklet level. => Use eth_bearer->bearer as a flag to throw away
- * incoming buffers, & postpone dev_remove_pack() to eth_media_stop() on exit.
- */
-
-static void disable_bearer(struct tipc_bearer *tb_ptr)
-{
-	((struct eth_bearer *)tb_ptr->usr_handle)->bearer = NULL;
-}
-
-/**
- * recv_notification - handle device updates from OS
- *
- * Change the state of the Ethernet bearer (if any) associated with the
- * specified device.
- */
-
-static int recv_notification(struct notifier_block *nb, unsigned long evt,
-			     void *dv)
-{
-	struct net_device *dev = (struct net_device *)dv;
-	struct eth_bearer *eb_ptr = &eth_bearers[0];
-	struct eth_bearer *stop = &eth_bearers[MAX_ETH_BEARERS];
-
-	if (!net_eq(dev_net(dev), &init_net))
-		return NOTIFY_DONE;
-
-	while ((eb_ptr->dev != dev)) {
-		if (++eb_ptr == stop)
-			return NOTIFY_DONE;	/* couldn't find device */
-	}
-	if (!eb_ptr->bearer)
-		return NOTIFY_DONE;		/* bearer had been disabled */
-
-	eb_ptr->bearer->mtu = dev->mtu;
-
-	switch (evt) {
-	case NETDEV_CHANGE:
-		if (netif_carrier_ok(dev))
-			tipc_continue(eb_ptr->bearer);
-		else
-			tipc_block_bearer(eb_ptr->bearer->name);
-		break;
-	case NETDEV_UP:
-		tipc_continue(eb_ptr->bearer);
-		break;
-	case NETDEV_DOWN:
-		tipc_block_bearer(eb_ptr->bearer->name);
-		break;
-	case NETDEV_CHANGEMTU:
-	case NETDEV_CHANGEADDR:
-		tipc_block_bearer(eb_ptr->bearer->name);
-		tipc_continue(eb_ptr->bearer);
-		break;
-	case NETDEV_UNREGISTER:
-	case NETDEV_CHANGENAME:
-		tipc_disable_bearer(eb_ptr->bearer->name);
-		break;
-	}
-	return NOTIFY_OK;
-}
-
-/**
- * eth_addr2str - convert Ethernet address to string
- */
-
-static char *eth_addr2str(struct tipc_media_addr *a, char *str_buf, int str_size)
-{
-	unchar *addr = (unchar *)&a->dev_addr;
-
-	if (str_size < 18)
-		*str_buf = '\0';
-	else
-		sprintf(str_buf, "%pM", addr);
-	return str_buf;
-}
-
-/**
- * tipc_eth_media_start - activate Ethernet bearer support
- *
- * Register Ethernet media type with TIPC bearer code.  Also register
- * with OS for notifications about device state changes.
- */
-
-int tipc_eth_media_start(void)
-{
-	struct tipc_media_addr bcast_addr;
-	int res;
-
-	if (eth_started)
-		return -EINVAL;
-
-	bcast_addr.type = htonl(TIPC_MEDIA_TYPE_ETH);
-	memset(&bcast_addr.dev_addr, 0xff, ETH_ALEN);
-
-	memset(eth_bearers, 0, sizeof(eth_bearers));
-
-	res = tipc_register_media(TIPC_MEDIA_TYPE_ETH, "eth",
-				  enable_bearer, disable_bearer, send_msg,
-				  eth_addr2str, &bcast_addr, ETH_LINK_PRIORITY,
-				  ETH_LINK_TOLERANCE, ETH_LINK_WINDOW);
-	if (res)
-		return res;
-
-	notifier.notifier_call = &recv_notification;
-	notifier.priority = 0;
-	res = register_netdevice_notifier(&notifier);
-	if (!res)
-		eth_started = 1;
-	return res;
-}
-
-/**
- * tipc_eth_media_stop - deactivate Ethernet bearer support
- */
-
-void tipc_eth_media_stop(void)
-{
-	int i;
-
-	if (!eth_started)
-		return;
-
-	unregister_netdevice_notifier(&notifier);
-	for (i = 0; i < MAX_ETH_BEARERS ; i++) {
-		if (eth_bearers[i].bearer) {
-			eth_bearers[i].bearer->blocked = 1;
-			eth_bearers[i].bearer = NULL;
-		}
-		if (eth_bearers[i].dev) {
-			dev_remove_pack(&eth_bearers[i].tipc_packet_type);
-			dev_put(eth_bearers[i].dev);
-		}
-	}
-	memset(&eth_bearers, 0, sizeof(eth_bearers));
-	eth_started = 0;
-}
diff -ruN linux-2.6.29/net/tipc/handler.c android_cluster/linux-2.6.29/net/tipc/handler.c
--- linux-2.6.29/net/tipc/handler.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/handler.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,132 +0,0 @@
-/*
- * net/tipc/handler.c: TIPC signal handling
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-
-struct queue_item {
-	struct list_head next_signal;
-	void (*handler) (unsigned long);
-	unsigned long data;
-};
-
-static struct kmem_cache *tipc_queue_item_cache;
-static struct list_head signal_queue_head;
-static DEFINE_SPINLOCK(qitem_lock);
-static int handler_enabled = 0;
-
-static void process_signal_queue(unsigned long dummy);
-
-static DECLARE_TASKLET_DISABLED(tipc_tasklet, process_signal_queue, 0);
-
-
-unsigned int tipc_k_signal(Handler routine, unsigned long argument)
-{
-	struct queue_item *item;
-
-	if (!handler_enabled) {
-		err("Signal request ignored by handler\n");
-		return -ENOPROTOOPT;
-	}
-
-	spin_lock_bh(&qitem_lock);
-	item = kmem_cache_alloc(tipc_queue_item_cache, GFP_ATOMIC);
-	if (!item) {
-		err("Signal queue out of memory\n");
-		spin_unlock_bh(&qitem_lock);
-		return -ENOMEM;
-	}
-	item->handler = routine;
-	item->data = argument;
-	list_add_tail(&item->next_signal, &signal_queue_head);
-	spin_unlock_bh(&qitem_lock);
-	tasklet_schedule(&tipc_tasklet);
-	return 0;
-}
-
-static void process_signal_queue(unsigned long dummy)
-{
-	struct queue_item *__volatile__ item;
-	struct list_head *l, *n;
-
-	spin_lock_bh(&qitem_lock);
-	list_for_each_safe(l, n, &signal_queue_head) {
-		item = list_entry(l, struct queue_item, next_signal);
-		list_del(&item->next_signal);
-		spin_unlock_bh(&qitem_lock);
-		item->handler(item->data);
-		spin_lock_bh(&qitem_lock);
-		kmem_cache_free(tipc_queue_item_cache, item);
-	}
-	spin_unlock_bh(&qitem_lock);
-}
-
-int tipc_handler_start(void)
-{
-	tipc_queue_item_cache =
-		kmem_cache_create("tipc_queue_items", sizeof(struct queue_item),
-				  0, SLAB_HWCACHE_ALIGN, NULL);
-	if (!tipc_queue_item_cache)
-		return -ENOMEM;
-
-	INIT_LIST_HEAD(&signal_queue_head);
-	tasklet_enable(&tipc_tasklet);
-	handler_enabled = 1;
-	return 0;
-}
-
-void tipc_handler_stop(void)
-{
-	struct list_head *l, *n;
-	struct queue_item *item;
-
-	if (!handler_enabled)
-		return;
-
-	handler_enabled = 0;
-	tasklet_disable(&tipc_tasklet);
-	tasklet_kill(&tipc_tasklet);
-
-	spin_lock_bh(&qitem_lock);
-	list_for_each_safe(l, n, &signal_queue_head) {
-		item = list_entry(l, struct queue_item, next_signal);
-		list_del(&item->next_signal);
-		kmem_cache_free(tipc_queue_item_cache, item);
-	}
-	spin_unlock_bh(&qitem_lock);
-
-	kmem_cache_destroy(tipc_queue_item_cache);
-}
-
diff -ruN linux-2.6.29/net/tipc/Kconfig android_cluster/linux-2.6.29/net/tipc/Kconfig
--- linux-2.6.29/net/tipc/Kconfig	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/Kconfig	2014-05-27 23:04:10.634023603 -0700
@@ -23,91 +23,168 @@
 if TIPC
 
 config TIPC_ADVANCED
-	bool "TIPC: Advanced configuration"
+	bool "Advanced TIPC configuration"
+	depends on TIPC
 	default n
 	help
-	  Saying Y here will open some advanced configuration
-          for TIPC. Most users do not need to bother, so if
-          unsure, just say N.
+	  Saying Y here will open some advanced configuration for TIPC. 
+	  Most users do not need to bother; if unsure, just say N.
 
-config TIPC_ZONES
-	int "Maximum number of zones in network"
-	depends on TIPC_ADVANCED
-	default "3"
+config TIPC_NETID
+	int "Network identifier"
+	depends on TIPC && TIPC_ADVANCED
+	default "4711"
+	help
+	  Specifies which TIPC network this node belongs to. 
+	  Can range from 1 to 9999; default is 4711.
+	  
+config TIPC_REMOTE_MNG
+	bool "Enable remote management"
+	depends on TIPC && TIPC_ADVANCED
+	default y
 	help
-	 Max number of zones inside TIPC network. Max supported value 
-         is 255 zones, minimum is 1
+ 	  Specifies if this node can be remotely managed from another node
+	  in the TIPC network.  By default, this capability is enabled.
 
-	 Default is 3 zones in a network; setting this to higher
-	 allows more zones but might use more memory.
+config TIPC_PORTS
+	int "Maximum number of ports in own node"
+	depends on TIPC && TIPC_ADVANCED
+	default "8191"
+	help
+	  Specifies how many ports can be supported by this node. 
+	  Can range from 127 to 65536; default is 8191. 
 
-config TIPC_CLUSTERS
-	int "Maximum number of clusters in a zone"
-	depends on TIPC_ADVANCED
-	default "1"
-	help
-          ***Only 1 (one cluster in a zone) is supported by current code.
-          Any value set here will be overridden.***
-
-          (Max number of clusters inside TIPC zone. Max supported 
-          value is 4095 clusters, minimum is 1.
-
-	  Default is 1; setting this to smaller value might save 
-          some memory, setting it to higher
-	  allows more clusters and might consume more memory.)
+	  Setting this to a smaller value saves some memory; 
+	  setting it to a higher value allows more ports.
 
 config TIPC_NODES
-	int "Maximum number of nodes in cluster"
-	depends on TIPC_ADVANCED
+	int "Maximum number of nodes in own cluster"
+	depends on TIPC && TIPC_ADVANCED
 	default "255"
 	help
-	  Maximum number of nodes inside a TIPC cluster. Maximum 
-          supported value is 2047 nodes, minimum is 8. 
+	  Specifies how many nodes can be supported in own TIPC cluster. 
+	  Can range from 8 to 4095 nodes; default is 255. 
 
-	  Setting this to a smaller value saves some memory, 
-	  setting it to higher allows more nodes.
+	  Setting this to a smaller value saves some memory; 
+	  setting it to a higher value allows more nodes.
 
-config TIPC_SLAVE_NODES
-	int "Maximum number of slave nodes in cluster"
-	depends on TIPC_ADVANCED
-	default "0"
+config TIPC_CLUSTERS
+	int "Maximum number of clusters in own zone"
+	depends on TIPC && TIPC_ADVANCED
+	default "8"
 	help
-          ***This capability is not supported by current code.***
-	  
-	  Maximum number of slave nodes inside a TIPC cluster. Maximum 
-          supported value is 2047 nodes, minimum is 0. 
+	  Specifies how many clusters can be supported in own TIPC zone. 
+	  Can range from 1 to 4095 clusters; default is 8. 
 
-	  Setting this to a smaller value saves some memory, 
-	  setting it to higher allows more nodes.
+	  Setting this to a smaller value saves some memory; 
+	  setting it to a higher value allows for more clusters.
 
-config TIPC_PORTS
-	int "Maximum number of ports in a node"
-	depends on TIPC_ADVANCED
-	default "8191"
+config TIPC_ZONES
+	int "Maximum number of zones in own network"
+	depends on TIPC && TIPC_ADVANCED
+	default "4"
+	help
+	  Specifies how many zones can be supported in own TIPC network. 
+	  Can range from 1 to 255 zones; default is 4. 
+
+	  Setting this to a smaller value saves some memory; 
+	  setting it to a higher value allows for more zones.
+
+config TIPC_REMOTES
+	int "Maximum number of neighbor nodes in other clusters"
+	depends on TIPC && TIPC_ADVANCED
+	default "8"
 	help
-	  Maximum number of ports within a node. Maximum 
-          supported value is 64535 nodes, minimum is 127. 
+	  Specifies how many nodes in other clusters this node can establish
+	  links to.  Can range from 0 to 255 nodes; default is 8. 
+
+	  Setting this to a smaller value saves some memory; 
+	  setting it to a higher value allows more inter-cluster links.
 
-	  Setting this to a smaller value saves some memory, 
-	  setting it to higher allows more ports.
+config TIPC_PUBL
+	int "Maximum number of name publications by own node"
+	depends on TIPC && TIPC_ADVANCED
+	default "10000"
+	help
+	  Specifies how many concurrent name publications this node can issue.
+	  Can range from 1 to 65535 publications; default is 10000. 
+
+	  Setting this to a smaller value saves some memory; 
+	  setting it to a higher value allows more name publications.
+
+config TIPC_SUBSCR
+	int "Maximum number of name subscriptions by own node"
+	depends on TIPC && TIPC_ADVANCED
+	default "2000"
+	help
+	  Specifies how many concurrent name subscriptions this node can handle.
+	  Can range from 1 to 65535 subscriptions; default is 2000. 
+
+	  Setting this to a smaller value saves some memory; 
+	  setting it to a higher value allows more name subscriptions.
 
 config TIPC_LOG
 	int "Size of log buffer"
-	depends on TIPC_ADVANCED
+	depends on TIPC && TIPC_ADVANCED
 	default 0
 	help
  	  Size (in bytes) of TIPC's internal log buffer, which records the
-	  occurrence of significant events.  Maximum supported value
-	  is 32768 bytes, minimum is 0.
+	  occurrence of significant events.  Can range from 0 to 32768 bytes;
+	  default is 0.
 
 	  There is no need to enable the log buffer unless the node will be
 	  managed remotely via TIPC.
 
+config TIPC_UNICLUSTER_FRIENDLY
+	bool "Inter-operate with uni-cluster nodes"
+	depends on TIPC
+	default y
+	help
+ 	  This allows TIPC to communicate with nodes in its cluster that
+	  pre-date the introduction of multi-cluster TIPC support (that is,
+	  nodes running TIPC 1.6 or earlier).
+
+config TIPC_MULTIPLE_LINKS
+	bool "Enable redundant link support"
+	depends on TIPC
+	default y
+	help
+ 	  This allows TIPC to establish multiple links to neighboring nodes
+	  wherever possible.  In the event of link failure TIPC will redirect
+	  messages on that link to an alternate link if one is available.
+
+config TIPC_CONFIG_SERVICE
+	bool "Enable configuration service"
+	depends on TIPC
+	default y
+	help
+ 	  This allows TIPC to be dynamically configured and monitored
+	  using the tipc-config tool.
+
+config TIPC_SOCKET_API
+	bool "Enable socket support"
+	depends on TIPC
+	default y
+	help
+ 	  This allows TIPC to support sockets of the AF_TIPC address family.
+
+config TIPC_SYSTEM_MSGS
+	bool "Enable system messages"
+	depends on TIPC
+	default y
+	help
+ 	  This allows TIPC to record the occurrence of significant events
+	  in the system log (and, if enabled, TIPC's own internal log buffer).
+	  These events include errors, warnings, and informatory notices, and
+	  can be useful in monitoring the operation of TIPC and in detecting
+	  and diagnosing problems.
+
 config TIPC_DEBUG
-	bool "Enable debugging support"
+	bool "Enable debug messages"
+	depends on TIPC
 	default n
 	help
- 	  This will enable debugging of TIPC.
+ 	  This enables debugging of TIPC.
 
 	  Only say Y here if you are having trouble with TIPC.  It will
 	  enable the display of detailed information about what is going on.
diff -ruN linux-2.6.29/net/tipc/link.c android_cluster/linux-2.6.29/net/tipc/link.c
--- linux-2.6.29/net/tipc/link.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/link.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,3358 +0,0 @@
-/*
- * net/tipc/link.c: TIPC link code
- *
- * Copyright (c) 1996-2007, Ericsson AB
- * Copyright (c) 2004-2007, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "dbg.h"
-#include "link.h"
-#include "net.h"
-#include "node.h"
-#include "port.h"
-#include "addr.h"
-#include "node_subscr.h"
-#include "name_distr.h"
-#include "bearer.h"
-#include "name_table.h"
-#include "discover.h"
-#include "config.h"
-#include "bcast.h"
-
-
-/*
- * Out-of-range value for link session numbers
- */
-
-#define INVALID_SESSION 0x10000
-
-/*
- * Limit for deferred reception queue:
- */
-
-#define DEF_QUEUE_LIMIT 256u
-
-/*
- * Link state events:
- */
-
-#define  STARTING_EVT    856384768	/* link processing trigger */
-#define  TRAFFIC_MSG_EVT 560815u	/* rx'd ??? */
-#define  TIMEOUT_EVT     560817u	/* link timer expired */
-
-/*
- * The following two 'message types' is really just implementation
- * data conveniently stored in the message header.
- * They must not be considered part of the protocol
- */
-#define OPEN_MSG   0
-#define CLOSED_MSG 1
-
-/*
- * State value stored in 'exp_msg_count'
- */
-
-#define START_CHANGEOVER 100000u
-
-/**
- * struct link_name - deconstructed link name
- * @addr_local: network address of node at this end
- * @if_local: name of interface at this end
- * @addr_peer: network address of node at far end
- * @if_peer: name of interface at far end
- */
-
-struct link_name {
-	u32 addr_local;
-	char if_local[TIPC_MAX_IF_NAME];
-	u32 addr_peer;
-	char if_peer[TIPC_MAX_IF_NAME];
-};
-
-#if 0
-
-/* LINK EVENT CODE IS NOT SUPPORTED AT PRESENT */
-
-/**
- * struct link_event - link up/down event notification
- */
-
-struct link_event {
-	u32 addr;
-	int up;
-	void (*fcn)(u32, char *, int);
-	char name[TIPC_MAX_LINK_NAME];
-};
-
-#endif
-
-static void link_handle_out_of_seq_msg(struct link *l_ptr,
-				       struct sk_buff *buf);
-static void link_recv_proto_msg(struct link *l_ptr, struct sk_buff *buf);
-static int  link_recv_changeover_msg(struct link **l_ptr, struct sk_buff **buf);
-static void link_set_supervision_props(struct link *l_ptr, u32 tolerance);
-static int  link_send_sections_long(struct port *sender,
-				    struct iovec const *msg_sect,
-				    u32 num_sect, u32 destnode);
-static void link_check_defragm_bufs(struct link *l_ptr);
-static void link_state_event(struct link *l_ptr, u32 event);
-static void link_reset_statistics(struct link *l_ptr);
-static void link_print(struct link *l_ptr, struct print_buf *buf,
-		       const char *str);
-
-/*
- * Debugging code used by link routines only
- *
- * When debugging link problems on a system that has multiple links,
- * the standard TIPC debugging routines may not be useful since they
- * allow the output from multiple links to be intermixed.  For this reason
- * routines of the form "dbg_link_XXX()" have been created that will capture
- * debug info into a link's personal print buffer, which can then be dumped
- * into the TIPC system log (TIPC_LOG) upon request.
- *
- * To enable per-link debugging, use LINK_LOG_BUF_SIZE to specify the size
- * of the print buffer used by each link.  If LINK_LOG_BUF_SIZE is set to 0,
- * the dbg_link_XXX() routines simply send their output to the standard
- * debug print buffer (DBG_OUTPUT), if it has been defined; this can be useful
- * when there is only a single link in the system being debugged.
- *
- * Notes:
- * - When enabled, LINK_LOG_BUF_SIZE should be set to at least TIPC_PB_MIN_SIZE
- * - "l_ptr" must be valid when using dbg_link_XXX() macros
- */
-
-#define LINK_LOG_BUF_SIZE 0
-
-#define dbg_link(fmt, arg...) \
-	do { \
-		if (LINK_LOG_BUF_SIZE) \
-			tipc_printf(&l_ptr->print_buf, fmt, ## arg); \
-	} while (0)
-#define dbg_link_msg(msg, txt) \
-	do { \
-		if (LINK_LOG_BUF_SIZE) \
-			tipc_msg_dbg(&l_ptr->print_buf, msg, txt); \
-	} while (0)
-#define dbg_link_state(txt) \
-	do { \
-		if (LINK_LOG_BUF_SIZE) \
-			link_print(l_ptr, &l_ptr->print_buf, txt); \
-	} while (0)
-#define dbg_link_dump() do { \
-	if (LINK_LOG_BUF_SIZE) { \
-		tipc_printf(LOG, "\n\nDumping link <%s>:\n", l_ptr->name); \
-		tipc_printbuf_move(LOG, &l_ptr->print_buf); \
-	} \
-} while (0)
-
-static void dbg_print_link(struct link *l_ptr, const char *str)
-{
-	if (DBG_OUTPUT != TIPC_NULL)
-		link_print(l_ptr, DBG_OUTPUT, str);
-}
-
-static void dbg_print_buf_chain(struct sk_buff *root_buf)
-{
-	if (DBG_OUTPUT != TIPC_NULL) {
-		struct sk_buff *buf = root_buf;
-
-		while (buf) {
-			msg_dbg(buf_msg(buf), "In chain: ");
-			buf = buf->next;
-		}
-	}
-}
-
-/*
- *  Simple link routines
- */
-
-static unsigned int align(unsigned int i)
-{
-	return (i + 3) & ~3u;
-}
-
-static int link_working_working(struct link *l_ptr)
-{
-	return (l_ptr->state == WORKING_WORKING);
-}
-
-static int link_working_unknown(struct link *l_ptr)
-{
-	return (l_ptr->state == WORKING_UNKNOWN);
-}
-
-static int link_reset_unknown(struct link *l_ptr)
-{
-	return (l_ptr->state == RESET_UNKNOWN);
-}
-
-static int link_reset_reset(struct link *l_ptr)
-{
-	return (l_ptr->state == RESET_RESET);
-}
-
-static int link_blocked(struct link *l_ptr)
-{
-	return (l_ptr->exp_msg_count || l_ptr->blocked);
-}
-
-static int link_congested(struct link *l_ptr)
-{
-	return (l_ptr->out_queue_size >= l_ptr->queue_limit[0]);
-}
-
-static u32 link_max_pkt(struct link *l_ptr)
-{
-	return l_ptr->max_pkt;
-}
-
-static void link_init_max_pkt(struct link *l_ptr)
-{
-	u32 max_pkt;
-
-	max_pkt = (l_ptr->b_ptr->publ.mtu & ~3);
-	if (max_pkt > MAX_MSG_SIZE)
-		max_pkt = MAX_MSG_SIZE;
-
-	l_ptr->max_pkt_target = max_pkt;
-	if (l_ptr->max_pkt_target < MAX_PKT_DEFAULT)
-		l_ptr->max_pkt = l_ptr->max_pkt_target;
-	else
-		l_ptr->max_pkt = MAX_PKT_DEFAULT;
-
-	l_ptr->max_pkt_probes = 0;
-}
-
-static u32 link_next_sent(struct link *l_ptr)
-{
-	if (l_ptr->next_out)
-		return msg_seqno(buf_msg(l_ptr->next_out));
-	return mod(l_ptr->next_out_no);
-}
-
-static u32 link_last_sent(struct link *l_ptr)
-{
-	return mod(link_next_sent(l_ptr) - 1);
-}
-
-/*
- *  Simple non-static link routines (i.e. referenced outside this file)
- */
-
-int tipc_link_is_up(struct link *l_ptr)
-{
-	if (!l_ptr)
-		return 0;
-	return (link_working_working(l_ptr) || link_working_unknown(l_ptr));
-}
-
-int tipc_link_is_active(struct link *l_ptr)
-{
-	return ((l_ptr->owner->active_links[0] == l_ptr) ||
-		(l_ptr->owner->active_links[1] == l_ptr));
-}
-
-/**
- * link_name_validate - validate & (optionally) deconstruct link name
- * @name - ptr to link name string
- * @name_parts - ptr to area for link name components (or NULL if not needed)
- *
- * Returns 1 if link name is valid, otherwise 0.
- */
-
-static int link_name_validate(const char *name, struct link_name *name_parts)
-{
-	char name_copy[TIPC_MAX_LINK_NAME];
-	char *addr_local;
-	char *if_local;
-	char *addr_peer;
-	char *if_peer;
-	char dummy;
-	u32 z_local, c_local, n_local;
-	u32 z_peer, c_peer, n_peer;
-	u32 if_local_len;
-	u32 if_peer_len;
-
-	/* copy link name & ensure length is OK */
-
-	name_copy[TIPC_MAX_LINK_NAME - 1] = 0;
-	/* need above in case non-Posix strncpy() doesn't pad with nulls */
-	strncpy(name_copy, name, TIPC_MAX_LINK_NAME);
-	if (name_copy[TIPC_MAX_LINK_NAME - 1] != 0)
-		return 0;
-
-	/* ensure all component parts of link name are present */
-
-	addr_local = name_copy;
-	if ((if_local = strchr(addr_local, ':')) == NULL)
-		return 0;
-	*(if_local++) = 0;
-	if ((addr_peer = strchr(if_local, '-')) == NULL)
-		return 0;
-	*(addr_peer++) = 0;
-	if_local_len = addr_peer - if_local;
-	if ((if_peer = strchr(addr_peer, ':')) == NULL)
-		return 0;
-	*(if_peer++) = 0;
-	if_peer_len = strlen(if_peer) + 1;
-
-	/* validate component parts of link name */
-
-	if ((sscanf(addr_local, "%u.%u.%u%c",
-		    &z_local, &c_local, &n_local, &dummy) != 3) ||
-	    (sscanf(addr_peer, "%u.%u.%u%c",
-		    &z_peer, &c_peer, &n_peer, &dummy) != 3) ||
-	    (z_local > 255) || (c_local > 4095) || (n_local > 4095) ||
-	    (z_peer  > 255) || (c_peer  > 4095) || (n_peer  > 4095) ||
-	    (if_local_len <= 1) || (if_local_len > TIPC_MAX_IF_NAME) ||
-	    (if_peer_len  <= 1) || (if_peer_len  > TIPC_MAX_IF_NAME) ||
-	    (strspn(if_local, tipc_alphabet) != (if_local_len - 1)) ||
-	    (strspn(if_peer, tipc_alphabet) != (if_peer_len - 1)))
-		return 0;
-
-	/* return link name components, if necessary */
-
-	if (name_parts) {
-		name_parts->addr_local = tipc_addr(z_local, c_local, n_local);
-		strcpy(name_parts->if_local, if_local);
-		name_parts->addr_peer = tipc_addr(z_peer, c_peer, n_peer);
-		strcpy(name_parts->if_peer, if_peer);
-	}
-	return 1;
-}
-
-/**
- * link_timeout - handle expiration of link timer
- * @l_ptr: pointer to link
- *
- * This routine must not grab "tipc_net_lock" to avoid a potential deadlock conflict
- * with tipc_link_delete().  (There is no risk that the node will be deleted by
- * another thread because tipc_link_delete() always cancels the link timer before
- * tipc_node_delete() is called.)
- */
-
-static void link_timeout(struct link *l_ptr)
-{
-	tipc_node_lock(l_ptr->owner);
-
-	/* update counters used in statistical profiling of send traffic */
-
-	l_ptr->stats.accu_queue_sz += l_ptr->out_queue_size;
-	l_ptr->stats.queue_sz_counts++;
-
-	if (l_ptr->out_queue_size > l_ptr->stats.max_queue_sz)
-		l_ptr->stats.max_queue_sz = l_ptr->out_queue_size;
-
-	if (l_ptr->first_out) {
-		struct tipc_msg *msg = buf_msg(l_ptr->first_out);
-		u32 length = msg_size(msg);
-
-		if ((msg_user(msg) == MSG_FRAGMENTER)
-		    && (msg_type(msg) == FIRST_FRAGMENT)) {
-			length = msg_size(msg_get_wrapped(msg));
-		}
-		if (length) {
-			l_ptr->stats.msg_lengths_total += length;
-			l_ptr->stats.msg_length_counts++;
-			if (length <= 64)
-				l_ptr->stats.msg_length_profile[0]++;
-			else if (length <= 256)
-				l_ptr->stats.msg_length_profile[1]++;
-			else if (length <= 1024)
-				l_ptr->stats.msg_length_profile[2]++;
-			else if (length <= 4096)
-				l_ptr->stats.msg_length_profile[3]++;
-			else if (length <= 16384)
-				l_ptr->stats.msg_length_profile[4]++;
-			else if (length <= 32768)
-				l_ptr->stats.msg_length_profile[5]++;
-			else
-				l_ptr->stats.msg_length_profile[6]++;
-		}
-	}
-
-	/* do all other link processing performed on a periodic basis */
-
-	link_check_defragm_bufs(l_ptr);
-
-	link_state_event(l_ptr, TIMEOUT_EVT);
-
-	if (l_ptr->next_out)
-		tipc_link_push_queue(l_ptr);
-
-	tipc_node_unlock(l_ptr->owner);
-}
-
-static void link_set_timer(struct link *l_ptr, u32 time)
-{
-	k_start_timer(&l_ptr->timer, time);
-}
-
-/**
- * tipc_link_create - create a new link
- * @b_ptr: pointer to associated bearer
- * @peer: network address of node at other end of link
- * @media_addr: media address to use when sending messages over link
- *
- * Returns pointer to link.
- */
-
-struct link *tipc_link_create(struct bearer *b_ptr, const u32 peer,
-			      const struct tipc_media_addr *media_addr)
-{
-	struct link *l_ptr;
-	struct tipc_msg *msg;
-	char *if_name;
-
-	l_ptr = kzalloc(sizeof(*l_ptr), GFP_ATOMIC);
-	if (!l_ptr) {
-		warn("Link creation failed, no memory\n");
-		return NULL;
-	}
-
-	if (LINK_LOG_BUF_SIZE) {
-		char *pb = kmalloc(LINK_LOG_BUF_SIZE, GFP_ATOMIC);
-
-		if (!pb) {
-			kfree(l_ptr);
-			warn("Link creation failed, no memory for print buffer\n");
-			return NULL;
-		}
-		tipc_printbuf_init(&l_ptr->print_buf, pb, LINK_LOG_BUF_SIZE);
-	}
-
-	l_ptr->addr = peer;
-	if_name = strchr(b_ptr->publ.name, ':') + 1;
-	sprintf(l_ptr->name, "%u.%u.%u:%s-%u.%u.%u:",
-		tipc_zone(tipc_own_addr), tipc_cluster(tipc_own_addr),
-		tipc_node(tipc_own_addr),
-		if_name,
-		tipc_zone(peer), tipc_cluster(peer), tipc_node(peer));
-		/* note: peer i/f is appended to link name by reset/activate */
-	memcpy(&l_ptr->media_addr, media_addr, sizeof(*media_addr));
-	l_ptr->checkpoint = 1;
-	l_ptr->b_ptr = b_ptr;
-	link_set_supervision_props(l_ptr, b_ptr->media->tolerance);
-	l_ptr->state = RESET_UNKNOWN;
-
-	l_ptr->pmsg = (struct tipc_msg *)&l_ptr->proto_msg;
-	msg = l_ptr->pmsg;
-	msg_init(msg, LINK_PROTOCOL, RESET_MSG, INT_H_SIZE, l_ptr->addr);
-	msg_set_size(msg, sizeof(l_ptr->proto_msg));
-	msg_set_session(msg, (tipc_random & 0xffff));
-	msg_set_bearer_id(msg, b_ptr->identity);
-	strcpy((char *)msg_data(msg), if_name);
-
-	l_ptr->priority = b_ptr->priority;
-	tipc_link_set_queue_limits(l_ptr, b_ptr->media->window);
-
-	link_init_max_pkt(l_ptr);
-
-	l_ptr->next_out_no = 1;
-	INIT_LIST_HEAD(&l_ptr->waiting_ports);
-
-	link_reset_statistics(l_ptr);
-
-	l_ptr->owner = tipc_node_attach_link(l_ptr);
-	if (!l_ptr->owner) {
-		if (LINK_LOG_BUF_SIZE)
-			kfree(l_ptr->print_buf.buf);
-		kfree(l_ptr);
-		return NULL;
-	}
-
-	k_init_timer(&l_ptr->timer, (Handler)link_timeout, (unsigned long)l_ptr);
-	list_add_tail(&l_ptr->link_list, &b_ptr->links);
-	tipc_k_signal((Handler)tipc_link_start, (unsigned long)l_ptr);
-
-	dbg("tipc_link_create(): tolerance = %u,cont intv = %u, abort_limit = %u\n",
-	    l_ptr->tolerance, l_ptr->continuity_interval, l_ptr->abort_limit);
-
-	return l_ptr;
-}
-
-/**
- * tipc_link_delete - delete a link
- * @l_ptr: pointer to link
- *
- * Note: 'tipc_net_lock' is write_locked, bearer is locked.
- * This routine must not grab the node lock until after link timer cancellation
- * to avoid a potential deadlock situation.
- */
-
-void tipc_link_delete(struct link *l_ptr)
-{
-	if (!l_ptr) {
-		err("Attempt to delete non-existent link\n");
-		return;
-	}
-
-	dbg("tipc_link_delete()\n");
-
-	k_cancel_timer(&l_ptr->timer);
-
-	tipc_node_lock(l_ptr->owner);
-	tipc_link_reset(l_ptr);
-	tipc_node_detach_link(l_ptr->owner, l_ptr);
-	tipc_link_stop(l_ptr);
-	list_del_init(&l_ptr->link_list);
-	if (LINK_LOG_BUF_SIZE)
-		kfree(l_ptr->print_buf.buf);
-	tipc_node_unlock(l_ptr->owner);
-	k_term_timer(&l_ptr->timer);
-	kfree(l_ptr);
-}
-
-void tipc_link_start(struct link *l_ptr)
-{
-	dbg("tipc_link_start %x\n", l_ptr);
-	link_state_event(l_ptr, STARTING_EVT);
-}
-
-/**
- * link_schedule_port - schedule port for deferred sending
- * @l_ptr: pointer to link
- * @origport: reference to sending port
- * @sz: amount of data to be sent
- *
- * Schedules port for renewed sending of messages after link congestion
- * has abated.
- */
-
-static int link_schedule_port(struct link *l_ptr, u32 origport, u32 sz)
-{
-	struct port *p_ptr;
-
-	spin_lock_bh(&tipc_port_list_lock);
-	p_ptr = tipc_port_lock(origport);
-	if (p_ptr) {
-		if (!p_ptr->wakeup)
-			goto exit;
-		if (!list_empty(&p_ptr->wait_list))
-			goto exit;
-		p_ptr->congested_link = l_ptr;
-		p_ptr->publ.congested = 1;
-		p_ptr->waiting_pkts = 1 + ((sz - 1) / link_max_pkt(l_ptr));
-		list_add_tail(&p_ptr->wait_list, &l_ptr->waiting_ports);
-		l_ptr->stats.link_congs++;
-exit:
-		tipc_port_unlock(p_ptr);
-	}
-	spin_unlock_bh(&tipc_port_list_lock);
-	return -ELINKCONG;
-}
-
-void tipc_link_wakeup_ports(struct link *l_ptr, int all)
-{
-	struct port *p_ptr;
-	struct port *temp_p_ptr;
-	int win = l_ptr->queue_limit[0] - l_ptr->out_queue_size;
-
-	if (all)
-		win = 100000;
-	if (win <= 0)
-		return;
-	if (!spin_trylock_bh(&tipc_port_list_lock))
-		return;
-	if (link_congested(l_ptr))
-		goto exit;
-	list_for_each_entry_safe(p_ptr, temp_p_ptr, &l_ptr->waiting_ports,
-				 wait_list) {
-		if (win <= 0)
-			break;
-		list_del_init(&p_ptr->wait_list);
-		p_ptr->congested_link = NULL;
-		spin_lock_bh(p_ptr->publ.lock);
-		p_ptr->publ.congested = 0;
-		p_ptr->wakeup(&p_ptr->publ);
-		win -= p_ptr->waiting_pkts;
-		spin_unlock_bh(p_ptr->publ.lock);
-	}
-
-exit:
-	spin_unlock_bh(&tipc_port_list_lock);
-}
-
-/**
- * link_release_outqueue - purge link's outbound message queue
- * @l_ptr: pointer to link
- */
-
-static void link_release_outqueue(struct link *l_ptr)
-{
-	struct sk_buff *buf = l_ptr->first_out;
-	struct sk_buff *next;
-
-	while (buf) {
-		next = buf->next;
-		buf_discard(buf);
-		buf = next;
-	}
-	l_ptr->first_out = NULL;
-	l_ptr->out_queue_size = 0;
-}
-
-/**
- * tipc_link_reset_fragments - purge link's inbound message fragments queue
- * @l_ptr: pointer to link
- */
-
-void tipc_link_reset_fragments(struct link *l_ptr)
-{
-	struct sk_buff *buf = l_ptr->defragm_buf;
-	struct sk_buff *next;
-
-	while (buf) {
-		next = buf->next;
-		buf_discard(buf);
-		buf = next;
-	}
-	l_ptr->defragm_buf = NULL;
-}
-
-/**
- * tipc_link_stop - purge all inbound and outbound messages associated with link
- * @l_ptr: pointer to link
- */
-
-void tipc_link_stop(struct link *l_ptr)
-{
-	struct sk_buff *buf;
-	struct sk_buff *next;
-
-	buf = l_ptr->oldest_deferred_in;
-	while (buf) {
-		next = buf->next;
-		buf_discard(buf);
-		buf = next;
-	}
-
-	buf = l_ptr->first_out;
-	while (buf) {
-		next = buf->next;
-		buf_discard(buf);
-		buf = next;
-	}
-
-	tipc_link_reset_fragments(l_ptr);
-
-	buf_discard(l_ptr->proto_msg_queue);
-	l_ptr->proto_msg_queue = NULL;
-}
-
-#if 0
-
-/* LINK EVENT CODE IS NOT SUPPORTED AT PRESENT */
-
-static void link_recv_event(struct link_event *ev)
-{
-	ev->fcn(ev->addr, ev->name, ev->up);
-	kfree(ev);
-}
-
-static void link_send_event(void (*fcn)(u32 a, char *n, int up),
-			    struct link *l_ptr, int up)
-{
-	struct link_event *ev;
-
-	ev = kmalloc(sizeof(*ev), GFP_ATOMIC);
-	if (!ev) {
-		warn("Link event allocation failure\n");
-		return;
-	}
-	ev->addr = l_ptr->addr;
-	ev->up = up;
-	ev->fcn = fcn;
-	memcpy(ev->name, l_ptr->name, TIPC_MAX_LINK_NAME);
-	tipc_k_signal((Handler)link_recv_event, (unsigned long)ev);
-}
-
-#else
-
-#define link_send_event(fcn, l_ptr, up) do { } while (0)
-
-#endif
-
-void tipc_link_reset(struct link *l_ptr)
-{
-	struct sk_buff *buf;
-	u32 prev_state = l_ptr->state;
-	u32 checkpoint = l_ptr->next_in_no;
-	int was_active_link = tipc_link_is_active(l_ptr);
-
-	msg_set_session(l_ptr->pmsg, ((msg_session(l_ptr->pmsg) + 1) & 0xffff));
-
-	/* Link is down, accept any session */
-	l_ptr->peer_session = INVALID_SESSION;
-
-	/* Prepare for max packet size negotiation */
-	link_init_max_pkt(l_ptr);
-
-	l_ptr->state = RESET_UNKNOWN;
-	dbg_link_state("Resetting Link\n");
-
-	if ((prev_state == RESET_UNKNOWN) || (prev_state == RESET_RESET))
-		return;
-
-	tipc_node_link_down(l_ptr->owner, l_ptr);
-	tipc_bearer_remove_dest(l_ptr->b_ptr, l_ptr->addr);
-#if 0
-	tipc_printf(TIPC_CONS, "\nReset link <%s>\n", l_ptr->name);
-	dbg_link_dump();
-#endif
-	if (was_active_link && tipc_node_has_active_links(l_ptr->owner) &&
-	    l_ptr->owner->permit_changeover) {
-		l_ptr->reset_checkpoint = checkpoint;
-		l_ptr->exp_msg_count = START_CHANGEOVER;
-	}
-
-	/* Clean up all queues: */
-
-	link_release_outqueue(l_ptr);
-	buf_discard(l_ptr->proto_msg_queue);
-	l_ptr->proto_msg_queue = NULL;
-	buf = l_ptr->oldest_deferred_in;
-	while (buf) {
-		struct sk_buff *next = buf->next;
-		buf_discard(buf);
-		buf = next;
-	}
-	if (!list_empty(&l_ptr->waiting_ports))
-		tipc_link_wakeup_ports(l_ptr, 1);
-
-	l_ptr->retransm_queue_head = 0;
-	l_ptr->retransm_queue_size = 0;
-	l_ptr->last_out = NULL;
-	l_ptr->first_out = NULL;
-	l_ptr->next_out = NULL;
-	l_ptr->unacked_window = 0;
-	l_ptr->checkpoint = 1;
-	l_ptr->next_out_no = 1;
-	l_ptr->deferred_inqueue_sz = 0;
-	l_ptr->oldest_deferred_in = NULL;
-	l_ptr->newest_deferred_in = NULL;
-	l_ptr->fsm_msg_cnt = 0;
-	l_ptr->stale_count = 0;
-	link_reset_statistics(l_ptr);
-
-	link_send_event(tipc_cfg_link_event, l_ptr, 0);
-	if (!in_own_cluster(l_ptr->addr))
-		link_send_event(tipc_disc_link_event, l_ptr, 0);
-}
-
-
-static void link_activate(struct link *l_ptr)
-{
-	l_ptr->next_in_no = l_ptr->stats.recv_info = 1;
-	tipc_node_link_up(l_ptr->owner, l_ptr);
-	tipc_bearer_add_dest(l_ptr->b_ptr, l_ptr->addr);
-	link_send_event(tipc_cfg_link_event, l_ptr, 1);
-	if (!in_own_cluster(l_ptr->addr))
-		link_send_event(tipc_disc_link_event, l_ptr, 1);
-}
-
-/**
- * link_state_event - link finite state machine
- * @l_ptr: pointer to link
- * @event: state machine event to process
- */
-
-static void link_state_event(struct link *l_ptr, unsigned event)
-{
-	struct link *other;
-	u32 cont_intv = l_ptr->continuity_interval;
-
-	if (!l_ptr->started && (event != STARTING_EVT))
-		return;		/* Not yet. */
-
-	if (link_blocked(l_ptr)) {
-		if (event == TIMEOUT_EVT) {
-			link_set_timer(l_ptr, cont_intv);
-		}
-		return;	  /* Changeover going on */
-	}
-	dbg_link("STATE_EV: <%s> ", l_ptr->name);
-
-	switch (l_ptr->state) {
-	case WORKING_WORKING:
-		dbg_link("WW/");
-		switch (event) {
-		case TRAFFIC_MSG_EVT:
-			dbg_link("TRF-");
-			/* fall through */
-		case ACTIVATE_MSG:
-			dbg_link("ACT\n");
-			break;
-		case TIMEOUT_EVT:
-			dbg_link("TIM ");
-			if (l_ptr->next_in_no != l_ptr->checkpoint) {
-				l_ptr->checkpoint = l_ptr->next_in_no;
-				if (tipc_bclink_acks_missing(l_ptr->owner)) {
-					tipc_link_send_proto_msg(l_ptr, STATE_MSG,
-								 0, 0, 0, 0, 0);
-					l_ptr->fsm_msg_cnt++;
-				} else if (l_ptr->max_pkt < l_ptr->max_pkt_target) {
-					tipc_link_send_proto_msg(l_ptr, STATE_MSG,
-								 1, 0, 0, 0, 0);
-					l_ptr->fsm_msg_cnt++;
-				}
-				link_set_timer(l_ptr, cont_intv);
-				break;
-			}
-			dbg_link(" -> WU\n");
-			l_ptr->state = WORKING_UNKNOWN;
-			l_ptr->fsm_msg_cnt = 0;
-			tipc_link_send_proto_msg(l_ptr, STATE_MSG, 1, 0, 0, 0, 0);
-			l_ptr->fsm_msg_cnt++;
-			link_set_timer(l_ptr, cont_intv / 4);
-			break;
-		case RESET_MSG:
-			dbg_link("RES -> RR\n");
-			info("Resetting link <%s>, requested by peer\n",
-			     l_ptr->name);
-			tipc_link_reset(l_ptr);
-			l_ptr->state = RESET_RESET;
-			l_ptr->fsm_msg_cnt = 0;
-			tipc_link_send_proto_msg(l_ptr, ACTIVATE_MSG, 0, 0, 0, 0, 0);
-			l_ptr->fsm_msg_cnt++;
-			link_set_timer(l_ptr, cont_intv);
-			break;
-		default:
-			err("Unknown link event %u in WW state\n", event);
-		}
-		break;
-	case WORKING_UNKNOWN:
-		dbg_link("WU/");
-		switch (event) {
-		case TRAFFIC_MSG_EVT:
-			dbg_link("TRF-");
-		case ACTIVATE_MSG:
-			dbg_link("ACT -> WW\n");
-			l_ptr->state = WORKING_WORKING;
-			l_ptr->fsm_msg_cnt = 0;
-			link_set_timer(l_ptr, cont_intv);
-			break;
-		case RESET_MSG:
-			dbg_link("RES -> RR\n");
-			info("Resetting link <%s>, requested by peer "
-			     "while probing\n", l_ptr->name);
-			tipc_link_reset(l_ptr);
-			l_ptr->state = RESET_RESET;
-			l_ptr->fsm_msg_cnt = 0;
-			tipc_link_send_proto_msg(l_ptr, ACTIVATE_MSG, 0, 0, 0, 0, 0);
-			l_ptr->fsm_msg_cnt++;
-			link_set_timer(l_ptr, cont_intv);
-			break;
-		case TIMEOUT_EVT:
-			dbg_link("TIM ");
-			if (l_ptr->next_in_no != l_ptr->checkpoint) {
-				dbg_link("-> WW \n");
-				l_ptr->state = WORKING_WORKING;
-				l_ptr->fsm_msg_cnt = 0;
-				l_ptr->checkpoint = l_ptr->next_in_no;
-				if (tipc_bclink_acks_missing(l_ptr->owner)) {
-					tipc_link_send_proto_msg(l_ptr, STATE_MSG,
-								 0, 0, 0, 0, 0);
-					l_ptr->fsm_msg_cnt++;
-				}
-				link_set_timer(l_ptr, cont_intv);
-			} else if (l_ptr->fsm_msg_cnt < l_ptr->abort_limit) {
-				dbg_link("Probing %u/%u,timer = %u ms)\n",
-					 l_ptr->fsm_msg_cnt, l_ptr->abort_limit,
-					 cont_intv / 4);
-				tipc_link_send_proto_msg(l_ptr, STATE_MSG,
-							 1, 0, 0, 0, 0);
-				l_ptr->fsm_msg_cnt++;
-				link_set_timer(l_ptr, cont_intv / 4);
-			} else {	/* Link has failed */
-				dbg_link("-> RU (%u probes unanswered)\n",
-					 l_ptr->fsm_msg_cnt);
-				warn("Resetting link <%s>, peer not responding\n",
-				     l_ptr->name);
-				tipc_link_reset(l_ptr);
-				l_ptr->state = RESET_UNKNOWN;
-				l_ptr->fsm_msg_cnt = 0;
-				tipc_link_send_proto_msg(l_ptr, RESET_MSG,
-							 0, 0, 0, 0, 0);
-				l_ptr->fsm_msg_cnt++;
-				link_set_timer(l_ptr, cont_intv);
-			}
-			break;
-		default:
-			err("Unknown link event %u in WU state\n", event);
-		}
-		break;
-	case RESET_UNKNOWN:
-		dbg_link("RU/");
-		switch (event) {
-		case TRAFFIC_MSG_EVT:
-			dbg_link("TRF-\n");
-			break;
-		case ACTIVATE_MSG:
-			other = l_ptr->owner->active_links[0];
-			if (other && link_working_unknown(other)) {
-				dbg_link("ACT\n");
-				break;
-			}
-			dbg_link("ACT -> WW\n");
-			l_ptr->state = WORKING_WORKING;
-			l_ptr->fsm_msg_cnt = 0;
-			link_activate(l_ptr);
-			tipc_link_send_proto_msg(l_ptr, STATE_MSG, 1, 0, 0, 0, 0);
-			l_ptr->fsm_msg_cnt++;
-			link_set_timer(l_ptr, cont_intv);
-			break;
-		case RESET_MSG:
-			dbg_link("RES \n");
-			dbg_link(" -> RR\n");
-			l_ptr->state = RESET_RESET;
-			l_ptr->fsm_msg_cnt = 0;
-			tipc_link_send_proto_msg(l_ptr, ACTIVATE_MSG, 1, 0, 0, 0, 0);
-			l_ptr->fsm_msg_cnt++;
-			link_set_timer(l_ptr, cont_intv);
-			break;
-		case STARTING_EVT:
-			dbg_link("START-");
-			l_ptr->started = 1;
-			/* fall through */
-		case TIMEOUT_EVT:
-			dbg_link("TIM \n");
-			tipc_link_send_proto_msg(l_ptr, RESET_MSG, 0, 0, 0, 0, 0);
-			l_ptr->fsm_msg_cnt++;
-			link_set_timer(l_ptr, cont_intv);
-			break;
-		default:
-			err("Unknown link event %u in RU state\n", event);
-		}
-		break;
-	case RESET_RESET:
-		dbg_link("RR/ ");
-		switch (event) {
-		case TRAFFIC_MSG_EVT:
-			dbg_link("TRF-");
-			/* fall through */
-		case ACTIVATE_MSG:
-			other = l_ptr->owner->active_links[0];
-			if (other && link_working_unknown(other)) {
-				dbg_link("ACT\n");
-				break;
-			}
-			dbg_link("ACT -> WW\n");
-			l_ptr->state = WORKING_WORKING;
-			l_ptr->fsm_msg_cnt = 0;
-			link_activate(l_ptr);
-			tipc_link_send_proto_msg(l_ptr, STATE_MSG, 1, 0, 0, 0, 0);
-			l_ptr->fsm_msg_cnt++;
-			link_set_timer(l_ptr, cont_intv);
-			break;
-		case RESET_MSG:
-			dbg_link("RES\n");
-			break;
-		case TIMEOUT_EVT:
-			dbg_link("TIM\n");
-			tipc_link_send_proto_msg(l_ptr, ACTIVATE_MSG, 0, 0, 0, 0, 0);
-			l_ptr->fsm_msg_cnt++;
-			link_set_timer(l_ptr, cont_intv);
-			dbg_link("fsm_msg_cnt %u\n", l_ptr->fsm_msg_cnt);
-			break;
-		default:
-			err("Unknown link event %u in RR state\n", event);
-		}
-		break;
-	default:
-		err("Unknown link state %u/%u\n", l_ptr->state, event);
-	}
-}
-
-/*
- * link_bundle_buf(): Append contents of a buffer to
- * the tail of an existing one.
- */
-
-static int link_bundle_buf(struct link *l_ptr,
-			   struct sk_buff *bundler,
-			   struct sk_buff *buf)
-{
-	struct tipc_msg *bundler_msg = buf_msg(bundler);
-	struct tipc_msg *msg = buf_msg(buf);
-	u32 size = msg_size(msg);
-	u32 bundle_size = msg_size(bundler_msg);
-	u32 to_pos = align(bundle_size);
-	u32 pad = to_pos - bundle_size;
-
-	if (msg_user(bundler_msg) != MSG_BUNDLER)
-		return 0;
-	if (msg_type(bundler_msg) != OPEN_MSG)
-		return 0;
-	if (skb_tailroom(bundler) < (pad + size))
-		return 0;
-	if (link_max_pkt(l_ptr) < (to_pos + size))
-		return 0;
-
-	skb_put(bundler, pad + size);
-	skb_copy_to_linear_data_offset(bundler, to_pos, buf->data, size);
-	msg_set_size(bundler_msg, to_pos + size);
-	msg_set_msgcnt(bundler_msg, msg_msgcnt(bundler_msg) + 1);
-	dbg("Packed msg # %u(%u octets) into pos %u in buf(#%u)\n",
-	    msg_msgcnt(bundler_msg), size, to_pos, msg_seqno(bundler_msg));
-	msg_dbg(msg, "PACKD:");
-	buf_discard(buf);
-	l_ptr->stats.sent_bundled++;
-	return 1;
-}
-
-static void link_add_to_outqueue(struct link *l_ptr,
-				 struct sk_buff *buf,
-				 struct tipc_msg *msg)
-{
-	u32 ack = mod(l_ptr->next_in_no - 1);
-	u32 seqno = mod(l_ptr->next_out_no++);
-
-	msg_set_word(msg, 2, ((ack << 16) | seqno));
-	msg_set_bcast_ack(msg, l_ptr->owner->bclink.last_in);
-	buf->next = NULL;
-	if (l_ptr->first_out) {
-		l_ptr->last_out->next = buf;
-		l_ptr->last_out = buf;
-	} else
-		l_ptr->first_out = l_ptr->last_out = buf;
-	l_ptr->out_queue_size++;
-}
-
-/*
- * tipc_link_send_buf() is the 'full path' for messages, called from
- * inside TIPC when the 'fast path' in tipc_send_buf
- * has failed, and from link_send()
- */
-
-int tipc_link_send_buf(struct link *l_ptr, struct sk_buff *buf)
-{
-	struct tipc_msg *msg = buf_msg(buf);
-	u32 size = msg_size(msg);
-	u32 dsz = msg_data_sz(msg);
-	u32 queue_size = l_ptr->out_queue_size;
-	u32 imp = msg_tot_importance(msg);
-	u32 queue_limit = l_ptr->queue_limit[imp];
-	u32 max_packet = link_max_pkt(l_ptr);
-
-	msg_set_prevnode(msg, tipc_own_addr);	/* If routed message */
-
-	/* Match msg importance against queue limits: */
-
-	if (unlikely(queue_size >= queue_limit)) {
-		if (imp <= TIPC_CRITICAL_IMPORTANCE) {
-			return link_schedule_port(l_ptr, msg_origport(msg),
-						  size);
-		}
-		msg_dbg(msg, "TIPC: Congestion, throwing away\n");
-		buf_discard(buf);
-		if (imp > CONN_MANAGER) {
-			warn("Resetting link <%s>, send queue full", l_ptr->name);
-			tipc_link_reset(l_ptr);
-		}
-		return dsz;
-	}
-
-	/* Fragmentation needed ? */
-
-	if (size > max_packet)
-		return tipc_link_send_long_buf(l_ptr, buf);
-
-	/* Packet can be queued or sent: */
-
-	if (queue_size > l_ptr->stats.max_queue_sz)
-		l_ptr->stats.max_queue_sz = queue_size;
-
-	if (likely(!tipc_bearer_congested(l_ptr->b_ptr, l_ptr) &&
-		   !link_congested(l_ptr))) {
-		link_add_to_outqueue(l_ptr, buf, msg);
-
-		if (likely(tipc_bearer_send(l_ptr->b_ptr, buf, &l_ptr->media_addr))) {
-			l_ptr->unacked_window = 0;
-		} else {
-			tipc_bearer_schedule(l_ptr->b_ptr, l_ptr);
-			l_ptr->stats.bearer_congs++;
-			l_ptr->next_out = buf;
-		}
-		return dsz;
-	}
-	/* Congestion: can message be bundled ?: */
-
-	if ((msg_user(msg) != CHANGEOVER_PROTOCOL) &&
-	    (msg_user(msg) != MSG_FRAGMENTER)) {
-
-		/* Try adding message to an existing bundle */
-
-		if (l_ptr->next_out &&
-		    link_bundle_buf(l_ptr, l_ptr->last_out, buf)) {
-			tipc_bearer_resolve_congestion(l_ptr->b_ptr, l_ptr);
-			return dsz;
-		}
-
-		/* Try creating a new bundle */
-
-		if (size <= max_packet * 2 / 3) {
-			struct sk_buff *bundler = buf_acquire(max_packet);
-			struct tipc_msg bundler_hdr;
-
-			if (bundler) {
-				msg_init(&bundler_hdr, MSG_BUNDLER, OPEN_MSG,
-					 INT_H_SIZE, l_ptr->addr);
-				skb_copy_to_linear_data(bundler, &bundler_hdr,
-							INT_H_SIZE);
-				skb_trim(bundler, INT_H_SIZE);
-				link_bundle_buf(l_ptr, bundler, buf);
-				buf = bundler;
-				msg = buf_msg(buf);
-				l_ptr->stats.sent_bundles++;
-			}
-		}
-	}
-	if (!l_ptr->next_out)
-		l_ptr->next_out = buf;
-	link_add_to_outqueue(l_ptr, buf, msg);
-	tipc_bearer_resolve_congestion(l_ptr->b_ptr, l_ptr);
-	return dsz;
-}
-
-/*
- * tipc_link_send(): same as tipc_link_send_buf(), but the link to use has
- * not been selected yet, and the the owner node is not locked
- * Called by TIPC internal users, e.g. the name distributor
- */
-
-int tipc_link_send(struct sk_buff *buf, u32 dest, u32 selector)
-{
-	struct link *l_ptr;
-	struct tipc_node *n_ptr;
-	int res = -ELINKCONG;
-
-	read_lock_bh(&tipc_net_lock);
-	n_ptr = tipc_node_select(dest, selector);
-	if (n_ptr) {
-		tipc_node_lock(n_ptr);
-		l_ptr = n_ptr->active_links[selector & 1];
-		if (l_ptr) {
-			dbg("tipc_link_send: found link %x for dest %x\n", l_ptr, dest);
-			res = tipc_link_send_buf(l_ptr, buf);
-		} else {
-			dbg("Attempt to send msg to unreachable node:\n");
-			msg_dbg(buf_msg(buf),">>>");
-			buf_discard(buf);
-		}
-		tipc_node_unlock(n_ptr);
-	} else {
-		dbg("Attempt to send msg to unknown node:\n");
-		msg_dbg(buf_msg(buf),">>>");
-		buf_discard(buf);
-	}
-	read_unlock_bh(&tipc_net_lock);
-	return res;
-}
-
-/*
- * link_send_buf_fast: Entry for data messages where the
- * destination link is known and the header is complete,
- * inclusive total message length. Very time critical.
- * Link is locked. Returns user data length.
- */
-
-static int link_send_buf_fast(struct link *l_ptr, struct sk_buff *buf,
-			      u32 *used_max_pkt)
-{
-	struct tipc_msg *msg = buf_msg(buf);
-	int res = msg_data_sz(msg);
-
-	if (likely(!link_congested(l_ptr))) {
-		if (likely(msg_size(msg) <= link_max_pkt(l_ptr))) {
-			if (likely(list_empty(&l_ptr->b_ptr->cong_links))) {
-				link_add_to_outqueue(l_ptr, buf, msg);
-				if (likely(tipc_bearer_send(l_ptr->b_ptr, buf,
-							    &l_ptr->media_addr))) {
-					l_ptr->unacked_window = 0;
-					msg_dbg(msg,"SENT_FAST:");
-					return res;
-				}
-				dbg("failed sent fast...\n");
-				tipc_bearer_schedule(l_ptr->b_ptr, l_ptr);
-				l_ptr->stats.bearer_congs++;
-				l_ptr->next_out = buf;
-				return res;
-			}
-		}
-		else
-			*used_max_pkt = link_max_pkt(l_ptr);
-	}
-	return tipc_link_send_buf(l_ptr, buf);  /* All other cases */
-}
-
-/*
- * tipc_send_buf_fast: Entry for data messages where the
- * destination node is known and the header is complete,
- * inclusive total message length.
- * Returns user data length.
- */
-int tipc_send_buf_fast(struct sk_buff *buf, u32 destnode)
-{
-	struct link *l_ptr;
-	struct tipc_node *n_ptr;
-	int res;
-	u32 selector = msg_origport(buf_msg(buf)) & 1;
-	u32 dummy;
-
-	if (destnode == tipc_own_addr)
-		return tipc_port_recv_msg(buf);
-
-	read_lock_bh(&tipc_net_lock);
-	n_ptr = tipc_node_select(destnode, selector);
-	if (likely(n_ptr)) {
-		tipc_node_lock(n_ptr);
-		l_ptr = n_ptr->active_links[selector];
-		dbg("send_fast: buf %x selected %x, destnode = %x\n",
-		    buf, l_ptr, destnode);
-		if (likely(l_ptr)) {
-			res = link_send_buf_fast(l_ptr, buf, &dummy);
-			tipc_node_unlock(n_ptr);
-			read_unlock_bh(&tipc_net_lock);
-			return res;
-		}
-		tipc_node_unlock(n_ptr);
-	}
-	read_unlock_bh(&tipc_net_lock);
-	res = msg_data_sz(buf_msg(buf));
-	tipc_reject_msg(buf, TIPC_ERR_NO_NODE);
-	return res;
-}
-
-
-/*
- * tipc_link_send_sections_fast: Entry for messages where the
- * destination processor is known and the header is complete,
- * except for total message length.
- * Returns user data length or errno.
- */
-int tipc_link_send_sections_fast(struct port *sender,
-				 struct iovec const *msg_sect,
-				 const u32 num_sect,
-				 u32 destaddr)
-{
-	struct tipc_msg *hdr = &sender->publ.phdr;
-	struct link *l_ptr;
-	struct sk_buff *buf;
-	struct tipc_node *node;
-	int res;
-	u32 selector = msg_origport(hdr) & 1;
-
-again:
-	/*
-	 * Try building message using port's max_pkt hint.
-	 * (Must not hold any locks while building message.)
-	 */
-
-	res = msg_build(hdr, msg_sect, num_sect, sender->publ.max_pkt,
-			!sender->user_port, &buf);
-
-	read_lock_bh(&tipc_net_lock);
-	node = tipc_node_select(destaddr, selector);
-	if (likely(node)) {
-		tipc_node_lock(node);
-		l_ptr = node->active_links[selector];
-		if (likely(l_ptr)) {
-			if (likely(buf)) {
-				res = link_send_buf_fast(l_ptr, buf,
-							 &sender->publ.max_pkt);
-				if (unlikely(res < 0))
-					buf_discard(buf);
-exit:
-				tipc_node_unlock(node);
-				read_unlock_bh(&tipc_net_lock);
-				return res;
-			}
-
-			/* Exit if build request was invalid */
-
-			if (unlikely(res < 0))
-				goto exit;
-
-			/* Exit if link (or bearer) is congested */
-
-			if (link_congested(l_ptr) ||
-			    !list_empty(&l_ptr->b_ptr->cong_links)) {
-				res = link_schedule_port(l_ptr,
-							 sender->publ.ref, res);
-				goto exit;
-			}
-
-			/*
-			 * Message size exceeds max_pkt hint; update hint,
-			 * then re-try fast path or fragment the message
-			 */
-
-			sender->publ.max_pkt = link_max_pkt(l_ptr);
-			tipc_node_unlock(node);
-			read_unlock_bh(&tipc_net_lock);
-
-
-			if ((msg_hdr_sz(hdr) + res) <= sender->publ.max_pkt)
-				goto again;
-
-			return link_send_sections_long(sender, msg_sect,
-						       num_sect, destaddr);
-		}
-		tipc_node_unlock(node);
-	}
-	read_unlock_bh(&tipc_net_lock);
-
-	/* Couldn't find a link to the destination node */
-
-	if (buf)
-		return tipc_reject_msg(buf, TIPC_ERR_NO_NODE);
-	if (res >= 0)
-		return tipc_port_reject_sections(sender, hdr, msg_sect, num_sect,
-						 TIPC_ERR_NO_NODE);
-	return res;
-}
-
-/*
- * link_send_sections_long(): Entry for long messages where the
- * destination node is known and the header is complete,
- * inclusive total message length.
- * Link and bearer congestion status have been checked to be ok,
- * and are ignored if they change.
- *
- * Note that fragments do not use the full link MTU so that they won't have
- * to undergo refragmentation if link changeover causes them to be sent
- * over another link with an additional tunnel header added as prefix.
- * (Refragmentation will still occur if the other link has a smaller MTU.)
- *
- * Returns user data length or errno.
- */
-static int link_send_sections_long(struct port *sender,
-				   struct iovec const *msg_sect,
-				   u32 num_sect,
-				   u32 destaddr)
-{
-	struct link *l_ptr;
-	struct tipc_node *node;
-	struct tipc_msg *hdr = &sender->publ.phdr;
-	u32 dsz = msg_data_sz(hdr);
-	u32 max_pkt,fragm_sz,rest;
-	struct tipc_msg fragm_hdr;
-	struct sk_buff *buf,*buf_chain,*prev;
-	u32 fragm_crs,fragm_rest,hsz,sect_rest;
-	const unchar *sect_crs;
-	int curr_sect;
-	u32 fragm_no;
-
-again:
-	fragm_no = 1;
-	max_pkt = sender->publ.max_pkt - INT_H_SIZE;
-		/* leave room for tunnel header in case of link changeover */
-	fragm_sz = max_pkt - INT_H_SIZE;
-		/* leave room for fragmentation header in each fragment */
-	rest = dsz;
-	fragm_crs = 0;
-	fragm_rest = 0;
-	sect_rest = 0;
-	sect_crs = NULL;
-	curr_sect = -1;
-
-	/* Prepare reusable fragment header: */
-
-	msg_dbg(hdr, ">FRAGMENTING>");
-	msg_init(&fragm_hdr, MSG_FRAGMENTER, FIRST_FRAGMENT,
-		 INT_H_SIZE, msg_destnode(hdr));
-	msg_set_link_selector(&fragm_hdr, sender->publ.ref);
-	msg_set_size(&fragm_hdr, max_pkt);
-	msg_set_fragm_no(&fragm_hdr, 1);
-
-	/* Prepare header of first fragment: */
-
-	buf_chain = buf = buf_acquire(max_pkt);
-	if (!buf)
-		return -ENOMEM;
-	buf->next = NULL;
-	skb_copy_to_linear_data(buf, &fragm_hdr, INT_H_SIZE);
-	hsz = msg_hdr_sz(hdr);
-	skb_copy_to_linear_data_offset(buf, INT_H_SIZE, hdr, hsz);
-	msg_dbg(buf_msg(buf), ">BUILD>");
-
-	/* Chop up message: */
-
-	fragm_crs = INT_H_SIZE + hsz;
-	fragm_rest = fragm_sz - hsz;
-
-	do {		/* For all sections */
-		u32 sz;
-
-		if (!sect_rest) {
-			sect_rest = msg_sect[++curr_sect].iov_len;
-			sect_crs = (const unchar *)msg_sect[curr_sect].iov_base;
-		}
-
-		if (sect_rest < fragm_rest)
-			sz = sect_rest;
-		else
-			sz = fragm_rest;
-
-		if (likely(!sender->user_port)) {
-			if (copy_from_user(buf->data + fragm_crs, sect_crs, sz)) {
-error:
-				for (; buf_chain; buf_chain = buf) {
-					buf = buf_chain->next;
-					buf_discard(buf_chain);
-				}
-				return -EFAULT;
-			}
-		} else
-			skb_copy_to_linear_data_offset(buf, fragm_crs,
-						       sect_crs, sz);
-		sect_crs += sz;
-		sect_rest -= sz;
-		fragm_crs += sz;
-		fragm_rest -= sz;
-		rest -= sz;
-
-		if (!fragm_rest && rest) {
-
-			/* Initiate new fragment: */
-			if (rest <= fragm_sz) {
-				fragm_sz = rest;
-				msg_set_type(&fragm_hdr,LAST_FRAGMENT);
-			} else {
-				msg_set_type(&fragm_hdr, FRAGMENT);
-			}
-			msg_set_size(&fragm_hdr, fragm_sz + INT_H_SIZE);
-			msg_set_fragm_no(&fragm_hdr, ++fragm_no);
-			prev = buf;
-			buf = buf_acquire(fragm_sz + INT_H_SIZE);
-			if (!buf)
-				goto error;
-
-			buf->next = NULL;
-			prev->next = buf;
-			skb_copy_to_linear_data(buf, &fragm_hdr, INT_H_SIZE);
-			fragm_crs = INT_H_SIZE;
-			fragm_rest = fragm_sz;
-			msg_dbg(buf_msg(buf),"  >BUILD>");
-		}
-	}
-	while (rest > 0);
-
-	/*
-	 * Now we have a buffer chain. Select a link and check
-	 * that packet size is still OK
-	 */
-	node = tipc_node_select(destaddr, sender->publ.ref & 1);
-	if (likely(node)) {
-		tipc_node_lock(node);
-		l_ptr = node->active_links[sender->publ.ref & 1];
-		if (!l_ptr) {
-			tipc_node_unlock(node);
-			goto reject;
-		}
-		if (link_max_pkt(l_ptr) < max_pkt) {
-			sender->publ.max_pkt = link_max_pkt(l_ptr);
-			tipc_node_unlock(node);
-			for (; buf_chain; buf_chain = buf) {
-				buf = buf_chain->next;
-				buf_discard(buf_chain);
-			}
-			goto again;
-		}
-	} else {
-reject:
-		for (; buf_chain; buf_chain = buf) {
-			buf = buf_chain->next;
-			buf_discard(buf_chain);
-		}
-		return tipc_port_reject_sections(sender, hdr, msg_sect, num_sect,
-						 TIPC_ERR_NO_NODE);
-	}
-
-	/* Append whole chain to send queue: */
-
-	buf = buf_chain;
-	l_ptr->long_msg_seq_no = mod(l_ptr->long_msg_seq_no + 1);
-	if (!l_ptr->next_out)
-		l_ptr->next_out = buf_chain;
-	l_ptr->stats.sent_fragmented++;
-	while (buf) {
-		struct sk_buff *next = buf->next;
-		struct tipc_msg *msg = buf_msg(buf);
-
-		l_ptr->stats.sent_fragments++;
-		msg_set_long_msgno(msg, l_ptr->long_msg_seq_no);
-		link_add_to_outqueue(l_ptr, buf, msg);
-		msg_dbg(msg, ">ADD>");
-		buf = next;
-	}
-
-	/* Send it, if possible: */
-
-	tipc_link_push_queue(l_ptr);
-	tipc_node_unlock(node);
-	return dsz;
-}
-
-/*
- * tipc_link_push_packet: Push one unsent packet to the media
- */
-u32 tipc_link_push_packet(struct link *l_ptr)
-{
-	struct sk_buff *buf = l_ptr->first_out;
-	u32 r_q_size = l_ptr->retransm_queue_size;
-	u32 r_q_head = l_ptr->retransm_queue_head;
-
-	/* Step to position where retransmission failed, if any,    */
-	/* consider that buffers may have been released in meantime */
-
-	if (r_q_size && buf) {
-		u32 last = lesser(mod(r_q_head + r_q_size),
-				  link_last_sent(l_ptr));
-		u32 first = msg_seqno(buf_msg(buf));
-
-		while (buf && less(first, r_q_head)) {
-			first = mod(first + 1);
-			buf = buf->next;
-		}
-		l_ptr->retransm_queue_head = r_q_head = first;
-		l_ptr->retransm_queue_size = r_q_size = mod(last - first);
-	}
-
-	/* Continue retransmission now, if there is anything: */
-
-	if (r_q_size && buf && !skb_cloned(buf)) {
-		msg_set_ack(buf_msg(buf), mod(l_ptr->next_in_no - 1));
-		msg_set_bcast_ack(buf_msg(buf), l_ptr->owner->bclink.last_in);
-		if (tipc_bearer_send(l_ptr->b_ptr, buf, &l_ptr->media_addr)) {
-			msg_dbg(buf_msg(buf), ">DEF-RETR>");
-			l_ptr->retransm_queue_head = mod(++r_q_head);
-			l_ptr->retransm_queue_size = --r_q_size;
-			l_ptr->stats.retransmitted++;
-			return 0;
-		} else {
-			l_ptr->stats.bearer_congs++;
-			msg_dbg(buf_msg(buf), "|>DEF-RETR>");
-			return PUSH_FAILED;
-		}
-	}
-
-	/* Send deferred protocol message, if any: */
-
-	buf = l_ptr->proto_msg_queue;
-	if (buf) {
-		msg_set_ack(buf_msg(buf), mod(l_ptr->next_in_no - 1));
-		msg_set_bcast_ack(buf_msg(buf),l_ptr->owner->bclink.last_in);
-		if (tipc_bearer_send(l_ptr->b_ptr, buf, &l_ptr->media_addr)) {
-			msg_dbg(buf_msg(buf), ">DEF-PROT>");
-			l_ptr->unacked_window = 0;
-			buf_discard(buf);
-			l_ptr->proto_msg_queue = NULL;
-			return 0;
-		} else {
-			msg_dbg(buf_msg(buf), "|>DEF-PROT>");
-			l_ptr->stats.bearer_congs++;
-			return PUSH_FAILED;
-		}
-	}
-
-	/* Send one deferred data message, if send window not full: */
-
-	buf = l_ptr->next_out;
-	if (buf) {
-		struct tipc_msg *msg = buf_msg(buf);
-		u32 next = msg_seqno(msg);
-		u32 first = msg_seqno(buf_msg(l_ptr->first_out));
-
-		if (mod(next - first) < l_ptr->queue_limit[0]) {
-			msg_set_ack(msg, mod(l_ptr->next_in_no - 1));
-			msg_set_bcast_ack(msg, l_ptr->owner->bclink.last_in);
-			if (tipc_bearer_send(l_ptr->b_ptr, buf, &l_ptr->media_addr)) {
-				if (msg_user(msg) == MSG_BUNDLER)
-					msg_set_type(msg, CLOSED_MSG);
-				msg_dbg(msg, ">PUSH-DATA>");
-				l_ptr->next_out = buf->next;
-				return 0;
-			} else {
-				msg_dbg(msg, "|PUSH-DATA|");
-				l_ptr->stats.bearer_congs++;
-				return PUSH_FAILED;
-			}
-		}
-	}
-	return PUSH_FINISHED;
-}
-
-/*
- * push_queue(): push out the unsent messages of a link where
- *               congestion has abated. Node is locked
- */
-void tipc_link_push_queue(struct link *l_ptr)
-{
-	u32 res;
-
-	if (tipc_bearer_congested(l_ptr->b_ptr, l_ptr))
-		return;
-
-	do {
-		res = tipc_link_push_packet(l_ptr);
-	} while (!res);
-
-	if (res == PUSH_FAILED)
-		tipc_bearer_schedule(l_ptr->b_ptr, l_ptr);
-}
-
-static void link_reset_all(unsigned long addr)
-{
-	struct tipc_node *n_ptr;
-	char addr_string[16];
-	u32 i;
-
-	read_lock_bh(&tipc_net_lock);
-	n_ptr = tipc_node_find((u32)addr);
-	if (!n_ptr) {
-		read_unlock_bh(&tipc_net_lock);
-		return;	/* node no longer exists */
-	}
-
-	tipc_node_lock(n_ptr);
-
-	warn("Resetting all links to %s\n",
-	     addr_string_fill(addr_string, n_ptr->addr));
-
-	for (i = 0; i < MAX_BEARERS; i++) {
-		if (n_ptr->links[i]) {
-			link_print(n_ptr->links[i], TIPC_OUTPUT,
-				   "Resetting link\n");
-			tipc_link_reset(n_ptr->links[i]);
-		}
-	}
-
-	tipc_node_unlock(n_ptr);
-	read_unlock_bh(&tipc_net_lock);
-}
-
-static void link_retransmit_failure(struct link *l_ptr, struct sk_buff *buf)
-{
-	struct tipc_msg *msg = buf_msg(buf);
-
-	warn("Retransmission failure on link <%s>\n", l_ptr->name);
-	tipc_msg_dbg(TIPC_OUTPUT, msg, ">RETR-FAIL>");
-
-	if (l_ptr->addr) {
-
-		/* Handle failure on standard link */
-
-		link_print(l_ptr, TIPC_OUTPUT, "Resetting link\n");
-		tipc_link_reset(l_ptr);
-
-	} else {
-
-		/* Handle failure on broadcast link */
-
-		struct tipc_node *n_ptr;
-		char addr_string[16];
-
-		tipc_printf(TIPC_OUTPUT, "Msg seq number: %u,  ", msg_seqno(msg));
-		tipc_printf(TIPC_OUTPUT, "Outstanding acks: %lu\n",
-				     (unsigned long) TIPC_SKB_CB(buf)->handle);
-
-		n_ptr = l_ptr->owner->next;
-		tipc_node_lock(n_ptr);
-
-		addr_string_fill(addr_string, n_ptr->addr);
-		tipc_printf(TIPC_OUTPUT, "Multicast link info for %s\n", addr_string);
-		tipc_printf(TIPC_OUTPUT, "Supported: %d,  ", n_ptr->bclink.supported);
-		tipc_printf(TIPC_OUTPUT, "Acked: %u\n", n_ptr->bclink.acked);
-		tipc_printf(TIPC_OUTPUT, "Last in: %u,  ", n_ptr->bclink.last_in);
-		tipc_printf(TIPC_OUTPUT, "Gap after: %u,  ", n_ptr->bclink.gap_after);
-		tipc_printf(TIPC_OUTPUT, "Gap to: %u\n", n_ptr->bclink.gap_to);
-		tipc_printf(TIPC_OUTPUT, "Nack sync: %u\n\n", n_ptr->bclink.nack_sync);
-
-		tipc_k_signal((Handler)link_reset_all, (unsigned long)n_ptr->addr);
-
-		tipc_node_unlock(n_ptr);
-
-		l_ptr->stale_count = 0;
-	}
-}
-
-void tipc_link_retransmit(struct link *l_ptr, struct sk_buff *buf,
-			  u32 retransmits)
-{
-	struct tipc_msg *msg;
-
-	if (!buf)
-		return;
-
-	msg = buf_msg(buf);
-
-	dbg("Retransmitting %u in link %x\n", retransmits, l_ptr);
-
-	if (tipc_bearer_congested(l_ptr->b_ptr, l_ptr)) {
-		if (!skb_cloned(buf)) {
-			msg_dbg(msg, ">NO_RETR->BCONG>");
-			dbg_print_link(l_ptr, "   ");
-			l_ptr->retransm_queue_head = msg_seqno(msg);
-			l_ptr->retransm_queue_size = retransmits;
-			return;
-		} else {
-			/* Don't retransmit if driver already has the buffer */
-		}
-	} else {
-		/* Detect repeated retransmit failures on uncongested bearer */
-
-		if (l_ptr->last_retransmitted == msg_seqno(msg)) {
-			if (++l_ptr->stale_count > 100) {
-				link_retransmit_failure(l_ptr, buf);
-				return;
-			}
-		} else {
-			l_ptr->last_retransmitted = msg_seqno(msg);
-			l_ptr->stale_count = 1;
-		}
-	}
-
-	while (retransmits && (buf != l_ptr->next_out) && buf && !skb_cloned(buf)) {
-		msg = buf_msg(buf);
-		msg_set_ack(msg, mod(l_ptr->next_in_no - 1));
-		msg_set_bcast_ack(msg, l_ptr->owner->bclink.last_in);
-		if (tipc_bearer_send(l_ptr->b_ptr, buf, &l_ptr->media_addr)) {
-			msg_dbg(buf_msg(buf), ">RETR>");
-			buf = buf->next;
-			retransmits--;
-			l_ptr->stats.retransmitted++;
-		} else {
-			tipc_bearer_schedule(l_ptr->b_ptr, l_ptr);
-			l_ptr->stats.bearer_congs++;
-			l_ptr->retransm_queue_head = msg_seqno(buf_msg(buf));
-			l_ptr->retransm_queue_size = retransmits;
-			return;
-		}
-	}
-
-	l_ptr->retransm_queue_head = l_ptr->retransm_queue_size = 0;
-}
-
-/**
- * link_insert_deferred_queue - insert deferred messages back into receive chain
- */
-
-static struct sk_buff *link_insert_deferred_queue(struct link *l_ptr,
-						  struct sk_buff *buf)
-{
-	u32 seq_no;
-
-	if (l_ptr->oldest_deferred_in == NULL)
-		return buf;
-
-	seq_no = msg_seqno(buf_msg(l_ptr->oldest_deferred_in));
-	if (seq_no == mod(l_ptr->next_in_no)) {
-		l_ptr->newest_deferred_in->next = buf;
-		buf = l_ptr->oldest_deferred_in;
-		l_ptr->oldest_deferred_in = NULL;
-		l_ptr->deferred_inqueue_sz = 0;
-	}
-	return buf;
-}
-
-/**
- * link_recv_buf_validate - validate basic format of received message
- *
- * This routine ensures a TIPC message has an acceptable header, and at least
- * as much data as the header indicates it should.  The routine also ensures
- * that the entire message header is stored in the main fragment of the message
- * buffer, to simplify future access to message header fields.
- *
- * Note: Having extra info present in the message header or data areas is OK.
- * TIPC will ignore the excess, under the assumption that it is optional info
- * introduced by a later release of the protocol.
- */
-
-static int link_recv_buf_validate(struct sk_buff *buf)
-{
-	static u32 min_data_hdr_size[8] = {
-		SHORT_H_SIZE, MCAST_H_SIZE, LONG_H_SIZE, DIR_MSG_H_SIZE,
-		MAX_H_SIZE, MAX_H_SIZE, MAX_H_SIZE, MAX_H_SIZE
-		};
-
-	struct tipc_msg *msg;
-	u32 tipc_hdr[2];
-	u32 size;
-	u32 hdr_size;
-	u32 min_hdr_size;
-
-	if (unlikely(buf->len < MIN_H_SIZE))
-		return 0;
-
-	msg = skb_header_pointer(buf, 0, sizeof(tipc_hdr), tipc_hdr);
-	if (msg == NULL)
-		return 0;
-
-	if (unlikely(msg_version(msg) != TIPC_VERSION))
-		return 0;
-
-	size = msg_size(msg);
-	hdr_size = msg_hdr_sz(msg);
-	min_hdr_size = msg_isdata(msg) ?
-		min_data_hdr_size[msg_type(msg)] : INT_H_SIZE;
-
-	if (unlikely((hdr_size < min_hdr_size) ||
-		     (size < hdr_size) ||
-		     (buf->len < size) ||
-		     (size - hdr_size > TIPC_MAX_USER_MSG_SIZE)))
-		return 0;
-
-	return pskb_may_pull(buf, hdr_size);
-}
-
-void tipc_recv_msg(struct sk_buff *head, struct tipc_bearer *tb_ptr)
-{
-	read_lock_bh(&tipc_net_lock);
-	while (head) {
-		struct bearer *b_ptr = (struct bearer *)tb_ptr;
-		struct tipc_node *n_ptr;
-		struct link *l_ptr;
-		struct sk_buff *crs;
-		struct sk_buff *buf = head;
-		struct tipc_msg *msg;
-		u32 seq_no;
-		u32 ackd;
-		u32 released = 0;
-		int type;
-
-		head = head->next;
-
-		/* Ensure message is well-formed */
-
-		if (unlikely(!link_recv_buf_validate(buf)))
-			goto cont;
-
-		/* Ensure message data is a single contiguous unit */
-
-		if (unlikely(buf_linearize(buf))) {
-			goto cont;
-		}
-
-		/* Handle arrival of a non-unicast link message */
-
-		msg = buf_msg(buf);
-
-		if (unlikely(msg_non_seq(msg))) {
-			if (msg_user(msg) ==  LINK_CONFIG)
-				tipc_disc_recv_msg(buf, b_ptr);
-			else
-				tipc_bclink_recv_pkt(buf);
-			continue;
-		}
-
-		if (unlikely(!msg_short(msg) &&
-			     (msg_destnode(msg) != tipc_own_addr)))
-			goto cont;
-
-		/* Locate unicast link endpoint that should handle message */
-
-		n_ptr = tipc_node_find(msg_prevnode(msg));
-		if (unlikely(!n_ptr))
-			goto cont;
-		tipc_node_lock(n_ptr);
-
-		l_ptr = n_ptr->links[b_ptr->identity];
-		if (unlikely(!l_ptr)) {
-			tipc_node_unlock(n_ptr);
-			goto cont;
-		}
-
-		/* Validate message sequence number info */
-
-		seq_no = msg_seqno(msg);
-		ackd = msg_ack(msg);
-
-		/* Release acked messages */
-
-		if (less(n_ptr->bclink.acked, msg_bcast_ack(msg))) {
-			if (tipc_node_is_up(n_ptr) && n_ptr->bclink.supported)
-				tipc_bclink_acknowledge(n_ptr, msg_bcast_ack(msg));
-		}
-
-		crs = l_ptr->first_out;
-		while ((crs != l_ptr->next_out) &&
-		       less_eq(msg_seqno(buf_msg(crs)), ackd)) {
-			struct sk_buff *next = crs->next;
-
-			buf_discard(crs);
-			crs = next;
-			released++;
-		}
-		if (released) {
-			l_ptr->first_out = crs;
-			l_ptr->out_queue_size -= released;
-		}
-
-		/* Try sending any messages link endpoint has pending */
-
-		if (unlikely(l_ptr->next_out))
-			tipc_link_push_queue(l_ptr);
-		if (unlikely(!list_empty(&l_ptr->waiting_ports)))
-			tipc_link_wakeup_ports(l_ptr, 0);
-		if (unlikely(++l_ptr->unacked_window >= TIPC_MIN_LINK_WIN)) {
-			l_ptr->stats.sent_acks++;
-			tipc_link_send_proto_msg(l_ptr, STATE_MSG, 0, 0, 0, 0, 0);
-		}
-
-		/* Now (finally!) process the incoming message */
-
-protocol_check:
-		if (likely(link_working_working(l_ptr))) {
-			if (likely(seq_no == mod(l_ptr->next_in_no))) {
-				l_ptr->next_in_no++;
-				if (unlikely(l_ptr->oldest_deferred_in))
-					head = link_insert_deferred_queue(l_ptr,
-									  head);
-				if (likely(msg_is_dest(msg, tipc_own_addr))) {
-deliver:
-					if (likely(msg_isdata(msg))) {
-						tipc_node_unlock(n_ptr);
-						tipc_port_recv_msg(buf);
-						continue;
-					}
-					switch (msg_user(msg)) {
-					case MSG_BUNDLER:
-						l_ptr->stats.recv_bundles++;
-						l_ptr->stats.recv_bundled +=
-							msg_msgcnt(msg);
-						tipc_node_unlock(n_ptr);
-						tipc_link_recv_bundle(buf);
-						continue;
-					case ROUTE_DISTRIBUTOR:
-						tipc_node_unlock(n_ptr);
-						tipc_cltr_recv_routing_table(buf);
-						continue;
-					case NAME_DISTRIBUTOR:
-						tipc_node_unlock(n_ptr);
-						tipc_named_recv(buf);
-						continue;
-					case CONN_MANAGER:
-						tipc_node_unlock(n_ptr);
-						tipc_port_recv_proto_msg(buf);
-						continue;
-					case MSG_FRAGMENTER:
-						l_ptr->stats.recv_fragments++;
-						if (tipc_link_recv_fragment(&l_ptr->defragm_buf,
-									    &buf, &msg)) {
-							l_ptr->stats.recv_fragmented++;
-							goto deliver;
-						}
-						break;
-					case CHANGEOVER_PROTOCOL:
-						type = msg_type(msg);
-						if (link_recv_changeover_msg(&l_ptr, &buf)) {
-							msg = buf_msg(buf);
-							seq_no = msg_seqno(msg);
-							if (type == ORIGINAL_MSG)
-								goto deliver;
-							goto protocol_check;
-						}
-						break;
-					}
-				}
-				tipc_node_unlock(n_ptr);
-				tipc_net_route_msg(buf);
-				continue;
-			}
-			link_handle_out_of_seq_msg(l_ptr, buf);
-			head = link_insert_deferred_queue(l_ptr, head);
-			tipc_node_unlock(n_ptr);
-			continue;
-		}
-
-		if (msg_user(msg) == LINK_PROTOCOL) {
-			link_recv_proto_msg(l_ptr, buf);
-			head = link_insert_deferred_queue(l_ptr, head);
-			tipc_node_unlock(n_ptr);
-			continue;
-		}
-		msg_dbg(msg,"NSEQ<REC<");
-		link_state_event(l_ptr, TRAFFIC_MSG_EVT);
-
-		if (link_working_working(l_ptr)) {
-			/* Re-insert in front of queue */
-			msg_dbg(msg,"RECV-REINS:");
-			buf->next = head;
-			head = buf;
-			tipc_node_unlock(n_ptr);
-			continue;
-		}
-		tipc_node_unlock(n_ptr);
-cont:
-		buf_discard(buf);
-	}
-	read_unlock_bh(&tipc_net_lock);
-}
-
-/*
- * link_defer_buf(): Sort a received out-of-sequence packet
- *                   into the deferred reception queue.
- * Returns the increase of the queue length,i.e. 0 or 1
- */
-
-u32 tipc_link_defer_pkt(struct sk_buff **head,
-			struct sk_buff **tail,
-			struct sk_buff *buf)
-{
-	struct sk_buff *prev = NULL;
-	struct sk_buff *crs = *head;
-	u32 seq_no = msg_seqno(buf_msg(buf));
-
-	buf->next = NULL;
-
-	/* Empty queue ? */
-	if (*head == NULL) {
-		*head = *tail = buf;
-		return 1;
-	}
-
-	/* Last ? */
-	if (less(msg_seqno(buf_msg(*tail)), seq_no)) {
-		(*tail)->next = buf;
-		*tail = buf;
-		return 1;
-	}
-
-	/* Scan through queue and sort it in */
-	do {
-		struct tipc_msg *msg = buf_msg(crs);
-
-		if (less(seq_no, msg_seqno(msg))) {
-			buf->next = crs;
-			if (prev)
-				prev->next = buf;
-			else
-				*head = buf;
-			return 1;
-		}
-		if (seq_no == msg_seqno(msg)) {
-			break;
-		}
-		prev = crs;
-		crs = crs->next;
-	}
-	while (crs);
-
-	/* Message is a duplicate of an existing message */
-
-	buf_discard(buf);
-	return 0;
-}
-
-/**
- * link_handle_out_of_seq_msg - handle arrival of out-of-sequence packet
- */
-
-static void link_handle_out_of_seq_msg(struct link *l_ptr,
-				       struct sk_buff *buf)
-{
-	u32 seq_no = msg_seqno(buf_msg(buf));
-
-	if (likely(msg_user(buf_msg(buf)) == LINK_PROTOCOL)) {
-		link_recv_proto_msg(l_ptr, buf);
-		return;
-	}
-
-	dbg("rx OOS msg: seq_no %u, expecting %u (%u)\n",
-	    seq_no, mod(l_ptr->next_in_no), l_ptr->next_in_no);
-
-	/* Record OOS packet arrival (force mismatch on next timeout) */
-
-	l_ptr->checkpoint--;
-
-	/*
-	 * Discard packet if a duplicate; otherwise add it to deferred queue
-	 * and notify peer of gap as per protocol specification
-	 */
-
-	if (less(seq_no, mod(l_ptr->next_in_no))) {
-		l_ptr->stats.duplicates++;
-		buf_discard(buf);
-		return;
-	}
-
-	if (tipc_link_defer_pkt(&l_ptr->oldest_deferred_in,
-				&l_ptr->newest_deferred_in, buf)) {
-		l_ptr->deferred_inqueue_sz++;
-		l_ptr->stats.deferred_recv++;
-		if ((l_ptr->deferred_inqueue_sz % 16) == 1)
-			tipc_link_send_proto_msg(l_ptr, STATE_MSG, 0, 0, 0, 0, 0);
-	} else
-		l_ptr->stats.duplicates++;
-}
-
-/*
- * Send protocol message to the other endpoint.
- */
-void tipc_link_send_proto_msg(struct link *l_ptr, u32 msg_typ, int probe_msg,
-			      u32 gap, u32 tolerance, u32 priority, u32 ack_mtu)
-{
-	struct sk_buff *buf = NULL;
-	struct tipc_msg *msg = l_ptr->pmsg;
-	u32 msg_size = sizeof(l_ptr->proto_msg);
-
-	if (link_blocked(l_ptr))
-		return;
-	msg_set_type(msg, msg_typ);
-	msg_set_net_plane(msg, l_ptr->b_ptr->net_plane);
-	msg_set_bcast_ack(msg, mod(l_ptr->owner->bclink.last_in));
-	msg_set_last_bcast(msg, tipc_bclink_get_last_sent());
-
-	if (msg_typ == STATE_MSG) {
-		u32 next_sent = mod(l_ptr->next_out_no);
-
-		if (!tipc_link_is_up(l_ptr))
-			return;
-		if (l_ptr->next_out)
-			next_sent = msg_seqno(buf_msg(l_ptr->next_out));
-		msg_set_next_sent(msg, next_sent);
-		if (l_ptr->oldest_deferred_in) {
-			u32 rec = msg_seqno(buf_msg(l_ptr->oldest_deferred_in));
-			gap = mod(rec - mod(l_ptr->next_in_no));
-		}
-		msg_set_seq_gap(msg, gap);
-		if (gap)
-			l_ptr->stats.sent_nacks++;
-		msg_set_link_tolerance(msg, tolerance);
-		msg_set_linkprio(msg, priority);
-		msg_set_max_pkt(msg, ack_mtu);
-		msg_set_ack(msg, mod(l_ptr->next_in_no - 1));
-		msg_set_probe(msg, probe_msg != 0);
-		if (probe_msg) {
-			u32 mtu = l_ptr->max_pkt;
-
-			if ((mtu < l_ptr->max_pkt_target) &&
-			    link_working_working(l_ptr) &&
-			    l_ptr->fsm_msg_cnt) {
-				msg_size = (mtu + (l_ptr->max_pkt_target - mtu)/2 + 2) & ~3;
-				if (l_ptr->max_pkt_probes == 10) {
-					l_ptr->max_pkt_target = (msg_size - 4);
-					l_ptr->max_pkt_probes = 0;
-					msg_size = (mtu + (l_ptr->max_pkt_target - mtu)/2 + 2) & ~3;
-				}
-				l_ptr->max_pkt_probes++;
-			}
-
-			l_ptr->stats.sent_probes++;
-		}
-		l_ptr->stats.sent_states++;
-	} else {		/* RESET_MSG or ACTIVATE_MSG */
-		msg_set_ack(msg, mod(l_ptr->reset_checkpoint - 1));
-		msg_set_seq_gap(msg, 0);
-		msg_set_next_sent(msg, 1);
-		msg_set_link_tolerance(msg, l_ptr->tolerance);
-		msg_set_linkprio(msg, l_ptr->priority);
-		msg_set_max_pkt(msg, l_ptr->max_pkt_target);
-	}
-
-	if (tipc_node_has_redundant_links(l_ptr->owner)) {
-		msg_set_redundant_link(msg);
-	} else {
-		msg_clear_redundant_link(msg);
-	}
-	msg_set_linkprio(msg, l_ptr->priority);
-
-	/* Ensure sequence number will not fit : */
-
-	msg_set_seqno(msg, mod(l_ptr->next_out_no + (0xffff/2)));
-
-	/* Congestion? */
-
-	if (tipc_bearer_congested(l_ptr->b_ptr, l_ptr)) {
-		if (!l_ptr->proto_msg_queue) {
-			l_ptr->proto_msg_queue =
-				buf_acquire(sizeof(l_ptr->proto_msg));
-		}
-		buf = l_ptr->proto_msg_queue;
-		if (!buf)
-			return;
-		skb_copy_to_linear_data(buf, msg, sizeof(l_ptr->proto_msg));
-		return;
-	}
-	msg_set_timestamp(msg, jiffies_to_msecs(jiffies));
-
-	/* Message can be sent */
-
-	msg_dbg(msg, ">>");
-
-	buf = buf_acquire(msg_size);
-	if (!buf)
-		return;
-
-	skb_copy_to_linear_data(buf, msg, sizeof(l_ptr->proto_msg));
-	msg_set_size(buf_msg(buf), msg_size);
-
-	if (tipc_bearer_send(l_ptr->b_ptr, buf, &l_ptr->media_addr)) {
-		l_ptr->unacked_window = 0;
-		buf_discard(buf);
-		return;
-	}
-
-	/* New congestion */
-	tipc_bearer_schedule(l_ptr->b_ptr, l_ptr);
-	l_ptr->proto_msg_queue = buf;
-	l_ptr->stats.bearer_congs++;
-}
-
-/*
- * Receive protocol message :
- * Note that network plane id propagates through the network, and may
- * change at any time. The node with lowest address rules
- */
-
-static void link_recv_proto_msg(struct link *l_ptr, struct sk_buff *buf)
-{
-	u32 rec_gap = 0;
-	u32 max_pkt_info;
-	u32 max_pkt_ack;
-	u32 msg_tol;
-	struct tipc_msg *msg = buf_msg(buf);
-
-	dbg("AT(%u):", jiffies_to_msecs(jiffies));
-	msg_dbg(msg, "<<");
-	if (link_blocked(l_ptr))
-		goto exit;
-
-	/* record unnumbered packet arrival (force mismatch on next timeout) */
-
-	l_ptr->checkpoint--;
-
-	if (l_ptr->b_ptr->net_plane != msg_net_plane(msg))
-		if (tipc_own_addr > msg_prevnode(msg))
-			l_ptr->b_ptr->net_plane = msg_net_plane(msg);
-
-	l_ptr->owner->permit_changeover = msg_redundant_link(msg);
-
-	switch (msg_type(msg)) {
-
-	case RESET_MSG:
-		if (!link_working_unknown(l_ptr) &&
-		    (l_ptr->peer_session != INVALID_SESSION)) {
-			if (msg_session(msg) == l_ptr->peer_session) {
-				dbg("Duplicate RESET: %u<->%u\n",
-				    msg_session(msg), l_ptr->peer_session);
-				break; /* duplicate: ignore */
-			}
-		}
-		/* fall thru' */
-	case ACTIVATE_MSG:
-		/* Update link settings according other endpoint's values */
-
-		strcpy((strrchr(l_ptr->name, ':') + 1), (char *)msg_data(msg));
-
-		if ((msg_tol = msg_link_tolerance(msg)) &&
-		    (msg_tol > l_ptr->tolerance))
-			link_set_supervision_props(l_ptr, msg_tol);
-
-		if (msg_linkprio(msg) > l_ptr->priority)
-			l_ptr->priority = msg_linkprio(msg);
-
-		max_pkt_info = msg_max_pkt(msg);
-		if (max_pkt_info) {
-			if (max_pkt_info < l_ptr->max_pkt_target)
-				l_ptr->max_pkt_target = max_pkt_info;
-			if (l_ptr->max_pkt > l_ptr->max_pkt_target)
-				l_ptr->max_pkt = l_ptr->max_pkt_target;
-		} else {
-			l_ptr->max_pkt = l_ptr->max_pkt_target;
-		}
-		l_ptr->owner->bclink.supported = (max_pkt_info != 0);
-
-		link_state_event(l_ptr, msg_type(msg));
-
-		l_ptr->peer_session = msg_session(msg);
-		l_ptr->peer_bearer_id = msg_bearer_id(msg);
-
-		/* Synchronize broadcast sequence numbers */
-		if (!tipc_node_has_redundant_links(l_ptr->owner)) {
-			l_ptr->owner->bclink.last_in = mod(msg_last_bcast(msg));
-		}
-		break;
-	case STATE_MSG:
-
-		if ((msg_tol = msg_link_tolerance(msg)))
-			link_set_supervision_props(l_ptr, msg_tol);
-
-		if (msg_linkprio(msg) &&
-		    (msg_linkprio(msg) != l_ptr->priority)) {
-			warn("Resetting link <%s>, priority change %u->%u\n",
-			     l_ptr->name, l_ptr->priority, msg_linkprio(msg));
-			l_ptr->priority = msg_linkprio(msg);
-			tipc_link_reset(l_ptr); /* Enforce change to take effect */
-			break;
-		}
-		link_state_event(l_ptr, TRAFFIC_MSG_EVT);
-		l_ptr->stats.recv_states++;
-		if (link_reset_unknown(l_ptr))
-			break;
-
-		if (less_eq(mod(l_ptr->next_in_no), msg_next_sent(msg))) {
-			rec_gap = mod(msg_next_sent(msg) -
-				      mod(l_ptr->next_in_no));
-		}
-
-		max_pkt_ack = msg_max_pkt(msg);
-		if (max_pkt_ack > l_ptr->max_pkt) {
-			dbg("Link <%s> updated MTU %u -> %u\n",
-			    l_ptr->name, l_ptr->max_pkt, max_pkt_ack);
-			l_ptr->max_pkt = max_pkt_ack;
-			l_ptr->max_pkt_probes = 0;
-		}
-
-		max_pkt_ack = 0;
-		if (msg_probe(msg)) {
-			l_ptr->stats.recv_probes++;
-			if (msg_size(msg) > sizeof(l_ptr->proto_msg)) {
-				max_pkt_ack = msg_size(msg);
-			}
-		}
-
-		/* Protocol message before retransmits, reduce loss risk */
-
-		tipc_bclink_check_gap(l_ptr->owner, msg_last_bcast(msg));
-
-		if (rec_gap || (msg_probe(msg))) {
-			tipc_link_send_proto_msg(l_ptr, STATE_MSG,
-						 0, rec_gap, 0, 0, max_pkt_ack);
-		}
-		if (msg_seq_gap(msg)) {
-			msg_dbg(msg, "With Gap:");
-			l_ptr->stats.recv_nacks++;
-			tipc_link_retransmit(l_ptr, l_ptr->first_out,
-					     msg_seq_gap(msg));
-		}
-		break;
-	default:
-		msg_dbg(buf_msg(buf), "<DISCARDING UNKNOWN<");
-	}
-exit:
-	buf_discard(buf);
-}
-
-
-/*
- * tipc_link_tunnel(): Send one message via a link belonging to
- * another bearer. Owner node is locked.
- */
-void tipc_link_tunnel(struct link *l_ptr,
-		      struct tipc_msg *tunnel_hdr,
-		      struct tipc_msg  *msg,
-		      u32 selector)
-{
-	struct link *tunnel;
-	struct sk_buff *buf;
-	u32 length = msg_size(msg);
-
-	tunnel = l_ptr->owner->active_links[selector & 1];
-	if (!tipc_link_is_up(tunnel)) {
-		warn("Link changeover error, "
-		     "tunnel link no longer available\n");
-		return;
-	}
-	msg_set_size(tunnel_hdr, length + INT_H_SIZE);
-	buf = buf_acquire(length + INT_H_SIZE);
-	if (!buf) {
-		warn("Link changeover error, "
-		     "unable to send tunnel msg\n");
-		return;
-	}
-	skb_copy_to_linear_data(buf, tunnel_hdr, INT_H_SIZE);
-	skb_copy_to_linear_data_offset(buf, INT_H_SIZE, msg, length);
-	dbg("%c->%c:", l_ptr->b_ptr->net_plane, tunnel->b_ptr->net_plane);
-	msg_dbg(buf_msg(buf), ">SEND>");
-	tipc_link_send_buf(tunnel, buf);
-}
-
-
-
-/*
- * changeover(): Send whole message queue via the remaining link
- *               Owner node is locked.
- */
-
-void tipc_link_changeover(struct link *l_ptr)
-{
-	u32 msgcount = l_ptr->out_queue_size;
-	struct sk_buff *crs = l_ptr->first_out;
-	struct link *tunnel = l_ptr->owner->active_links[0];
-	struct tipc_msg tunnel_hdr;
-	int split_bundles;
-
-	if (!tunnel)
-		return;
-
-	if (!l_ptr->owner->permit_changeover) {
-		warn("Link changeover error, "
-		     "peer did not permit changeover\n");
-		return;
-	}
-
-	msg_init(&tunnel_hdr, CHANGEOVER_PROTOCOL,
-		 ORIGINAL_MSG, INT_H_SIZE, l_ptr->addr);
-	msg_set_bearer_id(&tunnel_hdr, l_ptr->peer_bearer_id);
-	msg_set_msgcnt(&tunnel_hdr, msgcount);
-	dbg("Link changeover requires %u tunnel messages\n", msgcount);
-
-	if (!l_ptr->first_out) {
-		struct sk_buff *buf;
-
-		buf = buf_acquire(INT_H_SIZE);
-		if (buf) {
-			skb_copy_to_linear_data(buf, &tunnel_hdr, INT_H_SIZE);
-			msg_set_size(&tunnel_hdr, INT_H_SIZE);
-			dbg("%c->%c:", l_ptr->b_ptr->net_plane,
-			    tunnel->b_ptr->net_plane);
-			msg_dbg(&tunnel_hdr, "EMPTY>SEND>");
-			tipc_link_send_buf(tunnel, buf);
-		} else {
-			warn("Link changeover error, "
-			     "unable to send changeover msg\n");
-		}
-		return;
-	}
-
-	split_bundles = (l_ptr->owner->active_links[0] !=
-			 l_ptr->owner->active_links[1]);
-
-	while (crs) {
-		struct tipc_msg *msg = buf_msg(crs);
-
-		if ((msg_user(msg) == MSG_BUNDLER) && split_bundles) {
-			struct tipc_msg *m = msg_get_wrapped(msg);
-			unchar* pos = (unchar*)m;
-
-			msgcount = msg_msgcnt(msg);
-			while (msgcount--) {
-				msg_set_seqno(m,msg_seqno(msg));
-				tipc_link_tunnel(l_ptr, &tunnel_hdr, m,
-						 msg_link_selector(m));
-				pos += align(msg_size(m));
-				m = (struct tipc_msg *)pos;
-			}
-		} else {
-			tipc_link_tunnel(l_ptr, &tunnel_hdr, msg,
-					 msg_link_selector(msg));
-		}
-		crs = crs->next;
-	}
-}
-
-void tipc_link_send_duplicate(struct link *l_ptr, struct link *tunnel)
-{
-	struct sk_buff *iter;
-	struct tipc_msg tunnel_hdr;
-
-	msg_init(&tunnel_hdr, CHANGEOVER_PROTOCOL,
-		 DUPLICATE_MSG, INT_H_SIZE, l_ptr->addr);
-	msg_set_msgcnt(&tunnel_hdr, l_ptr->out_queue_size);
-	msg_set_bearer_id(&tunnel_hdr, l_ptr->peer_bearer_id);
-	iter = l_ptr->first_out;
-	while (iter) {
-		struct sk_buff *outbuf;
-		struct tipc_msg *msg = buf_msg(iter);
-		u32 length = msg_size(msg);
-
-		if (msg_user(msg) == MSG_BUNDLER)
-			msg_set_type(msg, CLOSED_MSG);
-		msg_set_ack(msg, mod(l_ptr->next_in_no - 1));	/* Update */
-		msg_set_bcast_ack(msg, l_ptr->owner->bclink.last_in);
-		msg_set_size(&tunnel_hdr, length + INT_H_SIZE);
-		outbuf = buf_acquire(length + INT_H_SIZE);
-		if (outbuf == NULL) {
-			warn("Link changeover error, "
-			     "unable to send duplicate msg\n");
-			return;
-		}
-		skb_copy_to_linear_data(outbuf, &tunnel_hdr, INT_H_SIZE);
-		skb_copy_to_linear_data_offset(outbuf, INT_H_SIZE, iter->data,
-					       length);
-		dbg("%c->%c:", l_ptr->b_ptr->net_plane,
-		    tunnel->b_ptr->net_plane);
-		msg_dbg(buf_msg(outbuf), ">SEND>");
-		tipc_link_send_buf(tunnel, outbuf);
-		if (!tipc_link_is_up(l_ptr))
-			return;
-		iter = iter->next;
-	}
-}
-
-
-
-/**
- * buf_extract - extracts embedded TIPC message from another message
- * @skb: encapsulating message buffer
- * @from_pos: offset to extract from
- *
- * Returns a new message buffer containing an embedded message.  The
- * encapsulating message itself is left unchanged.
- */
-
-static struct sk_buff *buf_extract(struct sk_buff *skb, u32 from_pos)
-{
-	struct tipc_msg *msg = (struct tipc_msg *)(skb->data + from_pos);
-	u32 size = msg_size(msg);
-	struct sk_buff *eb;
-
-	eb = buf_acquire(size);
-	if (eb)
-		skb_copy_to_linear_data(eb, msg, size);
-	return eb;
-}
-
-/*
- *  link_recv_changeover_msg(): Receive tunneled packet sent
- *  via other link. Node is locked. Return extracted buffer.
- */
-
-static int link_recv_changeover_msg(struct link **l_ptr,
-				    struct sk_buff **buf)
-{
-	struct sk_buff *tunnel_buf = *buf;
-	struct link *dest_link;
-	struct tipc_msg *msg;
-	struct tipc_msg *tunnel_msg = buf_msg(tunnel_buf);
-	u32 msg_typ = msg_type(tunnel_msg);
-	u32 msg_count = msg_msgcnt(tunnel_msg);
-
-	dest_link = (*l_ptr)->owner->links[msg_bearer_id(tunnel_msg)];
-	if (!dest_link) {
-		msg_dbg(tunnel_msg, "NOLINK/<REC<");
-		goto exit;
-	}
-	if (dest_link == *l_ptr) {
-		err("Unexpected changeover message on link <%s>\n",
-		    (*l_ptr)->name);
-		goto exit;
-	}
-	dbg("%c<-%c:", dest_link->b_ptr->net_plane,
-	    (*l_ptr)->b_ptr->net_plane);
-	*l_ptr = dest_link;
-	msg = msg_get_wrapped(tunnel_msg);
-
-	if (msg_typ == DUPLICATE_MSG) {
-		if (less(msg_seqno(msg), mod(dest_link->next_in_no))) {
-			msg_dbg(tunnel_msg, "DROP/<REC<");
-			goto exit;
-		}
-		*buf = buf_extract(tunnel_buf,INT_H_SIZE);
-		if (*buf == NULL) {
-			warn("Link changeover error, duplicate msg dropped\n");
-			goto exit;
-		}
-		msg_dbg(tunnel_msg, "TNL<REC<");
-		buf_discard(tunnel_buf);
-		return 1;
-	}
-
-	/* First original message ?: */
-
-	if (tipc_link_is_up(dest_link)) {
-		msg_dbg(tunnel_msg, "UP/FIRST/<REC<");
-		info("Resetting link <%s>, changeover initiated by peer\n",
-		     dest_link->name);
-		tipc_link_reset(dest_link);
-		dest_link->exp_msg_count = msg_count;
-		dbg("Expecting %u tunnelled messages\n", msg_count);
-		if (!msg_count)
-			goto exit;
-	} else if (dest_link->exp_msg_count == START_CHANGEOVER) {
-		msg_dbg(tunnel_msg, "BLK/FIRST/<REC<");
-		dest_link->exp_msg_count = msg_count;
-		dbg("Expecting %u tunnelled messages\n", msg_count);
-		if (!msg_count)
-			goto exit;
-	}
-
-	/* Receive original message */
-
-	if (dest_link->exp_msg_count == 0) {
-		warn("Link switchover error, "
-		     "got too many tunnelled messages\n");
-		msg_dbg(tunnel_msg, "OVERDUE/DROP/<REC<");
-		dbg_print_link(dest_link, "LINK:");
-		goto exit;
-	}
-	dest_link->exp_msg_count--;
-	if (less(msg_seqno(msg), dest_link->reset_checkpoint)) {
-		msg_dbg(tunnel_msg, "DROP/DUPL/<REC<");
-		goto exit;
-	} else {
-		*buf = buf_extract(tunnel_buf, INT_H_SIZE);
-		if (*buf != NULL) {
-			msg_dbg(tunnel_msg, "TNL<REC<");
-			buf_discard(tunnel_buf);
-			return 1;
-		} else {
-			warn("Link changeover error, original msg dropped\n");
-		}
-	}
-exit:
-	*buf = NULL;
-	buf_discard(tunnel_buf);
-	return 0;
-}
-
-/*
- *  Bundler functionality:
- */
-void tipc_link_recv_bundle(struct sk_buff *buf)
-{
-	u32 msgcount = msg_msgcnt(buf_msg(buf));
-	u32 pos = INT_H_SIZE;
-	struct sk_buff *obuf;
-
-	msg_dbg(buf_msg(buf), "<BNDL<: ");
-	while (msgcount--) {
-		obuf = buf_extract(buf, pos);
-		if (obuf == NULL) {
-			warn("Link unable to unbundle message(s)\n");
-			break;
-		}
-		pos += align(msg_size(buf_msg(obuf)));
-		msg_dbg(buf_msg(obuf), "     /");
-		tipc_net_route_msg(obuf);
-	}
-	buf_discard(buf);
-}
-
-/*
- *  Fragmentation/defragmentation:
- */
-
-
-/*
- * tipc_link_send_long_buf: Entry for buffers needing fragmentation.
- * The buffer is complete, inclusive total message length.
- * Returns user data length.
- */
-int tipc_link_send_long_buf(struct link *l_ptr, struct sk_buff *buf)
-{
-	struct tipc_msg *inmsg = buf_msg(buf);
-	struct tipc_msg fragm_hdr;
-	u32 insize = msg_size(inmsg);
-	u32 dsz = msg_data_sz(inmsg);
-	unchar *crs = buf->data;
-	u32 rest = insize;
-	u32 pack_sz = link_max_pkt(l_ptr);
-	u32 fragm_sz = pack_sz - INT_H_SIZE;
-	u32 fragm_no = 1;
-	u32 destaddr;
-
-	if (msg_short(inmsg))
-		destaddr = l_ptr->addr;
-	else
-		destaddr = msg_destnode(inmsg);
-
-	if (msg_routed(inmsg))
-		msg_set_prevnode(inmsg, tipc_own_addr);
-
-	/* Prepare reusable fragment header: */
-
-	msg_init(&fragm_hdr, MSG_FRAGMENTER, FIRST_FRAGMENT,
-		 INT_H_SIZE, destaddr);
-	msg_set_link_selector(&fragm_hdr, msg_link_selector(inmsg));
-	msg_set_long_msgno(&fragm_hdr, mod(l_ptr->long_msg_seq_no++));
-	msg_set_fragm_no(&fragm_hdr, fragm_no);
-	l_ptr->stats.sent_fragmented++;
-
-	/* Chop up message: */
-
-	while (rest > 0) {
-		struct sk_buff *fragm;
-
-		if (rest <= fragm_sz) {
-			fragm_sz = rest;
-			msg_set_type(&fragm_hdr, LAST_FRAGMENT);
-		}
-		fragm = buf_acquire(fragm_sz + INT_H_SIZE);
-		if (fragm == NULL) {
-			warn("Link unable to fragment message\n");
-			dsz = -ENOMEM;
-			goto exit;
-		}
-		msg_set_size(&fragm_hdr, fragm_sz + INT_H_SIZE);
-		skb_copy_to_linear_data(fragm, &fragm_hdr, INT_H_SIZE);
-		skb_copy_to_linear_data_offset(fragm, INT_H_SIZE, crs,
-					       fragm_sz);
-		/*  Send queued messages first, if any: */
-
-		l_ptr->stats.sent_fragments++;
-		tipc_link_send_buf(l_ptr, fragm);
-		if (!tipc_link_is_up(l_ptr))
-			return dsz;
-		msg_set_fragm_no(&fragm_hdr, ++fragm_no);
-		rest -= fragm_sz;
-		crs += fragm_sz;
-		msg_set_type(&fragm_hdr, FRAGMENT);
-	}
-exit:
-	buf_discard(buf);
-	return dsz;
-}
-
-/*
- * A pending message being re-assembled must store certain values
- * to handle subsequent fragments correctly. The following functions
- * help storing these values in unused, available fields in the
- * pending message. This makes dynamic memory allocation unecessary.
- */
-
-static void set_long_msg_seqno(struct sk_buff *buf, u32 seqno)
-{
-	msg_set_seqno(buf_msg(buf), seqno);
-}
-
-static u32 get_fragm_size(struct sk_buff *buf)
-{
-	return msg_ack(buf_msg(buf));
-}
-
-static void set_fragm_size(struct sk_buff *buf, u32 sz)
-{
-	msg_set_ack(buf_msg(buf), sz);
-}
-
-static u32 get_expected_frags(struct sk_buff *buf)
-{
-	return msg_bcast_ack(buf_msg(buf));
-}
-
-static void set_expected_frags(struct sk_buff *buf, u32 exp)
-{
-	msg_set_bcast_ack(buf_msg(buf), exp);
-}
-
-static u32 get_timer_cnt(struct sk_buff *buf)
-{
-	return msg_reroute_cnt(buf_msg(buf));
-}
-
-static void incr_timer_cnt(struct sk_buff *buf)
-{
-	msg_incr_reroute_cnt(buf_msg(buf));
-}
-
-/*
- * tipc_link_recv_fragment(): Called with node lock on. Returns
- * the reassembled buffer if message is complete.
- */
-int tipc_link_recv_fragment(struct sk_buff **pending, struct sk_buff **fb,
-			    struct tipc_msg **m)
-{
-	struct sk_buff *prev = NULL;
-	struct sk_buff *fbuf = *fb;
-	struct tipc_msg *fragm = buf_msg(fbuf);
-	struct sk_buff *pbuf = *pending;
-	u32 long_msg_seq_no = msg_long_msgno(fragm);
-
-	*fb = NULL;
-	msg_dbg(fragm,"FRG<REC<");
-
-	/* Is there an incomplete message waiting for this fragment? */
-
-	while (pbuf && ((msg_seqno(buf_msg(pbuf)) != long_msg_seq_no)
-			|| (msg_orignode(fragm) != msg_orignode(buf_msg(pbuf))))) {
-		prev = pbuf;
-		pbuf = pbuf->next;
-	}
-
-	if (!pbuf && (msg_type(fragm) == FIRST_FRAGMENT)) {
-		struct tipc_msg *imsg = (struct tipc_msg *)msg_data(fragm);
-		u32 msg_sz = msg_size(imsg);
-		u32 fragm_sz = msg_data_sz(fragm);
-		u32 exp_fragm_cnt = msg_sz/fragm_sz + !!(msg_sz % fragm_sz);
-		u32 max =  TIPC_MAX_USER_MSG_SIZE + LONG_H_SIZE;
-		if (msg_type(imsg) == TIPC_MCAST_MSG)
-			max = TIPC_MAX_USER_MSG_SIZE + MCAST_H_SIZE;
-		if (msg_size(imsg) > max) {
-			msg_dbg(fragm,"<REC<Oversized: ");
-			buf_discard(fbuf);
-			return 0;
-		}
-		pbuf = buf_acquire(msg_size(imsg));
-		if (pbuf != NULL) {
-			pbuf->next = *pending;
-			*pending = pbuf;
-			skb_copy_to_linear_data(pbuf, imsg,
-						msg_data_sz(fragm));
-			/*  Prepare buffer for subsequent fragments. */
-
-			set_long_msg_seqno(pbuf, long_msg_seq_no);
-			set_fragm_size(pbuf,fragm_sz);
-			set_expected_frags(pbuf,exp_fragm_cnt - 1);
-		} else {
-			warn("Link unable to reassemble fragmented message\n");
-		}
-		buf_discard(fbuf);
-		return 0;
-	} else if (pbuf && (msg_type(fragm) != FIRST_FRAGMENT)) {
-		u32 dsz = msg_data_sz(fragm);
-		u32 fsz = get_fragm_size(pbuf);
-		u32 crs = ((msg_fragm_no(fragm) - 1) * fsz);
-		u32 exp_frags = get_expected_frags(pbuf) - 1;
-		skb_copy_to_linear_data_offset(pbuf, crs,
-					       msg_data(fragm), dsz);
-		buf_discard(fbuf);
-
-		/* Is message complete? */
-
-		if (exp_frags == 0) {
-			if (prev)
-				prev->next = pbuf->next;
-			else
-				*pending = pbuf->next;
-			msg_reset_reroute_cnt(buf_msg(pbuf));
-			*fb = pbuf;
-			*m = buf_msg(pbuf);
-			return 1;
-		}
-		set_expected_frags(pbuf,exp_frags);
-		return 0;
-	}
-	dbg(" Discarding orphan fragment %x\n",fbuf);
-	msg_dbg(fragm,"ORPHAN:");
-	dbg("Pending long buffers:\n");
-	dbg_print_buf_chain(*pending);
-	buf_discard(fbuf);
-	return 0;
-}
-
-/**
- * link_check_defragm_bufs - flush stale incoming message fragments
- * @l_ptr: pointer to link
- */
-
-static void link_check_defragm_bufs(struct link *l_ptr)
-{
-	struct sk_buff *prev = NULL;
-	struct sk_buff *next = NULL;
-	struct sk_buff *buf = l_ptr->defragm_buf;
-
-	if (!buf)
-		return;
-	if (!link_working_working(l_ptr))
-		return;
-	while (buf) {
-		u32 cnt = get_timer_cnt(buf);
-
-		next = buf->next;
-		if (cnt < 4) {
-			incr_timer_cnt(buf);
-			prev = buf;
-		} else {
-			dbg(" Discarding incomplete long buffer\n");
-			msg_dbg(buf_msg(buf), "LONG:");
-			dbg_print_link(l_ptr, "curr:");
-			dbg("Pending long buffers:\n");
-			dbg_print_buf_chain(l_ptr->defragm_buf);
-			if (prev)
-				prev->next = buf->next;
-			else
-				l_ptr->defragm_buf = buf->next;
-			buf_discard(buf);
-		}
-		buf = next;
-	}
-}
-
-
-
-static void link_set_supervision_props(struct link *l_ptr, u32 tolerance)
-{
-	l_ptr->tolerance = tolerance;
-	l_ptr->continuity_interval =
-		((tolerance / 4) > 500) ? 500 : tolerance / 4;
-	l_ptr->abort_limit = tolerance / (l_ptr->continuity_interval / 4);
-}
-
-
-void tipc_link_set_queue_limits(struct link *l_ptr, u32 window)
-{
-	/* Data messages from this node, inclusive FIRST_FRAGM */
-	l_ptr->queue_limit[TIPC_LOW_IMPORTANCE] = window;
-	l_ptr->queue_limit[TIPC_MEDIUM_IMPORTANCE] = (window / 3) * 4;
-	l_ptr->queue_limit[TIPC_HIGH_IMPORTANCE] = (window / 3) * 5;
-	l_ptr->queue_limit[TIPC_CRITICAL_IMPORTANCE] = (window / 3) * 6;
-	/* Transiting data messages,inclusive FIRST_FRAGM */
-	l_ptr->queue_limit[TIPC_LOW_IMPORTANCE + 4] = 300;
-	l_ptr->queue_limit[TIPC_MEDIUM_IMPORTANCE + 4] = 600;
-	l_ptr->queue_limit[TIPC_HIGH_IMPORTANCE + 4] = 900;
-	l_ptr->queue_limit[TIPC_CRITICAL_IMPORTANCE + 4] = 1200;
-	l_ptr->queue_limit[CONN_MANAGER] = 1200;
-	l_ptr->queue_limit[ROUTE_DISTRIBUTOR] = 1200;
-	l_ptr->queue_limit[CHANGEOVER_PROTOCOL] = 2500;
-	l_ptr->queue_limit[NAME_DISTRIBUTOR] = 3000;
-	/* FRAGMENT and LAST_FRAGMENT packets */
-	l_ptr->queue_limit[MSG_FRAGMENTER] = 4000;
-}
-
-/**
- * link_find_link - locate link by name
- * @name - ptr to link name string
- * @node - ptr to area to be filled with ptr to associated node
- *
- * Caller must hold 'tipc_net_lock' to ensure node and bearer are not deleted;
- * this also prevents link deletion.
- *
- * Returns pointer to link (or 0 if invalid link name).
- */
-
-static struct link *link_find_link(const char *name, struct tipc_node **node)
-{
-	struct link_name link_name_parts;
-	struct bearer *b_ptr;
-	struct link *l_ptr;
-
-	if (!link_name_validate(name, &link_name_parts))
-		return NULL;
-
-	b_ptr = tipc_bearer_find_interface(link_name_parts.if_local);
-	if (!b_ptr)
-		return NULL;
-
-	*node = tipc_node_find(link_name_parts.addr_peer);
-	if (!*node)
-		return NULL;
-
-	l_ptr = (*node)->links[b_ptr->identity];
-	if (!l_ptr || strcmp(l_ptr->name, name))
-		return NULL;
-
-	return l_ptr;
-}
-
-struct sk_buff *tipc_link_cmd_config(const void *req_tlv_area, int req_tlv_space,
-				     u16 cmd)
-{
-	struct tipc_link_config *args;
-	u32 new_value;
-	struct link *l_ptr;
-	struct tipc_node *node;
-	int res;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_LINK_CONFIG))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	args = (struct tipc_link_config *)TLV_DATA(req_tlv_area);
-	new_value = ntohl(args->value);
-
-	if (!strcmp(args->name, tipc_bclink_name)) {
-		if ((cmd == TIPC_CMD_SET_LINK_WINDOW) &&
-		    (tipc_bclink_set_queue_limits(new_value) == 0))
-			return tipc_cfg_reply_none();
-		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
-						   " (cannot change setting on broadcast link)");
-	}
-
-	read_lock_bh(&tipc_net_lock);
-	l_ptr = link_find_link(args->name, &node);
-	if (!l_ptr) {
-		read_unlock_bh(&tipc_net_lock);
-		return tipc_cfg_reply_error_string("link not found");
-	}
-
-	tipc_node_lock(node);
-	res = -EINVAL;
-	switch (cmd) {
-	case TIPC_CMD_SET_LINK_TOL:
-		if ((new_value >= TIPC_MIN_LINK_TOL) &&
-		    (new_value <= TIPC_MAX_LINK_TOL)) {
-			link_set_supervision_props(l_ptr, new_value);
-			tipc_link_send_proto_msg(l_ptr, STATE_MSG,
-						 0, 0, new_value, 0, 0);
-			res = 0;
-		}
-		break;
-	case TIPC_CMD_SET_LINK_PRI:
-		if ((new_value >= TIPC_MIN_LINK_PRI) &&
-		    (new_value <= TIPC_MAX_LINK_PRI)) {
-			l_ptr->priority = new_value;
-			tipc_link_send_proto_msg(l_ptr, STATE_MSG,
-						 0, 0, 0, new_value, 0);
-			res = 0;
-		}
-		break;
-	case TIPC_CMD_SET_LINK_WINDOW:
-		if ((new_value >= TIPC_MIN_LINK_WIN) &&
-		    (new_value <= TIPC_MAX_LINK_WIN)) {
-			tipc_link_set_queue_limits(l_ptr, new_value);
-			res = 0;
-		}
-		break;
-	}
-	tipc_node_unlock(node);
-
-	read_unlock_bh(&tipc_net_lock);
-	if (res)
-		return tipc_cfg_reply_error_string("cannot change link setting");
-
-	return tipc_cfg_reply_none();
-}
-
-/**
- * link_reset_statistics - reset link statistics
- * @l_ptr: pointer to link
- */
-
-static void link_reset_statistics(struct link *l_ptr)
-{
-	memset(&l_ptr->stats, 0, sizeof(l_ptr->stats));
-	l_ptr->stats.sent_info = l_ptr->next_out_no;
-	l_ptr->stats.recv_info = l_ptr->next_in_no;
-}
-
-struct sk_buff *tipc_link_cmd_reset_stats(const void *req_tlv_area, int req_tlv_space)
-{
-	char *link_name;
-	struct link *l_ptr;
-	struct tipc_node *node;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_LINK_NAME))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	link_name = (char *)TLV_DATA(req_tlv_area);
-	if (!strcmp(link_name, tipc_bclink_name)) {
-		if (tipc_bclink_reset_stats())
-			return tipc_cfg_reply_error_string("link not found");
-		return tipc_cfg_reply_none();
-	}
-
-	read_lock_bh(&tipc_net_lock);
-	l_ptr = link_find_link(link_name, &node);
-	if (!l_ptr) {
-		read_unlock_bh(&tipc_net_lock);
-		return tipc_cfg_reply_error_string("link not found");
-	}
-
-	tipc_node_lock(node);
-	link_reset_statistics(l_ptr);
-	tipc_node_unlock(node);
-	read_unlock_bh(&tipc_net_lock);
-	return tipc_cfg_reply_none();
-}
-
-/**
- * percent - convert count to a percentage of total (rounding up or down)
- */
-
-static u32 percent(u32 count, u32 total)
-{
-	return (count * 100 + (total / 2)) / total;
-}
-
-/**
- * tipc_link_stats - print link statistics
- * @name: link name
- * @buf: print buffer area
- * @buf_size: size of print buffer area
- *
- * Returns length of print buffer data string (or 0 if error)
- */
-
-static int tipc_link_stats(const char *name, char *buf, const u32 buf_size)
-{
-	struct print_buf pb;
-	struct link *l_ptr;
-	struct tipc_node *node;
-	char *status;
-	u32 profile_total = 0;
-
-	if (!strcmp(name, tipc_bclink_name))
-		return tipc_bclink_stats(buf, buf_size);
-
-	tipc_printbuf_init(&pb, buf, buf_size);
-
-	read_lock_bh(&tipc_net_lock);
-	l_ptr = link_find_link(name, &node);
-	if (!l_ptr) {
-		read_unlock_bh(&tipc_net_lock);
-		return 0;
-	}
-	tipc_node_lock(node);
-
-	if (tipc_link_is_active(l_ptr))
-		status = "ACTIVE";
-	else if (tipc_link_is_up(l_ptr))
-		status = "STANDBY";
-	else
-		status = "DEFUNCT";
-	tipc_printf(&pb, "Link <%s>\n"
-			 "  %s  MTU:%u  Priority:%u  Tolerance:%u ms"
-			 "  Window:%u packets\n",
-		    l_ptr->name, status, link_max_pkt(l_ptr),
-		    l_ptr->priority, l_ptr->tolerance, l_ptr->queue_limit[0]);
-	tipc_printf(&pb, "  RX packets:%u fragments:%u/%u bundles:%u/%u\n",
-		    l_ptr->next_in_no - l_ptr->stats.recv_info,
-		    l_ptr->stats.recv_fragments,
-		    l_ptr->stats.recv_fragmented,
-		    l_ptr->stats.recv_bundles,
-		    l_ptr->stats.recv_bundled);
-	tipc_printf(&pb, "  TX packets:%u fragments:%u/%u bundles:%u/%u\n",
-		    l_ptr->next_out_no - l_ptr->stats.sent_info,
-		    l_ptr->stats.sent_fragments,
-		    l_ptr->stats.sent_fragmented,
-		    l_ptr->stats.sent_bundles,
-		    l_ptr->stats.sent_bundled);
-	profile_total = l_ptr->stats.msg_length_counts;
-	if (!profile_total)
-		profile_total = 1;
-	tipc_printf(&pb, "  TX profile sample:%u packets  average:%u octets\n"
-			 "  0-64:%u%% -256:%u%% -1024:%u%% -4096:%u%% "
-			 "-16354:%u%% -32768:%u%% -66000:%u%%\n",
-		    l_ptr->stats.msg_length_counts,
-		    l_ptr->stats.msg_lengths_total / profile_total,
-		    percent(l_ptr->stats.msg_length_profile[0], profile_total),
-		    percent(l_ptr->stats.msg_length_profile[1], profile_total),
-		    percent(l_ptr->stats.msg_length_profile[2], profile_total),
-		    percent(l_ptr->stats.msg_length_profile[3], profile_total),
-		    percent(l_ptr->stats.msg_length_profile[4], profile_total),
-		    percent(l_ptr->stats.msg_length_profile[5], profile_total),
-		    percent(l_ptr->stats.msg_length_profile[6], profile_total));
-	tipc_printf(&pb, "  RX states:%u probes:%u naks:%u defs:%u dups:%u\n",
-		    l_ptr->stats.recv_states,
-		    l_ptr->stats.recv_probes,
-		    l_ptr->stats.recv_nacks,
-		    l_ptr->stats.deferred_recv,
-		    l_ptr->stats.duplicates);
-	tipc_printf(&pb, "  TX states:%u probes:%u naks:%u acks:%u dups:%u\n",
-		    l_ptr->stats.sent_states,
-		    l_ptr->stats.sent_probes,
-		    l_ptr->stats.sent_nacks,
-		    l_ptr->stats.sent_acks,
-		    l_ptr->stats.retransmitted);
-	tipc_printf(&pb, "  Congestion bearer:%u link:%u  Send queue max:%u avg:%u\n",
-		    l_ptr->stats.bearer_congs,
-		    l_ptr->stats.link_congs,
-		    l_ptr->stats.max_queue_sz,
-		    l_ptr->stats.queue_sz_counts
-		    ? (l_ptr->stats.accu_queue_sz / l_ptr->stats.queue_sz_counts)
-		    : 0);
-
-	tipc_node_unlock(node);
-	read_unlock_bh(&tipc_net_lock);
-	return tipc_printbuf_validate(&pb);
-}
-
-#define MAX_LINK_STATS_INFO 2000
-
-struct sk_buff *tipc_link_cmd_show_stats(const void *req_tlv_area, int req_tlv_space)
-{
-	struct sk_buff *buf;
-	struct tlv_desc *rep_tlv;
-	int str_len;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_LINK_NAME))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	buf = tipc_cfg_reply_alloc(TLV_SPACE(MAX_LINK_STATS_INFO));
-	if (!buf)
-		return NULL;
-
-	rep_tlv = (struct tlv_desc *)buf->data;
-
-	str_len = tipc_link_stats((char *)TLV_DATA(req_tlv_area),
-				  (char *)TLV_DATA(rep_tlv), MAX_LINK_STATS_INFO);
-	if (!str_len) {
-		buf_discard(buf);
-		return tipc_cfg_reply_error_string("link not found");
-	}
-
-	skb_put(buf, TLV_SPACE(str_len));
-	TLV_SET(rep_tlv, TIPC_TLV_ULTRA_STRING, NULL, str_len);
-
-	return buf;
-}
-
-#if 0
-int link_control(const char *name, u32 op, u32 val)
-{
-	int res = -EINVAL;
-	struct link *l_ptr;
-	u32 bearer_id;
-	struct tipc_node * node;
-	u32 a;
-
-	a = link_name2addr(name, &bearer_id);
-	read_lock_bh(&tipc_net_lock);
-	node = tipc_node_find(a);
-	if (node) {
-		tipc_node_lock(node);
-		l_ptr = node->links[bearer_id];
-		if (l_ptr) {
-			if (op == TIPC_REMOVE_LINK) {
-				struct bearer *b_ptr = l_ptr->b_ptr;
-				spin_lock_bh(&b_ptr->publ.lock);
-				tipc_link_delete(l_ptr);
-				spin_unlock_bh(&b_ptr->publ.lock);
-			}
-			if (op == TIPC_CMD_BLOCK_LINK) {
-				tipc_link_reset(l_ptr);
-				l_ptr->blocked = 1;
-			}
-			if (op == TIPC_CMD_UNBLOCK_LINK) {
-				l_ptr->blocked = 0;
-			}
-			res = 0;
-		}
-		tipc_node_unlock(node);
-	}
-	read_unlock_bh(&tipc_net_lock);
-	return res;
-}
-#endif
-
-/**
- * tipc_link_get_max_pkt - get maximum packet size to use when sending to destination
- * @dest: network address of destination node
- * @selector: used to select from set of active links
- *
- * If no active link can be found, uses default maximum packet size.
- */
-
-u32 tipc_link_get_max_pkt(u32 dest, u32 selector)
-{
-	struct tipc_node *n_ptr;
-	struct link *l_ptr;
-	u32 res = MAX_PKT_DEFAULT;
-
-	if (dest == tipc_own_addr)
-		return MAX_MSG_SIZE;
-
-	read_lock_bh(&tipc_net_lock);
-	n_ptr = tipc_node_select(dest, selector);
-	if (n_ptr) {
-		tipc_node_lock(n_ptr);
-		l_ptr = n_ptr->active_links[selector & 1];
-		if (l_ptr)
-			res = link_max_pkt(l_ptr);
-		tipc_node_unlock(n_ptr);
-	}
-	read_unlock_bh(&tipc_net_lock);
-	return res;
-}
-
-#if 0
-static void link_dump_rec_queue(struct link *l_ptr)
-{
-	struct sk_buff *crs;
-
-	if (!l_ptr->oldest_deferred_in) {
-		info("Reception queue empty\n");
-		return;
-	}
-	info("Contents of Reception queue:\n");
-	crs = l_ptr->oldest_deferred_in;
-	while (crs) {
-		if (crs->data == (void *)0x0000a3a3) {
-			info("buffer %x invalid\n", crs);
-			return;
-		}
-		msg_dbg(buf_msg(crs), "In rec queue: \n");
-		crs = crs->next;
-	}
-}
-#endif
-
-static void link_dump_send_queue(struct link *l_ptr)
-{
-	if (l_ptr->next_out) {
-		info("\nContents of unsent queue:\n");
-		dbg_print_buf_chain(l_ptr->next_out);
-	}
-	info("\nContents of send queue:\n");
-	if (l_ptr->first_out) {
-		dbg_print_buf_chain(l_ptr->first_out);
-	}
-	info("Empty send queue\n");
-}
-
-static void link_print(struct link *l_ptr, struct print_buf *buf,
-		       const char *str)
-{
-	tipc_printf(buf, str);
-	if (link_reset_reset(l_ptr) || link_reset_unknown(l_ptr))
-		return;
-	tipc_printf(buf, "Link %x<%s>:",
-		    l_ptr->addr, l_ptr->b_ptr->publ.name);
-	tipc_printf(buf, ": NXO(%u):", mod(l_ptr->next_out_no));
-	tipc_printf(buf, "NXI(%u):", mod(l_ptr->next_in_no));
-	tipc_printf(buf, "SQUE");
-	if (l_ptr->first_out) {
-		tipc_printf(buf, "[%u..", msg_seqno(buf_msg(l_ptr->first_out)));
-		if (l_ptr->next_out)
-			tipc_printf(buf, "%u..",
-				    msg_seqno(buf_msg(l_ptr->next_out)));
-		tipc_printf(buf, "%u]",
-			    msg_seqno(buf_msg
-				      (l_ptr->last_out)), l_ptr->out_queue_size);
-		if ((mod(msg_seqno(buf_msg(l_ptr->last_out)) -
-			 msg_seqno(buf_msg(l_ptr->first_out)))
-		     != (l_ptr->out_queue_size - 1))
-		    || (l_ptr->last_out->next != NULL)) {
-			tipc_printf(buf, "\nSend queue inconsistency\n");
-			tipc_printf(buf, "first_out= %x ", l_ptr->first_out);
-			tipc_printf(buf, "next_out= %x ", l_ptr->next_out);
-			tipc_printf(buf, "last_out= %x ", l_ptr->last_out);
-			link_dump_send_queue(l_ptr);
-		}
-	} else
-		tipc_printf(buf, "[]");
-	tipc_printf(buf, "SQSIZ(%u)", l_ptr->out_queue_size);
-	if (l_ptr->oldest_deferred_in) {
-		u32 o = msg_seqno(buf_msg(l_ptr->oldest_deferred_in));
-		u32 n = msg_seqno(buf_msg(l_ptr->newest_deferred_in));
-		tipc_printf(buf, ":RQUE[%u..%u]", o, n);
-		if (l_ptr->deferred_inqueue_sz != mod((n + 1) - o)) {
-			tipc_printf(buf, ":RQSIZ(%u)",
-				    l_ptr->deferred_inqueue_sz);
-		}
-	}
-	if (link_working_unknown(l_ptr))
-		tipc_printf(buf, ":WU");
-	if (link_reset_reset(l_ptr))
-		tipc_printf(buf, ":RR");
-	if (link_reset_unknown(l_ptr))
-		tipc_printf(buf, ":RU");
-	if (link_working_working(l_ptr))
-		tipc_printf(buf, ":WW");
-	tipc_printf(buf, "\n");
-}
-
diff -ruN linux-2.6.29/net/tipc/link.h android_cluster/linux-2.6.29/net/tipc/link.h
--- linux-2.6.29/net/tipc/link.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/link.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,295 +0,0 @@
-/*
- * net/tipc/link.h: Include file for TIPC link code
- *
- * Copyright (c) 1995-2006, Ericsson AB
- * Copyright (c) 2004-2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_LINK_H
-#define _TIPC_LINK_H
-
-#include "dbg.h"
-#include "msg.h"
-#include "bearer.h"
-#include "node.h"
-
-#define PUSH_FAILED   1
-#define PUSH_FINISHED 2
-
-/*
- * Link states
- */
-
-#define WORKING_WORKING 560810u
-#define WORKING_UNKNOWN 560811u
-#define RESET_UNKNOWN   560812u
-#define RESET_RESET     560813u
-
-/*
- * Starting value for maximum packet size negotiation on unicast links
- * (unless bearer MTU is less)
- */
-
-#define MAX_PKT_DEFAULT 1500
-
-/**
- * struct link - TIPC link data structure
- * @addr: network address of link's peer node
- * @name: link name character string
- * @media_addr: media address to use when sending messages over link
- * @timer: link timer
- * @owner: pointer to peer node
- * @link_list: adjacent links in bearer's list of links
- * @started: indicates if link has been started
- * @checkpoint: reference point for triggering link continuity checking
- * @peer_session: link session # being used by peer end of link
- * @peer_bearer_id: bearer id used by link's peer endpoint
- * @b_ptr: pointer to bearer used by link
- * @tolerance: minimum link continuity loss needed to reset link [in ms]
- * @continuity_interval: link continuity testing interval [in ms]
- * @abort_limit: # of unacknowledged continuity probes needed to reset link
- * @state: current state of link FSM
- * @blocked: indicates if link has been administratively blocked
- * @fsm_msg_cnt: # of protocol messages link FSM has sent in current state
- * @proto_msg: template for control messages generated by link
- * @pmsg: convenience pointer to "proto_msg" field
- * @priority: current link priority
- * @queue_limit: outbound message queue congestion thresholds (indexed by user)
- * @exp_msg_count: # of tunnelled messages expected during link changeover
- * @reset_checkpoint: seq # of last acknowledged message at time of link reset
- * @max_pkt: current maximum packet size for this link
- * @max_pkt_target: desired maximum packet size for this link
- * @max_pkt_probes: # of probes based on current (max_pkt, max_pkt_target)
- * @out_queue_size: # of messages in outbound message queue
- * @first_out: ptr to first outbound message in queue
- * @last_out: ptr to last outbound message in queue
- * @next_out_no: next sequence number to use for outbound messages
- * @last_retransmitted: sequence number of most recently retransmitted message
- * @stale_count: # of identical retransmit requests made by peer
- * @next_in_no: next sequence number to expect for inbound messages
- * @deferred_inqueue_sz: # of messages in inbound message queue
- * @oldest_deferred_in: ptr to first inbound message in queue
- * @newest_deferred_in: ptr to last inbound message in queue
- * @unacked_window: # of inbound messages rx'd without ack'ing back to peer
- * @proto_msg_queue: ptr to (single) outbound control message
- * @retransm_queue_size: number of messages to retransmit
- * @retransm_queue_head: sequence number of first message to retransmit
- * @next_out: ptr to first unsent outbound message in queue
- * @waiting_ports: linked list of ports waiting for link congestion to abate
- * @long_msg_seq_no: next identifier to use for outbound fragmented messages
- * @defragm_buf: list of partially reassembled inbound message fragments
- * @stats: collects statistics regarding link activity
- * @print_buf: print buffer used to log link activity
- */
-
-struct link {
-	u32 addr;
-	char name[TIPC_MAX_LINK_NAME];
-	struct tipc_media_addr media_addr;
-	struct timer_list timer;
-	struct tipc_node *owner;
-	struct list_head link_list;
-
-	/* Management and link supervision data */
-	int started;
-	u32 checkpoint;
-	u32 peer_session;
-	u32 peer_bearer_id;
-	struct bearer *b_ptr;
-	u32 tolerance;
-	u32 continuity_interval;
-	u32 abort_limit;
-	int state;
-	int blocked;
-	u32 fsm_msg_cnt;
-	struct {
-		unchar hdr[INT_H_SIZE];
-		unchar body[TIPC_MAX_IF_NAME];
-	} proto_msg;
-	struct tipc_msg *pmsg;
-	u32 priority;
-	u32 queue_limit[15];	/* queue_limit[0]==window limit */
-
-	/* Changeover */
-	u32 exp_msg_count;
-	u32 reset_checkpoint;
-
-	/* Max packet negotiation */
-	u32 max_pkt;
-	u32 max_pkt_target;
-	u32 max_pkt_probes;
-
-	/* Sending */
-	u32 out_queue_size;
-	struct sk_buff *first_out;
-	struct sk_buff *last_out;
-	u32 next_out_no;
-	u32 last_retransmitted;
-	u32 stale_count;
-
-	/* Reception */
-	u32 next_in_no;
-	u32 deferred_inqueue_sz;
-	struct sk_buff *oldest_deferred_in;
-	struct sk_buff *newest_deferred_in;
-	u32 unacked_window;
-
-	/* Congestion handling */
-	struct sk_buff *proto_msg_queue;
-	u32 retransm_queue_size;
-	u32 retransm_queue_head;
-	struct sk_buff *next_out;
-	struct list_head waiting_ports;
-
-	/* Fragmentation/defragmentation */
-	u32 long_msg_seq_no;
-	struct sk_buff *defragm_buf;
-
-	/* Statistics */
-	struct {
-		u32 sent_info;		/* used in counting # sent packets */
-		u32 recv_info;		/* used in counting # recv'd packets */
-		u32 sent_states;
-		u32 recv_states;
-		u32 sent_probes;
-		u32 recv_probes;
-		u32 sent_nacks;
-		u32 recv_nacks;
-		u32 sent_acks;
-		u32 sent_bundled;
-		u32 sent_bundles;
-		u32 recv_bundled;
-		u32 recv_bundles;
-		u32 retransmitted;
-		u32 sent_fragmented;
-		u32 sent_fragments;
-		u32 recv_fragmented;
-		u32 recv_fragments;
-		u32 link_congs;		/* # port sends blocked by congestion */
-		u32 bearer_congs;
-		u32 deferred_recv;
-		u32 duplicates;
-
-		/* for statistical profiling of send queue size */
-
-		u32 max_queue_sz;
-		u32 accu_queue_sz;
-		u32 queue_sz_counts;
-
-		/* for statistical profiling of message lengths */
-
-		u32 msg_length_counts;
-		u32 msg_lengths_total;
-		u32 msg_length_profile[7];
-#if 0
-		u32 sent_tunneled;
-		u32 recv_tunneled;
-#endif
-	} stats;
-
-	struct print_buf print_buf;
-};
-
-struct port;
-
-struct link *tipc_link_create(struct bearer *b_ptr, const u32 peer,
-			      const struct tipc_media_addr *media_addr);
-void tipc_link_delete(struct link *l_ptr);
-void tipc_link_changeover(struct link *l_ptr);
-void tipc_link_send_duplicate(struct link *l_ptr, struct link *dest);
-void tipc_link_reset_fragments(struct link *l_ptr);
-int tipc_link_is_up(struct link *l_ptr);
-int tipc_link_is_active(struct link *l_ptr);
-void tipc_link_start(struct link *l_ptr);
-u32 tipc_link_push_packet(struct link *l_ptr);
-void tipc_link_stop(struct link *l_ptr);
-struct sk_buff *tipc_link_cmd_config(const void *req_tlv_area, int req_tlv_space, u16 cmd);
-struct sk_buff *tipc_link_cmd_show_stats(const void *req_tlv_area, int req_tlv_space);
-struct sk_buff *tipc_link_cmd_reset_stats(const void *req_tlv_area, int req_tlv_space);
-void tipc_link_reset(struct link *l_ptr);
-int tipc_link_send(struct sk_buff *buf, u32 dest, u32 selector);
-int tipc_link_send_buf(struct link *l_ptr, struct sk_buff *buf);
-u32 tipc_link_get_max_pkt(u32 dest,u32 selector);
-int tipc_link_send_sections_fast(struct port* sender,
-				 struct iovec const *msg_sect,
-				 const u32 num_sect,
-				 u32 destnode);
-int tipc_link_send_long_buf(struct link *l_ptr, struct sk_buff *buf);
-void tipc_link_tunnel(struct link *l_ptr, struct tipc_msg *tnl_hdr,
-		      struct tipc_msg *msg, u32 selector);
-void tipc_link_recv_bundle(struct sk_buff *buf);
-int  tipc_link_recv_fragment(struct sk_buff **pending,
-			     struct sk_buff **fb,
-			     struct tipc_msg **msg);
-void tipc_link_send_proto_msg(struct link *l_ptr, u32 msg_typ, int prob, u32 gap,
-			      u32 tolerance, u32 priority, u32 acked_mtu);
-void tipc_link_push_queue(struct link *l_ptr);
-u32 tipc_link_defer_pkt(struct sk_buff **head, struct sk_buff **tail,
-		   struct sk_buff *buf);
-void tipc_link_wakeup_ports(struct link *l_ptr, int all);
-void tipc_link_set_queue_limits(struct link *l_ptr, u32 window);
-void tipc_link_retransmit(struct link *l_ptr, struct sk_buff *start, u32 retransmits);
-
-/*
- * Link sequence number manipulation routines (uses modulo 2**16 arithmetic)
- */
-
-static inline u32 mod(u32 x)
-{
-	return x & 0xffffu;
-}
-
-static inline int between(u32 lower, u32 upper, u32 n)
-{
-	if ((lower < n) && (n < upper))
-		return 1;
-	if ((upper < lower) && ((n > lower) || (n < upper)))
-		return 1;
-	return 0;
-}
-
-static inline int less_eq(u32 left, u32 right)
-{
-	return (mod(right - left) < 32768u);
-}
-
-static inline int less(u32 left, u32 right)
-{
-	return (less_eq(left, right) && (mod(right) != mod(left)));
-}
-
-static inline u32 lesser(u32 left, u32 right)
-{
-	return less_eq(left, right) ? left : right;
-}
-
-#endif
diff -ruN linux-2.6.29/net/tipc/Makefile android_cluster/linux-2.6.29/net/tipc/Makefile
--- linux-2.6.29/net/tipc/Makefile	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/Makefile	2014-05-27 23:04:10.634023603 -0700
@@ -4,10 +4,10 @@
 
 obj-$(CONFIG_TIPC) := tipc.o
 
-tipc-y	+= addr.o bcast.o bearer.o config.o cluster.o \
-	   core.o handler.o link.o discover.o msg.o  \
-	   name_distr.o  subscr.o name_table.o net.o  \
-	   netlink.o node.o node_subscr.o port.o ref.o  \
-	   socket.o user_reg.o zone.o dbg.o eth_media.o
+tipc-y	+= tipc_addr.o tipc_bcast.o tipc_bearer.o tipc_cfgsrv.o  \
+	   tipc_core.o tipc_handler.o tipc_link.o tipc_discover.o tipc_msg.o  \
+	   tipc_name_distr.o tipc_topsrv.o tipc_name_table.o tipc_net.o  \
+	   tipc_netlink.o tipc_node.o tipc_port.o tipc_ref.o  \
+	   tipc_socket.o tipc_user_reg.o tipc_dbg.o tipc_eth_media.o
 
 # End of file
diff -ruN linux-2.6.29/net/tipc/msg.c android_cluster/linux-2.6.29/net/tipc/msg.c
--- linux-2.6.29/net/tipc/msg.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/msg.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,324 +0,0 @@
-/*
- * net/tipc/msg.c: TIPC message header routines
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "addr.h"
-#include "dbg.h"
-#include "msg.h"
-#include "bearer.h"
-
-
-#ifdef CONFIG_TIPC_DEBUG
-
-void tipc_msg_dbg(struct print_buf *buf, struct tipc_msg *msg, const char *str)
-{
-	u32 usr = msg_user(msg);
-	tipc_printf(buf, str);
-
-	switch (usr) {
-	case MSG_BUNDLER:
-		tipc_printf(buf, "BNDL::");
-		tipc_printf(buf, "MSGS(%u):", msg_msgcnt(msg));
-		break;
-	case BCAST_PROTOCOL:
-		tipc_printf(buf, "BCASTP::");
-		break;
-	case MSG_FRAGMENTER:
-		tipc_printf(buf, "FRAGM::");
-		switch (msg_type(msg)) {
-		case FIRST_FRAGMENT:
-			tipc_printf(buf, "FIRST:");
-			break;
-		case FRAGMENT:
-			tipc_printf(buf, "BODY:");
-			break;
-		case LAST_FRAGMENT:
-			tipc_printf(buf, "LAST:");
-			break;
-		default:
-			tipc_printf(buf, "UNKNOWN:%x",msg_type(msg));
-
-		}
-		tipc_printf(buf, "NO(%u/%u):",msg_long_msgno(msg),
-			    msg_fragm_no(msg));
-		break;
-	case TIPC_LOW_IMPORTANCE:
-	case TIPC_MEDIUM_IMPORTANCE:
-	case TIPC_HIGH_IMPORTANCE:
-	case TIPC_CRITICAL_IMPORTANCE:
-		tipc_printf(buf, "DAT%u:", msg_user(msg));
-		if (msg_short(msg)) {
-			tipc_printf(buf, "CON:");
-			break;
-		}
-		switch (msg_type(msg)) {
-		case TIPC_CONN_MSG:
-			tipc_printf(buf, "CON:");
-			break;
-		case TIPC_MCAST_MSG:
-			tipc_printf(buf, "MCST:");
-			break;
-		case TIPC_NAMED_MSG:
-			tipc_printf(buf, "NAM:");
-			break;
-		case TIPC_DIRECT_MSG:
-			tipc_printf(buf, "DIR:");
-			break;
-		default:
-			tipc_printf(buf, "UNKNOWN TYPE %u",msg_type(msg));
-		}
-		if (msg_routed(msg) && !msg_non_seq(msg))
-			tipc_printf(buf, "ROUT:");
-		if (msg_reroute_cnt(msg))
-			tipc_printf(buf, "REROUTED(%u):",
-				    msg_reroute_cnt(msg));
-		break;
-	case NAME_DISTRIBUTOR:
-		tipc_printf(buf, "NMD::");
-		switch (msg_type(msg)) {
-		case PUBLICATION:
-			tipc_printf(buf, "PUBL(%u):", (msg_size(msg) - msg_hdr_sz(msg)) / 20);	/* Items */
-			break;
-		case WITHDRAWAL:
-			tipc_printf(buf, "WDRW:");
-			break;
-		default:
-			tipc_printf(buf, "UNKNOWN:%x",msg_type(msg));
-		}
-		if (msg_routed(msg))
-			tipc_printf(buf, "ROUT:");
-		if (msg_reroute_cnt(msg))
-			tipc_printf(buf, "REROUTED(%u):",
-				    msg_reroute_cnt(msg));
-		break;
-	case CONN_MANAGER:
-		tipc_printf(buf, "CONN_MNG:");
-		switch (msg_type(msg)) {
-		case CONN_PROBE:
-			tipc_printf(buf, "PROBE:");
-			break;
-		case CONN_PROBE_REPLY:
-			tipc_printf(buf, "PROBE_REPLY:");
-			break;
-		case CONN_ACK:
-			tipc_printf(buf, "CONN_ACK:");
-			tipc_printf(buf, "ACK(%u):",msg_msgcnt(msg));
-			break;
-		default:
-			tipc_printf(buf, "UNKNOWN TYPE:%x",msg_type(msg));
-		}
-		if (msg_routed(msg))
-			tipc_printf(buf, "ROUT:");
-		if (msg_reroute_cnt(msg))
-			tipc_printf(buf, "REROUTED(%u):",msg_reroute_cnt(msg));
-		break;
-	case LINK_PROTOCOL:
-		tipc_printf(buf, "PROT:TIM(%u):",msg_timestamp(msg));
-		switch (msg_type(msg)) {
-		case STATE_MSG:
-			tipc_printf(buf, "STATE:");
-			tipc_printf(buf, "%s:",msg_probe(msg) ? "PRB" :"");
-			tipc_printf(buf, "NXS(%u):",msg_next_sent(msg));
-			tipc_printf(buf, "GAP(%u):",msg_seq_gap(msg));
-			tipc_printf(buf, "LSTBC(%u):",msg_last_bcast(msg));
-			break;
-		case RESET_MSG:
-			tipc_printf(buf, "RESET:");
-			if (msg_size(msg) != msg_hdr_sz(msg))
-				tipc_printf(buf, "BEAR:%s:",msg_data(msg));
-			break;
-		case ACTIVATE_MSG:
-			tipc_printf(buf, "ACTIVATE:");
-			break;
-		default:
-			tipc_printf(buf, "UNKNOWN TYPE:%x",msg_type(msg));
-		}
-		tipc_printf(buf, "PLANE(%c):",msg_net_plane(msg));
-		tipc_printf(buf, "SESS(%u):",msg_session(msg));
-		break;
-	case CHANGEOVER_PROTOCOL:
-		tipc_printf(buf, "TUNL:");
-		switch (msg_type(msg)) {
-		case DUPLICATE_MSG:
-			tipc_printf(buf, "DUPL:");
-			break;
-		case ORIGINAL_MSG:
-			tipc_printf(buf, "ORIG:");
-			tipc_printf(buf, "EXP(%u)",msg_msgcnt(msg));
-			break;
-		default:
-			tipc_printf(buf, "UNKNOWN TYPE:%x",msg_type(msg));
-		}
-		break;
-	case ROUTE_DISTRIBUTOR:
-		tipc_printf(buf, "ROUTING_MNG:");
-		switch (msg_type(msg)) {
-		case EXT_ROUTING_TABLE:
-			tipc_printf(buf, "EXT_TBL:");
-			tipc_printf(buf, "TO:%x:",msg_remote_node(msg));
-			break;
-		case LOCAL_ROUTING_TABLE:
-			tipc_printf(buf, "LOCAL_TBL:");
-			tipc_printf(buf, "TO:%x:",msg_remote_node(msg));
-			break;
-		case SLAVE_ROUTING_TABLE:
-			tipc_printf(buf, "DP_TBL:");
-			tipc_printf(buf, "TO:%x:",msg_remote_node(msg));
-			break;
-		case ROUTE_ADDITION:
-			tipc_printf(buf, "ADD:");
-			tipc_printf(buf, "TO:%x:",msg_remote_node(msg));
-			break;
-		case ROUTE_REMOVAL:
-			tipc_printf(buf, "REMOVE:");
-			tipc_printf(buf, "TO:%x:",msg_remote_node(msg));
-			break;
-		default:
-			tipc_printf(buf, "UNKNOWN TYPE:%x",msg_type(msg));
-		}
-		break;
-	case LINK_CONFIG:
-		tipc_printf(buf, "CFG:");
-		switch (msg_type(msg)) {
-		case DSC_REQ_MSG:
-			tipc_printf(buf, "DSC_REQ:");
-			break;
-		case DSC_RESP_MSG:
-			tipc_printf(buf, "DSC_RESP:");
-			break;
-		default:
-			tipc_printf(buf, "UNKNOWN TYPE:%x:",msg_type(msg));
-			break;
-		}
-		break;
-	default:
-		tipc_printf(buf, "UNKNOWN USER:");
-	}
-
-	switch (usr) {
-	case CONN_MANAGER:
-	case TIPC_LOW_IMPORTANCE:
-	case TIPC_MEDIUM_IMPORTANCE:
-	case TIPC_HIGH_IMPORTANCE:
-	case TIPC_CRITICAL_IMPORTANCE:
-		switch (msg_errcode(msg)) {
-		case TIPC_OK:
-			break;
-		case TIPC_ERR_NO_NAME:
-			tipc_printf(buf, "NO_NAME:");
-			break;
-		case TIPC_ERR_NO_PORT:
-			tipc_printf(buf, "NO_PORT:");
-			break;
-		case TIPC_ERR_NO_NODE:
-			tipc_printf(buf, "NO_PROC:");
-			break;
-		case TIPC_ERR_OVERLOAD:
-			tipc_printf(buf, "OVERLOAD:");
-			break;
-		case TIPC_CONN_SHUTDOWN:
-			tipc_printf(buf, "SHUTDOWN:");
-			break;
-		default:
-			tipc_printf(buf, "UNKNOWN ERROR(%x):",
-				    msg_errcode(msg));
-		}
-	default:{}
-	}
-
-	tipc_printf(buf, "HZ(%u):", msg_hdr_sz(msg));
-	tipc_printf(buf, "SZ(%u):", msg_size(msg));
-	tipc_printf(buf, "SQNO(%u):", msg_seqno(msg));
-
-	if (msg_non_seq(msg))
-		tipc_printf(buf, "NOSEQ:");
-	else {
-		tipc_printf(buf, "ACK(%u):", msg_ack(msg));
-	}
-	tipc_printf(buf, "BACK(%u):", msg_bcast_ack(msg));
-	tipc_printf(buf, "PRND(%x)", msg_prevnode(msg));
-
-	if (msg_isdata(msg)) {
-		if (msg_named(msg)) {
-			tipc_printf(buf, "NTYP(%u):", msg_nametype(msg));
-			tipc_printf(buf, "NINST(%u)", msg_nameinst(msg));
-		}
-	}
-
-	if ((usr != LINK_PROTOCOL) && (usr != LINK_CONFIG) &&
-	    (usr != MSG_BUNDLER)) {
-		if (!msg_short(msg)) {
-			tipc_printf(buf, ":ORIG(%x:%u):",
-				    msg_orignode(msg), msg_origport(msg));
-			tipc_printf(buf, ":DEST(%x:%u):",
-				    msg_destnode(msg), msg_destport(msg));
-		} else {
-			tipc_printf(buf, ":OPRT(%u):", msg_origport(msg));
-			tipc_printf(buf, ":DPRT(%u):", msg_destport(msg));
-		}
-		if (msg_routed(msg) && !msg_non_seq(msg))
-			tipc_printf(buf, ":TSEQN(%u)", msg_transp_seqno(msg));
-	}
-	if (msg_user(msg) == NAME_DISTRIBUTOR) {
-		tipc_printf(buf, ":ONOD(%x):", msg_orignode(msg));
-		tipc_printf(buf, ":DNOD(%x):", msg_destnode(msg));
-		if (msg_routed(msg)) {
-			tipc_printf(buf, ":CSEQN(%u)", msg_transp_seqno(msg));
-		}
-	}
-
-	if (msg_user(msg) ==  LINK_CONFIG) {
-		u32* raw = (u32*)msg;
-		struct tipc_media_addr* orig = (struct tipc_media_addr*)&raw[5];
-		tipc_printf(buf, ":REQL(%u):", msg_req_links(msg));
-		tipc_printf(buf, ":DDOM(%x):", msg_dest_domain(msg));
-		tipc_printf(buf, ":NETID(%u):", msg_bc_netid(msg));
-		tipc_media_addr_printf(buf, orig);
-	}
-	if (msg_user(msg) == BCAST_PROTOCOL) {
-		tipc_printf(buf, "BCNACK:AFTER(%u):", msg_bcgap_after(msg));
-		tipc_printf(buf, "TO(%u):", msg_bcgap_to(msg));
-	}
-	tipc_printf(buf, "\n");
-	if ((usr == CHANGEOVER_PROTOCOL) && (msg_msgcnt(msg))) {
-		tipc_msg_dbg(buf, msg_get_wrapped(msg), "      /");
-	}
-	if ((usr == MSG_FRAGMENTER) && (msg_type(msg) == FIRST_FRAGMENT)) {
-		tipc_msg_dbg(buf, msg_get_wrapped(msg), "      /");
-	}
-}
-
-#endif
diff -ruN linux-2.6.29/net/tipc/msg.h android_cluster/linux-2.6.29/net/tipc/msg.h
--- linux-2.6.29/net/tipc/msg.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/msg.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,816 +0,0 @@
-/*
- * net/tipc/msg.h: Include file for TIPC message header routines
- *
- * Copyright (c) 2000-2007, Ericsson AB
- * Copyright (c) 2005-2008, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_MSG_H
-#define _TIPC_MSG_H
-
-#include "core.h"
-
-#define TIPC_VERSION              2
-
-#define SHORT_H_SIZE              24	/* Connected, in-cluster messages */
-#define DIR_MSG_H_SIZE            32	/* Directly addressed messages */
-#define LONG_H_SIZE               40	/* Named messages */
-#define MCAST_H_SIZE              44	/* Multicast messages */
-#define INT_H_SIZE                40	/* Internal messages */
-#define MIN_H_SIZE                24	/* Smallest legal TIPC header size */
-#define MAX_H_SIZE                60	/* Largest possible TIPC header size */
-
-#define MAX_MSG_SIZE (MAX_H_SIZE + TIPC_MAX_USER_MSG_SIZE)
-
-
-/*
-		TIPC user data message header format, version 2
-
-	- Fundamental definitions available to privileged TIPC users
-	  are located in tipc_msg.h.
-	- Remaining definitions available to TIPC internal users appear below.
-*/
-
-
-static inline void msg_set_word(struct tipc_msg *m, u32 w, u32 val)
-{
-	m->hdr[w] = htonl(val);
-}
-
-static inline void msg_set_bits(struct tipc_msg *m, u32 w,
-				u32 pos, u32 mask, u32 val)
-{
-	val = (val & mask) << pos;
-	mask = mask << pos;
-	m->hdr[w] &= ~htonl(mask);
-	m->hdr[w] |= htonl(val);
-}
-
-static inline void msg_swap_words(struct tipc_msg *msg, u32 a, u32 b)
-{
-	u32 temp = msg->hdr[a];
-
-	msg->hdr[a] = msg->hdr[b];
-	msg->hdr[b] = temp;
-}
-
-/*
- * Word 0
- */
-
-static inline u32 msg_version(struct tipc_msg *m)
-{
-	return msg_bits(m, 0, 29, 7);
-}
-
-static inline void msg_set_version(struct tipc_msg *m)
-{
-	msg_set_bits(m, 0, 29, 7, TIPC_VERSION);
-}
-
-static inline u32 msg_user(struct tipc_msg *m)
-{
-	return msg_bits(m, 0, 25, 0xf);
-}
-
-static inline u32 msg_isdata(struct tipc_msg *m)
-{
-	return (msg_user(m) <= TIPC_CRITICAL_IMPORTANCE);
-}
-
-static inline void msg_set_user(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 0, 25, 0xf, n);
-}
-
-static inline void msg_set_importance(struct tipc_msg *m, u32 i)
-{
-	msg_set_user(m, i);
-}
-
-static inline void msg_set_hdr_sz(struct tipc_msg *m,u32 n)
-{
-	msg_set_bits(m, 0, 21, 0xf, n>>2);
-}
-
-static inline int msg_non_seq(struct tipc_msg *m)
-{
-	return msg_bits(m, 0, 20, 1);
-}
-
-static inline void msg_set_non_seq(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 0, 20, 1, n);
-}
-
-static inline int msg_dest_droppable(struct tipc_msg *m)
-{
-	return msg_bits(m, 0, 19, 1);
-}
-
-static inline void msg_set_dest_droppable(struct tipc_msg *m, u32 d)
-{
-	msg_set_bits(m, 0, 19, 1, d);
-}
-
-static inline int msg_src_droppable(struct tipc_msg *m)
-{
-	return msg_bits(m, 0, 18, 1);
-}
-
-static inline void msg_set_src_droppable(struct tipc_msg *m, u32 d)
-{
-	msg_set_bits(m, 0, 18, 1, d);
-}
-
-static inline void msg_set_size(struct tipc_msg *m, u32 sz)
-{
-	m->hdr[0] = htonl((msg_word(m, 0) & ~0x1ffff) | sz);
-}
-
-
-/*
- * Word 1
- */
-
-static inline void msg_set_type(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 1, 29, 0x7, n);
-}
-
-static inline void msg_set_errcode(struct tipc_msg *m, u32 err)
-{
-	msg_set_bits(m, 1, 25, 0xf, err);
-}
-
-static inline u32 msg_reroute_cnt(struct tipc_msg *m)
-{
-	return msg_bits(m, 1, 21, 0xf);
-}
-
-static inline void msg_incr_reroute_cnt(struct tipc_msg *m)
-{
-	msg_set_bits(m, 1, 21, 0xf, msg_reroute_cnt(m) + 1);
-}
-
-static inline void msg_reset_reroute_cnt(struct tipc_msg *m)
-{
-	msg_set_bits(m, 1, 21, 0xf, 0);
-}
-
-static inline u32 msg_lookup_scope(struct tipc_msg *m)
-{
-	return msg_bits(m, 1, 19, 0x3);
-}
-
-static inline void msg_set_lookup_scope(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 1, 19, 0x3, n);
-}
-
-static inline u32 msg_bcast_ack(struct tipc_msg *m)
-{
-	return msg_bits(m, 1, 0, 0xffff);
-}
-
-static inline void msg_set_bcast_ack(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 1, 0, 0xffff, n);
-}
-
-
-/*
- * Word 2
- */
-
-static inline u32 msg_ack(struct tipc_msg *m)
-{
-	return msg_bits(m, 2, 16, 0xffff);
-}
-
-static inline void msg_set_ack(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 2, 16, 0xffff, n);
-}
-
-static inline u32 msg_seqno(struct tipc_msg *m)
-{
-	return msg_bits(m, 2, 0, 0xffff);
-}
-
-static inline void msg_set_seqno(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 2, 0, 0xffff, n);
-}
-
-/*
- * TIPC may utilize the "link ack #" and "link seq #" fields of a short
- * message header to hold the destination node for the message, since the
- * normal "dest node" field isn't present.  This cache is only referenced
- * when required, so populating the cache of a longer message header is
- * harmless (as long as the header has the two link sequence fields present).
- *
- * Note: Host byte order is OK here, since the info never goes off-card.
- */
-
-static inline u32 msg_destnode_cache(struct tipc_msg *m)
-{
-	return m->hdr[2];
-}
-
-static inline void msg_set_destnode_cache(struct tipc_msg *m, u32 dnode)
-{
-	m->hdr[2] = dnode;
-}
-
-/*
- * Words 3-10
- */
-
-
-static inline void msg_set_prevnode(struct tipc_msg *m, u32 a)
-{
-	msg_set_word(m, 3, a);
-}
-
-static inline void msg_set_origport(struct tipc_msg *m, u32 p)
-{
-	msg_set_word(m, 4, p);
-}
-
-static inline void msg_set_destport(struct tipc_msg *m, u32 p)
-{
-	msg_set_word(m, 5, p);
-}
-
-static inline void msg_set_mc_netid(struct tipc_msg *m, u32 p)
-{
-	msg_set_word(m, 5, p);
-}
-
-static inline void msg_set_orignode(struct tipc_msg *m, u32 a)
-{
-	msg_set_word(m, 6, a);
-}
-
-static inline void msg_set_destnode(struct tipc_msg *m, u32 a)
-{
-	msg_set_word(m, 7, a);
-}
-
-static inline int msg_is_dest(struct tipc_msg *m, u32 d)
-{
-	return(msg_short(m) || (msg_destnode(m) == d));
-}
-
-static inline u32 msg_routed(struct tipc_msg *m)
-{
-	if (likely(msg_short(m)))
-		return 0;
-	return(msg_destnode(m) ^ msg_orignode(m)) >> 11;
-}
-
-static inline void msg_set_nametype(struct tipc_msg *m, u32 n)
-{
-	msg_set_word(m, 8, n);
-}
-
-static inline u32 msg_transp_seqno(struct tipc_msg *m)
-{
-	return msg_word(m, 8);
-}
-
-static inline void msg_set_timestamp(struct tipc_msg *m, u32 n)
-{
-	msg_set_word(m, 8, n);
-}
-
-static inline u32 msg_timestamp(struct tipc_msg *m)
-{
-	return msg_word(m, 8);
-}
-
-static inline void msg_set_transp_seqno(struct tipc_msg *m, u32 n)
-{
-	msg_set_word(m, 8, n);
-}
-
-static inline void msg_set_namelower(struct tipc_msg *m, u32 n)
-{
-	msg_set_word(m, 9, n);
-}
-
-static inline void msg_set_nameinst(struct tipc_msg *m, u32 n)
-{
-	msg_set_namelower(m, n);
-}
-
-static inline void msg_set_nameupper(struct tipc_msg *m, u32 n)
-{
-	msg_set_word(m, 10, n);
-}
-
-static inline struct tipc_msg *msg_get_wrapped(struct tipc_msg *m)
-{
-	return (struct tipc_msg *)msg_data(m);
-}
-
-
-/*
-		TIPC internal message header format, version 2
-
-       1 0 9 8 7 6 5 4|3 2 1 0 9 8 7 6|5 4 3 2 1 0 9 8|7 6 5 4 3 2 1 0
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w0:|vers |msg usr|hdr sz |n|resrv|            packet size          |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w1:|m typ|      sequence gap       |       broadcast ack no        |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w2:| link level ack no/bc_gap_from |     seq no / bcast_gap_to     |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w3:|                       previous node                           |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w4:|  next sent broadcast/fragm no | next sent pkt/ fragm msg no   |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w5:|          session no           |rsv=0|r|berid|link prio|netpl|p|
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w6:|                      originating node                         |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w7:|                      destination node                         |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w8:|                   transport sequence number                   |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-   w9:|   msg count / bcast tag       |       link tolerance          |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      \                                                               \
-      /                     User Specific Data                        /
-      \                                                               \
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-
-      NB: CONN_MANAGER use data message format. LINK_CONFIG has own format.
-*/
-
-/*
- * Internal users
- */
-
-#define  BCAST_PROTOCOL       5
-#define  MSG_BUNDLER          6
-#define  LINK_PROTOCOL        7
-#define  CONN_MANAGER         8
-#define  ROUTE_DISTRIBUTOR    9
-#define  CHANGEOVER_PROTOCOL  10
-#define  NAME_DISTRIBUTOR     11
-#define  MSG_FRAGMENTER       12
-#define  LINK_CONFIG          13
-#define  DSC_H_SIZE           40
-
-/*
- *  Connection management protocol messages
- */
-
-#define CONN_PROBE        0
-#define CONN_PROBE_REPLY  1
-#define CONN_ACK          2
-
-/*
- * Name distributor messages
- */
-
-#define PUBLICATION       0
-#define WITHDRAWAL        1
-
-
-/*
- * Word 1
- */
-
-static inline u32 msg_seq_gap(struct tipc_msg *m)
-{
-	return msg_bits(m, 1, 16, 0x1fff);
-}
-
-static inline void msg_set_seq_gap(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 1, 16, 0x1fff, n);
-}
-
-static inline u32 msg_req_links(struct tipc_msg *m)
-{
-	return msg_bits(m, 1, 16, 0xfff);
-}
-
-static inline void msg_set_req_links(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 1, 16, 0xfff, n);
-}
-
-
-/*
- * Word 2
- */
-
-static inline u32 msg_dest_domain(struct tipc_msg *m)
-{
-	return msg_word(m, 2);
-}
-
-static inline void msg_set_dest_domain(struct tipc_msg *m, u32 n)
-{
-	msg_set_word(m, 2, n);
-}
-
-static inline u32 msg_bcgap_after(struct tipc_msg *m)
-{
-	return msg_bits(m, 2, 16, 0xffff);
-}
-
-static inline void msg_set_bcgap_after(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 2, 16, 0xffff, n);
-}
-
-static inline u32 msg_bcgap_to(struct tipc_msg *m)
-{
-	return msg_bits(m, 2, 0, 0xffff);
-}
-
-static inline void msg_set_bcgap_to(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 2, 0, 0xffff, n);
-}
-
-
-/*
- * Word 4
- */
-
-static inline u32 msg_last_bcast(struct tipc_msg *m)
-{
-	return msg_bits(m, 4, 16, 0xffff);
-}
-
-static inline void msg_set_last_bcast(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 4, 16, 0xffff, n);
-}
-
-
-static inline u32 msg_fragm_no(struct tipc_msg *m)
-{
-	return msg_bits(m, 4, 16, 0xffff);
-}
-
-static inline void msg_set_fragm_no(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 4, 16, 0xffff, n);
-}
-
-
-static inline u32 msg_next_sent(struct tipc_msg *m)
-{
-	return msg_bits(m, 4, 0, 0xffff);
-}
-
-static inline void msg_set_next_sent(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 4, 0, 0xffff, n);
-}
-
-
-static inline u32 msg_long_msgno(struct tipc_msg *m)
-{
-	return msg_bits(m, 4, 0, 0xffff);
-}
-
-static inline void msg_set_long_msgno(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 4, 0, 0xffff, n);
-}
-
-static inline u32 msg_bc_netid(struct tipc_msg *m)
-{
-	return msg_word(m, 4);
-}
-
-static inline void msg_set_bc_netid(struct tipc_msg *m, u32 id)
-{
-	msg_set_word(m, 4, id);
-}
-
-static inline u32 msg_link_selector(struct tipc_msg *m)
-{
-	return msg_bits(m, 4, 0, 1);
-}
-
-static inline void msg_set_link_selector(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 4, 0, 1, (n & 1));
-}
-
-/*
- * Word 5
- */
-
-static inline u32 msg_session(struct tipc_msg *m)
-{
-	return msg_bits(m, 5, 16, 0xffff);
-}
-
-static inline void msg_set_session(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 5, 16, 0xffff, n);
-}
-
-static inline u32 msg_probe(struct tipc_msg *m)
-{
-	return msg_bits(m, 5, 0, 1);
-}
-
-static inline void msg_set_probe(struct tipc_msg *m, u32 val)
-{
-	msg_set_bits(m, 5, 0, 1, (val & 1));
-}
-
-static inline char msg_net_plane(struct tipc_msg *m)
-{
-	return msg_bits(m, 5, 1, 7) + 'A';
-}
-
-static inline void msg_set_net_plane(struct tipc_msg *m, char n)
-{
-	msg_set_bits(m, 5, 1, 7, (n - 'A'));
-}
-
-static inline u32 msg_linkprio(struct tipc_msg *m)
-{
-	return msg_bits(m, 5, 4, 0x1f);
-}
-
-static inline void msg_set_linkprio(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 5, 4, 0x1f, n);
-}
-
-static inline u32 msg_bearer_id(struct tipc_msg *m)
-{
-	return msg_bits(m, 5, 9, 0x7);
-}
-
-static inline void msg_set_bearer_id(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 5, 9, 0x7, n);
-}
-
-static inline u32 msg_redundant_link(struct tipc_msg *m)
-{
-	return msg_bits(m, 5, 12, 0x1);
-}
-
-static inline void msg_set_redundant_link(struct tipc_msg *m)
-{
-	msg_set_bits(m, 5, 12, 0x1, 1);
-}
-
-static inline void msg_clear_redundant_link(struct tipc_msg *m)
-{
-	msg_set_bits(m, 5, 12, 0x1, 0);
-}
-
-
-/*
- * Word 9
- */
-
-static inline u32 msg_msgcnt(struct tipc_msg *m)
-{
-	return msg_bits(m, 9, 16, 0xffff);
-}
-
-static inline void msg_set_msgcnt(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 9, 16, 0xffff, n);
-}
-
-static inline u32 msg_bcast_tag(struct tipc_msg *m)
-{
-	return msg_bits(m, 9, 16, 0xffff);
-}
-
-static inline void msg_set_bcast_tag(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 9, 16, 0xffff, n);
-}
-
-static inline u32 msg_max_pkt(struct tipc_msg *m)
-{
-	return (msg_bits(m, 9, 16, 0xffff) * 4);
-}
-
-static inline void msg_set_max_pkt(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 9, 16, 0xffff, (n / 4));
-}
-
-static inline u32 msg_link_tolerance(struct tipc_msg *m)
-{
-	return msg_bits(m, 9, 0, 0xffff);
-}
-
-static inline void msg_set_link_tolerance(struct tipc_msg *m, u32 n)
-{
-	msg_set_bits(m, 9, 0, 0xffff, n);
-}
-
-/*
- * Routing table message data
- */
-
-
-static inline u32 msg_remote_node(struct tipc_msg *m)
-{
-	return msg_word(m, msg_hdr_sz(m)/4);
-}
-
-static inline void msg_set_remote_node(struct tipc_msg *m, u32 a)
-{
-	msg_set_word(m, msg_hdr_sz(m)/4, a);
-}
-
-static inline void msg_set_dataoctet(struct tipc_msg *m, u32 pos)
-{
-	msg_data(m)[pos + 4] = 1;
-}
-
-/*
- * Segmentation message types
- */
-
-#define FIRST_FRAGMENT     0
-#define FRAGMENT           1
-#define LAST_FRAGMENT      2
-
-/*
- * Link management protocol message types
- */
-
-#define STATE_MSG       0
-#define RESET_MSG       1
-#define ACTIVATE_MSG    2
-
-/*
- * Changeover tunnel message types
- */
-#define DUPLICATE_MSG    0
-#define ORIGINAL_MSG     1
-
-/*
- * Routing table message types
- */
-#define EXT_ROUTING_TABLE    0
-#define LOCAL_ROUTING_TABLE  1
-#define SLAVE_ROUTING_TABLE  2
-#define ROUTE_ADDITION       3
-#define ROUTE_REMOVAL        4
-
-/*
- * Config protocol message types
- */
-
-#define DSC_REQ_MSG          0
-#define DSC_RESP_MSG         1
-
-static inline u32 msg_tot_importance(struct tipc_msg *m)
-{
-	if (likely(msg_isdata(m))) {
-		if (likely(msg_orignode(m) == tipc_own_addr))
-			return msg_importance(m);
-		return msg_importance(m) + 4;
-	}
-	if ((msg_user(m) == MSG_FRAGMENTER)  &&
-	    (msg_type(m) == FIRST_FRAGMENT))
-		return msg_importance(msg_get_wrapped(m));
-	return msg_importance(m);
-}
-
-
-static inline void msg_init(struct tipc_msg *m, u32 user, u32 type,
-			    u32 hsize, u32 destnode)
-{
-	memset(m, 0, hsize);
-	msg_set_version(m);
-	msg_set_user(m, user);
-	msg_set_hdr_sz(m, hsize);
-	msg_set_size(m, hsize);
-	msg_set_prevnode(m, tipc_own_addr);
-	msg_set_type(m, type);
-	if (!msg_short(m)) {
-		msg_set_orignode(m, tipc_own_addr);
-		msg_set_destnode(m, destnode);
-	}
-}
-
-/**
- * msg_calc_data_size - determine total data size for message
- */
-
-static inline int msg_calc_data_size(struct iovec const *msg_sect, u32 num_sect)
-{
-	int dsz = 0;
-	int i;
-
-	for (i = 0; i < num_sect; i++)
-		dsz += msg_sect[i].iov_len;
-	return dsz;
-}
-
-/**
- * msg_build - create message using specified header and data
- *
- * Note: Caller must not hold any locks in case copy_from_user() is interrupted!
- *
- * Returns message data size or errno
- */
-
-static inline int msg_build(struct tipc_msg *hdr,
-			    struct iovec const *msg_sect, u32 num_sect,
-			    int max_size, int usrmem, struct sk_buff** buf)
-{
-	int dsz, sz, hsz, pos, res, cnt;
-
-	dsz = msg_calc_data_size(msg_sect, num_sect);
-	if (unlikely(dsz > TIPC_MAX_USER_MSG_SIZE)) {
-		*buf = NULL;
-		return -EINVAL;
-	}
-
-	pos = hsz = msg_hdr_sz(hdr);
-	sz = hsz + dsz;
-	msg_set_size(hdr, sz);
-	if (unlikely(sz > max_size)) {
-		*buf = NULL;
-		return dsz;
-	}
-
-	*buf = buf_acquire(sz);
-	if (!(*buf))
-		return -ENOMEM;
-	skb_copy_to_linear_data(*buf, hdr, hsz);
-	for (res = 1, cnt = 0; res && (cnt < num_sect); cnt++) {
-		if (likely(usrmem))
-			res = !copy_from_user((*buf)->data + pos,
-					      msg_sect[cnt].iov_base,
-					      msg_sect[cnt].iov_len);
-		else
-			skb_copy_to_linear_data_offset(*buf, pos,
-						       msg_sect[cnt].iov_base,
-						       msg_sect[cnt].iov_len);
-		pos += msg_sect[cnt].iov_len;
-	}
-	if (likely(res))
-		return dsz;
-
-	buf_discard(*buf);
-	*buf = NULL;
-	return -EFAULT;
-}
-
-static inline void msg_set_media_addr(struct tipc_msg *m, struct tipc_media_addr *a)
-{
-	memcpy(&((int *)m)[5], a, sizeof(*a));
-}
-
-static inline void msg_get_media_addr(struct tipc_msg *m, struct tipc_media_addr *a)
-{
-	memcpy(a, &((int*)m)[5], sizeof(*a));
-}
-
-#endif
diff -ruN linux-2.6.29/net/tipc/name_distr.c android_cluster/linux-2.6.29/net/tipc/name_distr.c
--- linux-2.6.29/net/tipc/name_distr.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/name_distr.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,318 +0,0 @@
-/*
- * net/tipc/name_distr.c: TIPC name distribution code
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "cluster.h"
-#include "dbg.h"
-#include "link.h"
-#include "msg.h"
-#include "name_distr.h"
-
-#define ITEM_SIZE sizeof(struct distr_item)
-
-/**
- * struct distr_item - publication info distributed to other nodes
- * @type: name sequence type
- * @lower: name sequence lower bound
- * @upper: name sequence upper bound
- * @ref: publishing port reference
- * @key: publication key
- *
- * ===> All fields are stored in network byte order. <===
- *
- * First 3 fields identify (name or) name sequence being published.
- * Reference field uniquely identifies port that published name sequence.
- * Key field uniquely identifies publication, in the event a port has
- * multiple publications of the same name sequence.
- *
- * Note: There is no field that identifies the publishing node because it is
- * the same for all items contained within a publication message.
- */
-
-struct distr_item {
-	__be32 type;
-	__be32 lower;
-	__be32 upper;
-	__be32 ref;
-	__be32 key;
-};
-
-/**
- * List of externally visible publications by this node --
- * that is, all publications having scope > TIPC_NODE_SCOPE.
- */
-
-static LIST_HEAD(publ_root);
-static u32 publ_cnt = 0;
-
-/**
- * publ_to_item - add publication info to a publication message
- */
-
-static void publ_to_item(struct distr_item *i, struct publication *p)
-{
-	i->type = htonl(p->type);
-	i->lower = htonl(p->lower);
-	i->upper = htonl(p->upper);
-	i->ref = htonl(p->ref);
-	i->key = htonl(p->key);
-	dbg("publ_to_item: %u, %u, %u\n", p->type, p->lower, p->upper);
-}
-
-/**
- * named_prepare_buf - allocate & initialize a publication message
- */
-
-static struct sk_buff *named_prepare_buf(u32 type, u32 size, u32 dest)
-{
-	struct sk_buff *buf = buf_acquire(LONG_H_SIZE + size);
-	struct tipc_msg *msg;
-
-	if (buf != NULL) {
-		msg = buf_msg(buf);
-		msg_init(msg, NAME_DISTRIBUTOR, type, LONG_H_SIZE, dest);
-		msg_set_size(msg, LONG_H_SIZE + size);
-	}
-	return buf;
-}
-
-/**
- * tipc_named_publish - tell other nodes about a new publication by this node
- */
-
-void tipc_named_publish(struct publication *publ)
-{
-	struct sk_buff *buf;
-	struct distr_item *item;
-
-	list_add_tail(&publ->local_list, &publ_root);
-	publ_cnt++;
-
-	buf = named_prepare_buf(PUBLICATION, ITEM_SIZE, 0);
-	if (!buf) {
-		warn("Publication distribution failure\n");
-		return;
-	}
-
-	item = (struct distr_item *)msg_data(buf_msg(buf));
-	publ_to_item(item, publ);
-	dbg("tipc_named_withdraw: broadcasting publish msg\n");
-	tipc_cltr_broadcast(buf);
-}
-
-/**
- * tipc_named_withdraw - tell other nodes about a withdrawn publication by this node
- */
-
-void tipc_named_withdraw(struct publication *publ)
-{
-	struct sk_buff *buf;
-	struct distr_item *item;
-
-	list_del(&publ->local_list);
-	publ_cnt--;
-
-	buf = named_prepare_buf(WITHDRAWAL, ITEM_SIZE, 0);
-	if (!buf) {
-		warn("Withdrawl distribution failure\n");
-		return;
-	}
-
-	item = (struct distr_item *)msg_data(buf_msg(buf));
-	publ_to_item(item, publ);
-	dbg("tipc_named_withdraw: broadcasting withdraw msg\n");
-	tipc_cltr_broadcast(buf);
-}
-
-/**
- * tipc_named_node_up - tell specified node about all publications by this node
- */
-
-void tipc_named_node_up(unsigned long node)
-{
-	struct publication *publ;
-	struct distr_item *item = NULL;
-	struct sk_buff *buf = NULL;
-	u32 left = 0;
-	u32 rest;
-	u32 max_item_buf;
-
-	read_lock_bh(&tipc_nametbl_lock);
-	max_item_buf = TIPC_MAX_USER_MSG_SIZE / ITEM_SIZE;
-	max_item_buf *= ITEM_SIZE;
-	rest = publ_cnt * ITEM_SIZE;
-
-	list_for_each_entry(publ, &publ_root, local_list) {
-		if (!buf) {
-			left = (rest <= max_item_buf) ? rest : max_item_buf;
-			rest -= left;
-			buf = named_prepare_buf(PUBLICATION, left, node);
-			if (!buf) {
-				warn("Bulk publication distribution failure\n");
-				goto exit;
-			}
-			item = (struct distr_item *)msg_data(buf_msg(buf));
-		}
-		publ_to_item(item, publ);
-		item++;
-		left -= ITEM_SIZE;
-		if (!left) {
-			msg_set_link_selector(buf_msg(buf), node);
-			dbg("tipc_named_node_up: sending publish msg to "
-			    "<%u.%u.%u>\n", tipc_zone(node),
-			    tipc_cluster(node), tipc_node(node));
-			tipc_link_send(buf, node, node);
-			buf = NULL;
-		}
-	}
-exit:
-	read_unlock_bh(&tipc_nametbl_lock);
-}
-
-/**
- * node_is_down - remove publication associated with a failed node
- *
- * Invoked for each publication issued by a newly failed node.
- * Removes publication structure from name table & deletes it.
- * In rare cases the link may have come back up again when this
- * function is called, and we have two items representing the same
- * publication. Nudge this item's key to distinguish it from the other.
- * (Note: Publication's node subscription is already unsubscribed.)
- */
-
-static void node_is_down(struct publication *publ)
-{
-	struct publication *p;
-
-	write_lock_bh(&tipc_nametbl_lock);
-	dbg("node_is_down: withdrawing %u, %u, %u\n",
-	    publ->type, publ->lower, publ->upper);
-	publ->key += 1222345;
-	p = tipc_nametbl_remove_publ(publ->type, publ->lower,
-				     publ->node, publ->ref, publ->key);
-	write_unlock_bh(&tipc_nametbl_lock);
-
-	if (p != publ) {
-		err("Unable to remove publication from failed node\n"
-		    "(type=%u, lower=%u, node=0x%x, ref=%u, key=%u)\n",
-		    publ->type, publ->lower, publ->node, publ->ref, publ->key);
-	}
-
-	if (p) {
-		kfree(p);
-	}
-}
-
-/**
- * tipc_named_recv - process name table update message sent by another node
- */
-
-void tipc_named_recv(struct sk_buff *buf)
-{
-	struct publication *publ;
-	struct tipc_msg *msg = buf_msg(buf);
-	struct distr_item *item = (struct distr_item *)msg_data(msg);
-	u32 count = msg_data_sz(msg) / ITEM_SIZE;
-
-	write_lock_bh(&tipc_nametbl_lock);
-	while (count--) {
-		if (msg_type(msg) == PUBLICATION) {
-			dbg("tipc_named_recv: got publication for %u, %u, %u\n",
-			    ntohl(item->type), ntohl(item->lower),
-			    ntohl(item->upper));
-			publ = tipc_nametbl_insert_publ(ntohl(item->type),
-							ntohl(item->lower),
-							ntohl(item->upper),
-							TIPC_CLUSTER_SCOPE,
-							msg_orignode(msg),
-							ntohl(item->ref),
-							ntohl(item->key));
-			if (publ) {
-				tipc_nodesub_subscribe(&publ->subscr,
-						       msg_orignode(msg),
-						       publ,
-						       (net_ev_handler)node_is_down);
-			}
-		} else if (msg_type(msg) == WITHDRAWAL) {
-			dbg("tipc_named_recv: got withdrawl for %u, %u, %u\n",
-			    ntohl(item->type), ntohl(item->lower),
-			    ntohl(item->upper));
-			publ = tipc_nametbl_remove_publ(ntohl(item->type),
-							ntohl(item->lower),
-							msg_orignode(msg),
-							ntohl(item->ref),
-							ntohl(item->key));
-
-			if (publ) {
-				tipc_nodesub_unsubscribe(&publ->subscr);
-				kfree(publ);
-			} else {
-				err("Unable to remove publication by node 0x%x\n"
-				    "(type=%u, lower=%u, ref=%u, key=%u)\n",
-				    msg_orignode(msg),
-				    ntohl(item->type), ntohl(item->lower),
-				    ntohl(item->ref), ntohl(item->key));
-			}
-		} else {
-			warn("Unrecognized name table message received\n");
-		}
-		item++;
-	}
-	write_unlock_bh(&tipc_nametbl_lock);
-	buf_discard(buf);
-}
-
-/**
- * tipc_named_reinit - re-initialize local publication list
- *
- * This routine is called whenever TIPC networking is (re)enabled.
- * All existing publications by this node that have "cluster" or "zone" scope
- * are updated to reflect the node's current network address.
- * (If the node's address is unchanged, the update loop terminates immediately.)
- */
-
-void tipc_named_reinit(void)
-{
-	struct publication *publ;
-
-	write_lock_bh(&tipc_nametbl_lock);
-	list_for_each_entry(publ, &publ_root, local_list) {
-		if (publ->node == tipc_own_addr)
-			break;
-		publ->node = tipc_own_addr;
-	}
-	write_unlock_bh(&tipc_nametbl_lock);
-}
diff -ruN linux-2.6.29/net/tipc/name_distr.h android_cluster/linux-2.6.29/net/tipc/name_distr.h
--- linux-2.6.29/net/tipc/name_distr.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/name_distr.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,48 +0,0 @@
-/*
- * net/tipc/name_distr.h: Include file for TIPC name distribution code
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_NAME_DISTR_H
-#define _TIPC_NAME_DISTR_H
-
-#include "name_table.h"
-
-void tipc_named_publish(struct publication *publ);
-void tipc_named_withdraw(struct publication *publ);
-void tipc_named_node_up(unsigned long node);
-void tipc_named_recv(struct sk_buff *buf);
-void tipc_named_reinit(void);
-
-#endif
diff -ruN linux-2.6.29/net/tipc/name_table.c android_cluster/linux-2.6.29/net/tipc/name_table.c
--- linux-2.6.29/net/tipc/name_table.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/name_table.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,1104 +0,0 @@
-/*
- * net/tipc/name_table.c: TIPC name table code
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2004-2008, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "config.h"
-#include "dbg.h"
-#include "name_table.h"
-#include "name_distr.h"
-#include "addr.h"
-#include "node_subscr.h"
-#include "subscr.h"
-#include "port.h"
-#include "cluster.h"
-#include "bcast.h"
-
-static int tipc_nametbl_size = 1024;		/* must be a power of 2 */
-
-/**
- * struct sub_seq - container for all published instances of a name sequence
- * @lower: name sequence lower bound
- * @upper: name sequence upper bound
- * @node_list: circular list of publications made by own node
- * @cluster_list: circular list of publications made by own cluster
- * @zone_list: circular list of publications made by own zone
- * @node_list_size: number of entries in "node_list"
- * @cluster_list_size: number of entries in "cluster_list"
- * @zone_list_size: number of entries in "zone_list"
- *
- * Note: The zone list always contains at least one entry, since all
- *       publications of the associated name sequence belong to it.
- *       (The cluster and node lists may be empty.)
- */
-
-struct sub_seq {
-	u32 lower;
-	u32 upper;
-	struct publication *node_list;
-	struct publication *cluster_list;
-	struct publication *zone_list;
-	u32 node_list_size;
-	u32 cluster_list_size;
-	u32 zone_list_size;
-};
-
-/**
- * struct name_seq - container for all published instances of a name type
- * @type: 32 bit 'type' value for name sequence
- * @sseq: pointer to dynamically-sized array of sub-sequences of this 'type';
- *        sub-sequences are sorted in ascending order
- * @alloc: number of sub-sequences currently in array
- * @first_free: array index of first unused sub-sequence entry
- * @ns_list: links to adjacent name sequences in hash chain
- * @subscriptions: list of subscriptions for this 'type'
- * @lock: spinlock controlling access to publication lists of all sub-sequences
- */
-
-struct name_seq {
-	u32 type;
-	struct sub_seq *sseqs;
-	u32 alloc;
-	u32 first_free;
-	struct hlist_node ns_list;
-	struct list_head subscriptions;
-	spinlock_t lock;
-};
-
-/**
- * struct name_table - table containing all existing port name publications
- * @types: pointer to fixed-sized array of name sequence lists,
- *         accessed via hashing on 'type'; name sequence lists are *not* sorted
- * @local_publ_count: number of publications issued by this node
- */
-
-struct name_table {
-	struct hlist_head *types;
-	u32 local_publ_count;
-};
-
-static struct name_table table = { NULL } ;
-static atomic_t rsv_publ_ok = ATOMIC_INIT(0);
-DEFINE_RWLOCK(tipc_nametbl_lock);
-
-
-static int hash(int x)
-{
-	return(x & (tipc_nametbl_size - 1));
-}
-
-/**
- * publ_create - create a publication structure
- */
-
-static struct publication *publ_create(u32 type, u32 lower, u32 upper,
-				       u32 scope, u32 node, u32 port_ref,
-				       u32 key)
-{
-	struct publication *publ = kzalloc(sizeof(*publ), GFP_ATOMIC);
-	if (publ == NULL) {
-		warn("Publication creation failure, no memory\n");
-		return NULL;
-	}
-
-	publ->type = type;
-	publ->lower = lower;
-	publ->upper = upper;
-	publ->scope = scope;
-	publ->node = node;
-	publ->ref = port_ref;
-	publ->key = key;
-	INIT_LIST_HEAD(&publ->local_list);
-	INIT_LIST_HEAD(&publ->pport_list);
-	INIT_LIST_HEAD(&publ->subscr.nodesub_list);
-	return publ;
-}
-
-/**
- * tipc_subseq_alloc - allocate a specified number of sub-sequence structures
- */
-
-static struct sub_seq *tipc_subseq_alloc(u32 cnt)
-{
-	struct sub_seq *sseq = kcalloc(cnt, sizeof(struct sub_seq), GFP_ATOMIC);
-	return sseq;
-}
-
-/**
- * tipc_nameseq_create - create a name sequence structure for the specified 'type'
- *
- * Allocates a single sub-sequence structure and sets it to all 0's.
- */
-
-static struct name_seq *tipc_nameseq_create(u32 type, struct hlist_head *seq_head)
-{
-	struct name_seq *nseq = kzalloc(sizeof(*nseq), GFP_ATOMIC);
-	struct sub_seq *sseq = tipc_subseq_alloc(1);
-
-	if (!nseq || !sseq) {
-		warn("Name sequence creation failed, no memory\n");
-		kfree(nseq);
-		kfree(sseq);
-		return NULL;
-	}
-
-	spin_lock_init(&nseq->lock);
-	nseq->type = type;
-	nseq->sseqs = sseq;
-	dbg("tipc_nameseq_create(): nseq = %p, type %u, ssseqs %p, ff: %u\n",
-	    nseq, type, nseq->sseqs, nseq->first_free);
-	nseq->alloc = 1;
-	INIT_HLIST_NODE(&nseq->ns_list);
-	INIT_LIST_HEAD(&nseq->subscriptions);
-	hlist_add_head(&nseq->ns_list, seq_head);
-	return nseq;
-}
-
-/**
- * nameseq_find_subseq - find sub-sequence (if any) matching a name instance
- *
- * Very time-critical, so binary searches through sub-sequence array.
- */
-
-static struct sub_seq *nameseq_find_subseq(struct name_seq *nseq,
-					   u32 instance)
-{
-	struct sub_seq *sseqs = nseq->sseqs;
-	int low = 0;
-	int high = nseq->first_free - 1;
-	int mid;
-
-	while (low <= high) {
-		mid = (low + high) / 2;
-		if (instance < sseqs[mid].lower)
-			high = mid - 1;
-		else if (instance > sseqs[mid].upper)
-			low = mid + 1;
-		else
-			return &sseqs[mid];
-	}
-	return NULL;
-}
-
-/**
- * nameseq_locate_subseq - determine position of name instance in sub-sequence
- *
- * Returns index in sub-sequence array of the entry that contains the specified
- * instance value; if no entry contains that value, returns the position
- * where a new entry for it would be inserted in the array.
- *
- * Note: Similar to binary search code for locating a sub-sequence.
- */
-
-static u32 nameseq_locate_subseq(struct name_seq *nseq, u32 instance)
-{
-	struct sub_seq *sseqs = nseq->sseqs;
-	int low = 0;
-	int high = nseq->first_free - 1;
-	int mid;
-
-	while (low <= high) {
-		mid = (low + high) / 2;
-		if (instance < sseqs[mid].lower)
-			high = mid - 1;
-		else if (instance > sseqs[mid].upper)
-			low = mid + 1;
-		else
-			return mid;
-	}
-	return low;
-}
-
-/**
- * tipc_nameseq_insert_publ -
- */
-
-static struct publication *tipc_nameseq_insert_publ(struct name_seq *nseq,
-						    u32 type, u32 lower, u32 upper,
-						    u32 scope, u32 node, u32 port, u32 key)
-{
-	struct subscription *s;
-	struct subscription *st;
-	struct publication *publ;
-	struct sub_seq *sseq;
-	int created_subseq = 0;
-
-	sseq = nameseq_find_subseq(nseq, lower);
-	dbg("nameseq_ins: for seq %p, {%u,%u}, found sseq %p\n",
-	    nseq, type, lower, sseq);
-	if (sseq) {
-
-		/* Lower end overlaps existing entry => need an exact match */
-
-		if ((sseq->lower != lower) || (sseq->upper != upper)) {
-			warn("Cannot publish {%u,%u,%u}, overlap error\n",
-			     type, lower, upper);
-			return NULL;
-		}
-	} else {
-		u32 inspos;
-		struct sub_seq *freesseq;
-
-		/* Find where lower end should be inserted */
-
-		inspos = nameseq_locate_subseq(nseq, lower);
-
-		/* Fail if upper end overlaps into an existing entry */
-
-		if ((inspos < nseq->first_free) &&
-		    (upper >= nseq->sseqs[inspos].lower)) {
-			warn("Cannot publish {%u,%u,%u}, overlap error\n",
-			     type, lower, upper);
-			return NULL;
-		}
-
-		/* Ensure there is space for new sub-sequence */
-
-		if (nseq->first_free == nseq->alloc) {
-			struct sub_seq *sseqs = tipc_subseq_alloc(nseq->alloc * 2);
-
-			if (!sseqs) {
-				warn("Cannot publish {%u,%u,%u}, no memory\n",
-				     type, lower, upper);
-				return NULL;
-			}
-			dbg("Allocated %u more sseqs\n", nseq->alloc);
-			memcpy(sseqs, nseq->sseqs,
-			       nseq->alloc * sizeof(struct sub_seq));
-			kfree(nseq->sseqs);
-			nseq->sseqs = sseqs;
-			nseq->alloc *= 2;
-		}
-		dbg("Have %u sseqs for type %u\n", nseq->alloc, type);
-
-		/* Insert new sub-sequence */
-
-		dbg("ins in pos %u, ff = %u\n", inspos, nseq->first_free);
-		sseq = &nseq->sseqs[inspos];
-		freesseq = &nseq->sseqs[nseq->first_free];
-		memmove(sseq + 1, sseq, (freesseq - sseq) * sizeof (*sseq));
-		memset(sseq, 0, sizeof (*sseq));
-		nseq->first_free++;
-		sseq->lower = lower;
-		sseq->upper = upper;
-		created_subseq = 1;
-	}
-	dbg("inserting {%u,%u,%u} from <0x%x:%u> into sseq %p(%u,%u) of seq %p\n",
-	    type, lower, upper, node, port, sseq,
-	    sseq->lower, sseq->upper, nseq);
-
-	/* Insert a publication: */
-
-	publ = publ_create(type, lower, upper, scope, node, port, key);
-	if (!publ)
-		return NULL;
-	dbg("inserting publ %p, node=0x%x publ->node=0x%x, subscr->node=%p\n",
-	    publ, node, publ->node, publ->subscr.node);
-
-	sseq->zone_list_size++;
-	if (!sseq->zone_list)
-		sseq->zone_list = publ->zone_list_next = publ;
-	else {
-		publ->zone_list_next = sseq->zone_list->zone_list_next;
-		sseq->zone_list->zone_list_next = publ;
-	}
-
-	if (in_own_cluster(node)) {
-		sseq->cluster_list_size++;
-		if (!sseq->cluster_list)
-			sseq->cluster_list = publ->cluster_list_next = publ;
-		else {
-			publ->cluster_list_next =
-			sseq->cluster_list->cluster_list_next;
-			sseq->cluster_list->cluster_list_next = publ;
-		}
-	}
-
-	if (node == tipc_own_addr) {
-		sseq->node_list_size++;
-		if (!sseq->node_list)
-			sseq->node_list = publ->node_list_next = publ;
-		else {
-			publ->node_list_next = sseq->node_list->node_list_next;
-			sseq->node_list->node_list_next = publ;
-		}
-	}
-
-	/*
-	 * Any subscriptions waiting for notification?
-	 */
-	list_for_each_entry_safe(s, st, &nseq->subscriptions, nameseq_list) {
-		dbg("calling report_overlap()\n");
-		tipc_subscr_report_overlap(s,
-					   publ->lower,
-					   publ->upper,
-					   TIPC_PUBLISHED,
-					   publ->ref,
-					   publ->node,
-					   created_subseq);
-	}
-	return publ;
-}
-
-/**
- * tipc_nameseq_remove_publ -
- *
- * NOTE: There may be cases where TIPC is asked to remove a publication
- * that is not in the name table.  For example, if another node issues a
- * publication for a name sequence that overlaps an existing name sequence
- * the publication will not be recorded, which means the publication won't
- * be found when the name sequence is later withdrawn by that node.
- * A failed withdraw request simply returns a failure indication and lets the
- * caller issue any error or warning messages associated with such a problem.
- */
-
-static struct publication *tipc_nameseq_remove_publ(struct name_seq *nseq, u32 inst,
-						    u32 node, u32 ref, u32 key)
-{
-	struct publication *publ;
-	struct publication *curr;
-	struct publication *prev;
-	struct sub_seq *sseq = nameseq_find_subseq(nseq, inst);
-	struct sub_seq *free;
-	struct subscription *s, *st;
-	int removed_subseq = 0;
-
-	if (!sseq)
-		return NULL;
-
-	dbg("tipc_nameseq_remove_publ: seq: %p, sseq %p, {%u,%u}, key %u\n",
-	    nseq, sseq, nseq->type, inst, key);
-
-	/* Remove publication from zone scope list */
-
-	prev = sseq->zone_list;
-	publ = sseq->zone_list->zone_list_next;
-	while ((publ->key != key) || (publ->ref != ref) ||
-	       (publ->node && (publ->node != node))) {
-		prev = publ;
-		publ = publ->zone_list_next;
-		if (prev == sseq->zone_list) {
-
-			/* Prevent endless loop if publication not found */
-
-			return NULL;
-		}
-	}
-	if (publ != sseq->zone_list)
-		prev->zone_list_next = publ->zone_list_next;
-	else if (publ->zone_list_next != publ) {
-		prev->zone_list_next = publ->zone_list_next;
-		sseq->zone_list = publ->zone_list_next;
-	} else {
-		sseq->zone_list = NULL;
-	}
-	sseq->zone_list_size--;
-
-	/* Remove publication from cluster scope list, if present */
-
-	if (in_own_cluster(node)) {
-		prev = sseq->cluster_list;
-		curr = sseq->cluster_list->cluster_list_next;
-		while (curr != publ) {
-			prev = curr;
-			curr = curr->cluster_list_next;
-			if (prev == sseq->cluster_list) {
-
-				/* Prevent endless loop for malformed list */
-
-				err("Unable to de-list cluster publication\n"
-				    "{%u%u}, node=0x%x, ref=%u, key=%u)\n",
-				    publ->type, publ->lower, publ->node,
-				    publ->ref, publ->key);
-				goto end_cluster;
-			}
-		}
-		if (publ != sseq->cluster_list)
-			prev->cluster_list_next = publ->cluster_list_next;
-		else if (publ->cluster_list_next != publ) {
-			prev->cluster_list_next = publ->cluster_list_next;
-			sseq->cluster_list = publ->cluster_list_next;
-		} else {
-			sseq->cluster_list = NULL;
-		}
-		sseq->cluster_list_size--;
-	}
-end_cluster:
-
-	/* Remove publication from node scope list, if present */
-
-	if (node == tipc_own_addr) {
-		prev = sseq->node_list;
-		curr = sseq->node_list->node_list_next;
-		while (curr != publ) {
-			prev = curr;
-			curr = curr->node_list_next;
-			if (prev == sseq->node_list) {
-
-				/* Prevent endless loop for malformed list */
-
-				err("Unable to de-list node publication\n"
-				    "{%u%u}, node=0x%x, ref=%u, key=%u)\n",
-				    publ->type, publ->lower, publ->node,
-				    publ->ref, publ->key);
-				goto end_node;
-			}
-		}
-		if (publ != sseq->node_list)
-			prev->node_list_next = publ->node_list_next;
-		else if (publ->node_list_next != publ) {
-			prev->node_list_next = publ->node_list_next;
-			sseq->node_list = publ->node_list_next;
-		} else {
-			sseq->node_list = NULL;
-		}
-		sseq->node_list_size--;
-	}
-end_node:
-
-	/* Contract subseq list if no more publications for that subseq */
-
-	if (!sseq->zone_list) {
-		free = &nseq->sseqs[nseq->first_free--];
-		memmove(sseq, sseq + 1, (free - (sseq + 1)) * sizeof (*sseq));
-		removed_subseq = 1;
-	}
-
-	/* Notify any waiting subscriptions */
-
-	list_for_each_entry_safe(s, st, &nseq->subscriptions, nameseq_list) {
-		tipc_subscr_report_overlap(s,
-					   publ->lower,
-					   publ->upper,
-					   TIPC_WITHDRAWN,
-					   publ->ref,
-					   publ->node,
-					   removed_subseq);
-	}
-
-	return publ;
-}
-
-/**
- * tipc_nameseq_subscribe: attach a subscription, and issue
- * the prescribed number of events if there is any sub-
- * sequence overlapping with the requested sequence
- */
-
-static void tipc_nameseq_subscribe(struct name_seq *nseq, struct subscription *s)
-{
-	struct sub_seq *sseq = nseq->sseqs;
-
-	list_add(&s->nameseq_list, &nseq->subscriptions);
-
-	if (!sseq)
-		return;
-
-	while (sseq != &nseq->sseqs[nseq->first_free]) {
-		struct publication *zl = sseq->zone_list;
-		if (zl && tipc_subscr_overlap(s,sseq->lower,sseq->upper)) {
-			struct publication *crs = zl;
-			int must_report = 1;
-
-			do {
-				tipc_subscr_report_overlap(s,
-							   sseq->lower,
-							   sseq->upper,
-							   TIPC_PUBLISHED,
-							   crs->ref,
-							   crs->node,
-							   must_report);
-				must_report = 0;
-				crs = crs->zone_list_next;
-			} while (crs != zl);
-		}
-		sseq++;
-	}
-}
-
-static struct name_seq *nametbl_find_seq(u32 type)
-{
-	struct hlist_head *seq_head;
-	struct hlist_node *seq_node;
-	struct name_seq *ns;
-
-	dbg("find_seq %u,(%u,0x%x) table = %p, hash[type] = %u\n",
-	    type, htonl(type), type, table.types, hash(type));
-
-	seq_head = &table.types[hash(type)];
-	hlist_for_each_entry(ns, seq_node, seq_head, ns_list) {
-		if (ns->type == type) {
-			dbg("found %p\n", ns);
-			return ns;
-		}
-	}
-
-	return NULL;
-};
-
-struct publication *tipc_nametbl_insert_publ(u32 type, u32 lower, u32 upper,
-					     u32 scope, u32 node, u32 port, u32 key)
-{
-	struct name_seq *seq = nametbl_find_seq(type);
-
-	dbg("tipc_nametbl_insert_publ: {%u,%u,%u} found %p\n", type, lower, upper, seq);
-	if (lower > upper) {
-		warn("Failed to publish illegal {%u,%u,%u}\n",
-		     type, lower, upper);
-		return NULL;
-	}
-
-	dbg("Publishing {%u,%u,%u} from 0x%x\n", type, lower, upper, node);
-	if (!seq) {
-		seq = tipc_nameseq_create(type, &table.types[hash(type)]);
-		dbg("tipc_nametbl_insert_publ: created %p\n", seq);
-	}
-	if (!seq)
-		return NULL;
-
-	return tipc_nameseq_insert_publ(seq, type, lower, upper,
-					scope, node, port, key);
-}
-
-struct publication *tipc_nametbl_remove_publ(u32 type, u32 lower,
-					     u32 node, u32 ref, u32 key)
-{
-	struct publication *publ;
-	struct name_seq *seq = nametbl_find_seq(type);
-
-	if (!seq)
-		return NULL;
-
-	dbg("Withdrawing {%u,%u} from 0x%x\n", type, lower, node);
-	publ = tipc_nameseq_remove_publ(seq, lower, node, ref, key);
-
-	if (!seq->first_free && list_empty(&seq->subscriptions)) {
-		hlist_del_init(&seq->ns_list);
-		kfree(seq->sseqs);
-		kfree(seq);
-	}
-	return publ;
-}
-
-/*
- * tipc_nametbl_translate(): Translate tipc_name -> tipc_portid.
- *                      Very time-critical.
- *
- * Note: on entry 'destnode' is the search domain used during translation;
- *       on exit it passes back the node address of the matching port (if any)
- */
-
-u32 tipc_nametbl_translate(u32 type, u32 instance, u32 *destnode)
-{
-	struct sub_seq *sseq;
-	struct publication *publ = NULL;
-	struct name_seq *seq;
-	u32 ref;
-
-	if (!in_scope(*destnode, tipc_own_addr))
-		return 0;
-
-	read_lock_bh(&tipc_nametbl_lock);
-	seq = nametbl_find_seq(type);
-	if (unlikely(!seq))
-		goto not_found;
-	sseq = nameseq_find_subseq(seq, instance);
-	if (unlikely(!sseq))
-		goto not_found;
-	spin_lock_bh(&seq->lock);
-
-	/* Closest-First Algorithm: */
-	if (likely(!*destnode)) {
-		publ = sseq->node_list;
-		if (publ) {
-			sseq->node_list = publ->node_list_next;
-found:
-			ref = publ->ref;
-			*destnode = publ->node;
-			spin_unlock_bh(&seq->lock);
-			read_unlock_bh(&tipc_nametbl_lock);
-			return ref;
-		}
-		publ = sseq->cluster_list;
-		if (publ) {
-			sseq->cluster_list = publ->cluster_list_next;
-			goto found;
-		}
-		publ = sseq->zone_list;
-		if (publ) {
-			sseq->zone_list = publ->zone_list_next;
-			goto found;
-		}
-	}
-
-	/* Round-Robin Algorithm: */
-	else if (*destnode == tipc_own_addr) {
-		publ = sseq->node_list;
-		if (publ) {
-			sseq->node_list = publ->node_list_next;
-			goto found;
-		}
-	} else if (in_own_cluster(*destnode)) {
-		publ = sseq->cluster_list;
-		if (publ) {
-			sseq->cluster_list = publ->cluster_list_next;
-			goto found;
-		}
-	} else {
-		publ = sseq->zone_list;
-		if (publ) {
-			sseq->zone_list = publ->zone_list_next;
-			goto found;
-		}
-	}
-	spin_unlock_bh(&seq->lock);
-not_found:
-	*destnode = 0;
-	read_unlock_bh(&tipc_nametbl_lock);
-	return 0;
-}
-
-/**
- * tipc_nametbl_mc_translate - find multicast destinations
- *
- * Creates list of all local ports that overlap the given multicast address;
- * also determines if any off-node ports overlap.
- *
- * Note: Publications with a scope narrower than 'limit' are ignored.
- * (i.e. local node-scope publications mustn't receive messages arriving
- * from another node, even if the multcast link brought it here)
- *
- * Returns non-zero if any off-node ports overlap
- */
-
-int tipc_nametbl_mc_translate(u32 type, u32 lower, u32 upper, u32 limit,
-			      struct port_list *dports)
-{
-	struct name_seq *seq;
-	struct sub_seq *sseq;
-	struct sub_seq *sseq_stop;
-	int res = 0;
-
-	read_lock_bh(&tipc_nametbl_lock);
-	seq = nametbl_find_seq(type);
-	if (!seq)
-		goto exit;
-
-	spin_lock_bh(&seq->lock);
-
-	sseq = seq->sseqs + nameseq_locate_subseq(seq, lower);
-	sseq_stop = seq->sseqs + seq->first_free;
-	for (; sseq != sseq_stop; sseq++) {
-		struct publication *publ;
-
-		if (sseq->lower > upper)
-			break;
-
-		publ = sseq->node_list;
-		if (publ) {
-			do {
-				if (publ->scope <= limit)
-					tipc_port_list_add(dports, publ->ref);
-				publ = publ->node_list_next;
-			} while (publ != sseq->node_list);
-		}
-
-		if (sseq->cluster_list_size != sseq->node_list_size)
-			res = 1;
-	}
-
-	spin_unlock_bh(&seq->lock);
-exit:
-	read_unlock_bh(&tipc_nametbl_lock);
-	return res;
-}
-
-/**
- * tipc_nametbl_publish_rsv - publish port name using a reserved name type
- */
-
-int tipc_nametbl_publish_rsv(u32 ref, unsigned int scope,
-			struct tipc_name_seq const *seq)
-{
-	int res;
-
-	atomic_inc(&rsv_publ_ok);
-	res = tipc_publish(ref, scope, seq);
-	atomic_dec(&rsv_publ_ok);
-	return res;
-}
-
-/**
- * tipc_nametbl_publish - add name publication to network name tables
- */
-
-struct publication *tipc_nametbl_publish(u32 type, u32 lower, u32 upper,
-				    u32 scope, u32 port_ref, u32 key)
-{
-	struct publication *publ;
-
-	if (table.local_publ_count >= tipc_max_publications) {
-		warn("Publication failed, local publication limit reached (%u)\n",
-		     tipc_max_publications);
-		return NULL;
-	}
-	if ((type < TIPC_RESERVED_TYPES) && !atomic_read(&rsv_publ_ok)) {
-		warn("Publication failed, reserved name {%u,%u,%u}\n",
-		     type, lower, upper);
-		return NULL;
-	}
-
-	write_lock_bh(&tipc_nametbl_lock);
-	table.local_publ_count++;
-	publ = tipc_nametbl_insert_publ(type, lower, upper, scope,
-				   tipc_own_addr, port_ref, key);
-	if (publ && (scope != TIPC_NODE_SCOPE)) {
-		tipc_named_publish(publ);
-	}
-	write_unlock_bh(&tipc_nametbl_lock);
-	return publ;
-}
-
-/**
- * tipc_nametbl_withdraw - withdraw name publication from network name tables
- */
-
-int tipc_nametbl_withdraw(u32 type, u32 lower, u32 ref, u32 key)
-{
-	struct publication *publ;
-
-	dbg("tipc_nametbl_withdraw: {%u,%u}, key=%u\n", type, lower, key);
-	write_lock_bh(&tipc_nametbl_lock);
-	publ = tipc_nametbl_remove_publ(type, lower, tipc_own_addr, ref, key);
-	if (likely(publ)) {
-		table.local_publ_count--;
-		if (publ->scope != TIPC_NODE_SCOPE)
-			tipc_named_withdraw(publ);
-		write_unlock_bh(&tipc_nametbl_lock);
-		list_del_init(&publ->pport_list);
-		kfree(publ);
-		return 1;
-	}
-	write_unlock_bh(&tipc_nametbl_lock);
-	err("Unable to remove local publication\n"
-	    "(type=%u, lower=%u, ref=%u, key=%u)\n",
-	    type, lower, ref, key);
-	return 0;
-}
-
-/**
- * tipc_nametbl_subscribe - add a subscription object to the name table
- */
-
-void tipc_nametbl_subscribe(struct subscription *s)
-{
-	u32 type = s->seq.type;
-	struct name_seq *seq;
-
-	write_lock_bh(&tipc_nametbl_lock);
-	seq = nametbl_find_seq(type);
-	if (!seq) {
-		seq = tipc_nameseq_create(type, &table.types[hash(type)]);
-	}
-	if (seq){
-		spin_lock_bh(&seq->lock);
-		dbg("tipc_nametbl_subscribe:found %p for {%u,%u,%u}\n",
-		    seq, type, s->seq.lower, s->seq.upper);
-		tipc_nameseq_subscribe(seq, s);
-		spin_unlock_bh(&seq->lock);
-	} else {
-		warn("Failed to create subscription for {%u,%u,%u}\n",
-		     s->seq.type, s->seq.lower, s->seq.upper);
-	}
-	write_unlock_bh(&tipc_nametbl_lock);
-}
-
-/**
- * tipc_nametbl_unsubscribe - remove a subscription object from name table
- */
-
-void tipc_nametbl_unsubscribe(struct subscription *s)
-{
-	struct name_seq *seq;
-
-	write_lock_bh(&tipc_nametbl_lock);
-	seq = nametbl_find_seq(s->seq.type);
-	if (seq != NULL){
-		spin_lock_bh(&seq->lock);
-		list_del_init(&s->nameseq_list);
-		spin_unlock_bh(&seq->lock);
-		if ((seq->first_free == 0) && list_empty(&seq->subscriptions)) {
-			hlist_del_init(&seq->ns_list);
-			kfree(seq->sseqs);
-			kfree(seq);
-		}
-	}
-	write_unlock_bh(&tipc_nametbl_lock);
-}
-
-
-/**
- * subseq_list: print specified sub-sequence contents into the given buffer
- */
-
-static void subseq_list(struct sub_seq *sseq, struct print_buf *buf, u32 depth,
-			u32 index)
-{
-	char portIdStr[27];
-	char *scopeStr;
-	struct publication *publ = sseq->zone_list;
-
-	tipc_printf(buf, "%-10u %-10u ", sseq->lower, sseq->upper);
-
-	if (depth == 2 || !publ) {
-		tipc_printf(buf, "\n");
-		return;
-	}
-
-	do {
-		sprintf (portIdStr, "<%u.%u.%u:%u>",
-			 tipc_zone(publ->node), tipc_cluster(publ->node),
-			 tipc_node(publ->node), publ->ref);
-		tipc_printf(buf, "%-26s ", portIdStr);
-		if (depth > 3) {
-			if (publ->node != tipc_own_addr)
-				scopeStr = "";
-			else if (publ->scope == TIPC_NODE_SCOPE)
-				scopeStr = "node";
-			else if (publ->scope == TIPC_CLUSTER_SCOPE)
-				scopeStr = "cluster";
-			else
-				scopeStr = "zone";
-			tipc_printf(buf, "%-10u %s", publ->key, scopeStr);
-		}
-
-		publ = publ->zone_list_next;
-		if (publ == sseq->zone_list)
-			break;
-
-		tipc_printf(buf, "\n%33s", " ");
-	} while (1);
-
-	tipc_printf(buf, "\n");
-}
-
-/**
- * nameseq_list: print specified name sequence contents into the given buffer
- */
-
-static void nameseq_list(struct name_seq *seq, struct print_buf *buf, u32 depth,
-			 u32 type, u32 lowbound, u32 upbound, u32 index)
-{
-	struct sub_seq *sseq;
-	char typearea[11];
-
-	if (seq->first_free == 0)
-		return;
-
-	sprintf(typearea, "%-10u", seq->type);
-
-	if (depth == 1) {
-		tipc_printf(buf, "%s\n", typearea);
-		return;
-	}
-
-	for (sseq = seq->sseqs; sseq != &seq->sseqs[seq->first_free]; sseq++) {
-		if ((lowbound <= sseq->upper) && (upbound >= sseq->lower)) {
-			tipc_printf(buf, "%s ", typearea);
-			spin_lock_bh(&seq->lock);
-			subseq_list(sseq, buf, depth, index);
-			spin_unlock_bh(&seq->lock);
-			sprintf(typearea, "%10s", " ");
-		}
-	}
-}
-
-/**
- * nametbl_header - print name table header into the given buffer
- */
-
-static void nametbl_header(struct print_buf *buf, u32 depth)
-{
-	tipc_printf(buf, "Type       ");
-
-	if (depth > 1)
-		tipc_printf(buf, "Lower      Upper      ");
-	if (depth > 2)
-		tipc_printf(buf, "Port Identity              ");
-	if (depth > 3)
-		tipc_printf(buf, "Publication");
-
-	tipc_printf(buf, "\n-----------");
-
-	if (depth > 1)
-		tipc_printf(buf, "--------------------- ");
-	if (depth > 2)
-		tipc_printf(buf, "-------------------------- ");
-	if (depth > 3)
-		tipc_printf(buf, "------------------");
-
-	tipc_printf(buf, "\n");
-}
-
-/**
- * nametbl_list - print specified name table contents into the given buffer
- */
-
-static void nametbl_list(struct print_buf *buf, u32 depth_info,
-			 u32 type, u32 lowbound, u32 upbound)
-{
-	struct hlist_head *seq_head;
-	struct hlist_node *seq_node;
-	struct name_seq *seq;
-	int all_types;
-	u32 depth;
-	u32 i;
-
-	all_types = (depth_info & TIPC_NTQ_ALLTYPES);
-	depth = (depth_info & ~TIPC_NTQ_ALLTYPES);
-
-	if (depth == 0)
-		return;
-
-	if (all_types) {
-		/* display all entries in name table to specified depth */
-		nametbl_header(buf, depth);
-		lowbound = 0;
-		upbound = ~0;
-		for (i = 0; i < tipc_nametbl_size; i++) {
-			seq_head = &table.types[i];
-			hlist_for_each_entry(seq, seq_node, seq_head, ns_list) {
-				nameseq_list(seq, buf, depth, seq->type,
-					     lowbound, upbound, i);
-			}
-		}
-	} else {
-		/* display only the sequence that matches the specified type */
-		if (upbound < lowbound) {
-			tipc_printf(buf, "invalid name sequence specified\n");
-			return;
-		}
-		nametbl_header(buf, depth);
-		i = hash(type);
-		seq_head = &table.types[i];
-		hlist_for_each_entry(seq, seq_node, seq_head, ns_list) {
-			if (seq->type == type) {
-				nameseq_list(seq, buf, depth, type,
-					     lowbound, upbound, i);
-				break;
-			}
-		}
-	}
-}
-
-#if 0
-void tipc_nametbl_print(struct print_buf *buf, const char *str)
-{
-	tipc_printf(buf, str);
-	read_lock_bh(&tipc_nametbl_lock);
-	nametbl_list(buf, 0, 0, 0, 0);
-	read_unlock_bh(&tipc_nametbl_lock);
-}
-#endif
-
-#define MAX_NAME_TBL_QUERY 32768
-
-struct sk_buff *tipc_nametbl_get(const void *req_tlv_area, int req_tlv_space)
-{
-	struct sk_buff *buf;
-	struct tipc_name_table_query *argv;
-	struct tlv_desc *rep_tlv;
-	struct print_buf b;
-	int str_len;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_NAME_TBL_QUERY))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	buf = tipc_cfg_reply_alloc(TLV_SPACE(MAX_NAME_TBL_QUERY));
-	if (!buf)
-		return NULL;
-
-	rep_tlv = (struct tlv_desc *)buf->data;
-	tipc_printbuf_init(&b, TLV_DATA(rep_tlv), MAX_NAME_TBL_QUERY);
-	argv = (struct tipc_name_table_query *)TLV_DATA(req_tlv_area);
-	read_lock_bh(&tipc_nametbl_lock);
-	nametbl_list(&b, ntohl(argv->depth), ntohl(argv->type),
-		     ntohl(argv->lowbound), ntohl(argv->upbound));
-	read_unlock_bh(&tipc_nametbl_lock);
-	str_len = tipc_printbuf_validate(&b);
-
-	skb_put(buf, TLV_SPACE(str_len));
-	TLV_SET(rep_tlv, TIPC_TLV_ULTRA_STRING, NULL, str_len);
-
-	return buf;
-}
-
-#if 0
-void tipc_nametbl_dump(void)
-{
-	nametbl_list(TIPC_CONS, 0, 0, 0, 0);
-}
-#endif
-
-int tipc_nametbl_init(void)
-{
-	table.types = kcalloc(tipc_nametbl_size, sizeof(struct hlist_head),
-			      GFP_ATOMIC);
-	if (!table.types)
-		return -ENOMEM;
-
-	table.local_publ_count = 0;
-	return 0;
-}
-
-void tipc_nametbl_stop(void)
-{
-	u32 i;
-
-	if (!table.types)
-		return;
-
-	/* Verify name table is empty, then release it */
-
-	write_lock_bh(&tipc_nametbl_lock);
-	for (i = 0; i < tipc_nametbl_size; i++) {
-		if (!hlist_empty(&table.types[i]))
-			err("tipc_nametbl_stop(): hash chain %u is non-null\n", i);
-	}
-	kfree(table.types);
-	table.types = NULL;
-	write_unlock_bh(&tipc_nametbl_lock);
-}
-
diff -ruN linux-2.6.29/net/tipc/name_table.h android_cluster/linux-2.6.29/net/tipc/name_table.h
--- linux-2.6.29/net/tipc/name_table.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/name_table.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,108 +0,0 @@
-/*
- * net/tipc/name_table.h: Include file for TIPC name table code
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2004-2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_NAME_TABLE_H
-#define _TIPC_NAME_TABLE_H
-
-#include "node_subscr.h"
-
-struct subscription;
-struct port_list;
-
-/*
- * TIPC name types reserved for internal TIPC use (both current and planned)
- */
-
-#define TIPC_ZM_SRV 3  		/* zone master service name type */
-
-
-/**
- * struct publication - info about a published (name or) name sequence
- * @type: name sequence type
- * @lower: name sequence lower bound
- * @upper: name sequence upper bound
- * @scope: scope of publication
- * @node: network address of publishing port's node
- * @ref: publishing port
- * @key: publication key
- * @subscr: subscription to "node down" event (for off-node publications only)
- * @local_list: adjacent entries in list of publications made by this node
- * @pport_list: adjacent entries in list of publications made by this port
- * @node_list: next matching name seq publication with >= node scope
- * @cluster_list: next matching name seq publication with >= cluster scope
- * @zone_list: next matching name seq publication with >= zone scope
- *
- * Note that the node list, cluster list, and zone list are circular lists.
- */
-
-struct publication {
-	u32 type;
-	u32 lower;
-	u32 upper;
-	u32 scope;
-	u32 node;
-	u32 ref;
-	u32 key;
-	struct tipc_node_subscr subscr;
-	struct list_head local_list;
-	struct list_head pport_list;
-	struct publication *node_list_next;
-	struct publication *cluster_list_next;
-	struct publication *zone_list_next;
-};
-
-
-extern rwlock_t tipc_nametbl_lock;
-
-struct sk_buff *tipc_nametbl_get(const void *req_tlv_area, int req_tlv_space);
-u32 tipc_nametbl_translate(u32 type, u32 instance, u32 *node);
-int tipc_nametbl_mc_translate(u32 type, u32 lower, u32 upper, u32 limit,
-			 struct port_list *dports);
-int tipc_nametbl_publish_rsv(u32 ref, unsigned int scope,
-			struct tipc_name_seq const *seq);
-struct publication *tipc_nametbl_publish(u32 type, u32 lower, u32 upper,
-				    u32 scope, u32 port_ref, u32 key);
-int tipc_nametbl_withdraw(u32 type, u32 lower, u32 ref, u32 key);
-struct publication *tipc_nametbl_insert_publ(u32 type, u32 lower, u32 upper,
-					u32 scope, u32 node, u32 ref, u32 key);
-struct publication *tipc_nametbl_remove_publ(u32 type, u32 lower,
-					u32 node, u32 ref, u32 key);
-void tipc_nametbl_subscribe(struct subscription *s);
-void tipc_nametbl_unsubscribe(struct subscription *s);
-int tipc_nametbl_init(void);
-void tipc_nametbl_stop(void);
-
-#endif
diff -ruN linux-2.6.29/net/tipc/net.c android_cluster/linux-2.6.29/net/tipc/net.c
--- linux-2.6.29/net/tipc/net.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/net.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,313 +0,0 @@
-/*
- * net/tipc/net.c: TIPC network routing code
- *
- * Copyright (c) 1995-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "bearer.h"
-#include "net.h"
-#include "zone.h"
-#include "addr.h"
-#include "name_table.h"
-#include "name_distr.h"
-#include "subscr.h"
-#include "link.h"
-#include "msg.h"
-#include "port.h"
-#include "bcast.h"
-#include "discover.h"
-#include "config.h"
-
-/*
- * The TIPC locking policy is designed to ensure a very fine locking
- * granularity, permitting complete parallel access to individual
- * port and node/link instances. The code consists of three major
- * locking domains, each protected with their own disjunct set of locks.
- *
- * 1: The routing hierarchy.
- *    Comprises the structures 'zone', 'cluster', 'node', 'link'
- *    and 'bearer'. The whole hierarchy is protected by a big
- *    read/write lock, tipc_net_lock, to enssure that nothing is added
- *    or removed while code is accessing any of these structures.
- *    This layer must not be called from the two others while they
- *    hold any of their own locks.
- *    Neither must it itself do any upcalls to the other two before
- *    it has released tipc_net_lock and other protective locks.
- *
- *   Within the tipc_net_lock domain there are two sub-domains;'node' and
- *   'bearer', where local write operations are permitted,
- *   provided that those are protected by individual spin_locks
- *   per instance. Code holding tipc_net_lock(read) and a node spin_lock
- *   is permitted to poke around in both the node itself and its
- *   subordinate links. I.e, it can update link counters and queues,
- *   change link state, send protocol messages, and alter the
- *   "active_links" array in the node; but it can _not_ remove a link
- *   or a node from the overall structure.
- *   Correspondingly, individual bearers may change status within a
- *   tipc_net_lock(read), protected by an individual spin_lock ber bearer
- *   instance, but it needs tipc_net_lock(write) to remove/add any bearers.
- *
- *
- *  2: The transport level of the protocol.
- *     This consists of the structures port, (and its user level
- *     representations, such as user_port and tipc_sock), reference and
- *     tipc_user (port.c, reg.c, socket.c).
- *
- *     This layer has four different locks:
- *     - The tipc_port spin_lock. This is protecting each port instance
- *       from parallel data access and removal. Since we can not place
- *       this lock in the port itself, it has been placed in the
- *       corresponding reference table entry, which has the same life
- *       cycle as the module. This entry is difficult to access from
- *       outside the TIPC core, however, so a pointer to the lock has
- *       been added in the port instance, -to be used for unlocking
- *       only.
- *     - A read/write lock to protect the reference table itself (teg.c).
- *       (Nobody is using read-only access to this, so it can just as
- *       well be changed to a spin_lock)
- *     - A spin lock to protect the registry of kernel/driver users (reg.c)
- *     - A global spin_lock (tipc_port_lock), which only task is to ensure
- *       consistency where more than one port is involved in an operation,
- *       i.e., whe a port is part of a linked list of ports.
- *       There are two such lists; 'port_list', which is used for management,
- *       and 'wait_list', which is used to queue ports during congestion.
- *
- *  3: The name table (name_table.c, name_distr.c, subscription.c)
- *     - There is one big read/write-lock (tipc_nametbl_lock) protecting the
- *       overall name table structure. Nothing must be added/removed to
- *       this structure without holding write access to it.
- *     - There is one local spin_lock per sub_sequence, which can be seen
- *       as a sub-domain to the tipc_nametbl_lock domain. It is used only
- *       for translation operations, and is needed because a translation
- *       steps the root of the 'publication' linked list between each lookup.
- *       This is always used within the scope of a tipc_nametbl_lock(read).
- *     - A local spin_lock protecting the queue of subscriber events.
-*/
-
-DEFINE_RWLOCK(tipc_net_lock);
-struct network tipc_net = { NULL };
-
-struct tipc_node *tipc_net_select_remote_node(u32 addr, u32 ref)
-{
-	return tipc_zone_select_remote_node(tipc_net.zones[tipc_zone(addr)], addr, ref);
-}
-
-u32 tipc_net_select_router(u32 addr, u32 ref)
-{
-	return tipc_zone_select_router(tipc_net.zones[tipc_zone(addr)], addr, ref);
-}
-
-#if 0
-u32 tipc_net_next_node(u32 a)
-{
-	if (tipc_net.zones[tipc_zone(a)])
-		return tipc_zone_next_node(a);
-	return 0;
-}
-#endif
-
-void tipc_net_remove_as_router(u32 router)
-{
-	u32 z_num;
-
-	for (z_num = 1; z_num <= tipc_max_zones; z_num++) {
-		if (!tipc_net.zones[z_num])
-			continue;
-		tipc_zone_remove_as_router(tipc_net.zones[z_num], router);
-	}
-}
-
-void tipc_net_send_external_routes(u32 dest)
-{
-	u32 z_num;
-
-	for (z_num = 1; z_num <= tipc_max_zones; z_num++) {
-		if (tipc_net.zones[z_num])
-			tipc_zone_send_external_routes(tipc_net.zones[z_num], dest);
-	}
-}
-
-static int net_init(void)
-{
-	memset(&tipc_net, 0, sizeof(tipc_net));
-	tipc_net.zones = kcalloc(tipc_max_zones + 1, sizeof(struct _zone *), GFP_ATOMIC);
-	if (!tipc_net.zones) {
-		return -ENOMEM;
-	}
-	return 0;
-}
-
-static void net_stop(void)
-{
-	u32 z_num;
-
-	if (!tipc_net.zones)
-		return;
-
-	for (z_num = 1; z_num <= tipc_max_zones; z_num++) {
-		tipc_zone_delete(tipc_net.zones[z_num]);
-	}
-	kfree(tipc_net.zones);
-	tipc_net.zones = NULL;
-}
-
-static void net_route_named_msg(struct sk_buff *buf)
-{
-	struct tipc_msg *msg = buf_msg(buf);
-	u32 dnode;
-	u32 dport;
-
-	if (!msg_named(msg)) {
-		msg_dbg(msg, "tipc_net->drop_nam:");
-		buf_discard(buf);
-		return;
-	}
-
-	dnode = addr_domain(msg_lookup_scope(msg));
-	dport = tipc_nametbl_translate(msg_nametype(msg), msg_nameinst(msg), &dnode);
-	dbg("tipc_net->lookup<%u,%u>-><%u,%x>\n",
-	    msg_nametype(msg), msg_nameinst(msg), dport, dnode);
-	if (dport) {
-		msg_set_destnode(msg, dnode);
-		msg_set_destport(msg, dport);
-		tipc_net_route_msg(buf);
-		return;
-	}
-	msg_dbg(msg, "tipc_net->rej:NO NAME: ");
-	tipc_reject_msg(buf, TIPC_ERR_NO_NAME);
-}
-
-void tipc_net_route_msg(struct sk_buff *buf)
-{
-	struct tipc_msg *msg;
-	u32 dnode;
-
-	if (!buf)
-		return;
-	msg = buf_msg(buf);
-
-	msg_incr_reroute_cnt(msg);
-	if (msg_reroute_cnt(msg) > 6) {
-		if (msg_errcode(msg)) {
-			msg_dbg(msg, "NET>DISC>:");
-			buf_discard(buf);
-		} else {
-			msg_dbg(msg, "NET>REJ>:");
-			tipc_reject_msg(buf, msg_destport(msg) ?
-					TIPC_ERR_NO_PORT : TIPC_ERR_NO_NAME);
-		}
-		return;
-	}
-
-	msg_dbg(msg, "tipc_net->rout: ");
-
-	/* Handle message for this node */
-	dnode = msg_short(msg) ? tipc_own_addr : msg_destnode(msg);
-	if (in_scope(dnode, tipc_own_addr)) {
-		if (msg_isdata(msg)) {
-			if (msg_mcast(msg))
-				tipc_port_recv_mcast(buf, NULL);
-			else if (msg_destport(msg))
-				tipc_port_recv_msg(buf);
-			else
-				net_route_named_msg(buf);
-			return;
-		}
-		switch (msg_user(msg)) {
-		case ROUTE_DISTRIBUTOR:
-			tipc_cltr_recv_routing_table(buf);
-			break;
-		case NAME_DISTRIBUTOR:
-			tipc_named_recv(buf);
-			break;
-		case CONN_MANAGER:
-			tipc_port_recv_proto_msg(buf);
-			break;
-		default:
-			msg_dbg(msg,"DROP/NET/<REC<");
-			buf_discard(buf);
-		}
-		return;
-	}
-
-	/* Handle message for another node */
-	msg_dbg(msg, "NET>SEND>: ");
-	tipc_link_send(buf, dnode, msg_link_selector(msg));
-}
-
-int tipc_net_start(u32 addr)
-{
-	char addr_string[16];
-	int res;
-
-	if (tipc_mode != TIPC_NODE_MODE)
-		return -ENOPROTOOPT;
-
-	tipc_subscr_stop();
-	tipc_cfg_stop();
-
-	tipc_own_addr = addr;
-	tipc_mode = TIPC_NET_MODE;
-	tipc_named_reinit();
-	tipc_port_reinit();
-
-	if ((res = tipc_bearer_init()) ||
-	    (res = net_init()) ||
-	    (res = tipc_cltr_init()) ||
-	    (res = tipc_bclink_init())) {
-		return res;
-	}
-
-	tipc_k_signal((Handler)tipc_subscr_start, 0);
-	tipc_k_signal((Handler)tipc_cfg_init, 0);
-
-	info("Started in network mode\n");
-	info("Own node address %s, network identity %u\n",
-	     addr_string_fill(addr_string, tipc_own_addr), tipc_net_id);
-	return 0;
-}
-
-void tipc_net_stop(void)
-{
-	if (tipc_mode != TIPC_NET_MODE)
-		return;
-	write_lock_bh(&tipc_net_lock);
-	tipc_bearer_stop();
-	tipc_mode = TIPC_NODE_MODE;
-	tipc_bclink_stop();
-	net_stop();
-	write_unlock_bh(&tipc_net_lock);
-	info("Left network mode \n");
-}
-
diff -ruN linux-2.6.29/net/tipc/net.h android_cluster/linux-2.6.29/net/tipc/net.h
--- linux-2.6.29/net/tipc/net.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/net.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,64 +0,0 @@
-/*
- * net/tipc/net.h: Include file for TIPC network routing code
- *
- * Copyright (c) 1995-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_NET_H
-#define _TIPC_NET_H
-
-struct _zone;
-
-/**
- * struct network - TIPC network structure
- * @zones: array of pointers to all zones within network
- */
-
-struct network {
-	struct _zone **zones;
-};
-
-
-extern struct network tipc_net;
-extern rwlock_t tipc_net_lock;
-
-void tipc_net_remove_as_router(u32 router);
-void tipc_net_send_external_routes(u32 dest);
-void tipc_net_route_msg(struct sk_buff *buf);
-struct tipc_node *tipc_net_select_remote_node(u32 addr, u32 ref);
-u32 tipc_net_select_router(u32 addr, u32 ref);
-
-int tipc_net_start(u32 addr);
-void tipc_net_stop(void);
-
-#endif
diff -ruN linux-2.6.29/net/tipc/netlink.c android_cluster/linux-2.6.29/net/tipc/netlink.c
--- linux-2.6.29/net/tipc/netlink.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/netlink.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,114 +0,0 @@
-/*
- * net/tipc/netlink.c: TIPC configuration handling
- *
- * Copyright (c) 2005-2006, Ericsson AB
- * Copyright (c) 2005-2007, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "config.h"
-#include <net/genetlink.h>
-
-static int handle_cmd(struct sk_buff *skb, struct genl_info *info)
-{
-	struct sk_buff *rep_buf;
-	struct nlmsghdr *rep_nlh;
-	struct nlmsghdr *req_nlh = info->nlhdr;
-	struct tipc_genlmsghdr *req_userhdr = info->userhdr;
-	int hdr_space = NLMSG_SPACE(GENL_HDRLEN + TIPC_GENL_HDRLEN);
-	u16 cmd;
-
-	if ((req_userhdr->cmd & 0xC000) && (!capable(CAP_NET_ADMIN)))
-		cmd = TIPC_CMD_NOT_NET_ADMIN;
-	else
-		cmd = req_userhdr->cmd;
-
-	rep_buf = tipc_cfg_do_cmd(req_userhdr->dest, cmd,
-			NLMSG_DATA(req_nlh) + GENL_HDRLEN + TIPC_GENL_HDRLEN,
-			NLMSG_PAYLOAD(req_nlh, GENL_HDRLEN + TIPC_GENL_HDRLEN),
-			hdr_space);
-
-	if (rep_buf) {
-		skb_push(rep_buf, hdr_space);
-		rep_nlh = nlmsg_hdr(rep_buf);
-		memcpy(rep_nlh, req_nlh, hdr_space);
-		rep_nlh->nlmsg_len = rep_buf->len;
-		genlmsg_unicast(rep_buf, NETLINK_CB(skb).pid);
-	}
-
-	return 0;
-}
-
-static struct genl_family family = {
-	.id		= GENL_ID_GENERATE,
-	.name		= TIPC_GENL_NAME,
-	.version	= TIPC_GENL_VERSION,
-	.hdrsize	= TIPC_GENL_HDRLEN,
-	.maxattr	= 0,
-};
-
-static struct genl_ops ops = {
-	.cmd		= TIPC_GENL_CMD,
-	.doit		= handle_cmd,
-};
-
-static int family_registered = 0;
-
-int tipc_netlink_start(void)
-{
-
-
-	if (genl_register_family(&family))
-		goto err;
-
-	family_registered = 1;
-
-	if (genl_register_ops(&family, &ops))
-		goto err_unregister;
-
-	return 0;
-
- err_unregister:
-	genl_unregister_family(&family);
-	family_registered = 0;
- err:
-	err("Failed to register netlink interface\n");
-	return -EFAULT;
-}
-
-void tipc_netlink_stop(void)
-{
-	if (family_registered) {
-		genl_unregister_family(&family);
-		family_registered = 0;
-	}
-}
diff -ruN linux-2.6.29/net/tipc/node.c android_cluster/linux-2.6.29/net/tipc/node.c
--- linux-2.6.29/net/tipc/node.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/node.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,731 +0,0 @@
-/*
- * net/tipc/node.c: TIPC node management routines
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2005-2006, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "config.h"
-#include "node.h"
-#include "cluster.h"
-#include "net.h"
-#include "addr.h"
-#include "node_subscr.h"
-#include "link.h"
-#include "port.h"
-#include "bearer.h"
-#include "name_distr.h"
-
-void node_print(struct print_buf *buf, struct tipc_node *n_ptr, char *str);
-static void node_lost_contact(struct tipc_node *n_ptr);
-static void node_established_contact(struct tipc_node *n_ptr);
-
-struct tipc_node *tipc_nodes = NULL;	/* sorted list of nodes within cluster */
-
-static DEFINE_SPINLOCK(node_create_lock);
-
-u32 tipc_own_tag = 0;
-
-/**
- * tipc_node_create - create neighboring node
- *
- * Currently, this routine is called by neighbor discovery code, which holds
- * net_lock for reading only.  We must take node_create_lock to ensure a node
- * isn't created twice if two different bearers discover the node at the same
- * time.  (It would be preferable to switch to holding net_lock in write mode,
- * but this is a non-trivial change.)
- */
-
-struct tipc_node *tipc_node_create(u32 addr)
-{
-	struct cluster *c_ptr;
-	struct tipc_node *n_ptr;
-	struct tipc_node **curr_node;
-
-	spin_lock_bh(&node_create_lock);
-
-	for (n_ptr = tipc_nodes; n_ptr; n_ptr = n_ptr->next) {
-		if (addr < n_ptr->addr)
-			break;
-		if (addr == n_ptr->addr) {
-			spin_unlock_bh(&node_create_lock);
-			return n_ptr;
-		}
-	}
-
-	n_ptr = kzalloc(sizeof(*n_ptr),GFP_ATOMIC);
-	if (!n_ptr) {
-		spin_unlock_bh(&node_create_lock);
-		warn("Node creation failed, no memory\n");
-		return NULL;
-	}
-
-	c_ptr = tipc_cltr_find(addr);
-	if (!c_ptr) {
-		c_ptr = tipc_cltr_create(addr);
-	}
-	if (!c_ptr) {
-		spin_unlock_bh(&node_create_lock);
-		kfree(n_ptr);
-		return NULL;
-	}
-
-	n_ptr->addr = addr;
-		spin_lock_init(&n_ptr->lock);
-	INIT_LIST_HEAD(&n_ptr->nsub);
-	n_ptr->owner = c_ptr;
-	tipc_cltr_attach_node(c_ptr, n_ptr);
-	n_ptr->last_router = -1;
-
-	/* Insert node into ordered list */
-	for (curr_node = &tipc_nodes; *curr_node;
-	     curr_node = &(*curr_node)->next) {
-		if (addr < (*curr_node)->addr) {
-			n_ptr->next = *curr_node;
-			break;
-		}
-	}
-	(*curr_node) = n_ptr;
-	spin_unlock_bh(&node_create_lock);
-	return n_ptr;
-}
-
-void tipc_node_delete(struct tipc_node *n_ptr)
-{
-	if (!n_ptr)
-		return;
-
-#if 0
-	/* Not needed because links are already deleted via tipc_bearer_stop() */
-
-	u32 l_num;
-
-	for (l_num = 0; l_num < MAX_BEARERS; l_num++) {
-		link_delete(n_ptr->links[l_num]);
-	}
-#endif
-
-	dbg("node %x deleted\n", n_ptr->addr);
-	kfree(n_ptr);
-}
-
-
-/**
- * tipc_node_link_up - handle addition of link
- *
- * Link becomes active (alone or shared) or standby, depending on its priority.
- */
-
-void tipc_node_link_up(struct tipc_node *n_ptr, struct link *l_ptr)
-{
-	struct link **active = &n_ptr->active_links[0];
-
-	n_ptr->working_links++;
-
-	info("Established link <%s> on network plane %c\n",
-	     l_ptr->name, l_ptr->b_ptr->net_plane);
-
-	if (!active[0]) {
-		dbg(" link %x into %x/%x\n", l_ptr, &active[0], &active[1]);
-		active[0] = active[1] = l_ptr;
-		node_established_contact(n_ptr);
-		return;
-	}
-	if (l_ptr->priority < active[0]->priority) {
-		info("New link <%s> becomes standby\n", l_ptr->name);
-		return;
-	}
-	tipc_link_send_duplicate(active[0], l_ptr);
-	if (l_ptr->priority == active[0]->priority) {
-		active[0] = l_ptr;
-		return;
-	}
-	info("Old link <%s> becomes standby\n", active[0]->name);
-	if (active[1] != active[0])
-		info("Old link <%s> becomes standby\n", active[1]->name);
-	active[0] = active[1] = l_ptr;
-}
-
-/**
- * node_select_active_links - select active link
- */
-
-static void node_select_active_links(struct tipc_node *n_ptr)
-{
-	struct link **active = &n_ptr->active_links[0];
-	u32 i;
-	u32 highest_prio = 0;
-
-	active[0] = active[1] = NULL;
-
-	for (i = 0; i < MAX_BEARERS; i++) {
-		struct link *l_ptr = n_ptr->links[i];
-
-		if (!l_ptr || !tipc_link_is_up(l_ptr) ||
-		    (l_ptr->priority < highest_prio))
-			continue;
-
-		if (l_ptr->priority > highest_prio) {
-			highest_prio = l_ptr->priority;
-			active[0] = active[1] = l_ptr;
-		} else {
-			active[1] = l_ptr;
-		}
-	}
-}
-
-/**
- * tipc_node_link_down - handle loss of link
- */
-
-void tipc_node_link_down(struct tipc_node *n_ptr, struct link *l_ptr)
-{
-	struct link **active;
-
-	n_ptr->working_links--;
-
-	if (!tipc_link_is_active(l_ptr)) {
-		info("Lost standby link <%s> on network plane %c\n",
-		     l_ptr->name, l_ptr->b_ptr->net_plane);
-		return;
-	}
-	info("Lost link <%s> on network plane %c\n",
-		l_ptr->name, l_ptr->b_ptr->net_plane);
-
-	active = &n_ptr->active_links[0];
-	if (active[0] == l_ptr)
-		active[0] = active[1];
-	if (active[1] == l_ptr)
-		active[1] = active[0];
-	if (active[0] == l_ptr)
-		node_select_active_links(n_ptr);
-	if (tipc_node_is_up(n_ptr))
-		tipc_link_changeover(l_ptr);
-	else
-		node_lost_contact(n_ptr);
-}
-
-int tipc_node_has_active_links(struct tipc_node *n_ptr)
-{
-	return (n_ptr &&
-		((n_ptr->active_links[0]) || (n_ptr->active_links[1])));
-}
-
-int tipc_node_has_redundant_links(struct tipc_node *n_ptr)
-{
-	return (n_ptr->working_links > 1);
-}
-
-static int tipc_node_has_active_routes(struct tipc_node *n_ptr)
-{
-	return (n_ptr && (n_ptr->last_router >= 0));
-}
-
-int tipc_node_is_up(struct tipc_node *n_ptr)
-{
-	return (tipc_node_has_active_links(n_ptr) || tipc_node_has_active_routes(n_ptr));
-}
-
-struct tipc_node *tipc_node_attach_link(struct link *l_ptr)
-{
-	struct tipc_node *n_ptr = tipc_node_find(l_ptr->addr);
-
-	if (!n_ptr)
-		n_ptr = tipc_node_create(l_ptr->addr);
-	if (n_ptr) {
-		u32 bearer_id = l_ptr->b_ptr->identity;
-		char addr_string[16];
-
-		if (n_ptr->link_cnt >= 2) {
-			err("Attempt to create third link to %s\n",
-			    addr_string_fill(addr_string, n_ptr->addr));
-			return NULL;
-		}
-
-		if (!n_ptr->links[bearer_id]) {
-			n_ptr->links[bearer_id] = l_ptr;
-			tipc_net.zones[tipc_zone(l_ptr->addr)]->links++;
-			n_ptr->link_cnt++;
-			return n_ptr;
-		}
-		err("Attempt to establish second link on <%s> to %s \n",
-		    l_ptr->b_ptr->publ.name,
-		    addr_string_fill(addr_string, l_ptr->addr));
-	}
-	return NULL;
-}
-
-void tipc_node_detach_link(struct tipc_node *n_ptr, struct link *l_ptr)
-{
-	n_ptr->links[l_ptr->b_ptr->identity] = NULL;
-	tipc_net.zones[tipc_zone(l_ptr->addr)]->links--;
-	n_ptr->link_cnt--;
-}
-
-/*
- * Routing table management - five cases to handle:
- *
- * 1: A link towards a zone/cluster external node comes up.
- *    => Send a multicast message updating routing tables of all
- *    system nodes within own cluster that the new destination
- *    can be reached via this node.
- *    (node.establishedContact()=>cluster.multicastNewRoute())
- *
- * 2: A link towards a slave node comes up.
- *    => Send a multicast message updating routing tables of all
- *    system nodes within own cluster that the new destination
- *    can be reached via this node.
- *    (node.establishedContact()=>cluster.multicastNewRoute())
- *    => Send a  message to the slave node about existence
- *    of all system nodes within cluster:
- *    (node.establishedContact()=>cluster.sendLocalRoutes())
- *
- * 3: A new cluster local system node becomes available.
- *    => Send message(s) to this particular node containing
- *    information about all cluster external and slave
- *     nodes which can be reached via this node.
- *    (node.establishedContact()==>network.sendExternalRoutes())
- *    (node.establishedContact()==>network.sendSlaveRoutes())
- *    => Send messages to all directly connected slave nodes
- *    containing information about the existence of the new node
- *    (node.establishedContact()=>cluster.multicastNewRoute())
- *
- * 4: The link towards a zone/cluster external node or slave
- *    node goes down.
- *    => Send a multcast message updating routing tables of all
- *    nodes within cluster that the new destination can not any
- *    longer be reached via this node.
- *    (node.lostAllLinks()=>cluster.bcastLostRoute())
- *
- * 5: A cluster local system node becomes unavailable.
- *    => Remove all references to this node from the local
- *    routing tables. Note: This is a completely node
- *    local operation.
- *    (node.lostAllLinks()=>network.removeAsRouter())
- *    => Send messages to all directly connected slave nodes
- *    containing information about loss of the node
- *    (node.establishedContact()=>cluster.multicastLostRoute())
- *
- */
-
-static void node_established_contact(struct tipc_node *n_ptr)
-{
-	struct cluster *c_ptr;
-
-	dbg("node_established_contact:-> %x\n", n_ptr->addr);
-	if (!tipc_node_has_active_routes(n_ptr) && in_own_cluster(n_ptr->addr)) {
-		tipc_k_signal((Handler)tipc_named_node_up, n_ptr->addr);
-	}
-
-	/* Syncronize broadcast acks */
-	n_ptr->bclink.acked = tipc_bclink_get_last_sent();
-
-	if (is_slave(tipc_own_addr))
-		return;
-	if (!in_own_cluster(n_ptr->addr)) {
-		/* Usage case 1 (see above) */
-		c_ptr = tipc_cltr_find(tipc_own_addr);
-		if (!c_ptr)
-			c_ptr = tipc_cltr_create(tipc_own_addr);
-		if (c_ptr)
-			tipc_cltr_bcast_new_route(c_ptr, n_ptr->addr, 1,
-						  tipc_max_nodes);
-		return;
-	}
-
-	c_ptr = n_ptr->owner;
-	if (is_slave(n_ptr->addr)) {
-		/* Usage case 2 (see above) */
-		tipc_cltr_bcast_new_route(c_ptr, n_ptr->addr, 1, tipc_max_nodes);
-		tipc_cltr_send_local_routes(c_ptr, n_ptr->addr);
-		return;
-	}
-
-	if (n_ptr->bclink.supported) {
-		tipc_nmap_add(&tipc_cltr_bcast_nodes, n_ptr->addr);
-		if (n_ptr->addr < tipc_own_addr)
-			tipc_own_tag++;
-	}
-
-	/* Case 3 (see above) */
-	tipc_net_send_external_routes(n_ptr->addr);
-	tipc_cltr_send_slave_routes(c_ptr, n_ptr->addr);
-	tipc_cltr_bcast_new_route(c_ptr, n_ptr->addr, LOWEST_SLAVE,
-				  tipc_highest_allowed_slave);
-}
-
-static void node_lost_contact(struct tipc_node *n_ptr)
-{
-	struct cluster *c_ptr;
-	struct tipc_node_subscr *ns, *tns;
-	char addr_string[16];
-	u32 i;
-
-	/* Clean up broadcast reception remains */
-	n_ptr->bclink.gap_after = n_ptr->bclink.gap_to = 0;
-	while (n_ptr->bclink.deferred_head) {
-		struct sk_buff* buf = n_ptr->bclink.deferred_head;
-		n_ptr->bclink.deferred_head = buf->next;
-		buf_discard(buf);
-	}
-	if (n_ptr->bclink.defragm) {
-		buf_discard(n_ptr->bclink.defragm);
-		n_ptr->bclink.defragm = NULL;
-	}
-	if (in_own_cluster(n_ptr->addr) && n_ptr->bclink.supported) {
-		tipc_bclink_acknowledge(n_ptr, mod(n_ptr->bclink.acked + 10000));
-	}
-
-	/* Update routing tables */
-	if (is_slave(tipc_own_addr)) {
-		tipc_net_remove_as_router(n_ptr->addr);
-	} else {
-		if (!in_own_cluster(n_ptr->addr)) {
-			/* Case 4 (see above) */
-			c_ptr = tipc_cltr_find(tipc_own_addr);
-			tipc_cltr_bcast_lost_route(c_ptr, n_ptr->addr, 1,
-						   tipc_max_nodes);
-		} else {
-			/* Case 5 (see above) */
-			c_ptr = tipc_cltr_find(n_ptr->addr);
-			if (is_slave(n_ptr->addr)) {
-				tipc_cltr_bcast_lost_route(c_ptr, n_ptr->addr, 1,
-							   tipc_max_nodes);
-			} else {
-				if (n_ptr->bclink.supported) {
-					tipc_nmap_remove(&tipc_cltr_bcast_nodes,
-							 n_ptr->addr);
-					if (n_ptr->addr < tipc_own_addr)
-						tipc_own_tag--;
-				}
-				tipc_net_remove_as_router(n_ptr->addr);
-				tipc_cltr_bcast_lost_route(c_ptr, n_ptr->addr,
-							   LOWEST_SLAVE,
-							   tipc_highest_allowed_slave);
-			}
-		}
-	}
-	if (tipc_node_has_active_routes(n_ptr))
-		return;
-
-	info("Lost contact with %s\n",
-	     addr_string_fill(addr_string, n_ptr->addr));
-
-	/* Abort link changeover */
-	for (i = 0; i < MAX_BEARERS; i++) {
-		struct link *l_ptr = n_ptr->links[i];
-		if (!l_ptr)
-			continue;
-		l_ptr->reset_checkpoint = l_ptr->next_in_no;
-		l_ptr->exp_msg_count = 0;
-		tipc_link_reset_fragments(l_ptr);
-	}
-
-	/* Notify subscribers */
-	list_for_each_entry_safe(ns, tns, &n_ptr->nsub, nodesub_list) {
-		ns->node = NULL;
-		list_del_init(&ns->nodesub_list);
-		tipc_k_signal((Handler)ns->handle_node_down,
-			      (unsigned long)ns->usr_handle);
-	}
-}
-
-/**
- * tipc_node_select_next_hop - find the next-hop node for a message
- *
- * Called by when cluster local lookup has failed.
- */
-
-struct tipc_node *tipc_node_select_next_hop(u32 addr, u32 selector)
-{
-	struct tipc_node *n_ptr;
-	u32 router_addr;
-
-	if (!tipc_addr_domain_valid(addr))
-		return NULL;
-
-	/* Look for direct link to destination processsor */
-	n_ptr = tipc_node_find(addr);
-	if (n_ptr && tipc_node_has_active_links(n_ptr))
-		return n_ptr;
-
-	/* Cluster local system nodes *must* have direct links */
-	if (!is_slave(addr) && in_own_cluster(addr))
-		return NULL;
-
-	/* Look for cluster local router with direct link to node */
-	router_addr = tipc_node_select_router(n_ptr, selector);
-	if (router_addr)
-		return tipc_node_select(router_addr, selector);
-
-	/* Slave nodes can only be accessed within own cluster via a
-	   known router with direct link -- if no router was found,give up */
-	if (is_slave(addr))
-		return NULL;
-
-	/* Inter zone/cluster -- find any direct link to remote cluster */
-	addr = tipc_addr(tipc_zone(addr), tipc_cluster(addr), 0);
-	n_ptr = tipc_net_select_remote_node(addr, selector);
-	if (n_ptr && tipc_node_has_active_links(n_ptr))
-		return n_ptr;
-
-	/* Last resort -- look for any router to anywhere in remote zone */
-	router_addr =  tipc_net_select_router(addr, selector);
-	if (router_addr)
-		return tipc_node_select(router_addr, selector);
-
-	return NULL;
-}
-
-/**
- * tipc_node_select_router - select router to reach specified node
- *
- * Uses a deterministic and fair algorithm for selecting router node.
- */
-
-u32 tipc_node_select_router(struct tipc_node *n_ptr, u32 ref)
-{
-	u32 ulim;
-	u32 mask;
-	u32 start;
-	u32 r;
-
-	if (!n_ptr)
-		return 0;
-
-	if (n_ptr->last_router < 0)
-		return 0;
-	ulim = ((n_ptr->last_router + 1) * 32) - 1;
-
-	/* Start entry must be random */
-	mask = tipc_max_nodes;
-	while (mask > ulim)
-		mask >>= 1;
-	start = ref & mask;
-	r = start;
-
-	/* Lookup upwards with wrap-around */
-	do {
-		if (((n_ptr->routers[r / 32]) >> (r % 32)) & 1)
-			break;
-	} while (++r <= ulim);
-	if (r > ulim) {
-		r = 1;
-		do {
-			if (((n_ptr->routers[r / 32]) >> (r % 32)) & 1)
-				break;
-		} while (++r < start);
-		assert(r != start);
-	}
-	assert(r && (r <= ulim));
-	return tipc_addr(own_zone(), own_cluster(), r);
-}
-
-void tipc_node_add_router(struct tipc_node *n_ptr, u32 router)
-{
-	u32 r_num = tipc_node(router);
-
-	n_ptr->routers[r_num / 32] =
-		((1 << (r_num % 32)) | n_ptr->routers[r_num / 32]);
-	n_ptr->last_router = tipc_max_nodes / 32;
-	while ((--n_ptr->last_router >= 0) &&
-	       !n_ptr->routers[n_ptr->last_router]);
-}
-
-void tipc_node_remove_router(struct tipc_node *n_ptr, u32 router)
-{
-	u32 r_num = tipc_node(router);
-
-	if (n_ptr->last_router < 0)
-		return;		/* No routes */
-
-	n_ptr->routers[r_num / 32] =
-		((~(1 << (r_num % 32))) & (n_ptr->routers[r_num / 32]));
-	n_ptr->last_router = tipc_max_nodes / 32;
-	while ((--n_ptr->last_router >= 0) &&
-	       !n_ptr->routers[n_ptr->last_router]);
-
-	if (!tipc_node_is_up(n_ptr))
-		node_lost_contact(n_ptr);
-}
-
-#if 0
-void node_print(struct print_buf *buf, struct tipc_node *n_ptr, char *str)
-{
-	u32 i;
-
-	tipc_printf(buf, "\n\n%s", str);
-	for (i = 0; i < MAX_BEARERS; i++) {
-		if (!n_ptr->links[i])
-			continue;
-		tipc_printf(buf, "Links[%u]: %x, ", i, n_ptr->links[i]);
-	}
-	tipc_printf(buf, "Active links: [%x,%x]\n",
-		    n_ptr->active_links[0], n_ptr->active_links[1]);
-}
-#endif
-
-u32 tipc_available_nodes(const u32 domain)
-{
-	struct tipc_node *n_ptr;
-	u32 cnt = 0;
-
-	read_lock_bh(&tipc_net_lock);
-	for (n_ptr = tipc_nodes; n_ptr; n_ptr = n_ptr->next) {
-		if (!in_scope(domain, n_ptr->addr))
-			continue;
-		if (tipc_node_is_up(n_ptr))
-			cnt++;
-	}
-	read_unlock_bh(&tipc_net_lock);
-	return cnt;
-}
-
-struct sk_buff *tipc_node_get_nodes(const void *req_tlv_area, int req_tlv_space)
-{
-	u32 domain;
-	struct sk_buff *buf;
-	struct tipc_node *n_ptr;
-	struct tipc_node_info node_info;
-	u32 payload_size;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_NET_ADDR))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	domain = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
-	if (!tipc_addr_domain_valid(domain))
-		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
-						   " (network address)");
-
-	read_lock_bh(&tipc_net_lock);
-	if (!tipc_nodes) {
-		read_unlock_bh(&tipc_net_lock);
-		return tipc_cfg_reply_none();
-	}
-
-	/* For now, get space for all other nodes
-	   (will need to modify this when slave nodes are supported */
-
-	payload_size = TLV_SPACE(sizeof(node_info)) * (tipc_max_nodes - 1);
-	if (payload_size > 32768u) {
-		read_unlock_bh(&tipc_net_lock);
-		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
-						   " (too many nodes)");
-	}
-	buf = tipc_cfg_reply_alloc(payload_size);
-	if (!buf) {
-		read_unlock_bh(&tipc_net_lock);
-		return NULL;
-	}
-
-	/* Add TLVs for all nodes in scope */
-
-	for (n_ptr = tipc_nodes; n_ptr; n_ptr = n_ptr->next) {
-		if (!in_scope(domain, n_ptr->addr))
-			continue;
-		node_info.addr = htonl(n_ptr->addr);
-		node_info.up = htonl(tipc_node_is_up(n_ptr));
-		tipc_cfg_append_tlv(buf, TIPC_TLV_NODE_INFO,
-				    &node_info, sizeof(node_info));
-	}
-
-	read_unlock_bh(&tipc_net_lock);
-	return buf;
-}
-
-struct sk_buff *tipc_node_get_links(const void *req_tlv_area, int req_tlv_space)
-{
-	u32 domain;
-	struct sk_buff *buf;
-	struct tipc_node *n_ptr;
-	struct tipc_link_info link_info;
-	u32 payload_size;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_NET_ADDR))
-		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	domain = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
-	if (!tipc_addr_domain_valid(domain))
-		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
-						   " (network address)");
-
-	if (tipc_mode != TIPC_NET_MODE)
-		return tipc_cfg_reply_none();
-
-	read_lock_bh(&tipc_net_lock);
-
-	/* Get space for all unicast links + multicast link */
-
-	payload_size = TLV_SPACE(sizeof(link_info)) *
-		(tipc_net.zones[tipc_zone(tipc_own_addr)]->links + 1);
-	if (payload_size > 32768u) {
-		read_unlock_bh(&tipc_net_lock);
-		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
-						   " (too many links)");
-	}
-	buf = tipc_cfg_reply_alloc(payload_size);
-	if (!buf) {
-		read_unlock_bh(&tipc_net_lock);
-		return NULL;
-	}
-
-	/* Add TLV for broadcast link */
-
-	link_info.dest = htonl(tipc_own_addr & 0xfffff00);
-	link_info.up = htonl(1);
-	sprintf(link_info.str, tipc_bclink_name);
-	tipc_cfg_append_tlv(buf, TIPC_TLV_LINK_INFO, &link_info, sizeof(link_info));
-
-	/* Add TLVs for any other links in scope */
-
-	for (n_ptr = tipc_nodes; n_ptr; n_ptr = n_ptr->next) {
-		u32 i;
-
-		if (!in_scope(domain, n_ptr->addr))
-			continue;
-		tipc_node_lock(n_ptr);
-		for (i = 0; i < MAX_BEARERS; i++) {
-			if (!n_ptr->links[i])
-				continue;
-			link_info.dest = htonl(n_ptr->addr);
-			link_info.up = htonl(tipc_link_is_up(n_ptr->links[i]));
-			strcpy(link_info.str, n_ptr->links[i]->name);
-			tipc_cfg_append_tlv(buf, TIPC_TLV_LINK_INFO,
-					    &link_info, sizeof(link_info));
-		}
-		tipc_node_unlock(n_ptr);
-	}
-
-	read_unlock_bh(&tipc_net_lock);
-	return buf;
-}
diff -ruN linux-2.6.29/net/tipc/node.h android_cluster/linux-2.6.29/net/tipc/node.h
--- linux-2.6.29/net/tipc/node.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/node.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,146 +0,0 @@
-/*
- * net/tipc/node.h: Include file for TIPC node management routines
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_NODE_H
-#define _TIPC_NODE_H
-
-#include "node_subscr.h"
-#include "addr.h"
-#include "cluster.h"
-#include "bearer.h"
-
-/**
- * struct tipc_node - TIPC node structure
- * @addr: network address of node
- * @lock: spinlock governing access to structure
- * @owner: pointer to cluster that node belongs to
- * @next: pointer to next node in sorted list of cluster's nodes
- * @nsub: list of "node down" subscriptions monitoring node
- * @active_links: pointers to active links to node
- * @links: pointers to all links to node
- * @working_links: number of working links to node (both active and standby)
- * @link_cnt: number of links to node
- * @permit_changeover: non-zero if node has redundant links to this system
- * @routers: bitmap (used for multicluster communication)
- * @last_router: (used for multicluster communication)
- * @bclink: broadcast-related info
- *    @supported: non-zero if node supports TIPC b'cast capability
- *    @acked: sequence # of last outbound b'cast message acknowledged by node
- *    @last_in: sequence # of last in-sequence b'cast message received from node
- *    @gap_after: sequence # of last message not requiring a NAK request
- *    @gap_to: sequence # of last message requiring a NAK request
- *    @nack_sync: counter that determines when NAK requests should be sent
- *    @deferred_head: oldest OOS b'cast message received from node
- *    @deferred_tail: newest OOS b'cast message received from node
- *    @defragm: list of partially reassembled b'cast message fragments from node
- */
-
-struct tipc_node {
-	u32 addr;
-	spinlock_t lock;
-	struct cluster *owner;
-	struct tipc_node *next;
-	struct list_head nsub;
-	struct link *active_links[2];
-	struct link *links[MAX_BEARERS];
-	int link_cnt;
-	int working_links;
-	int permit_changeover;
-	u32 routers[512/32];
-	int last_router;
-	struct {
-		int supported;
-		u32 acked;
-		u32 last_in;
-		u32 gap_after;
-		u32 gap_to;
-		u32 nack_sync;
-		struct sk_buff *deferred_head;
-		struct sk_buff *deferred_tail;
-		struct sk_buff *defragm;
-	} bclink;
-};
-
-extern struct tipc_node *tipc_nodes;
-extern u32 tipc_own_tag;
-
-struct tipc_node *tipc_node_create(u32 addr);
-void tipc_node_delete(struct tipc_node *n_ptr);
-struct tipc_node *tipc_node_attach_link(struct link *l_ptr);
-void tipc_node_detach_link(struct tipc_node *n_ptr, struct link *l_ptr);
-void tipc_node_link_down(struct tipc_node *n_ptr, struct link *l_ptr);
-void tipc_node_link_up(struct tipc_node *n_ptr, struct link *l_ptr);
-int tipc_node_has_active_links(struct tipc_node *n_ptr);
-int tipc_node_has_redundant_links(struct tipc_node *n_ptr);
-u32 tipc_node_select_router(struct tipc_node *n_ptr, u32 ref);
-struct tipc_node *tipc_node_select_next_hop(u32 addr, u32 selector);
-int tipc_node_is_up(struct tipc_node *n_ptr);
-void tipc_node_add_router(struct tipc_node *n_ptr, u32 router);
-void tipc_node_remove_router(struct tipc_node *n_ptr, u32 router);
-struct sk_buff *tipc_node_get_links(const void *req_tlv_area, int req_tlv_space);
-struct sk_buff *tipc_node_get_nodes(const void *req_tlv_area, int req_tlv_space);
-
-static inline struct tipc_node *tipc_node_find(u32 addr)
-{
-	if (likely(in_own_cluster(addr)))
-		return tipc_local_nodes[tipc_node(addr)];
-	else if (tipc_addr_domain_valid(addr)) {
-		struct cluster *c_ptr = tipc_cltr_find(addr);
-
-		if (c_ptr)
-			return c_ptr->nodes[tipc_node(addr)];
-	}
-	return NULL;
-}
-
-static inline struct tipc_node *tipc_node_select(u32 addr, u32 selector)
-{
-	if (likely(in_own_cluster(addr)))
-		return tipc_local_nodes[tipc_node(addr)];
-	return tipc_node_select_next_hop(addr, selector);
-}
-
-static inline void tipc_node_lock(struct tipc_node *n_ptr)
-{
-	spin_lock_bh(&n_ptr->lock);
-}
-
-static inline void tipc_node_unlock(struct tipc_node *n_ptr)
-{
-	spin_unlock_bh(&n_ptr->lock);
-}
-
-#endif
diff -ruN linux-2.6.29/net/tipc/node_subscr.c android_cluster/linux-2.6.29/net/tipc/node_subscr.c
--- linux-2.6.29/net/tipc/node_subscr.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/node_subscr.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,80 +0,0 @@
-/*
- * net/tipc/node_subscr.c: TIPC "node down" subscription handling
- *
- * Copyright (c) 1995-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "dbg.h"
-#include "node_subscr.h"
-#include "node.h"
-#include "addr.h"
-
-/**
- * tipc_nodesub_subscribe - create "node down" subscription for specified node
- */
-
-void tipc_nodesub_subscribe(struct tipc_node_subscr *node_sub, u32 addr,
-		       void *usr_handle, net_ev_handler handle_down)
-{
-	if (addr == tipc_own_addr) {
-		node_sub->node = NULL;
-		return;
-	}
-
-	node_sub->node = tipc_node_find(addr);
-	if (!node_sub->node) {
-		warn("Node subscription rejected, unknown node 0x%x\n", addr);
-		return;
-	}
-	node_sub->handle_node_down = handle_down;
-	node_sub->usr_handle = usr_handle;
-
-	tipc_node_lock(node_sub->node);
-	list_add_tail(&node_sub->nodesub_list, &node_sub->node->nsub);
-	tipc_node_unlock(node_sub->node);
-}
-
-/**
- * tipc_nodesub_unsubscribe - cancel "node down" subscription (if any)
- */
-
-void tipc_nodesub_unsubscribe(struct tipc_node_subscr *node_sub)
-{
-	if (!node_sub->node)
-		return;
-
-	tipc_node_lock(node_sub->node);
-	list_del_init(&node_sub->nodesub_list);
-	tipc_node_unlock(node_sub->node);
-}
diff -ruN linux-2.6.29/net/tipc/port.c android_cluster/linux-2.6.29/net/tipc/port.c
--- linux-2.6.29/net/tipc/port.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/port.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,1706 +0,0 @@
-/*
- * net/tipc/port.c: TIPC port code
- *
- * Copyright (c) 1992-2007, Ericsson AB
- * Copyright (c) 2004-2008, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "config.h"
-#include "dbg.h"
-#include "port.h"
-#include "addr.h"
-#include "link.h"
-#include "node.h"
-#include "name_table.h"
-#include "user_reg.h"
-#include "msg.h"
-#include "bcast.h"
-
-/* Connection management: */
-#define PROBING_INTERVAL 3600000	/* [ms] => 1 h */
-#define CONFIRMED 0
-#define PROBING 1
-
-#define MAX_REJECT_SIZE 1024
-
-static struct sk_buff *msg_queue_head = NULL;
-static struct sk_buff *msg_queue_tail = NULL;
-
-DEFINE_SPINLOCK(tipc_port_list_lock);
-static DEFINE_SPINLOCK(queue_lock);
-
-static LIST_HEAD(ports);
-static void port_handle_node_down(unsigned long ref);
-static struct sk_buff* port_build_self_abort_msg(struct port *,u32 err);
-static struct sk_buff* port_build_peer_abort_msg(struct port *,u32 err);
-static void port_timeout(unsigned long ref);
-
-
-static u32 port_peernode(struct port *p_ptr)
-{
-	return msg_destnode(&p_ptr->publ.phdr);
-}
-
-static u32 port_peerport(struct port *p_ptr)
-{
-	return msg_destport(&p_ptr->publ.phdr);
-}
-
-static u32 port_out_seqno(struct port *p_ptr)
-{
-	return msg_transp_seqno(&p_ptr->publ.phdr);
-}
-
-static void port_incr_out_seqno(struct port *p_ptr)
-{
-	struct tipc_msg *m = &p_ptr->publ.phdr;
-
-	if (likely(!msg_routed(m)))
-		return;
-	msg_set_transp_seqno(m, (msg_transp_seqno(m) + 1));
-}
-
-/**
- * tipc_multicast - send a multicast message to local and remote destinations
- */
-
-int tipc_multicast(u32 ref, struct tipc_name_seq const *seq, u32 domain,
-		   u32 num_sect, struct iovec const *msg_sect)
-{
-	struct tipc_msg *hdr;
-	struct sk_buff *buf;
-	struct sk_buff *ibuf = NULL;
-	struct port_list dports = {0, NULL, };
-	struct port *oport = tipc_port_deref(ref);
-	int ext_targets;
-	int res;
-
-	if (unlikely(!oport))
-		return -EINVAL;
-
-	/* Create multicast message */
-
-	hdr = &oport->publ.phdr;
-	msg_set_type(hdr, TIPC_MCAST_MSG);
-	msg_set_nametype(hdr, seq->type);
-	msg_set_namelower(hdr, seq->lower);
-	msg_set_nameupper(hdr, seq->upper);
-	msg_set_hdr_sz(hdr, MCAST_H_SIZE);
-	res = msg_build(hdr, msg_sect, num_sect, MAX_MSG_SIZE,
-			!oport->user_port, &buf);
-	if (unlikely(!buf))
-		return res;
-
-	/* Figure out where to send multicast message */
-
-	ext_targets = tipc_nametbl_mc_translate(seq->type, seq->lower, seq->upper,
-						TIPC_NODE_SCOPE, &dports);
-
-	/* Send message to destinations (duplicate it only if necessary) */
-
-	if (ext_targets) {
-		if (dports.count != 0) {
-			ibuf = skb_copy(buf, GFP_ATOMIC);
-			if (ibuf == NULL) {
-				tipc_port_list_free(&dports);
-				buf_discard(buf);
-				return -ENOMEM;
-			}
-		}
-		res = tipc_bclink_send_msg(buf);
-		if ((res < 0) && (dports.count != 0)) {
-			buf_discard(ibuf);
-		}
-	} else {
-		ibuf = buf;
-	}
-
-	if (res >= 0) {
-		if (ibuf)
-			tipc_port_recv_mcast(ibuf, &dports);
-	} else {
-		tipc_port_list_free(&dports);
-	}
-	return res;
-}
-
-/**
- * tipc_port_recv_mcast - deliver multicast message to all destination ports
- *
- * If there is no port list, perform a lookup to create one
- */
-
-void tipc_port_recv_mcast(struct sk_buff *buf, struct port_list *dp)
-{
-	struct tipc_msg* msg;
-	struct port_list dports = {0, NULL, };
-	struct port_list *item = dp;
-	int cnt = 0;
-
-	msg = buf_msg(buf);
-
-	/* Create destination port list, if one wasn't supplied */
-
-	if (dp == NULL) {
-		tipc_nametbl_mc_translate(msg_nametype(msg),
-				     msg_namelower(msg),
-				     msg_nameupper(msg),
-				     TIPC_CLUSTER_SCOPE,
-				     &dports);
-		item = dp = &dports;
-	}
-
-	/* Deliver a copy of message to each destination port */
-
-	if (dp->count != 0) {
-		if (dp->count == 1) {
-			msg_set_destport(msg, dp->ports[0]);
-			tipc_port_recv_msg(buf);
-			tipc_port_list_free(dp);
-			return;
-		}
-		for (; cnt < dp->count; cnt++) {
-			int index = cnt % PLSIZE;
-			struct sk_buff *b = skb_clone(buf, GFP_ATOMIC);
-
-			if (b == NULL) {
-				warn("Unable to deliver multicast message(s)\n");
-				msg_dbg(msg, "LOST:");
-				goto exit;
-			}
-			if ((index == 0) && (cnt != 0)) {
-				item = item->next;
-			}
-			msg_set_destport(buf_msg(b),item->ports[index]);
-			tipc_port_recv_msg(b);
-		}
-	}
-exit:
-	buf_discard(buf);
-	tipc_port_list_free(dp);
-}
-
-/**
- * tipc_createport_raw - create a generic TIPC port
- *
- * Returns pointer to (locked) TIPC port, or NULL if unable to create it
- */
-
-struct tipc_port *tipc_createport_raw(void *usr_handle,
-			u32 (*dispatcher)(struct tipc_port *, struct sk_buff *),
-			void (*wakeup)(struct tipc_port *),
-			const u32 importance)
-{
-	struct port *p_ptr;
-	struct tipc_msg *msg;
-	u32 ref;
-
-	p_ptr = kzalloc(sizeof(*p_ptr), GFP_ATOMIC);
-	if (!p_ptr) {
-		warn("Port creation failed, no memory\n");
-		return NULL;
-	}
-	ref = tipc_ref_acquire(p_ptr, &p_ptr->publ.lock);
-	if (!ref) {
-		warn("Port creation failed, reference table exhausted\n");
-		kfree(p_ptr);
-		return NULL;
-	}
-
-	p_ptr->publ.usr_handle = usr_handle;
-	p_ptr->publ.max_pkt = MAX_PKT_DEFAULT;
-	p_ptr->publ.ref = ref;
-	msg = &p_ptr->publ.phdr;
-	msg_init(msg, importance, TIPC_NAMED_MSG, LONG_H_SIZE, 0);
-	msg_set_origport(msg, ref);
-	p_ptr->last_in_seqno = 41;
-	p_ptr->sent = 1;
-	INIT_LIST_HEAD(&p_ptr->wait_list);
-	INIT_LIST_HEAD(&p_ptr->subscription.nodesub_list);
-	p_ptr->congested_link = NULL;
-	p_ptr->dispatcher = dispatcher;
-	p_ptr->wakeup = wakeup;
-	p_ptr->user_port = NULL;
-	k_init_timer(&p_ptr->timer, (Handler)port_timeout, ref);
-	spin_lock_bh(&tipc_port_list_lock);
-	INIT_LIST_HEAD(&p_ptr->publications);
-	INIT_LIST_HEAD(&p_ptr->port_list);
-	list_add_tail(&p_ptr->port_list, &ports);
-	spin_unlock_bh(&tipc_port_list_lock);
-	return &(p_ptr->publ);
-}
-
-int tipc_deleteport(u32 ref)
-{
-	struct port *p_ptr;
-	struct sk_buff *buf = NULL;
-
-	tipc_withdraw(ref, 0, NULL);
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-
-	tipc_ref_discard(ref);
-	tipc_port_unlock(p_ptr);
-
-	k_cancel_timer(&p_ptr->timer);
-	if (p_ptr->publ.connected) {
-		buf = port_build_peer_abort_msg(p_ptr, TIPC_ERR_NO_PORT);
-		tipc_nodesub_unsubscribe(&p_ptr->subscription);
-	}
-	if (p_ptr->user_port) {
-		tipc_reg_remove_port(p_ptr->user_port);
-		kfree(p_ptr->user_port);
-	}
-
-	spin_lock_bh(&tipc_port_list_lock);
-	list_del(&p_ptr->port_list);
-	list_del(&p_ptr->wait_list);
-	spin_unlock_bh(&tipc_port_list_lock);
-	k_term_timer(&p_ptr->timer);
-	kfree(p_ptr);
-	dbg("Deleted port %u\n", ref);
-	tipc_net_route_msg(buf);
-	return 0;
-}
-
-/**
- * tipc_get_port() - return port associated with 'ref'
- *
- * Note: Port is not locked.
- */
-
-struct tipc_port *tipc_get_port(const u32 ref)
-{
-	return (struct tipc_port *)tipc_ref_deref(ref);
-}
-
-/**
- * tipc_get_handle - return user handle associated to port 'ref'
- */
-
-void *tipc_get_handle(const u32 ref)
-{
-	struct port *p_ptr;
-	void * handle;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return NULL;
-	handle = p_ptr->publ.usr_handle;
-	tipc_port_unlock(p_ptr);
-	return handle;
-}
-
-static int port_unreliable(struct port *p_ptr)
-{
-	return msg_src_droppable(&p_ptr->publ.phdr);
-}
-
-int tipc_portunreliable(u32 ref, unsigned int *isunreliable)
-{
-	struct port *p_ptr;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-	*isunreliable = port_unreliable(p_ptr);
-	tipc_port_unlock(p_ptr);
-	return 0;
-}
-
-int tipc_set_portunreliable(u32 ref, unsigned int isunreliable)
-{
-	struct port *p_ptr;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-	msg_set_src_droppable(&p_ptr->publ.phdr, (isunreliable != 0));
-	tipc_port_unlock(p_ptr);
-	return 0;
-}
-
-static int port_unreturnable(struct port *p_ptr)
-{
-	return msg_dest_droppable(&p_ptr->publ.phdr);
-}
-
-int tipc_portunreturnable(u32 ref, unsigned int *isunrejectable)
-{
-	struct port *p_ptr;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-	*isunrejectable = port_unreturnable(p_ptr);
-	tipc_port_unlock(p_ptr);
-	return 0;
-}
-
-int tipc_set_portunreturnable(u32 ref, unsigned int isunrejectable)
-{
-	struct port *p_ptr;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-	msg_set_dest_droppable(&p_ptr->publ.phdr, (isunrejectable != 0));
-	tipc_port_unlock(p_ptr);
-	return 0;
-}
-
-/*
- * port_build_proto_msg(): build a port level protocol
- * or a connection abortion message. Called with
- * tipc_port lock on.
- */
-static struct sk_buff *port_build_proto_msg(u32 destport, u32 destnode,
-					    u32 origport, u32 orignode,
-					    u32 usr, u32 type, u32 err,
-					    u32 seqno, u32 ack)
-{
-	struct sk_buff *buf;
-	struct tipc_msg *msg;
-
-	buf = buf_acquire(LONG_H_SIZE);
-	if (buf) {
-		msg = buf_msg(buf);
-		msg_init(msg, usr, type, LONG_H_SIZE, destnode);
-		msg_set_errcode(msg, err);
-		msg_set_destport(msg, destport);
-		msg_set_origport(msg, origport);
-		msg_set_orignode(msg, orignode);
-		msg_set_transp_seqno(msg, seqno);
-		msg_set_msgcnt(msg, ack);
-		msg_dbg(msg, "PORT>SEND>:");
-	}
-	return buf;
-}
-
-int tipc_reject_msg(struct sk_buff *buf, u32 err)
-{
-	struct tipc_msg *msg = buf_msg(buf);
-	struct sk_buff *rbuf;
-	struct tipc_msg *rmsg;
-	int hdr_sz;
-	u32 imp = msg_importance(msg);
-	u32 data_sz = msg_data_sz(msg);
-
-	if (data_sz > MAX_REJECT_SIZE)
-		data_sz = MAX_REJECT_SIZE;
-	if (msg_connected(msg) && (imp < TIPC_CRITICAL_IMPORTANCE))
-		imp++;
-	msg_dbg(msg, "port->rej: ");
-
-	/* discard rejected message if it shouldn't be returned to sender */
-	if (msg_errcode(msg) || msg_dest_droppable(msg)) {
-		buf_discard(buf);
-		return data_sz;
-	}
-
-	/* construct rejected message */
-	if (msg_mcast(msg))
-		hdr_sz = MCAST_H_SIZE;
-	else
-		hdr_sz = LONG_H_SIZE;
-	rbuf = buf_acquire(data_sz + hdr_sz);
-	if (rbuf == NULL) {
-		buf_discard(buf);
-		return data_sz;
-	}
-	rmsg = buf_msg(rbuf);
-	msg_init(rmsg, imp, msg_type(msg), hdr_sz, msg_orignode(msg));
-	msg_set_errcode(rmsg, err);
-	msg_set_destport(rmsg, msg_origport(msg));
-	msg_set_origport(rmsg, msg_destport(msg));
-	if (msg_short(msg)) {
-		msg_set_orignode(rmsg, tipc_own_addr);
-		/* leave name type & instance as zeroes */
-	} else {
-		msg_set_orignode(rmsg, msg_destnode(msg));
-		msg_set_nametype(rmsg, msg_nametype(msg));
-		msg_set_nameinst(rmsg, msg_nameinst(msg));
-	}
-	msg_set_size(rmsg, data_sz + hdr_sz);
-	skb_copy_to_linear_data_offset(rbuf, hdr_sz, msg_data(msg), data_sz);
-
-	/* send self-abort message when rejecting on a connected port */
-	if (msg_connected(msg)) {
-		struct sk_buff *abuf = NULL;
-		struct port *p_ptr = tipc_port_lock(msg_destport(msg));
-
-		if (p_ptr) {
-			if (p_ptr->publ.connected)
-				abuf = port_build_self_abort_msg(p_ptr, err);
-			tipc_port_unlock(p_ptr);
-		}
-		tipc_net_route_msg(abuf);
-	}
-
-	/* send rejected message */
-	buf_discard(buf);
-	tipc_net_route_msg(rbuf);
-	return data_sz;
-}
-
-int tipc_port_reject_sections(struct port *p_ptr, struct tipc_msg *hdr,
-			      struct iovec const *msg_sect, u32 num_sect,
-			      int err)
-{
-	struct sk_buff *buf;
-	int res;
-
-	res = msg_build(hdr, msg_sect, num_sect, MAX_MSG_SIZE,
-			!p_ptr->user_port, &buf);
-	if (!buf)
-		return res;
-
-	return tipc_reject_msg(buf, err);
-}
-
-static void port_timeout(unsigned long ref)
-{
-	struct port *p_ptr = tipc_port_lock(ref);
-	struct sk_buff *buf = NULL;
-
-	if (!p_ptr)
-		return;
-
-	if (!p_ptr->publ.connected) {
-		tipc_port_unlock(p_ptr);
-		return;
-	}
-
-	/* Last probe answered ? */
-	if (p_ptr->probing_state == PROBING) {
-		buf = port_build_self_abort_msg(p_ptr, TIPC_ERR_NO_PORT);
-	} else {
-		buf = port_build_proto_msg(port_peerport(p_ptr),
-					   port_peernode(p_ptr),
-					   p_ptr->publ.ref,
-					   tipc_own_addr,
-					   CONN_MANAGER,
-					   CONN_PROBE,
-					   TIPC_OK,
-					   port_out_seqno(p_ptr),
-					   0);
-		port_incr_out_seqno(p_ptr);
-		p_ptr->probing_state = PROBING;
-		k_start_timer(&p_ptr->timer, p_ptr->probing_interval);
-	}
-	tipc_port_unlock(p_ptr);
-	tipc_net_route_msg(buf);
-}
-
-
-static void port_handle_node_down(unsigned long ref)
-{
-	struct port *p_ptr = tipc_port_lock(ref);
-	struct sk_buff* buf = NULL;
-
-	if (!p_ptr)
-		return;
-	buf = port_build_self_abort_msg(p_ptr, TIPC_ERR_NO_NODE);
-	tipc_port_unlock(p_ptr);
-	tipc_net_route_msg(buf);
-}
-
-
-static struct sk_buff *port_build_self_abort_msg(struct port *p_ptr, u32 err)
-{
-	u32 imp = msg_importance(&p_ptr->publ.phdr);
-
-	if (!p_ptr->publ.connected)
-		return NULL;
-	if (imp < TIPC_CRITICAL_IMPORTANCE)
-		imp++;
-	return port_build_proto_msg(p_ptr->publ.ref,
-				    tipc_own_addr,
-				    port_peerport(p_ptr),
-				    port_peernode(p_ptr),
-				    imp,
-				    TIPC_CONN_MSG,
-				    err,
-				    p_ptr->last_in_seqno + 1,
-				    0);
-}
-
-
-static struct sk_buff *port_build_peer_abort_msg(struct port *p_ptr, u32 err)
-{
-	u32 imp = msg_importance(&p_ptr->publ.phdr);
-
-	if (!p_ptr->publ.connected)
-		return NULL;
-	if (imp < TIPC_CRITICAL_IMPORTANCE)
-		imp++;
-	return port_build_proto_msg(port_peerport(p_ptr),
-				    port_peernode(p_ptr),
-				    p_ptr->publ.ref,
-				    tipc_own_addr,
-				    imp,
-				    TIPC_CONN_MSG,
-				    err,
-				    port_out_seqno(p_ptr),
-				    0);
-}
-
-void tipc_port_recv_proto_msg(struct sk_buff *buf)
-{
-	struct tipc_msg *msg = buf_msg(buf);
-	struct port *p_ptr = tipc_port_lock(msg_destport(msg));
-	u32 err = TIPC_OK;
-	struct sk_buff *r_buf = NULL;
-	struct sk_buff *abort_buf = NULL;
-
-	msg_dbg(msg, "PORT<RECV<:");
-
-	if (!p_ptr) {
-		err = TIPC_ERR_NO_PORT;
-	} else if (p_ptr->publ.connected) {
-		if (port_peernode(p_ptr) != msg_orignode(msg))
-			err = TIPC_ERR_NO_PORT;
-		if (port_peerport(p_ptr) != msg_origport(msg))
-			err = TIPC_ERR_NO_PORT;
-		if (!err && msg_routed(msg)) {
-			u32 seqno = msg_transp_seqno(msg);
-			u32 myno =  ++p_ptr->last_in_seqno;
-			if (seqno != myno) {
-				err = TIPC_ERR_NO_PORT;
-				abort_buf = port_build_self_abort_msg(p_ptr, err);
-			}
-		}
-		if (msg_type(msg) == CONN_ACK) {
-			int wakeup = tipc_port_congested(p_ptr) &&
-				     p_ptr->publ.congested &&
-				     p_ptr->wakeup;
-			p_ptr->acked += msg_msgcnt(msg);
-			if (tipc_port_congested(p_ptr))
-				goto exit;
-			p_ptr->publ.congested = 0;
-			if (!wakeup)
-				goto exit;
-			p_ptr->wakeup(&p_ptr->publ);
-			goto exit;
-		}
-	} else if (p_ptr->publ.published) {
-		err = TIPC_ERR_NO_PORT;
-	}
-	if (err) {
-		r_buf = port_build_proto_msg(msg_origport(msg),
-					     msg_orignode(msg),
-					     msg_destport(msg),
-					     tipc_own_addr,
-					     TIPC_HIGH_IMPORTANCE,
-					     TIPC_CONN_MSG,
-					     err,
-					     0,
-					     0);
-		goto exit;
-	}
-
-	/* All is fine */
-	if (msg_type(msg) == CONN_PROBE) {
-		r_buf = port_build_proto_msg(msg_origport(msg),
-					     msg_orignode(msg),
-					     msg_destport(msg),
-					     tipc_own_addr,
-					     CONN_MANAGER,
-					     CONN_PROBE_REPLY,
-					     TIPC_OK,
-					     port_out_seqno(p_ptr),
-					     0);
-	}
-	p_ptr->probing_state = CONFIRMED;
-	port_incr_out_seqno(p_ptr);
-exit:
-	if (p_ptr)
-		tipc_port_unlock(p_ptr);
-	tipc_net_route_msg(r_buf);
-	tipc_net_route_msg(abort_buf);
-	buf_discard(buf);
-}
-
-static void port_print(struct port *p_ptr, struct print_buf *buf, int full_id)
-{
-	struct publication *publ;
-
-	if (full_id)
-		tipc_printf(buf, "<%u.%u.%u:%u>:",
-			    tipc_zone(tipc_own_addr), tipc_cluster(tipc_own_addr),
-			    tipc_node(tipc_own_addr), p_ptr->publ.ref);
-	else
-		tipc_printf(buf, "%-10u:", p_ptr->publ.ref);
-
-	if (p_ptr->publ.connected) {
-		u32 dport = port_peerport(p_ptr);
-		u32 destnode = port_peernode(p_ptr);
-
-		tipc_printf(buf, " connected to <%u.%u.%u:%u>",
-			    tipc_zone(destnode), tipc_cluster(destnode),
-			    tipc_node(destnode), dport);
-		if (p_ptr->publ.conn_type != 0)
-			tipc_printf(buf, " via {%u,%u}",
-				    p_ptr->publ.conn_type,
-				    p_ptr->publ.conn_instance);
-	}
-	else if (p_ptr->publ.published) {
-		tipc_printf(buf, " bound to");
-		list_for_each_entry(publ, &p_ptr->publications, pport_list) {
-			if (publ->lower == publ->upper)
-				tipc_printf(buf, " {%u,%u}", publ->type,
-					    publ->lower);
-			else
-				tipc_printf(buf, " {%u,%u,%u}", publ->type,
-					    publ->lower, publ->upper);
-		}
-	}
-	tipc_printf(buf, "\n");
-}
-
-#define MAX_PORT_QUERY 32768
-
-struct sk_buff *tipc_port_get_ports(void)
-{
-	struct sk_buff *buf;
-	struct tlv_desc *rep_tlv;
-	struct print_buf pb;
-	struct port *p_ptr;
-	int str_len;
-
-	buf = tipc_cfg_reply_alloc(TLV_SPACE(MAX_PORT_QUERY));
-	if (!buf)
-		return NULL;
-	rep_tlv = (struct tlv_desc *)buf->data;
-
-	tipc_printbuf_init(&pb, TLV_DATA(rep_tlv), MAX_PORT_QUERY);
-	spin_lock_bh(&tipc_port_list_lock);
-	list_for_each_entry(p_ptr, &ports, port_list) {
-		spin_lock_bh(p_ptr->publ.lock);
-		port_print(p_ptr, &pb, 0);
-		spin_unlock_bh(p_ptr->publ.lock);
-	}
-	spin_unlock_bh(&tipc_port_list_lock);
-	str_len = tipc_printbuf_validate(&pb);
-
-	skb_put(buf, TLV_SPACE(str_len));
-	TLV_SET(rep_tlv, TIPC_TLV_ULTRA_STRING, NULL, str_len);
-
-	return buf;
-}
-
-#if 0
-
-#define MAX_PORT_STATS 2000
-
-struct sk_buff *port_show_stats(const void *req_tlv_area, int req_tlv_space)
-{
-	u32 ref;
-	struct port *p_ptr;
-	struct sk_buff *buf;
-	struct tlv_desc *rep_tlv;
-	struct print_buf pb;
-	int str_len;
-
-	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_PORT_REF))
-		return cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
-
-	ref = *(u32 *)TLV_DATA(req_tlv_area);
-	ref = ntohl(ref);
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return cfg_reply_error_string("port not found");
-
-	buf = tipc_cfg_reply_alloc(TLV_SPACE(MAX_PORT_STATS));
-	if (!buf) {
-		tipc_port_unlock(p_ptr);
-		return NULL;
-	}
-	rep_tlv = (struct tlv_desc *)buf->data;
-
-	tipc_printbuf_init(&pb, TLV_DATA(rep_tlv), MAX_PORT_STATS);
-	port_print(p_ptr, &pb, 1);
-	/* NEED TO FILL IN ADDITIONAL PORT STATISTICS HERE */
-	tipc_port_unlock(p_ptr);
-	str_len = tipc_printbuf_validate(&pb);
-
-	skb_put(buf, TLV_SPACE(str_len));
-	TLV_SET(rep_tlv, TIPC_TLV_ULTRA_STRING, NULL, str_len);
-
-	return buf;
-}
-
-#endif
-
-void tipc_port_reinit(void)
-{
-	struct port *p_ptr;
-	struct tipc_msg *msg;
-
-	spin_lock_bh(&tipc_port_list_lock);
-	list_for_each_entry(p_ptr, &ports, port_list) {
-		msg = &p_ptr->publ.phdr;
-		if (msg_orignode(msg) == tipc_own_addr)
-			break;
-		msg_set_prevnode(msg, tipc_own_addr);
-		msg_set_orignode(msg, tipc_own_addr);
-	}
-	spin_unlock_bh(&tipc_port_list_lock);
-}
-
-
-/*
- *  port_dispatcher_sigh(): Signal handler for messages destinated
- *                          to the tipc_port interface.
- */
-
-static void port_dispatcher_sigh(void *dummy)
-{
-	struct sk_buff *buf;
-
-	spin_lock_bh(&queue_lock);
-	buf = msg_queue_head;
-	msg_queue_head = NULL;
-	spin_unlock_bh(&queue_lock);
-
-	while (buf) {
-		struct port *p_ptr;
-		struct user_port *up_ptr;
-		struct tipc_portid orig;
-		struct tipc_name_seq dseq;
-		void *usr_handle;
-		int connected;
-		int published;
-		u32 message_type;
-
-		struct sk_buff *next = buf->next;
-		struct tipc_msg *msg = buf_msg(buf);
-		u32 dref = msg_destport(msg);
-
-		message_type = msg_type(msg);
-		if (message_type > TIPC_DIRECT_MSG)
-			goto reject;	/* Unsupported message type */
-
-		p_ptr = tipc_port_lock(dref);
-		if (!p_ptr)
-			goto reject;	/* Port deleted while msg in queue */
-
-		orig.ref = msg_origport(msg);
-		orig.node = msg_orignode(msg);
-		up_ptr = p_ptr->user_port;
-		usr_handle = up_ptr->usr_handle;
-		connected = p_ptr->publ.connected;
-		published = p_ptr->publ.published;
-
-		if (unlikely(msg_errcode(msg)))
-			goto err;
-
-		switch (message_type) {
-
-		case TIPC_CONN_MSG:{
-				tipc_conn_msg_event cb = up_ptr->conn_msg_cb;
-				u32 peer_port = port_peerport(p_ptr);
-				u32 peer_node = port_peernode(p_ptr);
-
-				tipc_port_unlock(p_ptr);
-				if (unlikely(!cb))
-					goto reject;
-				if (unlikely(!connected)) {
-					if (tipc_connect2port(dref, &orig))
-						goto reject;
-				} else if ((msg_origport(msg) != peer_port) ||
-					   (msg_orignode(msg) != peer_node))
-					goto reject;
-				if (unlikely(++p_ptr->publ.conn_unacked >=
-					     TIPC_FLOW_CONTROL_WIN))
-					tipc_acknowledge(dref,
-							 p_ptr->publ.conn_unacked);
-				skb_pull(buf, msg_hdr_sz(msg));
-				cb(usr_handle, dref, &buf, msg_data(msg),
-				   msg_data_sz(msg));
-				break;
-			}
-		case TIPC_DIRECT_MSG:{
-				tipc_msg_event cb = up_ptr->msg_cb;
-
-				tipc_port_unlock(p_ptr);
-				if (unlikely(!cb || connected))
-					goto reject;
-				skb_pull(buf, msg_hdr_sz(msg));
-				cb(usr_handle, dref, &buf, msg_data(msg),
-				   msg_data_sz(msg), msg_importance(msg),
-				   &orig);
-				break;
-			}
-		case TIPC_MCAST_MSG:
-		case TIPC_NAMED_MSG:{
-				tipc_named_msg_event cb = up_ptr->named_msg_cb;
-
-				tipc_port_unlock(p_ptr);
-				if (unlikely(!cb || connected || !published))
-					goto reject;
-				dseq.type =  msg_nametype(msg);
-				dseq.lower = msg_nameinst(msg);
-				dseq.upper = (message_type == TIPC_NAMED_MSG)
-					? dseq.lower : msg_nameupper(msg);
-				skb_pull(buf, msg_hdr_sz(msg));
-				cb(usr_handle, dref, &buf, msg_data(msg),
-				   msg_data_sz(msg), msg_importance(msg),
-				   &orig, &dseq);
-				break;
-			}
-		}
-		if (buf)
-			buf_discard(buf);
-		buf = next;
-		continue;
-err:
-		switch (message_type) {
-
-		case TIPC_CONN_MSG:{
-				tipc_conn_shutdown_event cb =
-					up_ptr->conn_err_cb;
-				u32 peer_port = port_peerport(p_ptr);
-				u32 peer_node = port_peernode(p_ptr);
-
-				tipc_port_unlock(p_ptr);
-				if (!cb || !connected)
-					break;
-				if ((msg_origport(msg) != peer_port) ||
-				    (msg_orignode(msg) != peer_node))
-					break;
-				tipc_disconnect(dref);
-				skb_pull(buf, msg_hdr_sz(msg));
-				cb(usr_handle, dref, &buf, msg_data(msg),
-				   msg_data_sz(msg), msg_errcode(msg));
-				break;
-			}
-		case TIPC_DIRECT_MSG:{
-				tipc_msg_err_event cb = up_ptr->err_cb;
-
-				tipc_port_unlock(p_ptr);
-				if (!cb || connected)
-					break;
-				skb_pull(buf, msg_hdr_sz(msg));
-				cb(usr_handle, dref, &buf, msg_data(msg),
-				   msg_data_sz(msg), msg_errcode(msg), &orig);
-				break;
-			}
-		case TIPC_MCAST_MSG:
-		case TIPC_NAMED_MSG:{
-				tipc_named_msg_err_event cb =
-					up_ptr->named_err_cb;
-
-				tipc_port_unlock(p_ptr);
-				if (!cb || connected)
-					break;
-				dseq.type =  msg_nametype(msg);
-				dseq.lower = msg_nameinst(msg);
-				dseq.upper = (message_type == TIPC_NAMED_MSG)
-					? dseq.lower : msg_nameupper(msg);
-				skb_pull(buf, msg_hdr_sz(msg));
-				cb(usr_handle, dref, &buf, msg_data(msg),
-				   msg_data_sz(msg), msg_errcode(msg), &dseq);
-				break;
-			}
-		}
-		if (buf)
-			buf_discard(buf);
-		buf = next;
-		continue;
-reject:
-		tipc_reject_msg(buf, TIPC_ERR_NO_PORT);
-		buf = next;
-	}
-}
-
-/*
- *  port_dispatcher(): Dispatcher for messages destinated
- *  to the tipc_port interface. Called with port locked.
- */
-
-static u32 port_dispatcher(struct tipc_port *dummy, struct sk_buff *buf)
-{
-	buf->next = NULL;
-	spin_lock_bh(&queue_lock);
-	if (msg_queue_head) {
-		msg_queue_tail->next = buf;
-		msg_queue_tail = buf;
-	} else {
-		msg_queue_tail = msg_queue_head = buf;
-		tipc_k_signal((Handler)port_dispatcher_sigh, 0);
-	}
-	spin_unlock_bh(&queue_lock);
-	return 0;
-}
-
-/*
- * Wake up port after congestion: Called with port locked,
- *
- */
-
-static void port_wakeup_sh(unsigned long ref)
-{
-	struct port *p_ptr;
-	struct user_port *up_ptr;
-	tipc_continue_event cb = NULL;
-	void *uh = NULL;
-
-	p_ptr = tipc_port_lock(ref);
-	if (p_ptr) {
-		up_ptr = p_ptr->user_port;
-		if (up_ptr) {
-			cb = up_ptr->continue_event_cb;
-			uh = up_ptr->usr_handle;
-		}
-		tipc_port_unlock(p_ptr);
-	}
-	if (cb)
-		cb(uh, ref);
-}
-
-
-static void port_wakeup(struct tipc_port *p_ptr)
-{
-	tipc_k_signal((Handler)port_wakeup_sh, p_ptr->ref);
-}
-
-void tipc_acknowledge(u32 ref, u32 ack)
-{
-	struct port *p_ptr;
-	struct sk_buff *buf = NULL;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return;
-	if (p_ptr->publ.connected) {
-		p_ptr->publ.conn_unacked -= ack;
-		buf = port_build_proto_msg(port_peerport(p_ptr),
-					   port_peernode(p_ptr),
-					   ref,
-					   tipc_own_addr,
-					   CONN_MANAGER,
-					   CONN_ACK,
-					   TIPC_OK,
-					   port_out_seqno(p_ptr),
-					   ack);
-	}
-	tipc_port_unlock(p_ptr);
-	tipc_net_route_msg(buf);
-}
-
-/*
- * tipc_createport(): user level call. Will add port to
- *                    registry if non-zero user_ref.
- */
-
-int tipc_createport(u32 user_ref,
-		    void *usr_handle,
-		    unsigned int importance,
-		    tipc_msg_err_event error_cb,
-		    tipc_named_msg_err_event named_error_cb,
-		    tipc_conn_shutdown_event conn_error_cb,
-		    tipc_msg_event msg_cb,
-		    tipc_named_msg_event named_msg_cb,
-		    tipc_conn_msg_event conn_msg_cb,
-		    tipc_continue_event continue_event_cb,/* May be zero */
-		    u32 *portref)
-{
-	struct user_port *up_ptr;
-	struct port *p_ptr;
-
-	up_ptr = kmalloc(sizeof(*up_ptr), GFP_ATOMIC);
-	if (!up_ptr) {
-		warn("Port creation failed, no memory\n");
-		return -ENOMEM;
-	}
-	p_ptr = (struct port *)tipc_createport_raw(NULL, port_dispatcher,
-						   port_wakeup, importance);
-	if (!p_ptr) {
-		kfree(up_ptr);
-		return -ENOMEM;
-	}
-
-	p_ptr->user_port = up_ptr;
-	up_ptr->user_ref = user_ref;
-	up_ptr->usr_handle = usr_handle;
-	up_ptr->ref = p_ptr->publ.ref;
-	up_ptr->err_cb = error_cb;
-	up_ptr->named_err_cb = named_error_cb;
-	up_ptr->conn_err_cb = conn_error_cb;
-	up_ptr->msg_cb = msg_cb;
-	up_ptr->named_msg_cb = named_msg_cb;
-	up_ptr->conn_msg_cb = conn_msg_cb;
-	up_ptr->continue_event_cb = continue_event_cb;
-	INIT_LIST_HEAD(&up_ptr->uport_list);
-	tipc_reg_add_port(up_ptr);
-	*portref = p_ptr->publ.ref;
-	tipc_port_unlock(p_ptr);
-	return 0;
-}
-
-int tipc_ownidentity(u32 ref, struct tipc_portid *id)
-{
-	id->ref = ref;
-	id->node = tipc_own_addr;
-	return 0;
-}
-
-int tipc_portimportance(u32 ref, unsigned int *importance)
-{
-	struct port *p_ptr;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-	*importance = (unsigned int)msg_importance(&p_ptr->publ.phdr);
-	tipc_port_unlock(p_ptr);
-	return 0;
-}
-
-int tipc_set_portimportance(u32 ref, unsigned int imp)
-{
-	struct port *p_ptr;
-
-	if (imp > TIPC_CRITICAL_IMPORTANCE)
-		return -EINVAL;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-	msg_set_importance(&p_ptr->publ.phdr, (u32)imp);
-	tipc_port_unlock(p_ptr);
-	return 0;
-}
-
-
-int tipc_publish(u32 ref, unsigned int scope, struct tipc_name_seq const *seq)
-{
-	struct port *p_ptr;
-	struct publication *publ;
-	u32 key;
-	int res = -EINVAL;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-
-	dbg("tipc_publ %u, p_ptr = %x, conn = %x, scope = %x, "
-	    "lower = %u, upper = %u\n",
-	    ref, p_ptr, p_ptr->publ.connected, scope, seq->lower, seq->upper);
-	if (p_ptr->publ.connected)
-		goto exit;
-	if (seq->lower > seq->upper)
-		goto exit;
-	if ((scope < TIPC_ZONE_SCOPE) || (scope > TIPC_NODE_SCOPE))
-		goto exit;
-	key = ref + p_ptr->pub_count + 1;
-	if (key == ref) {
-		res = -EADDRINUSE;
-		goto exit;
-	}
-	publ = tipc_nametbl_publish(seq->type, seq->lower, seq->upper,
-				    scope, p_ptr->publ.ref, key);
-	if (publ) {
-		list_add(&publ->pport_list, &p_ptr->publications);
-		p_ptr->pub_count++;
-		p_ptr->publ.published = 1;
-		res = 0;
-	}
-exit:
-	tipc_port_unlock(p_ptr);
-	return res;
-}
-
-int tipc_withdraw(u32 ref, unsigned int scope, struct tipc_name_seq const *seq)
-{
-	struct port *p_ptr;
-	struct publication *publ;
-	struct publication *tpubl;
-	int res = -EINVAL;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-	if (!seq) {
-		list_for_each_entry_safe(publ, tpubl,
-					 &p_ptr->publications, pport_list) {
-			tipc_nametbl_withdraw(publ->type, publ->lower,
-					      publ->ref, publ->key);
-		}
-		res = 0;
-	} else {
-		list_for_each_entry_safe(publ, tpubl,
-					 &p_ptr->publications, pport_list) {
-			if (publ->scope != scope)
-				continue;
-			if (publ->type != seq->type)
-				continue;
-			if (publ->lower != seq->lower)
-				continue;
-			if (publ->upper != seq->upper)
-				break;
-			tipc_nametbl_withdraw(publ->type, publ->lower,
-					      publ->ref, publ->key);
-			res = 0;
-			break;
-		}
-	}
-	if (list_empty(&p_ptr->publications))
-		p_ptr->publ.published = 0;
-	tipc_port_unlock(p_ptr);
-	return res;
-}
-
-int tipc_connect2port(u32 ref, struct tipc_portid const *peer)
-{
-	struct port *p_ptr;
-	struct tipc_msg *msg;
-	int res = -EINVAL;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-	if (p_ptr->publ.published || p_ptr->publ.connected)
-		goto exit;
-	if (!peer->ref)
-		goto exit;
-
-	msg = &p_ptr->publ.phdr;
-	msg_set_destnode(msg, peer->node);
-	msg_set_destport(msg, peer->ref);
-	msg_set_orignode(msg, tipc_own_addr);
-	msg_set_origport(msg, p_ptr->publ.ref);
-	msg_set_transp_seqno(msg, 42);
-	msg_set_type(msg, TIPC_CONN_MSG);
-	if (!may_route(peer->node))
-		msg_set_hdr_sz(msg, SHORT_H_SIZE);
-	else
-		msg_set_hdr_sz(msg, LONG_H_SIZE);
-
-	p_ptr->probing_interval = PROBING_INTERVAL;
-	p_ptr->probing_state = CONFIRMED;
-	p_ptr->publ.connected = 1;
-	k_start_timer(&p_ptr->timer, p_ptr->probing_interval);
-
-	tipc_nodesub_subscribe(&p_ptr->subscription,peer->node,
-			  (void *)(unsigned long)ref,
-			  (net_ev_handler)port_handle_node_down);
-	res = 0;
-exit:
-	tipc_port_unlock(p_ptr);
-	p_ptr->publ.max_pkt = tipc_link_get_max_pkt(peer->node, ref);
-	return res;
-}
-
-/**
- * tipc_disconnect_port - disconnect port from peer
- *
- * Port must be locked.
- */
-
-int tipc_disconnect_port(struct tipc_port *tp_ptr)
-{
-	int res;
-
-	if (tp_ptr->connected) {
-		tp_ptr->connected = 0;
-		/* let timer expire on it's own to avoid deadlock! */
-		tipc_nodesub_unsubscribe(
-			&((struct port *)tp_ptr)->subscription);
-		res = 0;
-	} else {
-		res = -ENOTCONN;
-	}
-	return res;
-}
-
-/*
- * tipc_disconnect(): Disconnect port form peer.
- *                    This is a node local operation.
- */
-
-int tipc_disconnect(u32 ref)
-{
-	struct port *p_ptr;
-	int res;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-	res = tipc_disconnect_port((struct tipc_port *)p_ptr);
-	tipc_port_unlock(p_ptr);
-	return res;
-}
-
-/*
- * tipc_shutdown(): Send a SHUTDOWN msg to peer and disconnect
- */
-int tipc_shutdown(u32 ref)
-{
-	struct port *p_ptr;
-	struct sk_buff *buf = NULL;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-
-	if (p_ptr->publ.connected) {
-		u32 imp = msg_importance(&p_ptr->publ.phdr);
-		if (imp < TIPC_CRITICAL_IMPORTANCE)
-			imp++;
-		buf = port_build_proto_msg(port_peerport(p_ptr),
-					   port_peernode(p_ptr),
-					   ref,
-					   tipc_own_addr,
-					   imp,
-					   TIPC_CONN_MSG,
-					   TIPC_CONN_SHUTDOWN,
-					   port_out_seqno(p_ptr),
-					   0);
-	}
-	tipc_port_unlock(p_ptr);
-	tipc_net_route_msg(buf);
-	return tipc_disconnect(ref);
-}
-
-int tipc_isconnected(u32 ref, int *isconnected)
-{
-	struct port *p_ptr;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-	*isconnected = p_ptr->publ.connected;
-	tipc_port_unlock(p_ptr);
-	return 0;
-}
-
-int tipc_peer(u32 ref, struct tipc_portid *peer)
-{
-	struct port *p_ptr;
-	int res;
-
-	p_ptr = tipc_port_lock(ref);
-	if (!p_ptr)
-		return -EINVAL;
-	if (p_ptr->publ.connected) {
-		peer->ref = port_peerport(p_ptr);
-		peer->node = port_peernode(p_ptr);
-		res = 0;
-	} else
-		res = -ENOTCONN;
-	tipc_port_unlock(p_ptr);
-	return res;
-}
-
-int tipc_ref_valid(u32 ref)
-{
-	/* Works irrespective of type */
-	return !!tipc_ref_deref(ref);
-}
-
-
-/*
- *  tipc_port_recv_sections(): Concatenate and deliver sectioned
- *                        message for this node.
- */
-
-int tipc_port_recv_sections(struct port *sender, unsigned int num_sect,
-		       struct iovec const *msg_sect)
-{
-	struct sk_buff *buf;
-	int res;
-
-	res = msg_build(&sender->publ.phdr, msg_sect, num_sect,
-			MAX_MSG_SIZE, !sender->user_port, &buf);
-	if (likely(buf))
-		tipc_port_recv_msg(buf);
-	return res;
-}
-
-/**
- * tipc_send - send message sections on connection
- */
-
-int tipc_send(u32 ref, unsigned int num_sect, struct iovec const *msg_sect)
-{
-	struct port *p_ptr;
-	u32 destnode;
-	int res;
-
-	p_ptr = tipc_port_deref(ref);
-	if (!p_ptr || !p_ptr->publ.connected)
-		return -EINVAL;
-
-	p_ptr->publ.congested = 1;
-	if (!tipc_port_congested(p_ptr)) {
-		destnode = port_peernode(p_ptr);
-		if (likely(destnode != tipc_own_addr))
-			res = tipc_link_send_sections_fast(p_ptr, msg_sect, num_sect,
-							   destnode);
-		else
-			res = tipc_port_recv_sections(p_ptr, num_sect, msg_sect);
-
-		if (likely(res != -ELINKCONG)) {
-			port_incr_out_seqno(p_ptr);
-			p_ptr->publ.congested = 0;
-			p_ptr->sent++;
-			return res;
-		}
-	}
-	if (port_unreliable(p_ptr)) {
-		p_ptr->publ.congested = 0;
-		/* Just calculate msg length and return */
-		return msg_calc_data_size(msg_sect, num_sect);
-	}
-	return -ELINKCONG;
-}
-
-/**
- * tipc_send_buf - send message buffer on connection
- */
-
-int tipc_send_buf(u32 ref, struct sk_buff *buf, unsigned int dsz)
-{
-	struct port *p_ptr;
-	struct tipc_msg *msg;
-	u32 destnode;
-	u32 hsz;
-	u32 sz;
-	u32 res;
-
-	p_ptr = tipc_port_deref(ref);
-	if (!p_ptr || !p_ptr->publ.connected)
-		return -EINVAL;
-
-	msg = &p_ptr->publ.phdr;
-	hsz = msg_hdr_sz(msg);
-	sz = hsz + dsz;
-	msg_set_size(msg, sz);
-	if (skb_cow(buf, hsz))
-		return -ENOMEM;
-
-	skb_push(buf, hsz);
-	skb_copy_to_linear_data(buf, msg, hsz);
-	destnode = msg_destnode(msg);
-	p_ptr->publ.congested = 1;
-	if (!tipc_port_congested(p_ptr)) {
-		if (likely(destnode != tipc_own_addr))
-			res = tipc_send_buf_fast(buf, destnode);
-		else {
-			tipc_port_recv_msg(buf);
-			res = sz;
-		}
-		if (likely(res != -ELINKCONG)) {
-			port_incr_out_seqno(p_ptr);
-			p_ptr->sent++;
-			p_ptr->publ.congested = 0;
-			return res;
-		}
-	}
-	if (port_unreliable(p_ptr)) {
-		p_ptr->publ.congested = 0;
-		return dsz;
-	}
-	return -ELINKCONG;
-}
-
-/**
- * tipc_forward2name - forward message sections to port name
- */
-
-int tipc_forward2name(u32 ref,
-		      struct tipc_name const *name,
-		      u32 domain,
-		      u32 num_sect,
-		      struct iovec const *msg_sect,
-		      struct tipc_portid const *orig,
-		      unsigned int importance)
-{
-	struct port *p_ptr;
-	struct tipc_msg *msg;
-	u32 destnode = domain;
-	u32 destport = 0;
-	int res;
-
-	p_ptr = tipc_port_deref(ref);
-	if (!p_ptr || p_ptr->publ.connected)
-		return -EINVAL;
-
-	msg = &p_ptr->publ.phdr;
-	msg_set_type(msg, TIPC_NAMED_MSG);
-	msg_set_orignode(msg, orig->node);
-	msg_set_origport(msg, orig->ref);
-	msg_set_hdr_sz(msg, LONG_H_SIZE);
-	msg_set_nametype(msg, name->type);
-	msg_set_nameinst(msg, name->instance);
-	msg_set_lookup_scope(msg, addr_scope(domain));
-	if (importance <= TIPC_CRITICAL_IMPORTANCE)
-		msg_set_importance(msg,importance);
-	destport = tipc_nametbl_translate(name->type, name->instance, &destnode);
-	msg_set_destnode(msg, destnode);
-	msg_set_destport(msg, destport);
-
-	if (likely(destport || destnode)) {
-		p_ptr->sent++;
-		if (likely(destnode == tipc_own_addr))
-			return tipc_port_recv_sections(p_ptr, num_sect, msg_sect);
-		res = tipc_link_send_sections_fast(p_ptr, msg_sect, num_sect,
-						   destnode);
-		if (likely(res != -ELINKCONG))
-			return res;
-		if (port_unreliable(p_ptr)) {
-			/* Just calculate msg length and return */
-			return msg_calc_data_size(msg_sect, num_sect);
-		}
-		return -ELINKCONG;
-	}
-	return tipc_port_reject_sections(p_ptr, msg, msg_sect, num_sect,
-					 TIPC_ERR_NO_NAME);
-}
-
-/**
- * tipc_send2name - send message sections to port name
- */
-
-int tipc_send2name(u32 ref,
-		   struct tipc_name const *name,
-		   unsigned int domain,
-		   unsigned int num_sect,
-		   struct iovec const *msg_sect)
-{
-	struct tipc_portid orig;
-
-	orig.ref = ref;
-	orig.node = tipc_own_addr;
-	return tipc_forward2name(ref, name, domain, num_sect, msg_sect, &orig,
-				 TIPC_PORT_IMPORTANCE);
-}
-
-/**
- * tipc_forward_buf2name - forward message buffer to port name
- */
-
-int tipc_forward_buf2name(u32 ref,
-			  struct tipc_name const *name,
-			  u32 domain,
-			  struct sk_buff *buf,
-			  unsigned int dsz,
-			  struct tipc_portid const *orig,
-			  unsigned int importance)
-{
-	struct port *p_ptr;
-	struct tipc_msg *msg;
-	u32 destnode = domain;
-	u32 destport = 0;
-	int res;
-
-	p_ptr = (struct port *)tipc_ref_deref(ref);
-	if (!p_ptr || p_ptr->publ.connected)
-		return -EINVAL;
-
-	msg = &p_ptr->publ.phdr;
-	if (importance <= TIPC_CRITICAL_IMPORTANCE)
-		msg_set_importance(msg, importance);
-	msg_set_type(msg, TIPC_NAMED_MSG);
-	msg_set_orignode(msg, orig->node);
-	msg_set_origport(msg, orig->ref);
-	msg_set_nametype(msg, name->type);
-	msg_set_nameinst(msg, name->instance);
-	msg_set_lookup_scope(msg, addr_scope(domain));
-	msg_set_hdr_sz(msg, LONG_H_SIZE);
-	msg_set_size(msg, LONG_H_SIZE + dsz);
-	destport = tipc_nametbl_translate(name->type, name->instance, &destnode);
-	msg_set_destnode(msg, destnode);
-	msg_set_destport(msg, destport);
-	msg_dbg(msg, "forw2name ==> ");
-	if (skb_cow(buf, LONG_H_SIZE))
-		return -ENOMEM;
-	skb_push(buf, LONG_H_SIZE);
-	skb_copy_to_linear_data(buf, msg, LONG_H_SIZE);
-	msg_dbg(buf_msg(buf),"PREP:");
-	if (likely(destport || destnode)) {
-		p_ptr->sent++;
-		if (destnode == tipc_own_addr)
-			return tipc_port_recv_msg(buf);
-		res = tipc_send_buf_fast(buf, destnode);
-		if (likely(res != -ELINKCONG))
-			return res;
-		if (port_unreliable(p_ptr))
-			return dsz;
-		return -ELINKCONG;
-	}
-	return tipc_reject_msg(buf, TIPC_ERR_NO_NAME);
-}
-
-/**
- * tipc_send_buf2name - send message buffer to port name
- */
-
-int tipc_send_buf2name(u32 ref,
-		       struct tipc_name const *dest,
-		       u32 domain,
-		       struct sk_buff *buf,
-		       unsigned int dsz)
-{
-	struct tipc_portid orig;
-
-	orig.ref = ref;
-	orig.node = tipc_own_addr;
-	return tipc_forward_buf2name(ref, dest, domain, buf, dsz, &orig,
-				     TIPC_PORT_IMPORTANCE);
-}
-
-/**
- * tipc_forward2port - forward message sections to port identity
- */
-
-int tipc_forward2port(u32 ref,
-		      struct tipc_portid const *dest,
-		      unsigned int num_sect,
-		      struct iovec const *msg_sect,
-		      struct tipc_portid const *orig,
-		      unsigned int importance)
-{
-	struct port *p_ptr;
-	struct tipc_msg *msg;
-	int res;
-
-	p_ptr = tipc_port_deref(ref);
-	if (!p_ptr || p_ptr->publ.connected)
-		return -EINVAL;
-
-	msg = &p_ptr->publ.phdr;
-	msg_set_type(msg, TIPC_DIRECT_MSG);
-	msg_set_orignode(msg, orig->node);
-	msg_set_origport(msg, orig->ref);
-	msg_set_destnode(msg, dest->node);
-	msg_set_destport(msg, dest->ref);
-	msg_set_hdr_sz(msg, DIR_MSG_H_SIZE);
-	if (importance <= TIPC_CRITICAL_IMPORTANCE)
-		msg_set_importance(msg, importance);
-	p_ptr->sent++;
-	if (dest->node == tipc_own_addr)
-		return tipc_port_recv_sections(p_ptr, num_sect, msg_sect);
-	res = tipc_link_send_sections_fast(p_ptr, msg_sect, num_sect, dest->node);
-	if (likely(res != -ELINKCONG))
-		return res;
-	if (port_unreliable(p_ptr)) {
-		/* Just calculate msg length and return */
-		return msg_calc_data_size(msg_sect, num_sect);
-	}
-	return -ELINKCONG;
-}
-
-/**
- * tipc_send2port - send message sections to port identity
- */
-
-int tipc_send2port(u32 ref,
-		   struct tipc_portid const *dest,
-		   unsigned int num_sect,
-		   struct iovec const *msg_sect)
-{
-	struct tipc_portid orig;
-
-	orig.ref = ref;
-	orig.node = tipc_own_addr;
-	return tipc_forward2port(ref, dest, num_sect, msg_sect, &orig,
-				 TIPC_PORT_IMPORTANCE);
-}
-
-/**
- * tipc_forward_buf2port - forward message buffer to port identity
- */
-int tipc_forward_buf2port(u32 ref,
-			  struct tipc_portid const *dest,
-			  struct sk_buff *buf,
-			  unsigned int dsz,
-			  struct tipc_portid const *orig,
-			  unsigned int importance)
-{
-	struct port *p_ptr;
-	struct tipc_msg *msg;
-	int res;
-
-	p_ptr = (struct port *)tipc_ref_deref(ref);
-	if (!p_ptr || p_ptr->publ.connected)
-		return -EINVAL;
-
-	msg = &p_ptr->publ.phdr;
-	msg_set_type(msg, TIPC_DIRECT_MSG);
-	msg_set_orignode(msg, orig->node);
-	msg_set_origport(msg, orig->ref);
-	msg_set_destnode(msg, dest->node);
-	msg_set_destport(msg, dest->ref);
-	msg_set_hdr_sz(msg, DIR_MSG_H_SIZE);
-	if (importance <= TIPC_CRITICAL_IMPORTANCE)
-		msg_set_importance(msg, importance);
-	msg_set_size(msg, DIR_MSG_H_SIZE + dsz);
-	if (skb_cow(buf, DIR_MSG_H_SIZE))
-		return -ENOMEM;
-
-	skb_push(buf, DIR_MSG_H_SIZE);
-	skb_copy_to_linear_data(buf, msg, DIR_MSG_H_SIZE);
-	msg_dbg(msg, "buf2port: ");
-	p_ptr->sent++;
-	if (dest->node == tipc_own_addr)
-		return tipc_port_recv_msg(buf);
-	res = tipc_send_buf_fast(buf, dest->node);
-	if (likely(res != -ELINKCONG))
-		return res;
-	if (port_unreliable(p_ptr))
-		return dsz;
-	return -ELINKCONG;
-}
-
-/**
- * tipc_send_buf2port - send message buffer to port identity
- */
-
-int tipc_send_buf2port(u32 ref,
-		       struct tipc_portid const *dest,
-		       struct sk_buff *buf,
-		       unsigned int dsz)
-{
-	struct tipc_portid orig;
-
-	orig.ref = ref;
-	orig.node = tipc_own_addr;
-	return tipc_forward_buf2port(ref, dest, buf, dsz, &orig,
-				     TIPC_PORT_IMPORTANCE);
-}
-
diff -ruN linux-2.6.29/net/tipc/port.h android_cluster/linux-2.6.29/net/tipc/port.h
--- linux-2.6.29/net/tipc/port.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/port.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,207 +0,0 @@
-/*
- * net/tipc/port.h: Include file for TIPC port code
- *
- * Copyright (c) 1994-2007, Ericsson AB
- * Copyright (c) 2004-2007, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_PORT_H
-#define _TIPC_PORT_H
-
-#include "core.h"
-#include "ref.h"
-#include "net.h"
-#include "msg.h"
-#include "dbg.h"
-#include "node_subscr.h"
-
-/**
- * struct user_port - TIPC user port (used with native API)
- * @user_ref: id of user who created user port
- * @usr_handle: user-specified field
- * @ref: object reference to associated TIPC port
- * <various callback routines>
- * @uport_list: adjacent user ports in list of ports held by user
- */
-
-struct user_port {
-	u32 user_ref;
-	void *usr_handle;
-	u32 ref;
-	tipc_msg_err_event err_cb;
-	tipc_named_msg_err_event named_err_cb;
-	tipc_conn_shutdown_event conn_err_cb;
-	tipc_msg_event msg_cb;
-	tipc_named_msg_event named_msg_cb;
-	tipc_conn_msg_event conn_msg_cb;
-	tipc_continue_event continue_event_cb;
-	struct list_head uport_list;
-};
-
-/**
- * struct port - TIPC port structure
- * @publ: TIPC port info available to privileged users
- * @port_list: adjacent ports in TIPC's global list of ports
- * @dispatcher: ptr to routine which handles received messages
- * @wakeup: ptr to routine to call when port is no longer congested
- * @user_port: ptr to user port associated with port (if any)
- * @wait_list: adjacent ports in list of ports waiting on link congestion
- * @congested_link: ptr to congested link port is waiting on
- * @waiting_pkts:
- * @sent:
- * @acked:
- * @publications: list of publications for port
- * @pub_count: total # of publications port has made during its lifetime
- * @probing_state:
- * @probing_interval:
- * @last_in_seqno:
- * @timer_ref:
- * @subscription: "node down" subscription used to terminate failed connections
- */
-
-struct port {
-	struct tipc_port publ;
-	struct list_head port_list;
-	u32 (*dispatcher)(struct tipc_port *, struct sk_buff *);
-	void (*wakeup)(struct tipc_port *);
-	struct user_port *user_port;
-	struct list_head wait_list;
-	struct link *congested_link;
-	u32 waiting_pkts;
-	u32 sent;
-	u32 acked;
-	struct list_head publications;
-	u32 pub_count;
-	u32 probing_state;
-	u32 probing_interval;
-	u32 last_in_seqno;
-	struct timer_list timer;
-	struct tipc_node_subscr subscription;
-};
-
-extern spinlock_t tipc_port_list_lock;
-struct port_list;
-
-int tipc_port_recv_sections(struct port *p_ptr, u32 num_sect,
-			    struct iovec const *msg_sect);
-int tipc_port_reject_sections(struct port *p_ptr, struct tipc_msg *hdr,
-			      struct iovec const *msg_sect, u32 num_sect,
-			      int err);
-struct sk_buff *tipc_port_get_ports(void);
-struct sk_buff *port_show_stats(const void *req_tlv_area, int req_tlv_space);
-void tipc_port_recv_proto_msg(struct sk_buff *buf);
-void tipc_port_recv_mcast(struct sk_buff *buf, struct port_list *dp);
-void tipc_port_reinit(void);
-
-/**
- * tipc_port_lock - lock port instance referred to and return its pointer
- */
-
-static inline struct port *tipc_port_lock(u32 ref)
-{
-	return (struct port *)tipc_ref_lock(ref);
-}
-
-/**
- * tipc_port_unlock - unlock a port instance
- *
- * Can use pointer instead of tipc_ref_unlock() since port is already locked.
- */
-
-static inline void tipc_port_unlock(struct port *p_ptr)
-{
-	spin_unlock_bh(p_ptr->publ.lock);
-}
-
-static inline struct port* tipc_port_deref(u32 ref)
-{
-	return (struct port *)tipc_ref_deref(ref);
-}
-
-static inline u32 tipc_peer_port(struct port *p_ptr)
-{
-	return msg_destport(&p_ptr->publ.phdr);
-}
-
-static inline u32 tipc_peer_node(struct port *p_ptr)
-{
-	return msg_destnode(&p_ptr->publ.phdr);
-}
-
-static inline int tipc_port_congested(struct port *p_ptr)
-{
-	return((p_ptr->sent - p_ptr->acked) >= (TIPC_FLOW_CONTROL_WIN * 2));
-}
-
-/**
- * tipc_port_recv_msg - receive message from lower layer and deliver to port user
- */
-
-static inline int tipc_port_recv_msg(struct sk_buff *buf)
-{
-	struct port *p_ptr;
-	struct tipc_msg *msg = buf_msg(buf);
-	u32 destport = msg_destport(msg);
-	u32 dsz = msg_data_sz(msg);
-	u32 err;
-
-	/* forward unresolved named message */
-	if (unlikely(!destport)) {
-		tipc_net_route_msg(buf);
-		return dsz;
-	}
-
-	/* validate destination & pass to port, otherwise reject message */
-	p_ptr = tipc_port_lock(destport);
-	if (likely(p_ptr)) {
-		if (likely(p_ptr->publ.connected)) {
-			if ((unlikely(msg_origport(msg) != tipc_peer_port(p_ptr))) ||
-			    (unlikely(msg_orignode(msg) != tipc_peer_node(p_ptr))) ||
-			    (unlikely(!msg_connected(msg)))) {
-				err = TIPC_ERR_NO_PORT;
-				tipc_port_unlock(p_ptr);
-				goto reject;
-			}
-		}
-		err = p_ptr->dispatcher(&p_ptr->publ, buf);
-		tipc_port_unlock(p_ptr);
-		if (likely(!err))
-			return dsz;
-	} else {
-		err = TIPC_ERR_NO_PORT;
-	}
-reject:
-	dbg("port->rejecting, err = %x..\n",err);
-	return tipc_reject_msg(buf, err);
-}
-
-#endif
diff -ruN linux-2.6.29/net/tipc/ref.c android_cluster/linux-2.6.29/net/tipc/ref.c
--- linux-2.6.29/net/tipc/ref.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/ref.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,313 +0,0 @@
-/*
- * net/tipc/ref.c: TIPC object registry code
- *
- * Copyright (c) 1991-2006, Ericsson AB
- * Copyright (c) 2004-2007, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "ref.h"
-
-/**
- * struct reference - TIPC object reference entry
- * @object: pointer to object associated with reference entry
- * @lock: spinlock controlling access to object
- * @ref: reference value for object (combines instance & array index info)
- */
-
-struct reference {
-	void *object;
-	spinlock_t lock;
-	u32 ref;
-};
-
-/**
- * struct tipc_ref_table - table of TIPC object reference entries
- * @entries: pointer to array of reference entries
- * @capacity: array index of first unusable entry
- * @init_point: array index of first uninitialized entry
- * @first_free: array index of first unused object reference entry
- * @last_free: array index of last unused object reference entry
- * @index_mask: bitmask for array index portion of reference values
- * @start_mask: initial value for instance value portion of reference values
- */
-
-struct ref_table {
-	struct reference *entries;
-	u32 capacity;
-	u32 init_point;
-	u32 first_free;
-	u32 last_free;
-	u32 index_mask;
-	u32 start_mask;
-};
-
-/*
- * Object reference table consists of 2**N entries.
- *
- * State	Object ptr	Reference
- * -----        ----------      ---------
- * In use        non-NULL       XXXX|own index
- *				(XXXX changes each time entry is acquired)
- * Free            NULL         YYYY|next free index
- *				(YYYY is one more than last used XXXX)
- * Uninitialized   NULL         0
- *
- * Entry 0 is not used; this allows index 0 to denote the end of the free list.
- *
- * Note that a reference value of 0 does not necessarily indicate that an
- * entry is uninitialized, since the last entry in the free list could also
- * have a reference value of 0 (although this is unlikely).
- */
-
-static struct ref_table tipc_ref_table = { NULL };
-
-static DEFINE_RWLOCK(ref_table_lock);
-
-/**
- * tipc_ref_table_init - create reference table for objects
- */
-
-int tipc_ref_table_init(u32 requested_size, u32 start)
-{
-	struct reference *table;
-	u32 actual_size;
-
-	/* account for unused entry, then round up size to a power of 2 */
-
-	requested_size++;
-	for (actual_size = 16; actual_size < requested_size; actual_size <<= 1)
-		/* do nothing */ ;
-
-	/* allocate table & mark all entries as uninitialized */
-
-	table = __vmalloc(actual_size * sizeof(struct reference),
-			  GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO, PAGE_KERNEL);
-	if (table == NULL)
-		return -ENOMEM;
-
-	tipc_ref_table.entries = table;
-	tipc_ref_table.capacity = requested_size;
-	tipc_ref_table.init_point = 1;
-	tipc_ref_table.first_free = 0;
-	tipc_ref_table.last_free = 0;
-	tipc_ref_table.index_mask = actual_size - 1;
-	tipc_ref_table.start_mask = start & ~tipc_ref_table.index_mask;
-
-	return 0;
-}
-
-/**
- * tipc_ref_table_stop - destroy reference table for objects
- */
-
-void tipc_ref_table_stop(void)
-{
-	if (!tipc_ref_table.entries)
-		return;
-
-	vfree(tipc_ref_table.entries);
-	tipc_ref_table.entries = NULL;
-}
-
-/**
- * tipc_ref_acquire - create reference to an object
- *
- * Register an object pointer in reference table and lock the object.
- * Returns a unique reference value that is used from then on to retrieve the
- * object pointer, or to determine that the object has been deregistered.
- *
- * Note: The object is returned in the locked state so that the caller can
- * register a partially initialized object, without running the risk that
- * the object will be accessed before initialization is complete.
- */
-
-u32 tipc_ref_acquire(void *object, spinlock_t **lock)
-{
-	struct reference *entry;
-	u32 index;
-	u32 index_mask;
-	u32 next_plus_upper;
-	u32 ref;
-
-	if (!object) {
-		err("Attempt to acquire reference to non-existent object\n");
-		return 0;
-	}
-	if (!tipc_ref_table.entries) {
-		err("Reference table not found during acquisition attempt\n");
-		return 0;
-	}
-
-	/* take a free entry, if available; otherwise initialize a new entry */
-
-	write_lock_bh(&ref_table_lock);
-	if (tipc_ref_table.first_free) {
-		index = tipc_ref_table.first_free;
-		entry = &(tipc_ref_table.entries[index]);
-		index_mask = tipc_ref_table.index_mask;
-		/* take lock in case a previous user of entry still holds it */
-		spin_lock_bh(&entry->lock);
-		next_plus_upper = entry->ref;
-		tipc_ref_table.first_free = next_plus_upper & index_mask;
-		ref = (next_plus_upper & ~index_mask) + index;
-		entry->ref = ref;
-		entry->object = object;
-		*lock = &entry->lock;
-	}
-	else if (tipc_ref_table.init_point < tipc_ref_table.capacity) {
-		index = tipc_ref_table.init_point++;
-		entry = &(tipc_ref_table.entries[index]);
-		spin_lock_init(&entry->lock);
-		spin_lock_bh(&entry->lock);
-		ref = tipc_ref_table.start_mask + index;
-		entry->ref = ref;
-		entry->object = object;
-		*lock = &entry->lock;
-	}
-	else {
-		ref = 0;
-	}
-	write_unlock_bh(&ref_table_lock);
-
-	return ref;
-}
-
-/**
- * tipc_ref_discard - invalidate references to an object
- *
- * Disallow future references to an object and free up the entry for re-use.
- * Note: The entry's spin_lock may still be busy after discard
- */
-
-void tipc_ref_discard(u32 ref)
-{
-	struct reference *entry;
-	u32 index;
-	u32 index_mask;
-
-	if (!tipc_ref_table.entries) {
-		err("Reference table not found during discard attempt\n");
-		return;
-	}
-
-	index_mask = tipc_ref_table.index_mask;
-	index = ref & index_mask;
-	entry = &(tipc_ref_table.entries[index]);
-
-	write_lock_bh(&ref_table_lock);
-
-	if (!entry->object) {
-		err("Attempt to discard reference to non-existent object\n");
-		goto exit;
-	}
-	if (entry->ref != ref) {
-		err("Attempt to discard non-existent reference\n");
-		goto exit;
-	}
-
-	/*
-	 * mark entry as unused; increment instance part of entry's reference
-	 * to invalidate any subsequent references
-	 */
-
-	entry->object = NULL;
-	entry->ref = (ref & ~index_mask) + (index_mask + 1);
-
-	/* append entry to free entry list */
-
-	if (tipc_ref_table.first_free == 0)
-		tipc_ref_table.first_free = index;
-	else
-		tipc_ref_table.entries[tipc_ref_table.last_free].ref |= index;
-	tipc_ref_table.last_free = index;
-
-exit:
-	write_unlock_bh(&ref_table_lock);
-}
-
-/**
- * tipc_ref_lock - lock referenced object and return pointer to it
- */
-
-void *tipc_ref_lock(u32 ref)
-{
-	if (likely(tipc_ref_table.entries)) {
-		struct reference *entry;
-
-		entry = &tipc_ref_table.entries[ref &
-						tipc_ref_table.index_mask];
-		if (likely(entry->ref != 0)) {
-			spin_lock_bh(&entry->lock);
-			if (likely((entry->ref == ref) && (entry->object)))
-				return entry->object;
-			spin_unlock_bh(&entry->lock);
-		}
-	}
-	return NULL;
-}
-
-/**
- * tipc_ref_unlock - unlock referenced object
- */
-
-void tipc_ref_unlock(u32 ref)
-{
-	if (likely(tipc_ref_table.entries)) {
-		struct reference *entry;
-
-		entry = &tipc_ref_table.entries[ref &
-						tipc_ref_table.index_mask];
-		if (likely((entry->ref == ref) && (entry->object)))
-			spin_unlock_bh(&entry->lock);
-		else
-			err("Attempt to unlock non-existent reference\n");
-	}
-}
-
-/**
- * tipc_ref_deref - return pointer referenced object (without locking it)
- */
-
-void *tipc_ref_deref(u32 ref)
-{
-	if (likely(tipc_ref_table.entries)) {
-		struct reference *entry;
-
-		entry = &tipc_ref_table.entries[ref &
-						tipc_ref_table.index_mask];
-		if (likely(entry->ref == ref))
-			return entry->object;
-	}
-	return NULL;
-}
-
diff -ruN linux-2.6.29/net/tipc/ref.h android_cluster/linux-2.6.29/net/tipc/ref.h
--- linux-2.6.29/net/tipc/ref.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/ref.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,50 +0,0 @@
-/*
- * net/tipc/ref.h: Include file for TIPC object registry code
- *
- * Copyright (c) 1991-2006, Ericsson AB
- * Copyright (c) 2005-2006, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_REF_H
-#define _TIPC_REF_H
-
-int tipc_ref_table_init(u32 requested_size, u32 start);
-void tipc_ref_table_stop(void);
-
-u32 tipc_ref_acquire(void *object, spinlock_t **lock);
-void tipc_ref_discard(u32 ref);
-
-void *tipc_ref_lock(u32 ref);
-void tipc_ref_unlock(u32 ref);
-void *tipc_ref_deref(u32 ref);
-
-#endif
diff -ruN linux-2.6.29/net/tipc/socket.c android_cluster/linux-2.6.29/net/tipc/socket.c
--- linux-2.6.29/net/tipc/socket.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/socket.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,1892 +0,0 @@
-/*
- * net/tipc/socket.c: TIPC socket API
- *
- * Copyright (c) 2001-2007, Ericsson AB
- * Copyright (c) 2004-2008, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include <linux/module.h>
-#include <linux/types.h>
-#include <linux/net.h>
-#include <linux/socket.h>
-#include <linux/errno.h>
-#include <linux/mm.h>
-#include <linux/slab.h>
-#include <linux/poll.h>
-#include <linux/fcntl.h>
-#include <asm/string.h>
-#include <asm/atomic.h>
-#include <net/sock.h>
-
-#include <linux/tipc.h>
-#include <linux/tipc_config.h>
-#include <net/tipc/tipc_msg.h>
-#include <net/tipc/tipc_port.h>
-
-#include "core.h"
-
-#define SS_LISTENING	-1	/* socket is listening */
-#define SS_READY	-2	/* socket is connectionless */
-
-#define OVERLOAD_LIMIT_BASE	5000
-#define CONN_TIMEOUT_DEFAULT	8000	/* default connect timeout = 8s */
-
-struct tipc_sock {
-	struct sock sk;
-	struct tipc_port *p;
-	struct tipc_portid peer_name;
-};
-
-#define tipc_sk(sk) ((struct tipc_sock *)(sk))
-#define tipc_sk_port(sk) ((struct tipc_port *)(tipc_sk(sk)->p))
-
-static int backlog_rcv(struct sock *sk, struct sk_buff *skb);
-static u32 dispatch(struct tipc_port *tport, struct sk_buff *buf);
-static void wakeupdispatch(struct tipc_port *tport);
-
-static const struct proto_ops packet_ops;
-static const struct proto_ops stream_ops;
-static const struct proto_ops msg_ops;
-
-static struct proto tipc_proto;
-
-static int sockets_enabled = 0;
-
-static atomic_t tipc_queue_size = ATOMIC_INIT(0);
-
-/*
- * Revised TIPC socket locking policy:
- *
- * Most socket operations take the standard socket lock when they start
- * and hold it until they finish (or until they need to sleep).  Acquiring
- * this lock grants the owner exclusive access to the fields of the socket
- * data structures, with the exception of the backlog queue.  A few socket
- * operations can be done without taking the socket lock because they only
- * read socket information that never changes during the life of the socket.
- *
- * Socket operations may acquire the lock for the associated TIPC port if they
- * need to perform an operation on the port.  If any routine needs to acquire
- * both the socket lock and the port lock it must take the socket lock first
- * to avoid the risk of deadlock.
- *
- * The dispatcher handling incoming messages cannot grab the socket lock in
- * the standard fashion, since invoked it runs at the BH level and cannot block.
- * Instead, it checks to see if the socket lock is currently owned by someone,
- * and either handles the message itself or adds it to the socket's backlog
- * queue; in the latter case the queued message is processed once the process
- * owning the socket lock releases it.
- *
- * NOTE: Releasing the socket lock while an operation is sleeping overcomes
- * the problem of a blocked socket operation preventing any other operations
- * from occurring.  However, applications must be careful if they have
- * multiple threads trying to send (or receive) on the same socket, as these
- * operations might interfere with each other.  For example, doing a connect
- * and a receive at the same time might allow the receive to consume the
- * ACK message meant for the connect.  While additional work could be done
- * to try and overcome this, it doesn't seem to be worthwhile at the present.
- *
- * NOTE: Releasing the socket lock while an operation is sleeping also ensures
- * that another operation that must be performed in a non-blocking manner is
- * not delayed for very long because the lock has already been taken.
- *
- * NOTE: This code assumes that certain fields of a port/socket pair are
- * constant over its lifetime; such fields can be examined without taking
- * the socket lock and/or port lock, and do not need to be re-read even
- * after resuming processing after waiting.  These fields include:
- *   - socket type
- *   - pointer to socket sk structure (aka tipc_sock structure)
- *   - pointer to port structure
- *   - port reference
- */
-
-/**
- * advance_rx_queue - discard first buffer in socket receive queue
- *
- * Caller must hold socket lock
- */
-
-static void advance_rx_queue(struct sock *sk)
-{
-	buf_discard(__skb_dequeue(&sk->sk_receive_queue));
-	atomic_dec(&tipc_queue_size);
-}
-
-/**
- * discard_rx_queue - discard all buffers in socket receive queue
- *
- * Caller must hold socket lock
- */
-
-static void discard_rx_queue(struct sock *sk)
-{
-	struct sk_buff *buf;
-
-	while ((buf = __skb_dequeue(&sk->sk_receive_queue))) {
-		atomic_dec(&tipc_queue_size);
-		buf_discard(buf);
-	}
-}
-
-/**
- * reject_rx_queue - reject all buffers in socket receive queue
- *
- * Caller must hold socket lock
- */
-
-static void reject_rx_queue(struct sock *sk)
-{
-	struct sk_buff *buf;
-
-	while ((buf = __skb_dequeue(&sk->sk_receive_queue))) {
-		tipc_reject_msg(buf, TIPC_ERR_NO_PORT);
-		atomic_dec(&tipc_queue_size);
-	}
-}
-
-/**
- * tipc_create - create a TIPC socket
- * @net: network namespace (must be default network)
- * @sock: pre-allocated socket structure
- * @protocol: protocol indicator (must be 0)
- *
- * This routine creates additional data structures used by the TIPC socket,
- * initializes them, and links them together.
- *
- * Returns 0 on success, errno otherwise
- */
-
-static int tipc_create(struct net *net, struct socket *sock, int protocol)
-{
-	const struct proto_ops *ops;
-	socket_state state;
-	struct sock *sk;
-	struct tipc_port *tp_ptr;
-
-	/* Validate arguments */
-
-	if (net != &init_net)
-		return -EAFNOSUPPORT;
-
-	if (unlikely(protocol != 0))
-		return -EPROTONOSUPPORT;
-
-	switch (sock->type) {
-	case SOCK_STREAM:
-		ops = &stream_ops;
-		state = SS_UNCONNECTED;
-		break;
-	case SOCK_SEQPACKET:
-		ops = &packet_ops;
-		state = SS_UNCONNECTED;
-		break;
-	case SOCK_DGRAM:
-	case SOCK_RDM:
-		ops = &msg_ops;
-		state = SS_READY;
-		break;
-	default:
-		return -EPROTOTYPE;
-	}
-
-	/* Allocate socket's protocol area */
-
-	sk = sk_alloc(net, AF_TIPC, GFP_KERNEL, &tipc_proto);
-	if (sk == NULL)
-		return -ENOMEM;
-
-	/* Allocate TIPC port for socket to use */
-
-	tp_ptr = tipc_createport_raw(sk, &dispatch, &wakeupdispatch,
-				     TIPC_LOW_IMPORTANCE);
-	if (unlikely(!tp_ptr)) {
-		sk_free(sk);
-		return -ENOMEM;
-	}
-
-	/* Finish initializing socket data structures */
-
-	sock->ops = ops;
-	sock->state = state;
-
-	sock_init_data(sock, sk);
-	sk->sk_rcvtimeo = msecs_to_jiffies(CONN_TIMEOUT_DEFAULT);
-	sk->sk_backlog_rcv = backlog_rcv;
-	tipc_sk(sk)->p = tp_ptr;
-
-	spin_unlock_bh(tp_ptr->lock);
-
-	if (sock->state == SS_READY) {
-		tipc_set_portunreturnable(tp_ptr->ref, 1);
-		if (sock->type == SOCK_DGRAM)
-			tipc_set_portunreliable(tp_ptr->ref, 1);
-	}
-
-	atomic_inc(&tipc_user_count);
-	return 0;
-}
-
-/**
- * release - destroy a TIPC socket
- * @sock: socket to destroy
- *
- * This routine cleans up any messages that are still queued on the socket.
- * For DGRAM and RDM socket types, all queued messages are rejected.
- * For SEQPACKET and STREAM socket types, the first message is rejected
- * and any others are discarded.  (If the first message on a STREAM socket
- * is partially-read, it is discarded and the next one is rejected instead.)
- *
- * NOTE: Rejected messages are not necessarily returned to the sender!  They
- * are returned or discarded according to the "destination droppable" setting
- * specified for the message by the sender.
- *
- * Returns 0 on success, errno otherwise
- */
-
-static int release(struct socket *sock)
-{
-	struct sock *sk = sock->sk;
-	struct tipc_port *tport;
-	struct sk_buff *buf;
-	int res;
-
-	/*
-	 * Exit if socket isn't fully initialized (occurs when a failed accept()
-	 * releases a pre-allocated child socket that was never used)
-	 */
-
-	if (sk == NULL)
-		return 0;
-
-	tport = tipc_sk_port(sk);
-	lock_sock(sk);
-
-	/*
-	 * Reject all unreceived messages, except on an active connection
-	 * (which disconnects locally & sends a 'FIN+' to peer)
-	 */
-
-	while (sock->state != SS_DISCONNECTING) {
-		buf = __skb_dequeue(&sk->sk_receive_queue);
-		if (buf == NULL)
-			break;
-		atomic_dec(&tipc_queue_size);
-		if (TIPC_SKB_CB(buf)->handle != msg_data(buf_msg(buf)))
-			buf_discard(buf);
-		else {
-			if ((sock->state == SS_CONNECTING) ||
-			    (sock->state == SS_CONNECTED)) {
-				sock->state = SS_DISCONNECTING;
-				tipc_disconnect(tport->ref);
-			}
-			tipc_reject_msg(buf, TIPC_ERR_NO_PORT);
-		}
-	}
-
-	/*
-	 * Delete TIPC port; this ensures no more messages are queued
-	 * (also disconnects an active connection & sends a 'FIN-' to peer)
-	 */
-
-	res = tipc_deleteport(tport->ref);
-
-	/* Discard any remaining (connection-based) messages in receive queue */
-
-	discard_rx_queue(sk);
-
-	/* Reject any messages that accumulated in backlog queue */
-
-	sock->state = SS_DISCONNECTING;
-	release_sock(sk);
-
-	sock_put(sk);
-	sock->sk = NULL;
-
-	atomic_dec(&tipc_user_count);
-	return res;
-}
-
-/**
- * bind - associate or disassocate TIPC name(s) with a socket
- * @sock: socket structure
- * @uaddr: socket address describing name(s) and desired operation
- * @uaddr_len: size of socket address data structure
- *
- * Name and name sequence binding is indicated using a positive scope value;
- * a negative scope value unbinds the specified name.  Specifying no name
- * (i.e. a socket address length of 0) unbinds all names from the socket.
- *
- * Returns 0 on success, errno otherwise
- *
- * NOTE: This routine doesn't need to take the socket lock since it doesn't
- *       access any non-constant socket information.
- */
-
-static int bind(struct socket *sock, struct sockaddr *uaddr, int uaddr_len)
-{
-	struct sockaddr_tipc *addr = (struct sockaddr_tipc *)uaddr;
-	u32 portref = tipc_sk_port(sock->sk)->ref;
-
-	if (unlikely(!uaddr_len))
-		return tipc_withdraw(portref, 0, NULL);
-
-	if (uaddr_len < sizeof(struct sockaddr_tipc))
-		return -EINVAL;
-	if (addr->family != AF_TIPC)
-		return -EAFNOSUPPORT;
-
-	if (addr->addrtype == TIPC_ADDR_NAME)
-		addr->addr.nameseq.upper = addr->addr.nameseq.lower;
-	else if (addr->addrtype != TIPC_ADDR_NAMESEQ)
-		return -EAFNOSUPPORT;
-
-	return (addr->scope > 0) ?
-		tipc_publish(portref, addr->scope, &addr->addr.nameseq) :
-		tipc_withdraw(portref, -addr->scope, &addr->addr.nameseq);
-}
-
-/**
- * get_name - get port ID of socket or peer socket
- * @sock: socket structure
- * @uaddr: area for returned socket address
- * @uaddr_len: area for returned length of socket address
- * @peer: 0 = own ID, 1 = current peer ID, 2 = current/former peer ID
- *
- * Returns 0 on success, errno otherwise
- *
- * NOTE: This routine doesn't need to take the socket lock since it only
- *       accesses socket information that is unchanging (or which changes in
- * 	 a completely predictable manner).
- */
-
-static int get_name(struct socket *sock, struct sockaddr *uaddr,
-		    int *uaddr_len, int peer)
-{
-	struct sockaddr_tipc *addr = (struct sockaddr_tipc *)uaddr;
-	struct tipc_sock *tsock = tipc_sk(sock->sk);
-
-	if (peer) {
-		if ((sock->state != SS_CONNECTED) &&
-			((peer != 2) || (sock->state != SS_DISCONNECTING)))
-			return -ENOTCONN;
-		addr->addr.id.ref = tsock->peer_name.ref;
-		addr->addr.id.node = tsock->peer_name.node;
-	} else {
-		tipc_ownidentity(tsock->p->ref, &addr->addr.id);
-	}
-
-	*uaddr_len = sizeof(*addr);
-	addr->addrtype = TIPC_ADDR_ID;
-	addr->family = AF_TIPC;
-	addr->scope = 0;
-	addr->addr.name.domain = 0;
-
-	return 0;
-}
-
-/**
- * poll - read and possibly block on pollmask
- * @file: file structure associated with the socket
- * @sock: socket for which to calculate the poll bits
- * @wait: ???
- *
- * Returns pollmask value
- *
- * COMMENTARY:
- * It appears that the usual socket locking mechanisms are not useful here
- * since the pollmask info is potentially out-of-date the moment this routine
- * exits.  TCP and other protocols seem to rely on higher level poll routines
- * to handle any preventable race conditions, so TIPC will do the same ...
- *
- * TIPC sets the returned events as follows:
- * a) POLLRDNORM and POLLIN are set if the socket's receive queue is non-empty
- *    or if a connection-oriented socket is does not have an active connection
- *    (i.e. a read operation will not block).
- * b) POLLOUT is set except when a socket's connection has been terminated
- *    (i.e. a write operation will not block).
- * c) POLLHUP is set when a socket's connection has been terminated.
- *
- * IMPORTANT: The fact that a read or write operation will not block does NOT
- * imply that the operation will succeed!
- */
-
-static unsigned int poll(struct file *file, struct socket *sock,
-			 poll_table *wait)
-{
-	struct sock *sk = sock->sk;
-	u32 mask;
-
-	poll_wait(file, sk->sk_sleep, wait);
-
-	if (!skb_queue_empty(&sk->sk_receive_queue) ||
-	    (sock->state == SS_UNCONNECTED) ||
-	    (sock->state == SS_DISCONNECTING))
-		mask = (POLLRDNORM | POLLIN);
-	else
-		mask = 0;
-
-	if (sock->state == SS_DISCONNECTING)
-		mask |= POLLHUP;
-	else
-		mask |= POLLOUT;
-
-	return mask;
-}
-
-/**
- * dest_name_check - verify user is permitted to send to specified port name
- * @dest: destination address
- * @m: descriptor for message to be sent
- *
- * Prevents restricted configuration commands from being issued by
- * unauthorized users.
- *
- * Returns 0 if permission is granted, otherwise errno
- */
-
-static int dest_name_check(struct sockaddr_tipc *dest, struct msghdr *m)
-{
-	struct tipc_cfg_msg_hdr hdr;
-
-	if (likely(dest->addr.name.name.type >= TIPC_RESERVED_TYPES))
-		return 0;
-	if (likely(dest->addr.name.name.type == TIPC_TOP_SRV))
-		return 0;
-	if (likely(dest->addr.name.name.type != TIPC_CFG_SRV))
-		return -EACCES;
-
-	if (copy_from_user(&hdr, m->msg_iov[0].iov_base, sizeof(hdr)))
-		return -EFAULT;
-	if ((ntohs(hdr.tcm_type) & 0xC000) && (!capable(CAP_NET_ADMIN)))
-		return -EACCES;
-
-	return 0;
-}
-
-/**
- * send_msg - send message in connectionless manner
- * @iocb: if NULL, indicates that socket lock is already held
- * @sock: socket structure
- * @m: message to send
- * @total_len: length of message
- *
- * Message must have an destination specified explicitly.
- * Used for SOCK_RDM and SOCK_DGRAM messages,
- * and for 'SYN' messages on SOCK_SEQPACKET and SOCK_STREAM connections.
- * (Note: 'SYN+' is prohibited on SOCK_STREAM.)
- *
- * Returns the number of bytes sent on success, or errno otherwise
- */
-
-static int send_msg(struct kiocb *iocb, struct socket *sock,
-		    struct msghdr *m, size_t total_len)
-{
-	struct sock *sk = sock->sk;
-	struct tipc_port *tport = tipc_sk_port(sk);
-	struct sockaddr_tipc *dest = (struct sockaddr_tipc *)m->msg_name;
-	int needs_conn;
-	int res = -EINVAL;
-
-	if (unlikely(!dest))
-		return -EDESTADDRREQ;
-	if (unlikely((m->msg_namelen < sizeof(*dest)) ||
-		     (dest->family != AF_TIPC)))
-		return -EINVAL;
-
-	if (iocb)
-		lock_sock(sk);
-
-	needs_conn = (sock->state != SS_READY);
-	if (unlikely(needs_conn)) {
-		if (sock->state == SS_LISTENING) {
-			res = -EPIPE;
-			goto exit;
-		}
-		if (sock->state != SS_UNCONNECTED) {
-			res = -EISCONN;
-			goto exit;
-		}
-		if ((tport->published) ||
-		    ((sock->type == SOCK_STREAM) && (total_len != 0))) {
-			res = -EOPNOTSUPP;
-			goto exit;
-		}
-		if (dest->addrtype == TIPC_ADDR_NAME) {
-			tport->conn_type = dest->addr.name.name.type;
-			tport->conn_instance = dest->addr.name.name.instance;
-		}
-
-		/* Abort any pending connection attempts (very unlikely) */
-
-		reject_rx_queue(sk);
-	}
-
-	do {
-		if (dest->addrtype == TIPC_ADDR_NAME) {
-			if ((res = dest_name_check(dest, m)))
-				break;
-			res = tipc_send2name(tport->ref,
-					     &dest->addr.name.name,
-					     dest->addr.name.domain,
-					     m->msg_iovlen,
-					     m->msg_iov);
-		}
-		else if (dest->addrtype == TIPC_ADDR_ID) {
-			res = tipc_send2port(tport->ref,
-					     &dest->addr.id,
-					     m->msg_iovlen,
-					     m->msg_iov);
-		}
-		else if (dest->addrtype == TIPC_ADDR_MCAST) {
-			if (needs_conn) {
-				res = -EOPNOTSUPP;
-				break;
-			}
-			if ((res = dest_name_check(dest, m)))
-				break;
-			res = tipc_multicast(tport->ref,
-					     &dest->addr.nameseq,
-					     0,
-					     m->msg_iovlen,
-					     m->msg_iov);
-		}
-		if (likely(res != -ELINKCONG)) {
-			if (needs_conn && (res >= 0)) {
-				sock->state = SS_CONNECTING;
-			}
-			break;
-		}
-		if (m->msg_flags & MSG_DONTWAIT) {
-			res = -EWOULDBLOCK;
-			break;
-		}
-		release_sock(sk);
-		res = wait_event_interruptible(*sk->sk_sleep,
-					       !tport->congested);
-		lock_sock(sk);
-		if (res)
-			break;
-	} while (1);
-
-exit:
-	if (iocb)
-		release_sock(sk);
-	return res;
-}
-
-/**
- * send_packet - send a connection-oriented message
- * @iocb: if NULL, indicates that socket lock is already held
- * @sock: socket structure
- * @m: message to send
- * @total_len: length of message
- *
- * Used for SOCK_SEQPACKET messages and SOCK_STREAM data.
- *
- * Returns the number of bytes sent on success, or errno otherwise
- */
-
-static int send_packet(struct kiocb *iocb, struct socket *sock,
-		       struct msghdr *m, size_t total_len)
-{
-	struct sock *sk = sock->sk;
-	struct tipc_port *tport = tipc_sk_port(sk);
-	struct sockaddr_tipc *dest = (struct sockaddr_tipc *)m->msg_name;
-	int res;
-
-	/* Handle implied connection establishment */
-
-	if (unlikely(dest))
-		return send_msg(iocb, sock, m, total_len);
-
-	if (iocb)
-		lock_sock(sk);
-
-	do {
-		if (unlikely(sock->state != SS_CONNECTED)) {
-			if (sock->state == SS_DISCONNECTING)
-				res = -EPIPE;
-			else
-				res = -ENOTCONN;
-			break;
-		}
-
-		res = tipc_send(tport->ref, m->msg_iovlen, m->msg_iov);
-		if (likely(res != -ELINKCONG)) {
-			break;
-		}
-		if (m->msg_flags & MSG_DONTWAIT) {
-			res = -EWOULDBLOCK;
-			break;
-		}
-		release_sock(sk);
-		res = wait_event_interruptible(*sk->sk_sleep,
-			(!tport->congested || !tport->connected));
-		lock_sock(sk);
-		if (res)
-			break;
-	} while (1);
-
-	if (iocb)
-		release_sock(sk);
-	return res;
-}
-
-/**
- * send_stream - send stream-oriented data
- * @iocb: (unused)
- * @sock: socket structure
- * @m: data to send
- * @total_len: total length of data to be sent
- *
- * Used for SOCK_STREAM data.
- *
- * Returns the number of bytes sent on success (or partial success),
- * or errno if no data sent
- */
-
-static int send_stream(struct kiocb *iocb, struct socket *sock,
-		       struct msghdr *m, size_t total_len)
-{
-	struct sock *sk = sock->sk;
-	struct tipc_port *tport = tipc_sk_port(sk);
-	struct msghdr my_msg;
-	struct iovec my_iov;
-	struct iovec *curr_iov;
-	int curr_iovlen;
-	char __user *curr_start;
-	u32 hdr_size;
-	int curr_left;
-	int bytes_to_send;
-	int bytes_sent;
-	int res;
-
-	lock_sock(sk);
-
-	/* Handle special cases where there is no connection */
-
-	if (unlikely(sock->state != SS_CONNECTED)) {
-		if (sock->state == SS_UNCONNECTED) {
-			res = send_packet(NULL, sock, m, total_len);
-			goto exit;
-		} else if (sock->state == SS_DISCONNECTING) {
-			res = -EPIPE;
-			goto exit;
-		} else {
-			res = -ENOTCONN;
-			goto exit;
-		}
-	}
-
-	if (unlikely(m->msg_name)) {
-		res = -EISCONN;
-		goto exit;
-	}
-
-	/*
-	 * Send each iovec entry using one or more messages
-	 *
-	 * Note: This algorithm is good for the most likely case
-	 * (i.e. one large iovec entry), but could be improved to pass sets
-	 * of small iovec entries into send_packet().
-	 */
-
-	curr_iov = m->msg_iov;
-	curr_iovlen = m->msg_iovlen;
-	my_msg.msg_iov = &my_iov;
-	my_msg.msg_iovlen = 1;
-	my_msg.msg_flags = m->msg_flags;
-	my_msg.msg_name = NULL;
-	bytes_sent = 0;
-
-	hdr_size = msg_hdr_sz(&tport->phdr);
-
-	while (curr_iovlen--) {
-		curr_start = curr_iov->iov_base;
-		curr_left = curr_iov->iov_len;
-
-		while (curr_left) {
-			bytes_to_send = tport->max_pkt - hdr_size;
-			if (bytes_to_send > TIPC_MAX_USER_MSG_SIZE)
-				bytes_to_send = TIPC_MAX_USER_MSG_SIZE;
-			if (curr_left < bytes_to_send)
-				bytes_to_send = curr_left;
-			my_iov.iov_base = curr_start;
-			my_iov.iov_len = bytes_to_send;
-			if ((res = send_packet(NULL, sock, &my_msg, 0)) < 0) {
-				if (bytes_sent)
-					res = bytes_sent;
-				goto exit;
-			}
-			curr_left -= bytes_to_send;
-			curr_start += bytes_to_send;
-			bytes_sent += bytes_to_send;
-		}
-
-		curr_iov++;
-	}
-	res = bytes_sent;
-exit:
-	release_sock(sk);
-	return res;
-}
-
-/**
- * auto_connect - complete connection setup to a remote port
- * @sock: socket structure
- * @msg: peer's response message
- *
- * Returns 0 on success, errno otherwise
- */
-
-static int auto_connect(struct socket *sock, struct tipc_msg *msg)
-{
-	struct tipc_sock *tsock = tipc_sk(sock->sk);
-
-	if (msg_errcode(msg)) {
-		sock->state = SS_DISCONNECTING;
-		return -ECONNREFUSED;
-	}
-
-	tsock->peer_name.ref = msg_origport(msg);
-	tsock->peer_name.node = msg_orignode(msg);
-	tipc_connect2port(tsock->p->ref, &tsock->peer_name);
-	tipc_set_portimportance(tsock->p->ref, msg_importance(msg));
-	sock->state = SS_CONNECTED;
-	return 0;
-}
-
-/**
- * set_orig_addr - capture sender's address for received message
- * @m: descriptor for message info
- * @msg: received message header
- *
- * Note: Address is not captured if not requested by receiver.
- */
-
-static void set_orig_addr(struct msghdr *m, struct tipc_msg *msg)
-{
-	struct sockaddr_tipc *addr = (struct sockaddr_tipc *)m->msg_name;
-
-	if (addr) {
-		addr->family = AF_TIPC;
-		addr->addrtype = TIPC_ADDR_ID;
-		addr->addr.id.ref = msg_origport(msg);
-		addr->addr.id.node = msg_orignode(msg);
-		addr->addr.name.domain = 0;   	/* could leave uninitialized */
-		addr->scope = 0;   		/* could leave uninitialized */
-		m->msg_namelen = sizeof(struct sockaddr_tipc);
-	}
-}
-
-/**
- * anc_data_recv - optionally capture ancillary data for received message
- * @m: descriptor for message info
- * @msg: received message header
- * @tport: TIPC port associated with message
- *
- * Note: Ancillary data is not captured if not requested by receiver.
- *
- * Returns 0 if successful, otherwise errno
- */
-
-static int anc_data_recv(struct msghdr *m, struct tipc_msg *msg,
-				struct tipc_port *tport)
-{
-	u32 anc_data[3];
-	u32 err;
-	u32 dest_type;
-	int has_name;
-	int res;
-
-	if (likely(m->msg_controllen == 0))
-		return 0;
-
-	/* Optionally capture errored message object(s) */
-
-	err = msg ? msg_errcode(msg) : 0;
-	if (unlikely(err)) {
-		anc_data[0] = err;
-		anc_data[1] = msg_data_sz(msg);
-		if ((res = put_cmsg(m, SOL_TIPC, TIPC_ERRINFO, 8, anc_data)))
-			return res;
-		if (anc_data[1] &&
-		    (res = put_cmsg(m, SOL_TIPC, TIPC_RETDATA, anc_data[1],
-				    msg_data(msg))))
-			return res;
-	}
-
-	/* Optionally capture message destination object */
-
-	dest_type = msg ? msg_type(msg) : TIPC_DIRECT_MSG;
-	switch (dest_type) {
-	case TIPC_NAMED_MSG:
-		has_name = 1;
-		anc_data[0] = msg_nametype(msg);
-		anc_data[1] = msg_namelower(msg);
-		anc_data[2] = msg_namelower(msg);
-		break;
-	case TIPC_MCAST_MSG:
-		has_name = 1;
-		anc_data[0] = msg_nametype(msg);
-		anc_data[1] = msg_namelower(msg);
-		anc_data[2] = msg_nameupper(msg);
-		break;
-	case TIPC_CONN_MSG:
-		has_name = (tport->conn_type != 0);
-		anc_data[0] = tport->conn_type;
-		anc_data[1] = tport->conn_instance;
-		anc_data[2] = tport->conn_instance;
-		break;
-	default:
-		has_name = 0;
-	}
-	if (has_name &&
-	    (res = put_cmsg(m, SOL_TIPC, TIPC_DESTNAME, 12, anc_data)))
-		return res;
-
-	return 0;
-}
-
-/**
- * recv_msg - receive packet-oriented message
- * @iocb: (unused)
- * @m: descriptor for message info
- * @buf_len: total size of user buffer area
- * @flags: receive flags
- *
- * Used for SOCK_DGRAM, SOCK_RDM, and SOCK_SEQPACKET messages.
- * If the complete message doesn't fit in user area, truncate it.
- *
- * Returns size of returned message data, errno otherwise
- */
-
-static int recv_msg(struct kiocb *iocb, struct socket *sock,
-		    struct msghdr *m, size_t buf_len, int flags)
-{
-	struct sock *sk = sock->sk;
-	struct tipc_port *tport = tipc_sk_port(sk);
-	struct sk_buff *buf;
-	struct tipc_msg *msg;
-	unsigned int sz;
-	u32 err;
-	int res;
-
-	/* Catch invalid receive requests */
-
-	if (m->msg_iovlen != 1)
-		return -EOPNOTSUPP;   /* Don't do multiple iovec entries yet */
-
-	if (unlikely(!buf_len))
-		return -EINVAL;
-
-	lock_sock(sk);
-
-	if (unlikely(sock->state == SS_UNCONNECTED)) {
-		res = -ENOTCONN;
-		goto exit;
-	}
-
-restart:
-
-	/* Look for a message in receive queue; wait if necessary */
-
-	while (skb_queue_empty(&sk->sk_receive_queue)) {
-		if (sock->state == SS_DISCONNECTING) {
-			res = -ENOTCONN;
-			goto exit;
-		}
-		if (flags & MSG_DONTWAIT) {
-			res = -EWOULDBLOCK;
-			goto exit;
-		}
-		release_sock(sk);
-		res = wait_event_interruptible(*sk->sk_sleep,
-			(!skb_queue_empty(&sk->sk_receive_queue) ||
-			 (sock->state == SS_DISCONNECTING)));
-		lock_sock(sk);
-		if (res)
-			goto exit;
-	}
-
-	/* Look at first message in receive queue */
-
-	buf = skb_peek(&sk->sk_receive_queue);
-	msg = buf_msg(buf);
-	sz = msg_data_sz(msg);
-	err = msg_errcode(msg);
-
-	/* Complete connection setup for an implied connect */
-
-	if (unlikely(sock->state == SS_CONNECTING)) {
-		res = auto_connect(sock, msg);
-		if (res)
-			goto exit;
-	}
-
-	/* Discard an empty non-errored message & try again */
-
-	if ((!sz) && (!err)) {
-		advance_rx_queue(sk);
-		goto restart;
-	}
-
-	/* Capture sender's address (optional) */
-
-	set_orig_addr(m, msg);
-
-	/* Capture ancillary data (optional) */
-
-	res = anc_data_recv(m, msg, tport);
-	if (res)
-		goto exit;
-
-	/* Capture message data (if valid) & compute return value (always) */
-
-	if (!err) {
-		if (unlikely(buf_len < sz)) {
-			sz = buf_len;
-			m->msg_flags |= MSG_TRUNC;
-		}
-		if (unlikely(copy_to_user(m->msg_iov->iov_base, msg_data(msg),
-					  sz))) {
-			res = -EFAULT;
-			goto exit;
-		}
-		res = sz;
-	} else {
-		if ((sock->state == SS_READY) ||
-		    ((err == TIPC_CONN_SHUTDOWN) || m->msg_control))
-			res = 0;
-		else
-			res = -ECONNRESET;
-	}
-
-	/* Consume received message (optional) */
-
-	if (likely(!(flags & MSG_PEEK))) {
-		if ((sock->state != SS_READY) &&
-		    (++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))
-			tipc_acknowledge(tport->ref, tport->conn_unacked);
-		advance_rx_queue(sk);
-	}
-exit:
-	release_sock(sk);
-	return res;
-}
-
-/**
- * recv_stream - receive stream-oriented data
- * @iocb: (unused)
- * @m: descriptor for message info
- * @buf_len: total size of user buffer area
- * @flags: receive flags
- *
- * Used for SOCK_STREAM messages only.  If not enough data is available
- * will optionally wait for more; never truncates data.
- *
- * Returns size of returned message data, errno otherwise
- */
-
-static int recv_stream(struct kiocb *iocb, struct socket *sock,
-		       struct msghdr *m, size_t buf_len, int flags)
-{
-	struct sock *sk = sock->sk;
-	struct tipc_port *tport = tipc_sk_port(sk);
-	struct sk_buff *buf;
-	struct tipc_msg *msg;
-	unsigned int sz;
-	int sz_to_copy;
-	int sz_copied = 0;
-	int needed;
-	char __user *crs = m->msg_iov->iov_base;
-	unsigned char *buf_crs;
-	u32 err;
-	int res = 0;
-
-	/* Catch invalid receive attempts */
-
-	if (m->msg_iovlen != 1)
-		return -EOPNOTSUPP;   /* Don't do multiple iovec entries yet */
-
-	if (unlikely(!buf_len))
-		return -EINVAL;
-
-	lock_sock(sk);
-
-	if (unlikely((sock->state == SS_UNCONNECTED) ||
-		     (sock->state == SS_CONNECTING))) {
-		res = -ENOTCONN;
-		goto exit;
-	}
-
-restart:
-
-	/* Look for a message in receive queue; wait if necessary */
-
-	while (skb_queue_empty(&sk->sk_receive_queue)) {
-		if (sock->state == SS_DISCONNECTING) {
-			res = -ENOTCONN;
-			goto exit;
-		}
-		if (flags & MSG_DONTWAIT) {
-			res = -EWOULDBLOCK;
-			goto exit;
-		}
-		release_sock(sk);
-		res = wait_event_interruptible(*sk->sk_sleep,
-			(!skb_queue_empty(&sk->sk_receive_queue) ||
-			 (sock->state == SS_DISCONNECTING)));
-		lock_sock(sk);
-		if (res)
-			goto exit;
-	}
-
-	/* Look at first message in receive queue */
-
-	buf = skb_peek(&sk->sk_receive_queue);
-	msg = buf_msg(buf);
-	sz = msg_data_sz(msg);
-	err = msg_errcode(msg);
-
-	/* Discard an empty non-errored message & try again */
-
-	if ((!sz) && (!err)) {
-		advance_rx_queue(sk);
-		goto restart;
-	}
-
-	/* Optionally capture sender's address & ancillary data of first msg */
-
-	if (sz_copied == 0) {
-		set_orig_addr(m, msg);
-		res = anc_data_recv(m, msg, tport);
-		if (res)
-			goto exit;
-	}
-
-	/* Capture message data (if valid) & compute return value (always) */
-
-	if (!err) {
-		buf_crs = (unsigned char *)(TIPC_SKB_CB(buf)->handle);
-		sz = (unsigned char *)msg + msg_size(msg) - buf_crs;
-
-		needed = (buf_len - sz_copied);
-		sz_to_copy = (sz <= needed) ? sz : needed;
-		if (unlikely(copy_to_user(crs, buf_crs, sz_to_copy))) {
-			res = -EFAULT;
-			goto exit;
-		}
-		sz_copied += sz_to_copy;
-
-		if (sz_to_copy < sz) {
-			if (!(flags & MSG_PEEK))
-				TIPC_SKB_CB(buf)->handle = buf_crs + sz_to_copy;
-			goto exit;
-		}
-
-		crs += sz_to_copy;
-	} else {
-		if (sz_copied != 0)
-			goto exit; /* can't add error msg to valid data */
-
-		if ((err == TIPC_CONN_SHUTDOWN) || m->msg_control)
-			res = 0;
-		else
-			res = -ECONNRESET;
-	}
-
-	/* Consume received message (optional) */
-
-	if (likely(!(flags & MSG_PEEK))) {
-		if (unlikely(++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))
-			tipc_acknowledge(tport->ref, tport->conn_unacked);
-		advance_rx_queue(sk);
-	}
-
-	/* Loop around if more data is required */
-
-	if ((sz_copied < buf_len)    /* didn't get all requested data */
-	    && (!skb_queue_empty(&sk->sk_receive_queue) ||
-		(flags & MSG_WAITALL))
-				     /* ... and more is ready or required */
-	    && (!(flags & MSG_PEEK)) /* ... and aren't just peeking at data */
-	    && (!err)                /* ... and haven't reached a FIN */
-	    )
-		goto restart;
-
-exit:
-	release_sock(sk);
-	return sz_copied ? sz_copied : res;
-}
-
-/**
- * rx_queue_full - determine if receive queue can accept another message
- * @msg: message to be added to queue
- * @queue_size: current size of queue
- * @base: nominal maximum size of queue
- *
- * Returns 1 if queue is unable to accept message, 0 otherwise
- */
-
-static int rx_queue_full(struct tipc_msg *msg, u32 queue_size, u32 base)
-{
-	u32 threshold;
-	u32 imp = msg_importance(msg);
-
-	if (imp == TIPC_LOW_IMPORTANCE)
-		threshold = base;
-	else if (imp == TIPC_MEDIUM_IMPORTANCE)
-		threshold = base * 2;
-	else if (imp == TIPC_HIGH_IMPORTANCE)
-		threshold = base * 100;
-	else
-		return 0;
-
-	if (msg_connected(msg))
-		threshold *= 4;
-
-	return (queue_size >= threshold);
-}
-
-/**
- * filter_rcv - validate incoming message
- * @sk: socket
- * @buf: message
- *
- * Enqueues message on receive queue if acceptable; optionally handles
- * disconnect indication for a connected socket.
- *
- * Called with socket lock already taken; port lock may also be taken.
- *
- * Returns TIPC error status code (TIPC_OK if message is not to be rejected)
- */
-
-static u32 filter_rcv(struct sock *sk, struct sk_buff *buf)
-{
-	struct socket *sock = sk->sk_socket;
-	struct tipc_msg *msg = buf_msg(buf);
-	u32 recv_q_len;
-
-	/* Reject message if it is wrong sort of message for socket */
-
-	/*
-	 * WOULD IT BE BETTER TO JUST DISCARD THESE MESSAGES INSTEAD?
-	 * "NO PORT" ISN'T REALLY THE RIGHT ERROR CODE, AND THERE MAY
-	 * BE SECURITY IMPLICATIONS INHERENT IN REJECTING INVALID TRAFFIC
-	 */
-
-	if (sock->state == SS_READY) {
-		if (msg_connected(msg)) {
-			msg_dbg(msg, "dispatch filter 1\n");
-			return TIPC_ERR_NO_PORT;
-		}
-	} else {
-		if (msg_mcast(msg)) {
-			msg_dbg(msg, "dispatch filter 2\n");
-			return TIPC_ERR_NO_PORT;
-		}
-		if (sock->state == SS_CONNECTED) {
-			if (!msg_connected(msg)) {
-				msg_dbg(msg, "dispatch filter 3\n");
-				return TIPC_ERR_NO_PORT;
-			}
-		}
-		else if (sock->state == SS_CONNECTING) {
-			if (!msg_connected(msg) && (msg_errcode(msg) == 0)) {
-				msg_dbg(msg, "dispatch filter 4\n");
-				return TIPC_ERR_NO_PORT;
-			}
-		}
-		else if (sock->state == SS_LISTENING) {
-			if (msg_connected(msg) || msg_errcode(msg)) {
-				msg_dbg(msg, "dispatch filter 5\n");
-				return TIPC_ERR_NO_PORT;
-			}
-		}
-		else if (sock->state == SS_DISCONNECTING) {
-			msg_dbg(msg, "dispatch filter 6\n");
-			return TIPC_ERR_NO_PORT;
-		}
-		else /* (sock->state == SS_UNCONNECTED) */ {
-			if (msg_connected(msg) || msg_errcode(msg)) {
-				msg_dbg(msg, "dispatch filter 7\n");
-				return TIPC_ERR_NO_PORT;
-			}
-		}
-	}
-
-	/* Reject message if there isn't room to queue it */
-
-	recv_q_len = (u32)atomic_read(&tipc_queue_size);
-	if (unlikely(recv_q_len >= OVERLOAD_LIMIT_BASE)) {
-		if (rx_queue_full(msg, recv_q_len, OVERLOAD_LIMIT_BASE))
-			return TIPC_ERR_OVERLOAD;
-	}
-	recv_q_len = skb_queue_len(&sk->sk_receive_queue);
-	if (unlikely(recv_q_len >= (OVERLOAD_LIMIT_BASE / 2))) {
-		if (rx_queue_full(msg, recv_q_len, OVERLOAD_LIMIT_BASE / 2))
-			return TIPC_ERR_OVERLOAD;
-	}
-
-	/* Enqueue message (finally!) */
-
-	msg_dbg(msg, "<DISP<: ");
-	TIPC_SKB_CB(buf)->handle = msg_data(msg);
-	atomic_inc(&tipc_queue_size);
-	__skb_queue_tail(&sk->sk_receive_queue, buf);
-
-	/* Initiate connection termination for an incoming 'FIN' */
-
-	if (unlikely(msg_errcode(msg) && (sock->state == SS_CONNECTED))) {
-		sock->state = SS_DISCONNECTING;
-		tipc_disconnect_port(tipc_sk_port(sk));
-	}
-
-	if (waitqueue_active(sk->sk_sleep))
-		wake_up_interruptible(sk->sk_sleep);
-	return TIPC_OK;
-}
-
-/**
- * backlog_rcv - handle incoming message from backlog queue
- * @sk: socket
- * @buf: message
- *
- * Caller must hold socket lock, but not port lock.
- *
- * Returns 0
- */
-
-static int backlog_rcv(struct sock *sk, struct sk_buff *buf)
-{
-	u32 res;
-
-	res = filter_rcv(sk, buf);
-	if (res)
-		tipc_reject_msg(buf, res);
-	return 0;
-}
-
-/**
- * dispatch - handle incoming message
- * @tport: TIPC port that received message
- * @buf: message
- *
- * Called with port lock already taken.
- *
- * Returns TIPC error status code (TIPC_OK if message is not to be rejected)
- */
-
-static u32 dispatch(struct tipc_port *tport, struct sk_buff *buf)
-{
-	struct sock *sk = (struct sock *)tport->usr_handle;
-	u32 res;
-
-	/*
-	 * Process message if socket is unlocked; otherwise add to backlog queue
-	 *
-	 * This code is based on sk_receive_skb(), but must be distinct from it
-	 * since a TIPC-specific filter/reject mechanism is utilized
-	 */
-
-	bh_lock_sock(sk);
-	if (!sock_owned_by_user(sk)) {
-		res = filter_rcv(sk, buf);
-	} else {
-		sk_add_backlog(sk, buf);
-		res = TIPC_OK;
-	}
-	bh_unlock_sock(sk);
-
-	return res;
-}
-
-/**
- * wakeupdispatch - wake up port after congestion
- * @tport: port to wakeup
- *
- * Called with port lock already taken.
- */
-
-static void wakeupdispatch(struct tipc_port *tport)
-{
-	struct sock *sk = (struct sock *)tport->usr_handle;
-
-	if (waitqueue_active(sk->sk_sleep))
-		wake_up_interruptible(sk->sk_sleep);
-}
-
-/**
- * connect - establish a connection to another TIPC port
- * @sock: socket structure
- * @dest: socket address for destination port
- * @destlen: size of socket address data structure
- * @flags: file-related flags associated with socket
- *
- * Returns 0 on success, errno otherwise
- */
-
-static int connect(struct socket *sock, struct sockaddr *dest, int destlen,
-		   int flags)
-{
-	struct sock *sk = sock->sk;
-	struct sockaddr_tipc *dst = (struct sockaddr_tipc *)dest;
-	struct msghdr m = {NULL,};
-	struct sk_buff *buf;
-	struct tipc_msg *msg;
-	int res;
-
-	lock_sock(sk);
-
-	/* For now, TIPC does not allow use of connect() with DGRAM/RDM types */
-
-	if (sock->state == SS_READY) {
-		res = -EOPNOTSUPP;
-		goto exit;
-	}
-
-	/* For now, TIPC does not support the non-blocking form of connect() */
-
-	if (flags & O_NONBLOCK) {
-		res = -EWOULDBLOCK;
-		goto exit;
-	}
-
-	/* Issue Posix-compliant error code if socket is in the wrong state */
-
-	if (sock->state == SS_LISTENING) {
-		res = -EOPNOTSUPP;
-		goto exit;
-	}
-	if (sock->state == SS_CONNECTING) {
-		res = -EALREADY;
-		goto exit;
-	}
-	if (sock->state != SS_UNCONNECTED) {
-		res = -EISCONN;
-		goto exit;
-	}
-
-	/*
-	 * Reject connection attempt using multicast address
-	 *
-	 * Note: send_msg() validates the rest of the address fields,
-	 *       so there's no need to do it here
-	 */
-
-	if (dst->addrtype == TIPC_ADDR_MCAST) {
-		res = -EINVAL;
-		goto exit;
-	}
-
-	/* Reject any messages already in receive queue (very unlikely) */
-
-	reject_rx_queue(sk);
-
-	/* Send a 'SYN-' to destination */
-
-	m.msg_name = dest;
-	m.msg_namelen = destlen;
-	res = send_msg(NULL, sock, &m, 0);
-	if (res < 0) {
-		goto exit;
-	}
-
-	/* Wait until an 'ACK' or 'RST' arrives, or a timeout occurs */
-
-	release_sock(sk);
-	res = wait_event_interruptible_timeout(*sk->sk_sleep,
-			(!skb_queue_empty(&sk->sk_receive_queue) ||
-			(sock->state != SS_CONNECTING)),
-			sk->sk_rcvtimeo);
-	lock_sock(sk);
-
-	if (res > 0) {
-		buf = skb_peek(&sk->sk_receive_queue);
-		if (buf != NULL) {
-			msg = buf_msg(buf);
-			res = auto_connect(sock, msg);
-			if (!res) {
-				if (!msg_data_sz(msg))
-					advance_rx_queue(sk);
-			}
-		} else {
-			if (sock->state == SS_CONNECTED) {
-				res = -EISCONN;
-			} else {
-				res = -ECONNREFUSED;
-			}
-		}
-	} else {
-		if (res == 0)
-			res = -ETIMEDOUT;
-		else
-			; /* leave "res" unchanged */
-		sock->state = SS_DISCONNECTING;
-	}
-
-exit:
-	release_sock(sk);
-	return res;
-}
-
-/**
- * listen - allow socket to listen for incoming connections
- * @sock: socket structure
- * @len: (unused)
- *
- * Returns 0 on success, errno otherwise
- */
-
-static int listen(struct socket *sock, int len)
-{
-	struct sock *sk = sock->sk;
-	int res;
-
-	lock_sock(sk);
-
-	if (sock->state == SS_READY)
-		res = -EOPNOTSUPP;
-	else if (sock->state != SS_UNCONNECTED)
-		res = -EINVAL;
-	else {
-		sock->state = SS_LISTENING;
-		res = 0;
-	}
-
-	release_sock(sk);
-	return res;
-}
-
-/**
- * accept - wait for connection request
- * @sock: listening socket
- * @newsock: new socket that is to be connected
- * @flags: file-related flags associated with socket
- *
- * Returns 0 on success, errno otherwise
- */
-
-static int accept(struct socket *sock, struct socket *new_sock, int flags)
-{
-	struct sock *sk = sock->sk;
-	struct sk_buff *buf;
-	int res;
-
-	lock_sock(sk);
-
-	if (sock->state == SS_READY) {
-		res = -EOPNOTSUPP;
-		goto exit;
-	}
-	if (sock->state != SS_LISTENING) {
-		res = -EINVAL;
-		goto exit;
-	}
-
-	while (skb_queue_empty(&sk->sk_receive_queue)) {
-		if (flags & O_NONBLOCK) {
-			res = -EWOULDBLOCK;
-			goto exit;
-		}
-		release_sock(sk);
-		res = wait_event_interruptible(*sk->sk_sleep,
-				(!skb_queue_empty(&sk->sk_receive_queue)));
-		lock_sock(sk);
-		if (res)
-			goto exit;
-	}
-
-	buf = skb_peek(&sk->sk_receive_queue);
-
-	res = tipc_create(sock_net(sock->sk), new_sock, 0);
-	if (!res) {
-		struct sock *new_sk = new_sock->sk;
-		struct tipc_sock *new_tsock = tipc_sk(new_sk);
-		struct tipc_port *new_tport = new_tsock->p;
-		u32 new_ref = new_tport->ref;
-		struct tipc_msg *msg = buf_msg(buf);
-
-		lock_sock(new_sk);
-
-		/*
-		 * Reject any stray messages received by new socket
-		 * before the socket lock was taken (very, very unlikely)
-		 */
-
-		reject_rx_queue(new_sk);
-
-		/* Connect new socket to it's peer */
-
-		new_tsock->peer_name.ref = msg_origport(msg);
-		new_tsock->peer_name.node = msg_orignode(msg);
-		tipc_connect2port(new_ref, &new_tsock->peer_name);
-		new_sock->state = SS_CONNECTED;
-
-		tipc_set_portimportance(new_ref, msg_importance(msg));
-		if (msg_named(msg)) {
-			new_tport->conn_type = msg_nametype(msg);
-			new_tport->conn_instance = msg_nameinst(msg);
-		}
-
-		/*
-		 * Respond to 'SYN-' by discarding it & returning 'ACK'-.
-		 * Respond to 'SYN+' by queuing it on new socket.
-		 */
-
-		msg_dbg(msg,"<ACC<: ");
-		if (!msg_data_sz(msg)) {
-			struct msghdr m = {NULL,};
-
-			advance_rx_queue(sk);
-			send_packet(NULL, new_sock, &m, 0);
-		} else {
-			__skb_dequeue(&sk->sk_receive_queue);
-			__skb_queue_head(&new_sk->sk_receive_queue, buf);
-		}
-		release_sock(new_sk);
-	}
-exit:
-	release_sock(sk);
-	return res;
-}
-
-/**
- * shutdown - shutdown socket connection
- * @sock: socket structure
- * @how: direction to close (must be SHUT_RDWR)
- *
- * Terminates connection (if necessary), then purges socket's receive queue.
- *
- * Returns 0 on success, errno otherwise
- */
-
-static int shutdown(struct socket *sock, int how)
-{
-	struct sock *sk = sock->sk;
-	struct tipc_port *tport = tipc_sk_port(sk);
-	struct sk_buff *buf;
-	int res;
-
-	if (how != SHUT_RDWR)
-		return -EINVAL;
-
-	lock_sock(sk);
-
-	switch (sock->state) {
-	case SS_CONNECTING:
-	case SS_CONNECTED:
-
-		/* Disconnect and send a 'FIN+' or 'FIN-' message to peer */
-restart:
-		buf = __skb_dequeue(&sk->sk_receive_queue);
-		if (buf) {
-			atomic_dec(&tipc_queue_size);
-			if (TIPC_SKB_CB(buf)->handle != msg_data(buf_msg(buf))) {
-				buf_discard(buf);
-				goto restart;
-			}
-			tipc_disconnect(tport->ref);
-			tipc_reject_msg(buf, TIPC_CONN_SHUTDOWN);
-		} else {
-			tipc_shutdown(tport->ref);
-		}
-
-		sock->state = SS_DISCONNECTING;
-
-		/* fall through */
-
-	case SS_DISCONNECTING:
-
-		/* Discard any unreceived messages; wake up sleeping tasks */
-
-		discard_rx_queue(sk);
-		if (waitqueue_active(sk->sk_sleep))
-			wake_up_interruptible(sk->sk_sleep);
-		res = 0;
-		break;
-
-	default:
-		res = -ENOTCONN;
-	}
-
-	release_sock(sk);
-	return res;
-}
-
-/**
- * setsockopt - set socket option
- * @sock: socket structure
- * @lvl: option level
- * @opt: option identifier
- * @ov: pointer to new option value
- * @ol: length of option value
- *
- * For stream sockets only, accepts and ignores all IPPROTO_TCP options
- * (to ease compatibility).
- *
- * Returns 0 on success, errno otherwise
- */
-
-static int setsockopt(struct socket *sock,
-		      int lvl, int opt, char __user *ov, int ol)
-{
-	struct sock *sk = sock->sk;
-	struct tipc_port *tport = tipc_sk_port(sk);
-	u32 value;
-	int res;
-
-	if ((lvl == IPPROTO_TCP) && (sock->type == SOCK_STREAM))
-		return 0;
-	if (lvl != SOL_TIPC)
-		return -ENOPROTOOPT;
-	if (ol < sizeof(value))
-		return -EINVAL;
-	if ((res = get_user(value, (u32 __user *)ov)))
-		return res;
-
-	lock_sock(sk);
-
-	switch (opt) {
-	case TIPC_IMPORTANCE:
-		res = tipc_set_portimportance(tport->ref, value);
-		break;
-	case TIPC_SRC_DROPPABLE:
-		if (sock->type != SOCK_STREAM)
-			res = tipc_set_portunreliable(tport->ref, value);
-		else
-			res = -ENOPROTOOPT;
-		break;
-	case TIPC_DEST_DROPPABLE:
-		res = tipc_set_portunreturnable(tport->ref, value);
-		break;
-	case TIPC_CONN_TIMEOUT:
-		sk->sk_rcvtimeo = msecs_to_jiffies(value);
-		/* no need to set "res", since already 0 at this point */
-		break;
-	default:
-		res = -EINVAL;
-	}
-
-	release_sock(sk);
-
-	return res;
-}
-
-/**
- * getsockopt - get socket option
- * @sock: socket structure
- * @lvl: option level
- * @opt: option identifier
- * @ov: receptacle for option value
- * @ol: receptacle for length of option value
- *
- * For stream sockets only, returns 0 length result for all IPPROTO_TCP options
- * (to ease compatibility).
- *
- * Returns 0 on success, errno otherwise
- */
-
-static int getsockopt(struct socket *sock,
-		      int lvl, int opt, char __user *ov, int __user *ol)
-{
-	struct sock *sk = sock->sk;
-	struct tipc_port *tport = tipc_sk_port(sk);
-	int len;
-	u32 value;
-	int res;
-
-	if ((lvl == IPPROTO_TCP) && (sock->type == SOCK_STREAM))
-		return put_user(0, ol);
-	if (lvl != SOL_TIPC)
-		return -ENOPROTOOPT;
-	if ((res = get_user(len, ol)))
-		return res;
-
-	lock_sock(sk);
-
-	switch (opt) {
-	case TIPC_IMPORTANCE:
-		res = tipc_portimportance(tport->ref, &value);
-		break;
-	case TIPC_SRC_DROPPABLE:
-		res = tipc_portunreliable(tport->ref, &value);
-		break;
-	case TIPC_DEST_DROPPABLE:
-		res = tipc_portunreturnable(tport->ref, &value);
-		break;
-	case TIPC_CONN_TIMEOUT:
-		value = jiffies_to_msecs(sk->sk_rcvtimeo);
-		/* no need to set "res", since already 0 at this point */
-		break;
-	default:
-		res = -EINVAL;
-	}
-
-	release_sock(sk);
-
-	if (res) {
-		/* "get" failed */
-	}
-	else if (len < sizeof(value)) {
-		res = -EINVAL;
-	}
-	else if (copy_to_user(ov, &value, sizeof(value))) {
-		res = -EFAULT;
-	}
-	else {
-		res = put_user(sizeof(value), ol);
-	}
-
-	return res;
-}
-
-/**
- * Protocol switches for the various types of TIPC sockets
- */
-
-static const struct proto_ops msg_ops = {
-	.owner 		= THIS_MODULE,
-	.family		= AF_TIPC,
-	.release	= release,
-	.bind		= bind,
-	.connect	= connect,
-	.socketpair	= sock_no_socketpair,
-	.accept		= accept,
-	.getname	= get_name,
-	.poll		= poll,
-	.ioctl		= sock_no_ioctl,
-	.listen		= listen,
-	.shutdown	= shutdown,
-	.setsockopt	= setsockopt,
-	.getsockopt	= getsockopt,
-	.sendmsg	= send_msg,
-	.recvmsg	= recv_msg,
-	.mmap		= sock_no_mmap,
-	.sendpage	= sock_no_sendpage
-};
-
-static const struct proto_ops packet_ops = {
-	.owner 		= THIS_MODULE,
-	.family		= AF_TIPC,
-	.release	= release,
-	.bind		= bind,
-	.connect	= connect,
-	.socketpair	= sock_no_socketpair,
-	.accept		= accept,
-	.getname	= get_name,
-	.poll		= poll,
-	.ioctl		= sock_no_ioctl,
-	.listen		= listen,
-	.shutdown	= shutdown,
-	.setsockopt	= setsockopt,
-	.getsockopt	= getsockopt,
-	.sendmsg	= send_packet,
-	.recvmsg	= recv_msg,
-	.mmap		= sock_no_mmap,
-	.sendpage	= sock_no_sendpage
-};
-
-static const struct proto_ops stream_ops = {
-	.owner 		= THIS_MODULE,
-	.family		= AF_TIPC,
-	.release	= release,
-	.bind		= bind,
-	.connect	= connect,
-	.socketpair	= sock_no_socketpair,
-	.accept		= accept,
-	.getname	= get_name,
-	.poll		= poll,
-	.ioctl		= sock_no_ioctl,
-	.listen		= listen,
-	.shutdown	= shutdown,
-	.setsockopt	= setsockopt,
-	.getsockopt	= getsockopt,
-	.sendmsg	= send_stream,
-	.recvmsg	= recv_stream,
-	.mmap		= sock_no_mmap,
-	.sendpage	= sock_no_sendpage
-};
-
-static const struct net_proto_family tipc_family_ops = {
-	.owner 		= THIS_MODULE,
-	.family		= AF_TIPC,
-	.create		= tipc_create
-};
-
-static struct proto tipc_proto = {
-	.name		= "TIPC",
-	.owner		= THIS_MODULE,
-	.obj_size	= sizeof(struct tipc_sock)
-};
-
-/**
- * tipc_socket_init - initialize TIPC socket interface
- *
- * Returns 0 on success, errno otherwise
- */
-int tipc_socket_init(void)
-{
-	int res;
-
-	res = proto_register(&tipc_proto, 1);
-	if (res) {
-		err("Failed to register TIPC protocol type\n");
-		goto out;
-	}
-
-	res = sock_register(&tipc_family_ops);
-	if (res) {
-		err("Failed to register TIPC socket type\n");
-		proto_unregister(&tipc_proto);
-		goto out;
-	}
-
-	sockets_enabled = 1;
- out:
-	return res;
-}
-
-/**
- * tipc_socket_stop - stop TIPC socket interface
- */
-
-void tipc_socket_stop(void)
-{
-	if (!sockets_enabled)
-		return;
-
-	sockets_enabled = 0;
-	sock_unregister(tipc_family_ops.family);
-	proto_unregister(&tipc_proto);
-}
-
diff -ruN linux-2.6.29/net/tipc/subscr.c android_cluster/linux-2.6.29/net/tipc/subscr.c
--- linux-2.6.29/net/tipc/subscr.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/subscr.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,617 +0,0 @@
-/*
- * net/tipc/subscr.c: TIPC network topology service
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2005-2007, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "dbg.h"
-#include "name_table.h"
-#include "port.h"
-#include "ref.h"
-#include "subscr.h"
-
-/**
- * struct subscriber - TIPC network topology subscriber
- * @port_ref: object reference to server port connecting to subscriber
- * @lock: pointer to spinlock controlling access to subscriber's server port
- * @subscriber_list: adjacent subscribers in top. server's list of subscribers
- * @subscription_list: list of subscription objects for this subscriber
- */
-
-struct subscriber {
-	u32 port_ref;
-	spinlock_t *lock;
-	struct list_head subscriber_list;
-	struct list_head subscription_list;
-};
-
-/**
- * struct top_srv - TIPC network topology subscription service
- * @user_ref: TIPC userid of subscription service
- * @setup_port: reference to TIPC port that handles subscription requests
- * @subscription_count: number of active subscriptions (not subscribers!)
- * @subscriber_list: list of ports subscribing to service
- * @lock: spinlock govering access to subscriber list
- */
-
-struct top_srv {
-	u32 user_ref;
-	u32 setup_port;
-	atomic_t subscription_count;
-	struct list_head subscriber_list;
-	spinlock_t lock;
-};
-
-static struct top_srv topsrv = { 0 };
-
-/**
- * htohl - convert value to endianness used by destination
- * @in: value to convert
- * @swap: non-zero if endianness must be reversed
- *
- * Returns converted value
- */
-
-static u32 htohl(u32 in, int swap)
-{
-	return swap ? swab32(in) : in;
-}
-
-/**
- * subscr_send_event - send a message containing a tipc_event to the subscriber
- *
- * Note: Must not hold subscriber's server port lock, since tipc_send() will
- *       try to take the lock if the message is rejected and returned!
- */
-
-static void subscr_send_event(struct subscription *sub,
-			      u32 found_lower,
-			      u32 found_upper,
-			      u32 event,
-			      u32 port_ref,
-			      u32 node)
-{
-	struct iovec msg_sect;
-
-	msg_sect.iov_base = (void *)&sub->evt;
-	msg_sect.iov_len = sizeof(struct tipc_event);
-
-	sub->evt.event = htohl(event, sub->swap);
-	sub->evt.found_lower = htohl(found_lower, sub->swap);
-	sub->evt.found_upper = htohl(found_upper, sub->swap);
-	sub->evt.port.ref = htohl(port_ref, sub->swap);
-	sub->evt.port.node = htohl(node, sub->swap);
-	tipc_send(sub->server_ref, 1, &msg_sect);
-}
-
-/**
- * tipc_subscr_overlap - test for subscription overlap with the given values
- *
- * Returns 1 if there is overlap, otherwise 0.
- */
-
-int tipc_subscr_overlap(struct subscription *sub,
-			u32 found_lower,
-			u32 found_upper)
-
-{
-	if (found_lower < sub->seq.lower)
-		found_lower = sub->seq.lower;
-	if (found_upper > sub->seq.upper)
-		found_upper = sub->seq.upper;
-	if (found_lower > found_upper)
-		return 0;
-	return 1;
-}
-
-/**
- * tipc_subscr_report_overlap - issue event if there is subscription overlap
- *
- * Protected by nameseq.lock in name_table.c
- */
-
-void tipc_subscr_report_overlap(struct subscription *sub,
-				u32 found_lower,
-				u32 found_upper,
-				u32 event,
-				u32 port_ref,
-				u32 node,
-				int must)
-{
-	if (!tipc_subscr_overlap(sub, found_lower, found_upper))
-		return;
-	if (!must && !(sub->filter & TIPC_SUB_PORTS))
-		return;
-
-	sub->event_cb(sub, found_lower, found_upper, event, port_ref, node);
-}
-
-/**
- * subscr_timeout - subscription timeout has occurred
- */
-
-static void subscr_timeout(struct subscription *sub)
-{
-	struct port *server_port;
-
-	/* Validate server port reference (in case subscriber is terminating) */
-
-	server_port = tipc_port_lock(sub->server_ref);
-	if (server_port == NULL)
-		return;
-
-	/* Validate timeout (in case subscription is being cancelled) */
-
-	if (sub->timeout == TIPC_WAIT_FOREVER) {
-		tipc_port_unlock(server_port);
-		return;
-	}
-
-	/* Unlink subscription from name table */
-
-	tipc_nametbl_unsubscribe(sub);
-
-	/* Unlink subscription from subscriber */
-
-	list_del(&sub->subscription_list);
-
-	/* Release subscriber's server port */
-
-	tipc_port_unlock(server_port);
-
-	/* Notify subscriber of timeout */
-
-	subscr_send_event(sub, sub->evt.s.seq.lower, sub->evt.s.seq.upper,
-			  TIPC_SUBSCR_TIMEOUT, 0, 0);
-
-	/* Now destroy subscription */
-
-	k_term_timer(&sub->timer);
-	kfree(sub);
-	atomic_dec(&topsrv.subscription_count);
-}
-
-/**
- * subscr_del - delete a subscription within a subscription list
- *
- * Called with subscriber port locked.
- */
-
-static void subscr_del(struct subscription *sub)
-{
-	tipc_nametbl_unsubscribe(sub);
-	list_del(&sub->subscription_list);
-	kfree(sub);
-	atomic_dec(&topsrv.subscription_count);
-}
-
-/**
- * subscr_terminate - terminate communication with a subscriber
- *
- * Called with subscriber port locked.  Routine must temporarily release lock
- * to enable subscription timeout routine(s) to finish without deadlocking;
- * the lock is then reclaimed to allow caller to release it upon return.
- * (This should work even in the unlikely event some other thread creates
- * a new object reference in the interim that uses this lock; this routine will
- * simply wait for it to be released, then claim it.)
- */
-
-static void subscr_terminate(struct subscriber *subscriber)
-{
-	u32 port_ref;
-	struct subscription *sub;
-	struct subscription *sub_temp;
-
-	/* Invalidate subscriber reference */
-
-	port_ref = subscriber->port_ref;
-	subscriber->port_ref = 0;
-	spin_unlock_bh(subscriber->lock);
-
-	/* Sever connection to subscriber */
-
-	tipc_shutdown(port_ref);
-	tipc_deleteport(port_ref);
-
-	/* Destroy any existing subscriptions for subscriber */
-
-	list_for_each_entry_safe(sub, sub_temp, &subscriber->subscription_list,
-				 subscription_list) {
-		if (sub->timeout != TIPC_WAIT_FOREVER) {
-			k_cancel_timer(&sub->timer);
-			k_term_timer(&sub->timer);
-		}
-		dbg("Term: Removing sub %u,%u,%u from subscriber %x list\n",
-		    sub->seq.type, sub->seq.lower, sub->seq.upper, subscriber);
-		subscr_del(sub);
-	}
-
-	/* Remove subscriber from topology server's subscriber list */
-
-	spin_lock_bh(&topsrv.lock);
-	list_del(&subscriber->subscriber_list);
-	spin_unlock_bh(&topsrv.lock);
-
-	/* Reclaim subscriber lock */
-
-	spin_lock_bh(subscriber->lock);
-
-	/* Now destroy subscriber */
-
-	kfree(subscriber);
-}
-
-/**
- * subscr_cancel - handle subscription cancellation request
- *
- * Called with subscriber port locked.  Routine must temporarily release lock
- * to enable the subscription timeout routine to finish without deadlocking;
- * the lock is then reclaimed to allow caller to release it upon return.
- *
- * Note that fields of 's' use subscriber's endianness!
- */
-
-static void subscr_cancel(struct tipc_subscr *s,
-			  struct subscriber *subscriber)
-{
-	struct subscription *sub;
-	struct subscription *sub_temp;
-	int found = 0;
-
-	/* Find first matching subscription, exit if not found */
-
-	list_for_each_entry_safe(sub, sub_temp, &subscriber->subscription_list,
-				 subscription_list) {
-		if (!memcmp(s, &sub->evt.s, sizeof(struct tipc_subscr))) {
-			found = 1;
-			break;
-		}
-	}
-	if (!found)
-		return;
-
-	/* Cancel subscription timer (if used), then delete subscription */
-
-	if (sub->timeout != TIPC_WAIT_FOREVER) {
-		sub->timeout = TIPC_WAIT_FOREVER;
-		spin_unlock_bh(subscriber->lock);
-		k_cancel_timer(&sub->timer);
-		k_term_timer(&sub->timer);
-		spin_lock_bh(subscriber->lock);
-	}
-	dbg("Cancel: removing sub %u,%u,%u from subscriber %x list\n",
-	    sub->seq.type, sub->seq.lower, sub->seq.upper, subscriber);
-	subscr_del(sub);
-}
-
-/**
- * subscr_subscribe - create subscription for subscriber
- *
- * Called with subscriber port locked.
- */
-
-static struct subscription *subscr_subscribe(struct tipc_subscr *s,
-					     struct subscriber *subscriber)
-{
-	struct subscription *sub;
-	int swap;
-
-	/* Determine subscriber's endianness */
-
-	swap = !(s->filter & (TIPC_SUB_PORTS | TIPC_SUB_SERVICE));
-
-	/* Detect & process a subscription cancellation request */
-
-	if (s->filter & htohl(TIPC_SUB_CANCEL, swap)) {
-		s->filter &= ~htohl(TIPC_SUB_CANCEL, swap);
-		subscr_cancel(s, subscriber);
-		return NULL;
-	}
-
-	/* Refuse subscription if global limit exceeded */
-
-	if (atomic_read(&topsrv.subscription_count) >= tipc_max_subscriptions) {
-		warn("Subscription rejected, subscription limit reached (%u)\n",
-		     tipc_max_subscriptions);
-		subscr_terminate(subscriber);
-		return NULL;
-	}
-
-	/* Allocate subscription object */
-
-	sub = kmalloc(sizeof(*sub), GFP_ATOMIC);
-	if (!sub) {
-		warn("Subscription rejected, no memory\n");
-		subscr_terminate(subscriber);
-		return NULL;
-	}
-
-	/* Initialize subscription object */
-
-	sub->seq.type = htohl(s->seq.type, swap);
-	sub->seq.lower = htohl(s->seq.lower, swap);
-	sub->seq.upper = htohl(s->seq.upper, swap);
-	sub->timeout = htohl(s->timeout, swap);
-	sub->filter = htohl(s->filter, swap);
-	if ((!(sub->filter & TIPC_SUB_PORTS)
-	     == !(sub->filter & TIPC_SUB_SERVICE))
-	    || (sub->seq.lower > sub->seq.upper)) {
-		warn("Subscription rejected, illegal request\n");
-		kfree(sub);
-		subscr_terminate(subscriber);
-		return NULL;
-	}
-	sub->event_cb = subscr_send_event;
-	INIT_LIST_HEAD(&sub->nameseq_list);
-	list_add(&sub->subscription_list, &subscriber->subscription_list);
-	sub->server_ref = subscriber->port_ref;
-	sub->swap = swap;
-	memcpy(&sub->evt.s, s, sizeof(struct tipc_subscr));
-	atomic_inc(&topsrv.subscription_count);
-	if (sub->timeout != TIPC_WAIT_FOREVER) {
-		k_init_timer(&sub->timer,
-			     (Handler)subscr_timeout, (unsigned long)sub);
-		k_start_timer(&sub->timer, sub->timeout);
-	}
-
-	return sub;
-}
-
-/**
- * subscr_conn_shutdown_event - handle termination request from subscriber
- *
- * Called with subscriber's server port unlocked.
- */
-
-static void subscr_conn_shutdown_event(void *usr_handle,
-				       u32 port_ref,
-				       struct sk_buff **buf,
-				       unsigned char const *data,
-				       unsigned int size,
-				       int reason)
-{
-	struct subscriber *subscriber = usr_handle;
-	spinlock_t *subscriber_lock;
-
-	if (tipc_port_lock(port_ref) == NULL)
-		return;
-
-	subscriber_lock = subscriber->lock;
-	subscr_terminate(subscriber);
-	spin_unlock_bh(subscriber_lock);
-}
-
-/**
- * subscr_conn_msg_event - handle new subscription request from subscriber
- *
- * Called with subscriber's server port unlocked.
- */
-
-static void subscr_conn_msg_event(void *usr_handle,
-				  u32 port_ref,
-				  struct sk_buff **buf,
-				  const unchar *data,
-				  u32 size)
-{
-	struct subscriber *subscriber = usr_handle;
-	spinlock_t *subscriber_lock;
-	struct subscription *sub;
-
-	/*
-	 * Lock subscriber's server port (& make a local copy of lock pointer,
-	 * in case subscriber is deleted while processing subscription request)
-	 */
-
-	if (tipc_port_lock(port_ref) == NULL)
-		return;
-
-	subscriber_lock = subscriber->lock;
-
-	if (size != sizeof(struct tipc_subscr)) {
-		subscr_terminate(subscriber);
-		spin_unlock_bh(subscriber_lock);
-	} else {
-		sub = subscr_subscribe((struct tipc_subscr *)data, subscriber);
-		spin_unlock_bh(subscriber_lock);
-		if (sub != NULL) {
-
-			/*
-			 * We must release the server port lock before adding a
-			 * subscription to the name table since TIPC needs to be
-			 * able to (re)acquire the port lock if an event message
-			 * issued by the subscription process is rejected and
-			 * returned.  The subscription cannot be deleted while
-			 * it is being added to the name table because:
-			 * a) the single-threading of the native API port code
-			 *    ensures the subscription cannot be cancelled and
-			 *    the subscriber connection cannot be broken, and
-			 * b) the name table lock ensures the subscription
-			 *    timeout code cannot delete the subscription,
-			 * so the subscription object is still protected.
-			 */
-
-			tipc_nametbl_subscribe(sub);
-		}
-	}
-}
-
-/**
- * subscr_named_msg_event - handle request to establish a new subscriber
- */
-
-static void subscr_named_msg_event(void *usr_handle,
-				   u32 port_ref,
-				   struct sk_buff **buf,
-				   const unchar *data,
-				   u32 size,
-				   u32 importance,
-				   struct tipc_portid const *orig,
-				   struct tipc_name_seq const *dest)
-{
-	static struct iovec msg_sect = {NULL, 0};
-
-	struct subscriber *subscriber;
-	u32 server_port_ref;
-
-	/* Create subscriber object */
-
-	subscriber = kzalloc(sizeof(struct subscriber), GFP_ATOMIC);
-	if (subscriber == NULL) {
-		warn("Subscriber rejected, no memory\n");
-		return;
-	}
-	INIT_LIST_HEAD(&subscriber->subscription_list);
-	INIT_LIST_HEAD(&subscriber->subscriber_list);
-
-	/* Create server port & establish connection to subscriber */
-
-	tipc_createport(topsrv.user_ref,
-			subscriber,
-			importance,
-			NULL,
-			NULL,
-			subscr_conn_shutdown_event,
-			NULL,
-			NULL,
-			subscr_conn_msg_event,
-			NULL,
-			&subscriber->port_ref);
-	if (subscriber->port_ref == 0) {
-		warn("Subscriber rejected, unable to create port\n");
-		kfree(subscriber);
-		return;
-	}
-	tipc_connect2port(subscriber->port_ref, orig);
-
-	/* Lock server port (& save lock address for future use) */
-
-	subscriber->lock = tipc_port_lock(subscriber->port_ref)->publ.lock;
-
-	/* Add subscriber to topology server's subscriber list */
-
-	spin_lock_bh(&topsrv.lock);
-	list_add(&subscriber->subscriber_list, &topsrv.subscriber_list);
-	spin_unlock_bh(&topsrv.lock);
-
-	/* Unlock server port */
-
-	server_port_ref = subscriber->port_ref;
-	spin_unlock_bh(subscriber->lock);
-
-	/* Send an ACK- to complete connection handshaking */
-
-	tipc_send(server_port_ref, 1, &msg_sect);
-
-	/* Handle optional subscription request */
-
-	if (size != 0) {
-		subscr_conn_msg_event(subscriber, server_port_ref,
-				      buf, data, size);
-	}
-}
-
-int tipc_subscr_start(void)
-{
-	struct tipc_name_seq seq = {TIPC_TOP_SRV, TIPC_TOP_SRV, TIPC_TOP_SRV};
-	int res = -1;
-
-	memset(&topsrv, 0, sizeof (topsrv));
-	spin_lock_init(&topsrv.lock);
-	INIT_LIST_HEAD(&topsrv.subscriber_list);
-
-	spin_lock_bh(&topsrv.lock);
-	res = tipc_attach(&topsrv.user_ref, NULL, NULL);
-	if (res) {
-		spin_unlock_bh(&topsrv.lock);
-		return res;
-	}
-
-	res = tipc_createport(topsrv.user_ref,
-			      NULL,
-			      TIPC_CRITICAL_IMPORTANCE,
-			      NULL,
-			      NULL,
-			      NULL,
-			      NULL,
-			      subscr_named_msg_event,
-			      NULL,
-			      NULL,
-			      &topsrv.setup_port);
-	if (res)
-		goto failed;
-
-	res = tipc_nametbl_publish_rsv(topsrv.setup_port, TIPC_NODE_SCOPE, &seq);
-	if (res)
-		goto failed;
-
-	spin_unlock_bh(&topsrv.lock);
-	return 0;
-
-failed:
-	err("Failed to create subscription service\n");
-	tipc_detach(topsrv.user_ref);
-	topsrv.user_ref = 0;
-	spin_unlock_bh(&topsrv.lock);
-	return res;
-}
-
-void tipc_subscr_stop(void)
-{
-	struct subscriber *subscriber;
-	struct subscriber *subscriber_temp;
-	spinlock_t *subscriber_lock;
-
-	if (topsrv.user_ref) {
-		tipc_deleteport(topsrv.setup_port);
-		list_for_each_entry_safe(subscriber, subscriber_temp,
-					 &topsrv.subscriber_list,
-					 subscriber_list) {
-			subscriber_lock = subscriber->lock;
-			spin_lock_bh(subscriber_lock);
-			subscr_terminate(subscriber);
-			spin_unlock_bh(subscriber_lock);
-		}
-		tipc_detach(topsrv.user_ref);
-		topsrv.user_ref = 0;
-	}
-}
-
-
-int tipc_ispublished(struct tipc_name const *name)
-{
-	u32 domain = 0;
-
-	return(tipc_nametbl_translate(name->type, name->instance,&domain) != 0);
-}
-
diff -ruN linux-2.6.29/net/tipc/subscr.h android_cluster/linux-2.6.29/net/tipc/subscr.h
--- linux-2.6.29/net/tipc/subscr.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/subscr.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,90 +0,0 @@
-/*
- * net/tipc/subscr.h: Include file for TIPC network topology service
- *
- * Copyright (c) 2003-2006, Ericsson AB
- * Copyright (c) 2005-2007, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_SUBSCR_H
-#define _TIPC_SUBSCR_H
-
-struct subscription;
-
-typedef void (*tipc_subscr_event) (struct subscription *sub,
-				   u32 found_lower, u32 found_upper,
-				   u32 event, u32 port_ref, u32 node);
-
-/**
- * struct subscription - TIPC network topology subscription object
- * @seq: name sequence associated with subscription
- * @timeout: duration of subscription (in ms)
- * @filter: event filtering to be done for subscription
- * @event_cb: routine invoked when a subscription event is detected
- * @timer: timer governing subscription duration (optional)
- * @nameseq_list: adjacent subscriptions in name sequence's subscription list
- * @subscription_list: adjacent subscriptions in subscriber's subscription list
- * @server_ref: object reference of server port associated with subscription
- * @swap: indicates if subscriber uses opposite endianness in its messages
- * @evt: template for events generated by subscription
- */
-
-struct subscription {
-	struct tipc_name_seq seq;
-	u32 timeout;
-	u32 filter;
-	tipc_subscr_event event_cb;
-	struct timer_list timer;
-	struct list_head nameseq_list;
-	struct list_head subscription_list;
-	u32 server_ref;
-	int swap;
-	struct tipc_event evt;
-};
-
-int tipc_subscr_overlap(struct subscription *sub,
-			u32 found_lower,
-			u32 found_upper);
-
-void tipc_subscr_report_overlap(struct subscription *sub,
-				u32 found_lower,
-				u32 found_upper,
-				u32 event,
-				u32 port_ref,
-				u32 node,
-				int must_report);
-
-int tipc_subscr_start(void);
-
-void tipc_subscr_stop(void);
-
-
-#endif
diff -ruN linux-2.6.29/net/tipc/tipc_addr.c android_cluster/linux-2.6.29/net/tipc/tipc_addr.c
--- linux-2.6.29/net/tipc/tipc_addr.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_addr.c	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,104 @@
+/*
+ * net/tipc/tipc_addr.c: TIPC address utility routines
+ *
+ * Copyright (c) 2000-2006, Ericsson AB
+ * Copyright (c) 2004-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_dbg.h"
+#include "tipc_addr.h"
+#include "tipc_net.h"
+
+u32 tipc_get_addr(void)
+{
+	return tipc_own_addr;
+}
+
+/**
+ * tipc_addr_domain_valid - validates a network domain address
+ *
+ * Accepts <Z.C.N>, <Z.C.0>, <Z.0.0>, and <0.0.0>, where Z, C, & N are non-zero.
+ * 
+ * Returns 1 if domain address is valid, otherwise 0
+ */
+
+int tipc_addr_domain_valid(u32 addr)
+{
+	u32 n = tipc_node(addr);
+	u32 c = tipc_cluster(addr);
+	u32 z = tipc_zone(addr);
+
+	if (n && (!z || !c))
+		return 0;
+	if (c && !z)
+		return 0;
+	return 1;
+}
+
+/**
+ * tipc_addr_node_valid - validates a proposed network address for this node
+ *
+ * Accepts <Z.C.N>, where Z, C, and N are non-zero.
+ * 
+ * Returns 1 if address can be used, otherwise 0
+ */
+
+int tipc_addr_node_valid(u32 addr)
+{
+	return (tipc_addr_domain_valid(addr) && tipc_node(addr));
+}
+
+/**
+ * tipc_in_scope - determines if network address lies within specified domain
+ */
+
+int tipc_in_scope(u32 domain, u32 addr)
+{
+        if (likely(domain == addr))
+                return 1;
+	if (domain == 0)
+		return 1;
+	if (domain == addr_cluster(addr)) /* domain <Z.C.0> */
+		return 1;
+	if (domain == addr_zone(addr)) /* domain <Z.0.0> */
+		return 1;
+	return 0;
+}
+
+char *tipc_addr_string_fill(char *string, u32 addr)
+{
+	snprintf(string, 16, "<%u.%u.%u>",
+		 tipc_zone(addr), tipc_cluster(addr), tipc_node(addr));
+	return string;
+}
+
diff -ruN linux-2.6.29/net/tipc/tipc_addr.h android_cluster/linux-2.6.29/net/tipc/tipc_addr.h
--- linux-2.6.29/net/tipc/tipc_addr.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_addr.h	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,141 @@
+/*
+ * net/tipc/tipc_addr.h: Include file for TIPC address utility routines
+ *
+ * Copyright (c) 2000-2006, Ericsson AB
+ * Copyright (c) 2004-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_ADDR_H
+#define _TIPC_ADDR_H
+
+
+static inline u32 addr_zone(u32 addr)
+{
+        return addr & 0xff000000u;
+}
+
+static inline u32 addr_cluster(u32 addr)
+{
+        return addr & 0xfffff000u;
+}
+
+static inline u32 own_node(void)
+{
+	return tipc_node(tipc_own_addr);
+}
+
+static inline u32 own_cluster(void)
+{
+	return tipc_cluster(tipc_own_addr);
+}
+
+static inline u32 own_zone(void)
+{
+	return tipc_zone(tipc_own_addr);
+}
+
+static inline int in_own_cluster(u32 addr)
+{
+	return !((addr ^ tipc_own_addr) >> 12);
+}
+
+static inline int in_own_zone(u32 addr)
+{
+	return !((addr ^ tipc_own_addr) >> 24);
+}
+
+static inline int is_slave(u32 addr)
+{
+	return addr & 0x800;
+}
+
+static inline int may_route(u32 addr)
+{
+	return(addr ^ tipc_own_addr) >> 11;
+}
+
+/**
+ * addr_in_node - test for node inclusion; <0.0.0> always matches
+ */
+
+static inline int addr_in_node(u32 addr)
+{
+	return (addr == tipc_own_addr) || !addr;
+}
+
+/**
+ * addr_in_node - test for cluster inclusion; <0.0.0> always matches
+ */
+
+static inline int addr_in_cluster(u32 addr)
+{
+	return in_own_cluster(addr) || !addr;
+}
+
+/**
+ * addr_scope - convert message lookup domain to equivalent 2-bit scope value
+ */
+
+static inline int addr_scope(u32 domain)
+{
+	if (likely(!domain))
+		return TIPC_ZONE_SCOPE;
+	if (tipc_node(domain))
+		return TIPC_NODE_SCOPE;
+	if (tipc_cluster(domain))
+		return TIPC_CLUSTER_SCOPE;
+	return TIPC_ZONE_SCOPE;
+}
+
+/**
+ * addr_domain - convert 2-bit scope value to equivalent message lookup domain
+ *
+ * Needed when address of a named message must be looked up a second time
+ * after a network hop.
+ */
+
+static inline int addr_domain(int sc)
+{
+	if (likely(sc == TIPC_NODE_SCOPE))
+		return tipc_own_addr;
+	if (sc == TIPC_CLUSTER_SCOPE)
+		return tipc_addr(tipc_zone(tipc_own_addr),
+				 tipc_cluster(tipc_own_addr), 0);
+	return tipc_addr(tipc_zone(tipc_own_addr), 0, 0);
+}
+
+int tipc_addr_domain_valid(u32);
+int tipc_addr_node_valid(u32 addr);
+int tipc_in_scope(u32 domain, u32 addr);
+char *tipc_addr_string_fill(char *string, u32 addr);
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_bcast.c android_cluster/linux-2.6.29/net/tipc/tipc_bcast.c
--- linux-2.6.29/net/tipc/tipc_bcast.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_bcast.c	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,946 @@
+/*
+ * net/tipc/tipc_bcast.c: TIPC broadcast code
+ *
+ * Copyright (c) 2004-2006, Ericsson AB
+ * Copyright (c) 2004, Intel Corporation.
+ * Copyright (c) 2005-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_msg.h"
+#include "tipc_dbg.h"
+#include "tipc_link.h"
+#include "tipc_net.h"
+#include "tipc_node.h"
+#include "tipc_port.h"
+#include "tipc_addr.h"
+#include "tipc_name_distr.h"
+#include "tipc_bearer.h"
+#include "tipc_name_table.h"
+#include "tipc_bcast.h"
+
+#define MAX_PKT_DEFAULT_MCAST 1500	/* bcast link max packet size (fixed) */
+
+#define BCLINK_WIN_DEFAULT 20		/* bcast link window size (default) */
+
+#define BCLINK_LOG_BUF_SIZE 0
+
+/*
+ * Loss rate for incoming broadcast frames; used to test retransmission code.
+ * Set to N to cause every N'th frame to be discarded; 0 => don't discard any.
+ */
+
+#define TIPC_BCAST_LOSS_RATE 0
+
+/**
+ * struct bcbearer_pair - a pair of bearers used by broadcast link
+ * @primary: pointer to primary bearer
+ * @secondary: pointer to secondary bearer
+ *
+ * Bearers must have same priority and same set of reachable destinations
+ * to be paired.
+ */
+
+struct bcbearer_pair {
+	struct bearer *primary;
+	struct bearer *secondary;
+};
+
+/**
+ * struct bcbearer - bearer used by broadcast link
+ * @bearer: (non-standard) broadcast bearer structure
+ * @media: (non-standard) broadcast media structure
+ * @bpairs: array of bearer pairs
+ * @bpairs_temp: temporary array of bearer pairs used by tipc_bcbearer_sort()
+ * @remains: temporary node map used by tipc_bcbearer_send()
+ * @remains_new: temporary node map used tipc_bcbearer_send()
+ *
+ * Note: The fields labelled "temporary" are incorporated into the bearer
+ * to avoid consuming potentially limited stack space through the use of
+ * large local variables within multicast routines.  Concurrent access is
+ * prevented through use of the spinlock "bc_lock".
+ */
+
+struct bcbearer {
+	struct bearer bearer;
+	struct tipc_media media;
+	struct bcbearer_pair bpairs[TIPC_MAX_BEARERS];
+	struct bcbearer_pair bpairs_temp[TIPC_MAX_LINK_PRI + 1];
+	struct tipc_node_map remains;
+	struct tipc_node_map remains_new;
+};
+
+/**
+ * struct bclink - link used for broadcast messages
+ * @link: (non-standard) broadcast link structure
+ * @node: (non-standard) node structure representing b'cast link's peer node
+ * @bcast_nodes: map of b'cast capable nodes in cluster
+ * 
+ * Handles sequence numbering, fragmentation, bundling, etc.
+ */
+
+struct bclink {
+	struct link link;
+	struct tipc_node node;
+	struct tipc_node_map bcast_nodes;
+};
+
+
+static struct bcbearer *bcbearer = NULL;
+static struct bclink *bclink = NULL;
+static struct link *bcl = NULL;
+static DEFINE_SPINLOCK(bc_lock);
+
+char tipc_bclink_name[] = "broadcast-link";
+
+
+static u32 bcbuf_acks(struct sk_buff *buf)
+{
+	return (u32)(unsigned long)buf_handle(buf);
+}
+
+static void bcbuf_set_acks(struct sk_buff *buf, u32 acks)
+{
+	buf_set_handle(buf, (void *)(unsigned long)acks);
+}
+
+static void bcbuf_decr_acks(struct sk_buff *buf)
+{
+	bcbuf_set_acks(buf, bcbuf_acks(buf) - 1);
+}
+
+void tipc_bclink_add_node(u32 addr)
+{
+	spin_lock_bh(&bc_lock);
+	tipc_nmap_add(&bclink->bcast_nodes, addr);
+	spin_unlock_bh(&bc_lock);
+}
+
+void tipc_bclink_remove_node(u32 addr)
+{
+	spin_lock_bh(&bc_lock);
+	tipc_nmap_remove(&bclink->bcast_nodes, addr);
+	spin_unlock_bh(&bc_lock);
+}
+
+static void bclink_set_last_sent(void)
+{
+	if (bcl->next_out)
+		bcl->fsm_msg_cnt = mod(buf_seqno(bcl->next_out) - 1);
+	else
+		bcl->fsm_msg_cnt = mod(bcl->next_out_no - 1);
+}
+
+u32 tipc_bclink_get_last_sent(void)
+{
+	return bcl->fsm_msg_cnt;
+}
+
+static void bclink_update_last_sent(struct tipc_node *node, u32 seqno)
+{
+	node->bclink.last_sent = greater(node->bclink.last_sent, seqno);
+}
+
+/**
+ * bclink_retransmit_pkt - retransmit broadcast packets
+ * @after: sequence number of last packet to *not* retransmit
+ * @to: sequence number of last packet to retransmit
+ *
+ * Called with bc_lock locked
+ */
+
+static void bclink_retransmit_pkt(u32 after, u32 to)
+{
+	struct sk_buff *buf;
+
+	buf = bcl->first_out;
+	while (buf && less_eq(buf_seqno(buf), after)) {
+		buf = buf->next;
+	}
+	tipc_link_retransmit(bcl, buf, mod(to - after));
+}
+
+/**
+ * tipc_bclink_acknowledge - handle acknowledgement of broadcast packets
+ * @n_ptr: node that sent acknowledgement info
+ * @acked: broadcast sequence # that has been acknowledged
+ *
+ * Node is locked, bc_lock unlocked.
+ */
+
+void tipc_bclink_acknowledge(struct tipc_node *n_ptr, u32 acked)
+{
+	struct sk_buff *crs;
+	struct sk_buff *next;
+	unsigned int released = 0;
+
+	if (less_eq(acked, n_ptr->bclink.acked))
+		return;
+
+	spin_lock_bh(&bc_lock);
+
+	/* Skip over packets that node has previously acknowledged */
+
+	crs = bcl->first_out;
+	while (crs && less_eq(buf_seqno(crs), n_ptr->bclink.acked)) {
+		crs = crs->next;
+	}
+
+	/* Update packets that node is now acknowledging */
+
+	while (crs && less_eq(buf_seqno(crs), acked)) {
+		next = crs->next;
+		bcbuf_decr_acks(crs);
+		if (bcbuf_acks(crs) == 0) {
+			bcl->first_out = next;
+			bcl->out_queue_size--;
+			buf_discard(crs);
+			released = 1;
+		}
+		crs = next;
+	}
+	n_ptr->bclink.acked = acked;
+
+	/* Try resolving broadcast link congestion, if necessary */
+
+	if (unlikely(bcl->next_out)) {
+		tipc_link_push_queue(bcl);
+		bclink_set_last_sent();
+	}
+	if (unlikely(released && !list_empty(&bcl->waiting_ports)))
+		tipc_link_wakeup_ports(bcl, 0);
+	spin_unlock_bh(&bc_lock);
+}
+
+/**
+ * tipc_bclink_update_link_state - update broadcast link state
+ *
+ * tipc_net_lock and node lock set
+ */
+
+void tipc_bclink_update_link_state(struct tipc_node *n_ptr, u32 last_sent)
+{
+	struct sk_buff *buf;
+
+	/* Ignore "stale" link state info */
+
+	if (less_eq(last_sent, n_ptr->bclink.last_in))
+		return;
+
+	/* Update link synchronization state; quit if in sync */
+
+	bclink_update_last_sent(n_ptr, last_sent);
+
+	if (n_ptr->bclink.last_sent == n_ptr->bclink.last_in)
+		return;
+ 
+	/* Update out-of-sync state; quit if loss is still unconfirmed */
+
+	if ((++n_ptr->bclink.oos_state) == 1) {
+		if (n_ptr->bclink.deferred_size < (TIPC_MIN_LINK_WIN / 2))
+		    return;
+		n_ptr->bclink.oos_state++;
+	}
+
+	/* Don't NACK if one has been recently sent (or seen) */
+
+	if (n_ptr->bclink.oos_state & 0x1)
+		return;
+
+	/* Send NACK */
+
+	buf = buf_acquire(INT_H_SIZE);
+	if (buf) {
+		struct tipc_msg *msg = buf_msg(buf);
+
+		tipc_msg_init(msg, BCAST_PROTOCOL, STATE_MSG,
+			      INT_H_SIZE, n_ptr->elm.addr);
+		msg_set_non_seq(msg, 1);
+		msg_set_mc_netid(msg, tipc_net_id);
+		msg_set_bcast_ack(msg, n_ptr->bclink.last_in); 
+		msg_set_bcgap_after(msg, n_ptr->bclink.last_in);
+		msg_set_bcgap_to(msg, n_ptr->bclink.deferred_head
+				 ? buf_seqno(n_ptr->bclink.deferred_head) - 1
+				 : n_ptr->bclink.last_sent);
+
+		tipc_bearer_send(&bcbearer->bearer, buf, NULL);
+		buf_discard(buf);
+
+		spin_lock_bh(&bc_lock);
+		bcl->stats.sent_nacks++;
+		spin_unlock_bh(&bc_lock);
+
+		n_ptr->bclink.oos_state++;
+	}
+}
+
+/**
+ * bclink_peek_nack - monitor retransmission requests sent by other nodes
+ *
+ * Delay any upcoming NACK by this node if another node has already
+ * requested the first message this node is going to ask for.
+ *
+ * Only tipc_net_lock set.
+ */
+
+static void bclink_peek_nack(struct tipc_msg *msg)
+{
+	struct tipc_node *n_ptr;
+
+	n_ptr = tipc_net_find_node(msg_destnode(msg));
+	if (unlikely(!n_ptr))
+		return;
+
+	tipc_node_lock(n_ptr);
+
+	if (tipc_node_is_up(n_ptr) && n_ptr->bclink.supported &&
+	    (n_ptr->bclink.last_in != n_ptr->bclink.last_sent) &&
+	    (n_ptr->bclink.last_in == msg_bcgap_after(msg)))
+		n_ptr->bclink.oos_state = 2;
+
+	tipc_node_unlock(n_ptr);
+}
+
+/**
+ * tipc_bclink_send_msg - broadcast a packet to all nodes in cluster
+ */
+
+int tipc_bclink_send_msg(struct sk_buff *buf)
+{
+	int res;
+
+	spin_lock_bh(&bc_lock);
+
+	if (!bclink->bcast_nodes.count) {
+		res = msg_data_sz(buf_msg(buf));
+		buf_discard(buf);
+		goto exit;
+	}
+
+	res = tipc_link_send_buf(bcl, buf);
+	if (unlikely(res == -ELINKCONG))
+		buf_discard(buf);
+	else
+		bclink_set_last_sent();
+
+	if (bcl->out_queue_size > bcl->stats.max_queue_sz)
+		bcl->stats.max_queue_sz = bcl->out_queue_size;
+	bcl->stats.queue_sz_counts++;
+	bcl->stats.accu_queue_sz += bcl->out_queue_size;
+exit:
+	spin_unlock_bh(&bc_lock);
+	return res;
+}
+
+/**
+ * tipc_bclink_recv_pkt - receive a broadcast packet, and deliver upwards
+ *
+ * tipc_net_lock is read_locked, no other locks set
+ */
+
+void tipc_bclink_recv_pkt(struct sk_buff *buf)
+{
+	struct tipc_msg *msg;
+	struct tipc_node *node;
+	u32 seqno;
+	u32 next_in;
+	int deferred;
+
+#if (TIPC_BCAST_LOSS_RATE)
+	static int rx_count = 0;
+
+	if (++rx_count == TIPC_BCAST_LOSS_RATE) {
+		rx_count = 0;
+		goto exit;
+	}
+#endif
+
+	/* Screen out unwanted broadcast messages */
+
+	msg = buf_msg(buf);
+	if (msg_mc_netid(msg) != tipc_net_id)
+		goto exit;
+	
+	node = tipc_net_find_node(msg_prevnode(msg));
+	if (unlikely(!node))
+		goto exit;
+
+	tipc_node_lock(node);
+	if (unlikely(!tipc_node_is_up(node) || !node->bclink.supported))
+		goto unlock;
+
+	/* Handle broadcast protocol message */
+
+	if (unlikely(msg_user(msg) == BCAST_PROTOCOL)) {
+		if (msg_destnode(msg) == tipc_own_addr) {
+			tipc_bclink_acknowledge(node, msg_bcast_ack(msg));
+			tipc_node_unlock(node);
+			spin_lock_bh(&bc_lock);
+			bcl->stats.recv_nacks++;
+			/* remember retransmit requester */
+			bcl->owner->node_list.next = 
+				(struct list_head *)node;
+			bclink_retransmit_pkt(msg_bcgap_after(msg),
+					      msg_bcgap_to(msg));
+			spin_unlock_bh(&bc_lock);
+		} else {
+			tipc_node_unlock(node);
+			bclink_peek_nack(msg);
+		}
+		goto exit;
+	}
+
+	/* Handle in-sequence broadcast message */
+
+	seqno = msg_seqno(msg);
+	next_in = mod(node->bclink.last_in + 1);
+
+	if (seqno == next_in) {
+		bclink_update_last_sent(node, seqno);
+receive:
+		node->bclink.last_in = seqno;
+		node->bclink.oos_state = 0;
+
+		spin_lock_bh(&bc_lock);
+		bcl->stats.recv_info++;
+
+		/*
+		 * Unicast an ACK periodically, ensuring that
+		 * all nodes in the cluster don't ACK at the same time
+		 */
+
+		if (((seqno - tipc_own_addr) % TIPC_MIN_LINK_WIN) == 0) {
+			tipc_link_send_proto_msg(
+				node->active_links[node->elm.addr & 1],
+				STATE_MSG, 0, 0, 0, 0, 0, 0);
+			bcl->stats.sent_acks++;
+		}
+
+		/* Deliver message to destination */
+
+		if (likely(msg_isdata(msg))) {
+			spin_unlock_bh(&bc_lock);
+			tipc_node_unlock(node);
+			tipc_port_recv_mcast(buf, NULL);
+		} else if (msg_user(msg) == MSG_BUNDLER) {
+			bcl->stats.recv_bundles++;
+			bcl->stats.recv_bundled += msg_msgcnt(msg);
+			spin_unlock_bh(&bc_lock);
+			tipc_node_unlock(node);
+			tipc_link_recv_bundle(buf);
+		} else if (msg_user(msg) == MSG_FRAGMENTER) {
+			bcl->stats.recv_fragments++;
+			if (tipc_link_recv_fragment(&node->bclink.defragm,
+						    &buf, &msg)) {
+				bcl->stats.recv_fragmented++;
+				msg_set_destnode_cache(msg, tipc_own_addr);
+			}
+			spin_unlock_bh(&bc_lock);
+			tipc_node_unlock(node);
+			tipc_net_route_msg(buf);
+		} else if (msg_user(msg) == NAME_DISTRIBUTOR) {
+			spin_unlock_bh(&bc_lock);
+			tipc_node_unlock(node);
+			tipc_named_recv(buf);
+		} else if (msg_user(msg) == ROUTE_DISTRIBUTOR) {
+			spin_unlock_bh(&bc_lock);
+			tipc_node_unlock(node);
+			tipc_route_recv(buf);
+		} else {
+			spin_unlock_bh(&bc_lock);
+			tipc_node_unlock(node);
+			tipc_net_route_msg(buf);
+		}
+
+		buf = NULL;
+
+		/* Determine new synchronization state */
+
+		tipc_node_lock(node);
+		if (unlikely(!tipc_node_is_up(node)))
+			goto unlock;
+
+		if (node->bclink.last_in == node->bclink.last_sent)
+			goto unlock;
+
+		if (!node->bclink.deferred_head) {
+			node->bclink.oos_state = 1;
+			goto unlock;
+		}
+
+		msg = buf_msg(node->bclink.deferred_head);
+		seqno = msg_seqno(msg);
+		next_in = mod(next_in + 1);
+		if (seqno != next_in)
+			goto unlock;
+
+		/* Take in-sequence message from deferred queue & deliver it */
+
+		buf = node->bclink.deferred_head;
+		node->bclink.deferred_head = buf->next;
+		node->bclink.deferred_size--;
+		goto receive;
+	}
+
+	/* Handle out-of-sequence broadcast message */
+
+	if (less(next_in, seqno)) {
+		deferred = tipc_link_defer_pkt(&node->bclink.deferred_head,
+					       &node->bclink.deferred_tail,
+					       buf, seqno);
+		node->bclink.deferred_size += deferred;
+		buf = NULL;
+		bclink_update_last_sent(node, seqno);
+	} else
+		deferred = 0;
+
+	spin_lock_bh(&bc_lock);
+	if (deferred)
+		bcl->stats.deferred_recv++;
+	else
+		bcl->stats.duplicates++;
+	spin_unlock_bh(&bc_lock);
+
+unlock:
+	tipc_node_unlock(node);
+exit:
+	buf_discard(buf);
+}
+
+u32 tipc_bclink_acks_missing(struct tipc_node *n_ptr)
+{
+	return (n_ptr->bclink.supported &&
+		(tipc_bclink_get_last_sent() != n_ptr->bclink.acked));
+}
+
+
+/**
+ * tipc_bcbearer_send - send a packet through the broadcast pseudo-bearer
+ *
+ * Send through as many bearers as necessary to reach all nodes
+ * that support TIPC multicasting.
+ *
+ * Returns 0 if packet sent successfully, non-zero if not
+ */
+
+static int tipc_bcbearer_send(struct sk_buff *buf,
+			      struct tipc_bearer *unused1,
+			      struct tipc_media_addr *unused2)
+{
+	int bp_index;
+
+	/*
+	 * Prepare broadcast link message for reliable transmission,
+	 * if first time trying to send it
+	 *
+	 * Note: Preparation is skipped for broadcast link protocol messages
+	 * since they are sent in an unreliable manner and don't need it
+	 */
+
+	if (likely(!msg_non_seq(buf_msg(buf)))) {
+		struct tipc_msg *msg;
+
+		assert(bclink->bcast_nodes.count != 0);
+		bcbuf_set_acks(buf, bclink->bcast_nodes.count);
+		msg = buf_msg(buf);
+		msg_set_non_seq(msg, 1);
+		msg_set_mc_netid(msg, tipc_net_id);
+		bcl->stats.sent_info++;
+	}
+
+	/* Send buffer over bearers until all targets reached */
+	
+	bcbearer->remains = bclink->bcast_nodes;
+
+	for (bp_index = 0; bp_index < TIPC_MAX_BEARERS; bp_index++) {
+		struct bearer *p = bcbearer->bpairs[bp_index].primary;
+		struct bearer *s = bcbearer->bpairs[bp_index].secondary;
+
+		if (!p)
+			break;	/* no more bearers to try */
+
+		tipc_nmap_diff(&bcbearer->remains, &p->nodes, &bcbearer->remains_new);
+		if (bcbearer->remains_new.count == bcbearer->remains.count)
+			continue;	/* bearer pair doesn't add anything */
+
+		if (p->publ.blocked ||
+		    p->media->send_msg(buf, &p->publ, &p->media->bcast_addr)) {
+			/* unable to send on primary bearer */
+			if (!s || s->publ.blocked ||
+			    s->media->send_msg(buf, &s->publ,
+					       &s->media->bcast_addr)) {
+				/* unable to send on either bearer */
+				continue;
+			}
+		}
+
+		if (s) {
+			bcbearer->bpairs[bp_index].primary = s;
+			bcbearer->bpairs[bp_index].secondary = p;
+		}
+
+		if (bcbearer->remains_new.count == 0)
+			return 0;
+
+		bcbearer->remains = bcbearer->remains_new;
+	}
+	
+	/* 
+	 * Unable to reach all targets (indicate success, since currently 
+	 * there isn't code in place to properly block & unblock the
+	 * pseudo-bearer used by the broadcast link)
+	 */
+	
+	return 0;
+}
+
+/**
+ * tipc_bcbearer_sort - create sets of bearer pairs used by broadcast bearer
+ */
+
+void tipc_bcbearer_sort(void)
+{
+	struct bcbearer_pair *bp_temp = bcbearer->bpairs_temp;
+	struct bcbearer_pair *bp_curr;
+	int b_index;
+	int pri;
+
+	spin_lock_bh(&bc_lock);
+
+	/* Group bearers by priority (can assume max of two per priority) */
+
+	memset(bp_temp, 0, sizeof(bcbearer->bpairs_temp));
+
+	for (b_index = 0; b_index < TIPC_MAX_BEARERS; b_index++) {
+		struct bearer *b = &tipc_bearers[b_index];
+
+		if (!b->active || !b->nodes.count)
+			continue;
+
+		if (!bp_temp[b->priority].primary)
+			bp_temp[b->priority].primary = b;
+		else
+			bp_temp[b->priority].secondary = b;
+	}
+
+	/* Create array of bearer pairs for broadcasting */
+
+	bp_curr = bcbearer->bpairs;
+	memset(bcbearer->bpairs, 0, sizeof(bcbearer->bpairs));
+
+	for (pri = TIPC_MAX_LINK_PRI; pri >= 0; pri--) {
+
+		if (!bp_temp[pri].primary)
+			continue;
+
+		bp_curr->primary = bp_temp[pri].primary;
+
+		if (bp_temp[pri].secondary) {
+			if (tipc_nmap_equal(&bp_temp[pri].primary->nodes,
+					    &bp_temp[pri].secondary->nodes)) {
+				bp_curr->secondary = bp_temp[pri].secondary;
+			} else {
+				bp_curr++;
+				bp_curr->primary = bp_temp[pri].secondary;
+			}
+		}
+
+		bp_curr++;
+	}
+
+	spin_unlock_bh(&bc_lock);
+}
+
+/**
+ * tipc_bcbearer_push - resolve bearer congestion
+ *
+ * Forces bclink to push out any unsent packets, until all packets are gone
+ * or congestion reoccurs.
+ * No locks set when function called
+ */
+
+void tipc_bcbearer_push(void)
+{
+	struct bearer *b_ptr;
+
+	spin_lock_bh(&bc_lock);
+	b_ptr = &bcbearer->bearer;
+	if (b_ptr->publ.blocked) {
+		b_ptr->publ.blocked = 0;
+		tipc_bearer_lock_push(b_ptr);
+	}
+	spin_unlock_bh(&bc_lock);
+}
+
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+int tipc_bclink_stats(char *buf, const u32 buf_size)
+{
+	struct print_buf pb;
+
+	if (!bcl)
+		return 0;
+
+	tipc_printbuf_init(&pb, buf, buf_size);
+
+	spin_lock_bh(&bc_lock);
+
+	tipc_printf(&pb, "Link <%s>\n"
+			 "  Window:%u packets\n",
+		    bcl->name, bcl->queue_limit[0]);
+	tipc_printf(&pb, "  RX packets:%u fragments:%u/%u bundles:%u/%u\n",
+		    bcl->stats.recv_info,
+		    bcl->stats.recv_fragments,
+		    bcl->stats.recv_fragmented,
+		    bcl->stats.recv_bundles,
+		    bcl->stats.recv_bundled);
+	tipc_printf(&pb, "  TX packets:%u fragments:%u/%u bundles:%u/%u\n",
+		    bcl->stats.sent_info,
+		    bcl->stats.sent_fragments,
+		    bcl->stats.sent_fragmented,
+		    bcl->stats.sent_bundles,
+		    bcl->stats.sent_bundled);
+	tipc_printf(&pb, "  RX naks:%u defs:%u dups:%u\n",
+		    bcl->stats.recv_nacks,
+		    bcl->stats.deferred_recv,
+		    bcl->stats.duplicates);
+	tipc_printf(&pb, "  TX naks:%u acks:%u dups:%u\n",
+		    bcl->stats.sent_nacks,
+		    bcl->stats.sent_acks,
+		    bcl->stats.retransmitted);
+	tipc_printf(&pb, "  Congestion bearer:%u link:%u  Send queue max:%u avg:%u\n",
+		    bcl->stats.bearer_congs,
+		    bcl->stats.link_congs,
+		    bcl->stats.max_queue_sz,
+		    bcl->stats.queue_sz_counts
+		    ? (bcl->stats.accu_queue_sz / bcl->stats.queue_sz_counts)
+		    : 0);
+
+	spin_unlock_bh(&bc_lock);
+	return tipc_printbuf_validate(&pb);
+}
+
+int tipc_bclink_reset_stats(void)
+{
+	if (!bcl)
+		return -ENOPROTOOPT;
+
+	spin_lock_bh(&bc_lock);
+	memset(&bcl->stats, 0, sizeof(bcl->stats));
+	spin_unlock_bh(&bc_lock);
+	return 0;
+}
+
+#endif
+
+int tipc_bclink_set_queue_limits(u32 limit)
+{
+	if (!bcl)
+		return -ENOPROTOOPT;
+	if ((limit < TIPC_MIN_LINK_WIN) || (limit > TIPC_MAX_LINK_WIN))
+		return -EINVAL;
+
+	spin_lock_bh(&bc_lock);
+	tipc_link_set_queue_limits(bcl, limit);
+	spin_unlock_bh(&bc_lock);
+	return 0;
+}
+
+int tipc_bclink_init(void)
+{
+	bcbearer = kzalloc(sizeof(*bcbearer), GFP_ATOMIC);
+	bclink = kzalloc(sizeof(*bclink), GFP_ATOMIC);
+	if (!bcbearer || !bclink) {
+ nomem:
+		warn("Broadcast link creation failed, no memory\n");
+		kfree(bcbearer);
+		bcbearer = NULL;
+		kfree(bclink);
+		bclink = NULL;
+		return -ENOMEM;
+	}
+
+	INIT_LIST_HEAD(&bcbearer->bearer.cong_links);
+	bcbearer->bearer.media = &bcbearer->media;
+	bcbearer->media.send_msg = tipc_bcbearer_send;
+	sprintf(bcbearer->media.name, "tipc-broadcast");
+
+	bcl = &bclink->link;
+	INIT_LIST_HEAD(&bcl->waiting_ports);
+	bcl->next_out_no = 1;
+	spin_lock_init(&bclink->node.elm.lock);
+	bcl->owner = &bclink->node;
+	bcl->max_pkt = MAX_PKT_DEFAULT_MCAST;
+	tipc_link_set_queue_limits(bcl, BCLINK_WIN_DEFAULT);
+	bcl->b_ptr = &bcbearer->bearer;
+	bcl->state = WORKING_WORKING;
+	sprintf(bcl->name, tipc_bclink_name);
+
+	if (BCLINK_LOG_BUF_SIZE) {
+		char *pb = kmalloc(BCLINK_LOG_BUF_SIZE, GFP_ATOMIC);
+
+		if (!pb)
+			goto nomem;
+		tipc_printbuf_init(&bcl->print_buf, pb, BCLINK_LOG_BUF_SIZE);
+	}
+
+	return 0;
+}
+
+void tipc_bclink_stop(void)
+{
+	spin_lock_bh(&bc_lock);
+	if (bcbearer) {
+		tipc_link_stop(bcl);
+		if (BCLINK_LOG_BUF_SIZE)
+			kfree(bcl->print_buf.buf);
+		bcl = NULL;
+		spin_lock_term(&bclink->node.elm.lock);
+		kfree(bclink);
+		bclink = NULL;
+		kfree(bcbearer);
+		bcbearer = NULL;
+	}
+	spin_unlock_bh(&bc_lock);
+}
+
+/**
+ * tipc_nmap_add - add a node to a node map
+ */
+
+void tipc_nmap_add(struct tipc_node_map *nm_ptr, u32 node)
+{
+	int n = tipc_node(node);
+	int w = n / WSIZE;
+	u32 mask = (1 << (n % WSIZE));
+
+	if ((nm_ptr->map[w] & mask) == 0) {
+		nm_ptr->count++;
+		nm_ptr->map[w] |= mask;
+	}
+}
+
+/** 
+ * tipc_nmap_remove - remove a node from a node map
+ */
+
+void tipc_nmap_remove(struct tipc_node_map *nm_ptr, u32 node)
+{
+	int n = tipc_node(node);
+	int w = n / WSIZE;
+	u32 mask = (1 << (n % WSIZE));
+
+	if ((nm_ptr->map[w] & mask) != 0) {
+		nm_ptr->map[w] &= ~mask;
+		nm_ptr->count--;
+	}
+}
+
+/**
+ * tipc_nmap_diff - find differences between node maps
+ * @nm_a: input node map A
+ * @nm_b: input node map B
+ * @nm_diff: output node map A-B (i.e. nodes of A that are not in B)
+ */
+
+void tipc_nmap_diff(struct tipc_node_map *nm_a, struct tipc_node_map *nm_b,
+		    struct tipc_node_map *nm_diff)
+{
+	int stop = sizeof(nm_a->map) / sizeof(u32);
+	int w;
+	int b;
+	u32 map;
+
+	memset(nm_diff, 0, sizeof(*nm_diff));
+	for (w = 0; w < stop; w++) {
+		map = nm_a->map[w] ^ (nm_a->map[w] & nm_b->map[w]);
+		nm_diff->map[w] = map;
+		if (map != 0) {
+			for (b = 0 ; b < WSIZE; b++) {
+				if (map & (1 << b))
+					nm_diff->count++;
+			}
+		}
+	}
+}
+
+/**
+ * tipc_port_list_add - add a port to a port list, ensuring no duplicates
+ */
+
+void tipc_port_list_add(struct port_list *pl_ptr, u32 port)
+{
+	struct port_list *item = pl_ptr;
+	int i;
+	int item_sz = PLSIZE;
+	int cnt = pl_ptr->count;
+
+	for (; ; cnt -= item_sz, item = item->next) {
+		if (cnt < PLSIZE)
+			item_sz = cnt;
+		for (i = 0; i < item_sz; i++)
+			if (item->ports[i] == port)
+				return;
+		if (i < PLSIZE) {
+			item->ports[i] = port;
+			pl_ptr->count++;
+			return;
+		}
+		if (!item->next) {
+			item->next = kmalloc(sizeof(*item), GFP_ATOMIC);
+			if (!item->next) {
+				warn("Incomplete multicast delivery, no memory\n");
+				return;
+			}
+			item->next->next = NULL;
+		}
+	}
+}
+
+/**
+ * tipc_port_list_free - free dynamically created entries in port_list chain
+ * 
+ * Note: First item is on stack, so it doesn't need to be released
+ */
+
+void tipc_port_list_free(struct port_list *pl_ptr)
+{
+	struct port_list *item;
+	struct port_list *next;
+
+	for (item = pl_ptr->next; item; item = next) {
+		next = item->next;
+		kfree(item);
+	}
+}
+
diff -ruN linux-2.6.29/net/tipc/tipc_bcast.h android_cluster/linux-2.6.29/net/tipc/tipc_bcast.h
--- linux-2.6.29/net/tipc/tipc_bcast.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_bcast.h	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,106 @@
+/*
+ * net/tipc/tipc_bcast.h: Include file for TIPC broadcast code
+ *
+ * Copyright (c) 2003-2006, Ericsson AB
+ * Copyright (c) 2005-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_BCAST_H
+#define _TIPC_BCAST_H
+
+#define MAX_NODES 4096
+#define WSIZE 32
+
+/**
+ * struct tipc_node_map - set of node identifiers
+ * @count: # of nodes in set
+ * @map: bitmap of node identifiers that are in the set
+ */
+
+struct tipc_node_map {
+	u32 count;
+	u32 map[MAX_NODES / WSIZE];
+};
+
+
+#define PLSIZE 32
+
+/**
+ * struct port_list - set of node local destination ports
+ * @count: # of ports in set (only valid for first entry in list)
+ * @next: pointer to next entry in list
+ * @ports: array of port references
+ */
+
+struct port_list {
+	int count;
+	struct port_list *next;
+	u32 ports[PLSIZE];
+};
+
+
+struct tipc_node;
+
+extern char tipc_bclink_name[];
+
+
+void tipc_nmap_add(struct tipc_node_map *nm_ptr, u32 node);
+void tipc_nmap_remove(struct tipc_node_map *nm_ptr, u32 node);
+void tipc_nmap_diff(struct tipc_node_map *nm_a, struct tipc_node_map *nm_b,
+		    struct tipc_node_map *nm_diff);
+
+static inline int tipc_nmap_equal(struct tipc_node_map *nm_a,
+				  struct tipc_node_map *nm_b)
+{
+	return !memcmp(nm_a, nm_b, sizeof(*nm_a));
+}
+
+void tipc_port_list_add(struct port_list *pl_ptr, u32 port);
+void tipc_port_list_free(struct port_list *pl_ptr);
+
+int  tipc_bclink_init(void);
+void tipc_bclink_stop(void);
+void tipc_bclink_add_node(u32 addr);
+void tipc_bclink_remove_node(u32 addr);
+void tipc_bclink_acknowledge(struct tipc_node *n_ptr, u32 acked);
+int  tipc_bclink_send_msg(struct sk_buff *buf);
+void tipc_bclink_recv_pkt(struct sk_buff *buf);
+u32  tipc_bclink_get_last_sent(void);
+u32  tipc_bclink_acks_missing(struct tipc_node *n_ptr);
+void tipc_bclink_update_link_state(struct tipc_node *n_ptr, u32 last_sent);
+int  tipc_bclink_stats(char *stats_buf, const u32 buf_size);
+int  tipc_bclink_reset_stats(void);
+int  tipc_bclink_set_queue_limits(u32 limit);
+void tipc_bcbearer_sort(void);
+void tipc_bcbearer_push(void);
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_bearer.c android_cluster/linux-2.6.29/net/tipc/tipc_bearer.c
--- linux-2.6.29/net/tipc/tipc_bearer.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_bearer.c	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,775 @@
+/*
+ * net/tipc/tipc_bearer.c: TIPC bearer code
+ *
+ * Copyright (c) 1996-2006, Ericsson AB
+ * Copyright (c) 2004-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_cfgsrv.h"
+#include "tipc_dbg.h"
+#include "tipc_bearer.h"
+#include "tipc_link.h"
+#include "tipc_port.h"
+#include "tipc_discover.h"
+#include "tipc_bcast.h"
+
+#define TIPC_MAX_ADDR_STR 32
+
+#define MAX_MEDIA 4
+
+static struct tipc_media *media_list[MAX_MEDIA];
+static u32 media_count = 0;
+
+struct bearer *tipc_bearers = NULL;
+
+/**
+ * media_name_valid - validate media name
+ *
+ * Returns 1 if media name is valid, otherwise 0.
+ */
+
+static int media_name_valid(const char *name)
+{
+	u32 len;
+
+	len = strlen(name);
+	if ((len + 1) > TIPC_MAX_MEDIA_NAME)
+		return 0;
+	return (strspn(name, tipc_alphabet) == len);
+}
+
+/**
+ * tipc_media_find_name - locates specified media object by name
+ */
+
+struct tipc_media *tipc_media_find_name(const char *name)
+{
+	u32 i;
+
+	for (i = 0; i < media_count; i++) {
+		if (!strcmp(media_list[i]->name, name))
+			return media_list[i];
+	}
+	return NULL;
+}
+
+/**
+ * media_find_id - locates specified media object by media identifier
+ */
+
+static struct tipc_media *media_find_id(u8 type)
+{
+	u32 i;
+
+	for (i = 0; i < media_count; i++) {
+		if (media_list[i]->media_id == type)
+			return media_list[i];
+	}
+	return NULL;
+}
+
+
+/**
+ * tipc_register_media - register a media type
+ *
+ * Bearers for this media type must be activated separately at a later stage.
+ */
+
+int  tipc_register_media(struct tipc_media *m_ptr)
+{
+	int res = -EINVAL;
+
+	write_lock_bh(&tipc_net_lock);
+
+	if (m_ptr->media_id == TIPC_MEDIA_ID_INVALID) {
+		goto exit;
+	}
+	if (!media_name_valid(m_ptr->name)) {
+		goto exit;
+	}
+	if ((m_ptr->priority < TIPC_MIN_LINK_PRI) &&
+	    (m_ptr->priority > TIPC_MAX_LINK_PRI)) {
+		goto exit;
+	}
+	if ((m_ptr->tolerance < TIPC_MIN_LINK_TOL) || 
+	    (m_ptr->tolerance > TIPC_MAX_LINK_TOL)) {
+		goto exit;
+	}
+	if ((m_ptr->bcast_addr.media_id != m_ptr->media_id) ||
+	    (m_ptr->bcast_addr.broadcast == 0)) {
+		goto exit;
+	}
+	if (!m_ptr->send_msg || 
+	    !m_ptr->enable_bearer || !m_ptr->disable_bearer ||
+	    !m_ptr->addr2str || !m_ptr->str2addr ||
+	    !m_ptr->addr2msg || !m_ptr->msg2addr) {
+		goto exit;
+	}
+
+	if (media_count >= MAX_MEDIA) {
+		goto exit;
+	}
+	if (media_find_id(m_ptr->media_id) || tipc_media_find_name(m_ptr->name)) {
+		goto exit;
+	}
+
+	media_list[media_count++] = m_ptr;
+	res = 0;
+exit:
+	write_unlock_bh(&tipc_net_lock);
+	if (res)
+		warn("Media <%s> rejected\n", m_ptr->name);
+	else
+		dbg("Media <%s> registered\n", m_ptr->name);
+	return res;
+}
+
+/**
+ * tipc_media_addr_printf - record media address in print buffer
+ */
+
+void tipc_media_addr_printf(struct print_buf *pb, struct tipc_media_addr *a)
+{
+#if defined(CONFIG_TIPC_CONFIG_SERVICE) \
+    || defined(CONFIG_TIPC_SYSTEM_MSGS) \
+    || defined(CONFIG_TIPC_DEBUG)
+
+	char addr_str[TIPC_MAX_ADDR_STR];
+	struct tipc_media *m_ptr;
+
+	m_ptr = media_find_id(a->media_id);
+	if ((m_ptr != NULL) && 
+	    (m_ptr->addr2str(a, addr_str, sizeof(addr_str)) == 0)) {
+		tipc_printf(pb, "%s(%s)", m_ptr->name, addr_str);
+	} else {
+		unchar *addr = (unchar *)&a->value;
+		int i;
+
+		tipc_printf(pb, "UNKNOWN(%u)", a->media_id);
+		for (i = 0; i < sizeof(a->value); i++) {
+			tipc_printf(pb, "-%02x", addr[i]);
+		}
+	}
+#endif
+}
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+/**
+ * tipc_media_get_names - record names of registered media in buffer
+ */
+
+struct sk_buff *tipc_media_get_names(void)
+{
+	struct sk_buff *buf;
+	int i;
+
+	buf = tipc_cfg_reply_alloc(MAX_MEDIA * TLV_SPACE(TIPC_MAX_MEDIA_NAME));
+	if (!buf)
+		return NULL;
+
+	read_lock_bh(&tipc_net_lock);
+	for (i = 0; i < media_count; i++) {
+		tipc_cfg_append_tlv(buf, TIPC_TLV_MEDIA_NAME, 
+				    media_list[i]->name, 
+				    strlen(media_list[i]->name) + 1);
+	}
+	read_unlock_bh(&tipc_net_lock);
+	return buf;
+}
+
+#endif
+
+/**
+ * bearer_name_validate - validate & (optionally) deconstruct bearer name
+ * @name - ptr to bearer name string
+ * @name_parts - ptr to area for bearer name components (or NULL if not needed)
+ *
+ * Returns 1 if bearer name is valid, otherwise 0.
+ */
+
+static int bearer_name_validate(const char *name,
+				struct bearer_name *name_parts)
+{
+	char name_copy[TIPC_MAX_BEARER_NAME];
+	char *media_name;
+	char *if_name;
+	u32 media_len;
+	u32 if_len;
+
+	/* copy bearer name & ensure length is OK */
+
+	name_copy[TIPC_MAX_BEARER_NAME - 1] = 0;
+	/* need above in case non-Posix strncpy() doesn't pad with nulls */
+	strncpy(name_copy, name, TIPC_MAX_BEARER_NAME);
+	if (name_copy[TIPC_MAX_BEARER_NAME - 1] != 0)
+		return 0;
+
+	/* ensure all component parts of bearer name are present */
+
+	media_name = name_copy;
+	if ((if_name = strchr(media_name, ':')) == NULL)
+		return 0;
+	*(if_name++) = 0;
+	media_len = if_name - media_name;
+	if_len = strlen(if_name) + 1;
+
+	/* validate component parts of bearer name */
+
+	if ((media_len <= 1) || (media_len > TIPC_MAX_MEDIA_NAME) ||
+	    (if_len <= 1) || (if_len > TIPC_MAX_IF_NAME) ||
+	    (strspn(media_name, tipc_alphabet) != (media_len - 1)) ||
+	    (strspn(if_name, tipc_alphabet) != (if_len - 1)))
+		return 0;
+
+	/* return bearer name components, if necessary */
+
+	if (name_parts) {
+		strcpy(name_parts->media_name, media_name);
+		strcpy(name_parts->if_name, if_name);
+	}
+	return 1;
+}
+
+/**
+ * bearer_find_interface - locates bearer object with matching interface name
+ */
+
+static struct bearer *bearer_find_interface(const char *if_name)
+{
+	struct bearer *b_ptr;
+	char *b_if_name;
+	u32 i;
+
+	for (i = 0, b_ptr = tipc_bearers; i < TIPC_MAX_BEARERS; i++, b_ptr++) {
+		if (!b_ptr->active)
+			continue;
+		b_if_name = strchr(b_ptr->publ.name, ':') + 1;
+		if (!strcmp(b_if_name, if_name))
+			return b_ptr;
+	}
+	return NULL;
+}
+
+/**
+ * tipc_bearer_find - locates bearer object with matching bearer name or interface name
+ */
+
+struct bearer *tipc_bearer_find(const char *name)
+{
+	struct bearer *b_ptr;
+	int i;
+
+	if (tipc_mode != TIPC_NET_MODE)
+		return NULL;
+
+	if (strchr(name,':') == NULL)
+		return bearer_find_interface(name);
+
+	for (i = 0, b_ptr = tipc_bearers; i < TIPC_MAX_BEARERS; i++, b_ptr++) {
+		if (b_ptr->active && (!strcmp(b_ptr->publ.name, name)))
+			return b_ptr;
+	}
+	return NULL;
+}
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+/**
+ * tipc_bearer_get_names - record names of bearers in buffer
+ */
+
+struct sk_buff *tipc_bearer_get_names(void)
+{
+	struct sk_buff *buf;
+	struct bearer *b_ptr;
+	int i, j;
+
+	buf = tipc_cfg_reply_alloc(TIPC_MAX_BEARERS * TLV_SPACE(TIPC_MAX_BEARER_NAME));
+	if (!buf)
+		return NULL;
+
+	read_lock_bh(&tipc_net_lock);
+	for (i = 0; i < media_count; i++) {
+		for (j = 0; j < TIPC_MAX_BEARERS; j++) {
+			b_ptr = &tipc_bearers[j];
+			if (b_ptr->active && (b_ptr->media == media_list[i])) {
+				tipc_cfg_append_tlv(buf, TIPC_TLV_BEARER_NAME, 
+						    b_ptr->publ.name, 
+						    strlen(b_ptr->publ.name) + 1);
+			}
+		}
+	}
+	read_unlock_bh(&tipc_net_lock);
+	return buf;
+}
+
+#endif
+
+void tipc_bearer_add_dest(struct bearer *b_ptr, u32 dest,
+			  struct tipc_media_addr *maddr)
+{
+	struct discoverer *d_ptr;
+	struct discoverer *temp_d_ptr;
+
+	if (in_own_cluster(dest)) {
+		tipc_nmap_add(&b_ptr->nodes, dest);
+		tipc_bcbearer_sort();
+	}
+
+	list_for_each_entry_safe(d_ptr, temp_d_ptr, &b_ptr->disc_list, disc_list) {
+		if (tipc_in_scope(d_ptr->domain, dest)) {
+			d_ptr->num_nodes++;
+			/* tipc_disc_update(d_ptr); */
+		}
+	}
+}
+
+void tipc_bearer_remove_dest(struct bearer *b_ptr, u32 dest,
+			  struct tipc_media_addr *maddr)
+{
+	struct discoverer *d_ptr;
+	struct discoverer *temp_d_ptr;
+
+	if (in_own_cluster(dest)) {
+		tipc_nmap_remove(&b_ptr->nodes, dest);
+		tipc_bcbearer_sort();
+	}
+
+	list_for_each_entry_safe(d_ptr, temp_d_ptr, &b_ptr->disc_list, disc_list) {
+		if (tipc_in_scope(d_ptr->domain, dest)) {
+			d_ptr->num_nodes--;
+			tipc_disc_update(d_ptr);
+		}
+	}
+}
+
+
+/*  
+ * tipc_bearer_send_discover: 'Individual' discoverer's, i.e. those having a
+ * fully specified address, are controlled by the corresponding link's timer,
+ * instead of the discovery timer.
+ */
+
+void tipc_bearer_send_discover(struct bearer *b_ptr, u32 dest)
+{
+	/* TODO: This needs to be reworked */
+
+	struct discoverer *d_ptr;
+	struct discoverer *temp_d_ptr;
+
+	list_for_each_entry_safe(d_ptr, temp_d_ptr, &b_ptr->disc_list, disc_list) {
+		if (d_ptr->domain == dest) {
+			tipc_disc_send_msg(d_ptr);
+			break;
+		}
+	}
+}
+
+/**
+ * tipc_bearer_remove_discoverer(): 
+ * Remove the discovery item for 'dest' from bearer's list.
+ * Note: bearer item is locked. tipc_net_lock is write_locked.
+ */
+
+void tipc_bearer_remove_discoverer(struct bearer *b_ptr, u32 dest)
+{
+	struct discoverer *d_ptr;
+	struct discoverer *temp_d_ptr;
+
+	if (in_own_cluster(dest))
+		return;
+
+	list_for_each_entry_safe(d_ptr, temp_d_ptr, &b_ptr->disc_list, disc_list) {
+		if (tipc_in_scope(dest, d_ptr->domain)) {
+			tipc_disc_deactivate(d_ptr);
+			tipc_disc_delete(d_ptr);
+		}
+	}
+}
+
+/*
+ * bearer_push(): Resolve bearer congestion. Force the waiting
+ * links to push out their unsent packets, one packet per link
+ * per iteration, until all packets are gone or congestion reoccurs.
+ * 'tipc_net_lock' is read_locked when this function is called
+ * bearer.lock must be taken before calling
+ * Returns binary true(1) ore false(0)
+ */
+static int bearer_push(struct bearer *b_ptr)
+{
+	u32 res = 0;
+	struct link *ln, *tln;
+
+	if (b_ptr->publ.blocked)
+		return 0;
+
+	while (!list_empty(&b_ptr->cong_links) && (res != PUSH_FAILED)) {
+		list_for_each_entry_safe(ln, tln, &b_ptr->cong_links, link_list) {
+			res = tipc_link_push_packet(ln);
+			if (res == PUSH_FAILED)
+				break;
+			if (res == PUSH_FINISHED)
+				list_move_tail(&ln->link_list, &b_ptr->links);
+		}
+	}
+	return list_empty(&b_ptr->cong_links);
+}
+
+void tipc_bearer_lock_push(struct bearer *b_ptr)
+{
+	int res;
+
+	spin_lock_bh(&b_ptr->publ.lock);
+	res = bearer_push(b_ptr);
+	spin_unlock_bh(&b_ptr->publ.lock);
+	if (res)
+		tipc_bcbearer_push();
+}
+
+
+/*
+ * Interrupt enabling new requests after bearer congestion or blocking:
+ * See bearer_send().
+ */
+void tipc_continue(struct tipc_bearer *tb_ptr)
+{
+	struct bearer *b_ptr = (struct bearer *)tb_ptr;
+
+	spin_lock_bh(&b_ptr->publ.lock);
+	b_ptr->continue_count++;
+	if (!list_empty(&b_ptr->cong_links))
+		tipc_k_signal((Handler)tipc_bearer_lock_push, (unsigned long)b_ptr);
+	b_ptr->publ.blocked = 0;
+	spin_unlock_bh(&b_ptr->publ.lock);
+}
+
+/*
+ * Schedule link for sending of messages after the bearer
+ * has been deblocked by 'continue()'. This method is called
+ * when somebody tries to send a message via this link while
+ * the bearer is congested. 'tipc_net_lock' is in read_lock here
+ * bearer.lock is busy
+ */
+
+static void tipc_bearer_schedule_unlocked(struct bearer *b_ptr, struct link *l_ptr)
+{
+	list_move_tail(&l_ptr->link_list, &b_ptr->cong_links);
+}
+
+/*
+ * Schedule link for sending of messages after the bearer
+ * has been deblocked by 'continue()'. This method is called
+ * when somebody tries to send a message via this link while
+ * the bearer is congested. 'tipc_net_lock' is in read_lock here,
+ * bearer.lock is free
+ */
+
+void tipc_bearer_schedule(struct bearer *b_ptr, struct link *l_ptr)
+{
+	spin_lock_bh(&b_ptr->publ.lock);
+	tipc_bearer_schedule_unlocked(b_ptr, l_ptr);
+	spin_unlock_bh(&b_ptr->publ.lock);
+}
+
+
+/*
+ * tipc_bearer_resolve_congestion(): Check if there is bearer congestion,
+ * and if there is, try to resolve it before returning.
+ * 'tipc_net_lock' is read_locked when this function is called
+ */
+int tipc_bearer_resolve_congestion(struct bearer *b_ptr, struct link *l_ptr)
+{
+	int res = 1;
+
+	if (list_empty(&b_ptr->cong_links))
+		return 1;
+	spin_lock_bh(&b_ptr->publ.lock);
+	if (!bearer_push(b_ptr)) {
+		tipc_bearer_schedule_unlocked(b_ptr, l_ptr);
+		res = 0;
+	}
+	spin_unlock_bh(&b_ptr->publ.lock);
+	return res;
+}
+
+
+/**
+ * tipc_bearer_congested - determines if bearer is currently congested
+ */
+
+int tipc_bearer_congested(struct bearer *b_ptr, struct link *l_ptr)
+{
+	if (unlikely(b_ptr->publ.blocked))
+		return 1;
+	if (likely(list_empty(&b_ptr->cong_links)))
+		return 0;
+	return !tipc_bearer_resolve_congestion(b_ptr, l_ptr);
+}
+
+/**
+ * tipc_enable_bearer - enable bearer with the given name
+ */
+
+int tipc_enable_bearer(const char *name, u32 disc_domain, u32 priority)
+{
+	struct bearer *b_ptr;
+	struct tipc_media *m_ptr;
+	struct bearer_name b_name;
+	char addr_string[16];
+	u32 bearer_id;
+	u32 with_this_prio;
+	u32 i;
+	int res = -EINVAL;
+
+	if (tipc_mode != TIPC_NET_MODE) {
+		warn("Bearer <%s> rejected, not supported in standalone mode\n",
+		     name);
+		return -ENOPROTOOPT;
+	}
+	if (!bearer_name_validate(name, &b_name)) {
+		warn("Bearer <%s> rejected, illegal name\n", name);
+		return -EINVAL;
+	}
+	if (!tipc_addr_domain_valid(disc_domain)) {
+		warn("Bearer <%s> rejected, illegal discovery domain\n", name);
+		return -EINVAL;
+	}
+	if ((priority < TIPC_MIN_LINK_PRI ||
+	     priority > TIPC_MAX_LINK_PRI) &&
+	    (priority != TIPC_MEDIA_LINK_PRI)) {
+		warn("Bearer <%s> rejected, illegal priority\n", name);
+		return -EINVAL;
+	}
+
+	write_lock_bh(&tipc_net_lock);
+
+	m_ptr = tipc_media_find_name(b_name.media_name);
+	if (!m_ptr) {
+		warn("Bearer <%s> rejected, media <%s> not registered\n", name,
+		     b_name.media_name);
+		goto failed;
+	}
+	if (priority == TIPC_MEDIA_LINK_PRI)
+		priority = m_ptr->priority;
+
+restart:
+	bearer_id = TIPC_MAX_BEARERS;
+	with_this_prio = 1;
+	for (i = TIPC_MAX_BEARERS; i-- != 0; ) {
+		if (!tipc_bearers[i].in_use) {
+			bearer_id = i;
+			continue;
+		}
+		if (!strcmp(name, tipc_bearers[i].publ.name)) {
+			warn("Bearer <%s> rejected, already enabled\n", name);
+			goto failed;
+		}
+		if ((tipc_bearers[i].priority == priority) &&
+		    (++with_this_prio > 2)) {
+			if (priority-- == 0) {
+				warn("Bearer <%s> rejected, duplicate priority\n",
+				     name);
+				goto failed;
+			}
+			warn("Bearer <%s> priority adjustment required %u->%u\n",
+			     name, priority + 1, priority);
+			goto restart;
+		}
+	}
+	if (bearer_id >= TIPC_MAX_BEARERS) {
+		warn("Bearer <%s> rejected, bearer limit reached (%u)\n", 
+		     name, TIPC_MAX_BEARERS);
+		goto failed;
+	}
+
+	b_ptr = &tipc_bearers[bearer_id];
+	b_ptr->in_use = 1;
+	strcpy(b_ptr->publ.name, name);
+	b_ptr->priority = priority;
+
+	write_unlock_bh(&tipc_net_lock);
+	res = m_ptr->enable_bearer(&b_ptr->publ);
+	if (res) {
+		b_ptr->in_use = 0;
+		warn("Bearer <%s> rejected, enable failure (%d)\n", name, -res);
+		return res;
+	}
+	write_lock_bh(&tipc_net_lock);
+
+	b_ptr->identity = bearer_id;
+	b_ptr->media = m_ptr;
+	b_ptr->tolerance = m_ptr->tolerance;
+	b_ptr->window = m_ptr->window;
+	b_ptr->net_plane = bearer_id + 'A';
+
+	INIT_LIST_HEAD(&b_ptr->cong_links);
+	INIT_LIST_HEAD(&b_ptr->links);
+	INIT_LIST_HEAD(&b_ptr->disc_list);
+	if (disc_domain != tipc_own_addr) {
+		tipc_disc_create(b_ptr, &m_ptr->bcast_addr, disc_domain);
+	}
+	spin_lock_init(&b_ptr->publ.lock);
+	b_ptr->active = 1;
+
+	write_unlock_bh(&tipc_net_lock);
+
+	tipc_addr_string_fill(addr_string, disc_domain);
+	info("Enabled bearer <%s>, discovery domain %s, priority %u\n",
+	     name, addr_string, priority);
+	return 0;
+failed:
+	write_unlock_bh(&tipc_net_lock);
+	return res;
+}
+
+/**
+ * tipc_block_bearer(): Block the bearer with the given name,
+ *                      and reset all its links
+ */
+
+int tipc_block_bearer(const char *name)
+{
+	struct bearer *b_ptr = NULL;
+	struct link *l_ptr;
+	struct link *temp_l_ptr;
+
+	read_lock_bh(&tipc_net_lock);
+	b_ptr = tipc_bearer_find(name);
+	if (!b_ptr) {
+		warn("Attempt to block unknown bearer <%s>\n", name);
+		read_unlock_bh(&tipc_net_lock);
+		return -EINVAL;
+	}
+
+	info("Blocking bearer <%s>\n", name);
+	spin_lock_bh(&b_ptr->publ.lock);
+	b_ptr->publ.blocked = 1;
+	list_for_each_entry_safe(l_ptr, temp_l_ptr, &b_ptr->links, link_list) {
+		struct tipc_node *n_ptr = l_ptr->owner;
+
+		tipc_node_lock(n_ptr);
+		tipc_link_reset(l_ptr);
+		tipc_node_unlock(n_ptr);
+	}
+	spin_unlock_bh(&b_ptr->publ.lock);
+	read_unlock_bh(&tipc_net_lock);
+	return 0;
+}
+
+/**
+ * bearer_disable -
+ *
+ * Note: This routine assumes caller holds tipc_net_lock.
+ */
+
+static int bearer_disable(struct bearer *b_ptr)
+{
+	struct link *l_ptr;
+	struct link *temp_l_ptr;
+	struct discoverer *d_ptr;
+	struct discoverer *temp_d_ptr;
+
+	info("Disabling bearer <%s>\n", b_ptr->publ.name);
+	spin_lock_bh(&b_ptr->publ.lock);
+	b_ptr->publ.blocked = 1;
+	b_ptr->media->disable_bearer(&b_ptr->publ);
+	list_for_each_entry_safe(l_ptr, temp_l_ptr, &b_ptr->links, link_list) {
+		tipc_link_delete(l_ptr);
+	}
+	spin_unlock_bh(&b_ptr->publ.lock);
+
+	/* Safe to delete discovery struct here. Bearer is inactive now */
+
+	list_for_each_entry_safe(d_ptr, temp_d_ptr, &b_ptr->disc_list, disc_list) {
+		tipc_disc_deactivate(d_ptr);
+		tipc_disc_delete(d_ptr);
+	}
+
+	spin_lock_term(&b_ptr->publ.lock); 
+	memset(b_ptr, 0, sizeof(struct bearer));
+	return 0;
+}
+
+int tipc_disable_bearer(const char *name)
+{
+	struct bearer *b_ptr;
+	int res;
+
+	write_lock_bh(&tipc_net_lock);
+	b_ptr = tipc_bearer_find(name);
+	if (b_ptr == NULL) {
+		warn("Attempt to disable unknown bearer <%s>\n", name);
+		res = -EINVAL;
+	}
+	else {
+		res = bearer_disable(b_ptr);
+	}
+	write_unlock_bh(&tipc_net_lock);
+	return res;
+}
+
+
+int tipc_bearer_init(void)
+{
+	int res;
+
+	write_lock_bh(&tipc_net_lock);
+	tipc_bearers = kcalloc(TIPC_MAX_BEARERS, sizeof(struct bearer), GFP_ATOMIC);
+	if (tipc_bearers) {
+		res = 0;
+	} else {
+		kfree(tipc_bearers);
+		tipc_bearers = NULL;
+		res = -ENOMEM;
+	}
+	write_unlock_bh(&tipc_net_lock);
+	return res;
+}
+
+void tipc_bearer_stop(void)
+{
+	u32 i;
+
+	if (!tipc_bearers)
+		return;
+
+	for (i = 0; i < TIPC_MAX_BEARERS; i++) {
+		if (tipc_bearers[i].active)
+			bearer_disable(&tipc_bearers[i]);
+	}
+	kfree(tipc_bearers);
+	tipc_bearers = NULL;
+	media_count = 0;
+}
+
diff -ruN linux-2.6.29/net/tipc/tipc_bearer.h android_cluster/linux-2.6.29/net/tipc/tipc_bearer.h
--- linux-2.6.29/net/tipc/tipc_bearer.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_bearer.h	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,133 @@
+/*
+ * net/tipc/tipc_bearer.h: Include file for TIPC bearer code
+ * 
+ * Copyright (c) 1996-2006, Ericsson AB
+ * Copyright (c) 2005-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_BEARER_H
+#define _TIPC_BEARER_H
+
+#include "tipc_core.h"
+#include "tipc_bcast.h"
+
+
+/**
+ * struct bearer - TIPC bearer information
+ * @publ: bearer information available to privileged users
+ * @media: ptr to media structure associated with bearer
+ * @priority: default link priority for bearer
+ * @window: default window size for bearer
+ * @tolerance: default link tolerance for bearer
+ * @identity: array index of this bearer within TIPC bearer array
+ * @disc_list: list of neighbor discovery objects
+ * @links: list of non-congested links associated with bearer
+ * @cong_links: list of congested links associated with bearer
+ * @continue_count: # of times bearer has resumed after congestion or blocking
+ * @in_use: if zero, bearer entry is unassigned (fully disabled)
+ * @active: if non-zero, bearer entry is bound to an interface (fully enabled)
+ * @net_plane: network plane ('A' through 'H') currently associated with bearer
+ * @nodes: indicates which nodes in cluster can be reached through bearer
+ */
+ 
+struct bearer {
+	struct tipc_bearer publ;
+	struct tipc_media *media;
+	u32 priority;
+	u32 window;
+	u32 tolerance;
+	u32 identity;
+	struct list_head disc_list;
+	struct list_head links;
+	struct list_head cong_links;
+	u32 continue_count;
+	int in_use;
+	int active;
+	char net_plane;
+	struct tipc_node_map nodes;
+};
+
+struct bearer_name {
+	char media_name[TIPC_MAX_MEDIA_NAME];
+	char if_name[TIPC_MAX_IF_NAME];
+};
+
+struct link;
+
+extern struct bearer *tipc_bearers;
+
+int tipc_media_set_priority(const char *name, u32 new_value);
+int tipc_media_set_window(const char *name, u32 new_value);
+void tipc_media_addr_printf(struct print_buf *pb, struct tipc_media_addr *a);
+struct sk_buff *tipc_media_get_names(void);
+
+struct sk_buff *tipc_bearer_get_names(void);
+void tipc_bearer_add_dest(struct bearer *b_ptr, u32 dest,
+			  struct tipc_media_addr *maddr);
+void tipc_bearer_remove_dest(struct bearer *b_ptr, u32 dest,
+			     struct tipc_media_addr *maddr);
+void tipc_bearer_remove_discoverer(struct bearer *b_ptr, u32 dest);
+void tipc_bearer_schedule(struct bearer *b_ptr, struct link *l_ptr);
+struct bearer *tipc_bearer_find(const char *name);
+struct tipc_media *tipc_media_find_name(const char *name);
+int tipc_bearer_resolve_congestion(struct bearer *b_ptr, struct link *l_ptr);
+int tipc_bearer_congested(struct bearer *b_ptr, struct link *l_ptr);
+int tipc_bearer_init(void);
+void tipc_bearer_stop(void);
+void tipc_bearer_lock_push(struct bearer *b_ptr);
+void tipc_bearer_send_discover(struct bearer *b_ptr, u32 dest);
+
+/**
+ * tipc_bearer_send- sends buffer to destination over bearer
+ *
+ * Returns true (1) if successful, or false (0) if unable to send
+ *
+ * IMPORTANT:
+ * The media send routine must not alter the buffer being passed in
+ * as it may be needed for later retransmission!
+ *
+ * If the media send routine returns a non-zero value (indicating that
+ * it was unable to send the buffer), it must:
+ *   1) mark the bearer as blocked,
+ *   2) call tipc_continue() once the bearer is able to send again.
+ * Media types that are unable to meet these two critera must ensure their
+ * send routine always returns success -- even if the buffer was not sent --
+ * and let TIPC's link code deal with the undelivered message.
+ */
+
+static inline int tipc_bearer_send(struct bearer *b_ptr, struct sk_buff *buf,
+				   struct tipc_media_addr *dest)
+{
+	return !b_ptr->media->send_msg(buf, &b_ptr->publ, dest);
+}
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_cfgsrv.c android_cluster/linux-2.6.29/net/tipc/tipc_cfgsrv.c
--- linux-2.6.29/net/tipc/tipc_cfgsrv.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_cfgsrv.c	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,779 @@
+/*
+ * net/tipc/tipc_cfgsrv.c: TIPC configuration service code
+ *
+ * Copyright (c) 2002-2006, Ericsson AB
+ * Copyright (c) 2004-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_dbg.h"
+#include "tipc_bearer.h"
+#include "tipc_port.h"
+#include "tipc_link.h"
+#include "tipc_net.h"
+#include "tipc_addr.h"
+#include "tipc_name_table.h"
+#include "tipc_node.h"
+#include "tipc_cfgsrv.h"
+#include "tipc_discover.h"
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+struct manager {
+	u32 user_ref;
+	u32 port_ref;
+};
+
+static struct manager mng = { 0 };
+
+static DEFINE_SPINLOCK(config_lock);
+
+static const void *req_tlv_area;	/* request message TLV area */
+static int req_tlv_space;		/* request message TLV area size */
+static int rep_headroom;		/* reply message headroom to use */
+
+
+struct sk_buff *tipc_cfg_reply_alloc(int payload_size)
+{
+	struct sk_buff *buf;
+
+	buf = alloc_skb(rep_headroom + payload_size, GFP_ATOMIC);
+	if (buf)
+		skb_reserve(buf, rep_headroom);
+	return buf;
+}
+
+int tipc_cfg_append_tlv(struct sk_buff *buf, int tlv_type,
+			void *tlv_data, int tlv_data_size)
+{
+	struct tlv_desc *tlv = (struct tlv_desc *)skb_tail_pointer(buf);
+	int new_tlv_space = TLV_SPACE(tlv_data_size);
+
+	if (skb_tailroom(buf) < new_tlv_space) {
+		err("Can't append TLV to configuration message\n");
+		return 0;
+	}
+	skb_put(buf, new_tlv_space);
+	tlv->tlv_type = htons(tlv_type);
+	tlv->tlv_len  = htons(TLV_LENGTH(tlv_data_size));
+	if (tlv_data_size && tlv_data)
+		memcpy(TLV_DATA(tlv), tlv_data, tlv_data_size);
+	return 1;
+}
+
+struct sk_buff *tipc_cfg_reply_unsigned_type(u16 tlv_type, u32 value)
+{
+	struct sk_buff *buf;
+	__be32 value_net;
+
+	buf = tipc_cfg_reply_alloc(TLV_SPACE(sizeof(value)));
+	if (buf) {
+		value_net = htonl(value);
+		tipc_cfg_append_tlv(buf, tlv_type, &value_net,
+				    sizeof(value_net));
+	}
+	return buf;
+}
+
+struct sk_buff *tipc_cfg_reply_string_type(u16 tlv_type, char *string)
+{
+	struct sk_buff *buf;
+	int string_len = strlen(string) + 1;
+
+	buf = tipc_cfg_reply_alloc(TLV_SPACE(string_len));
+	if (buf)
+		tipc_cfg_append_tlv(buf, tlv_type, string, string_len);
+	return buf;
+}
+
+
+#if 0
+
+/* Now obsolete code for handling commands not yet implemented the new way */
+
+/*
+ * Some of this code assumed that the manager structure contains two added
+ * fields:
+ *	u32 link_subscriptions;
+ *	struct list_head link_subscribers;
+ * which are currently not present.  These fields may need to be re-introduced
+ * if and when support for link subscriptions is added.
+ */
+
+struct subscr_data {
+	char usr_handle[8];
+	u32 domain;
+	u32 port_ref;
+	struct list_head subd_list;
+};
+
+
+void tipc_cfg_link_event(u32 addr, char *name, int up)
+{
+	/* TIPC DOESN'T HANDLE LINK EVENT SUBSCRIPTIONS AT THE MOMENT */
+}
+
+int tipc_cfg_cmd(const struct tipc_cmd_msg * msg,
+		 char *data,
+		 u32 sz,
+		 u32 *ret_size,
+		 struct tipc_portid *orig)
+{
+	int rv = -EINVAL;
+	u32 cmd = msg->cmd;
+
+	*ret_size = 0;
+	switch (cmd) {
+	case TIPC_REMOVE_LINK:
+	case TIPC_CMD_BLOCK_LINK:
+	case TIPC_CMD_UNBLOCK_LINK:
+		if (!cfg_check_connection(orig))
+			rv = link_control(msg->argv.link_name, msg->cmd, 0);
+		break;
+	case TIPC_ESTABLISH:
+		{
+			int connected;
+
+			tipc_isconnected(mng.conn_port_ref, &connected);
+			if (connected || !orig) {
+				rv = TIPC_FAILURE;
+				break;
+			}
+			rv = tipc_connect2port(mng.conn_port_ref, orig);
+			if (rv == TIPC_OK)
+				orig = 0;
+			break;
+		}
+	case TIPC_GET_PEER_ADDRESS:
+		*ret_size = link_peer_addr(msg->argv.link_name, data, sz);
+		break;
+	default: {}
+	}
+	if (*ret_size)
+		rv = TIPC_OK;
+	return rv;
+}
+
+static void cfg_cmd_event(struct tipc_cmd_msg *msg,
+			  char *data,
+			  u32 sz,
+			  struct tipc_portid const *orig)
+{
+	int rv = -EINVAL;
+	struct tipc_cmd_result_msg rmsg;
+	struct iovec msg_sect[2];
+	int *arg;
+
+	msg->cmd = ntohl(msg->cmd);
+
+	cfg_prepare_res_msg(msg->cmd, msg->usr_handle, rv, &rmsg, msg_sect,
+			    data, 0);
+	if (ntohl(msg->magic) != TIPC_MAGIC)
+		goto exit;
+
+	switch (msg->cmd) {
+	case TIPC_CREATE_LINK:
+		if (!cfg_check_connection(orig))
+			rv = disc_create_link(&msg->argv.create_link);
+		break;
+	case TIPC_LINK_SUBSCRIBE:
+		{
+			struct subscr_data *sub;
+
+			if (mng.link_subscriptions > 64)
+				break;
+			sub = (struct subscr_data *)kmalloc(sizeof(*sub),
+							    GFP_ATOMIC);
+			if (sub == NULL) {
+				warn("Memory squeeze; dropped remote link subscription\n");
+				break;
+			}
+			INIT_LIST_HEAD(&sub->subd_list);
+			tipc_createport(mng.user_ref,
+					(void *)sub,
+					TIPC_HIGH_IMPORTANCE,
+					0,
+					0,
+					(tipc_conn_shutdown_event)cfg_linksubscr_cancel,
+					0,
+					0,
+					(tipc_conn_msg_event)cfg_linksubscr_cancel,
+					0,
+					&sub->port_ref);
+			if (!sub->port_ref) {
+				kfree(sub);
+				break;
+			}
+			memcpy(sub->usr_handle,msg->usr_handle,
+			       sizeof(sub->usr_handle));
+			sub->domain = msg->argv.domain;
+			list_add_tail(&sub->subd_list, &mng.link_subscribers);
+			tipc_connect2port(sub->port_ref, orig);
+			rmsg.retval = TIPC_OK;
+			tipc_send(sub->port_ref, 2u, msg_sect);
+			mng.link_subscriptions++;
+			return;
+		}
+	default:
+		rv = tipc_cfg_cmd(msg, data, sz, (u32 *)&msg_sect[1].iov_len, orig);
+	}
+exit:
+	rmsg.result_len = htonl(msg_sect[1].iov_len);
+	rmsg.retval = htonl(rv);
+	tipc_cfg_respond(msg_sect, 2u, orig);
+}
+
+#endif
+
+#define MAX_STATS_INFO 2000
+
+static struct sk_buff *tipc_show_stats(void)
+{
+	struct sk_buff *buf;
+	struct tlv_desc *rep_tlv;
+	struct print_buf pb;
+	int str_len;
+	u32 value;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	value = ntohl(*(u32 *)TLV_DATA(req_tlv_area));
+	if (value != 0)
+		return tipc_cfg_reply_error_string("unsupported argument");
+
+	buf = tipc_cfg_reply_alloc(TLV_SPACE(MAX_STATS_INFO));
+	if (buf == NULL)
+		return NULL;
+
+	rep_tlv = (struct tlv_desc *)buf->data;
+	tipc_printbuf_init(&pb, (char *)TLV_DATA(rep_tlv), MAX_STATS_INFO);
+
+	tipc_printf(&pb, "TIPC version " TIPC_MOD_VER "\n");
+
+	/* Use additional tipc_printf()'s to return more info ... */
+
+	str_len = tipc_printbuf_validate(&pb);
+	skb_put(buf, TLV_SPACE(str_len));
+	TLV_SET(rep_tlv, TIPC_TLV_ULTRA_STRING, NULL, str_len);
+
+	return buf;
+}
+
+static struct sk_buff *cfg_enable_bearer(void)
+{
+	struct tipc_bearer_config *args;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_BEARER_CONFIG))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	args = (struct tipc_bearer_config *)TLV_DATA(req_tlv_area);
+	if (tipc_enable_bearer(args->name,
+			       ntohl(args->disc_domain),
+			       ntohl(args->priority)))
+		return tipc_cfg_reply_error_string("unable to enable bearer");
+
+	return tipc_cfg_reply_none();
+}
+
+static struct sk_buff *cfg_disable_bearer(void)
+{
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_BEARER_NAME))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	if (tipc_disable_bearer((char *)TLV_DATA(req_tlv_area)))
+		return tipc_cfg_reply_error_string("unable to disable bearer");
+
+	return tipc_cfg_reply_none();
+}
+
+static struct sk_buff *cfg_set_own_addr(void)
+{
+	u32 addr;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_NET_ADDR))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	addr = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	if (addr == tipc_own_addr)
+		return tipc_cfg_reply_none();
+	if (!tipc_addr_node_valid(addr))
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+						   " (node address)");
+	if (tipc_mode == TIPC_NET_MODE)
+		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
+						   " (cannot change node address once assigned)");
+
+	tipc_core_start_net(addr);
+
+	return tipc_cfg_reply_none();
+}
+
+static struct sk_buff *cfg_set_remote_mng(void)
+{
+	u32 value;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	tipc_remote_management = (value != 0);
+	return tipc_cfg_reply_none();
+}
+
+static struct sk_buff *cfg_set_max_publications(void)
+{
+	u32 value;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	if (value != delimit(value, 1, 65535))
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+						   " (max publications must be 1-65535)");
+	tipc_max_publications = value;
+	return tipc_cfg_reply_none();
+}
+
+static struct sk_buff *cfg_set_max_subscriptions(void)
+{
+	u32 value;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	if (value != delimit(value, 1, 65535))
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+						   " (max subscriptions must be 1-65535");
+	tipc_max_subscriptions = value;
+	return tipc_cfg_reply_none();
+}
+
+static struct sk_buff *cfg_set_max_ports(void)
+{
+	u32 value;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	if (value == tipc_max_ports)
+		return tipc_cfg_reply_none();
+	if (value != delimit(value, 127, 65535))
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+						   " (max ports must be 127-65535)");
+	if (tipc_mode != TIPC_NOT_RUNNING)
+		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
+			" (cannot change max ports while TIPC is active)");
+	tipc_max_ports = value;
+	return tipc_cfg_reply_none();
+}
+
+static struct sk_buff *cfg_set_max_zones(void)
+{
+	u32 value;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	if (value == tipc_max_zones)
+		return tipc_cfg_reply_none();
+	if (value != delimit(value, 1, 255))
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+						   " (max zones must be 1-255)");
+	if (tipc_mode == TIPC_NET_MODE)
+		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
+			" (cannot change max zones once TIPC has joined a network)");
+	tipc_max_zones = value;
+	return tipc_cfg_reply_none();
+}
+
+static struct sk_buff *cfg_set_max_clusters(void)
+{
+	u32 value;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	if (value == tipc_max_clusters)
+		return tipc_cfg_reply_none();
+	if (value != delimit(value, 1, 4095))
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+						   " (max clusters must be 1-4095)");
+	if (tipc_mode == TIPC_NET_MODE)
+		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
+			" (cannot change max clusters once TIPC has joined a network)");
+	tipc_max_clusters = value;
+	return tipc_cfg_reply_none();
+}
+
+static struct sk_buff *cfg_set_max_nodes(void)
+{
+	u32 value;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	if (value == tipc_max_nodes)
+		return tipc_cfg_reply_none();
+	if (value != delimit(value, 8, 4095))
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+						   " (max nodes must be 8-4095)");
+	if (tipc_mode == TIPC_NET_MODE)
+		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
+			" (cannot change max nodes once TIPC has joined a network)");
+	tipc_max_nodes = value;
+	return tipc_cfg_reply_none();
+}
+
+static struct sk_buff *cfg_set_max_remotes(void)
+{
+	u32 value;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	if (value == tipc_max_remotes)
+		return tipc_cfg_reply_none();
+	if (value != delimit(value, 0, 255))
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+						   " (max remotes must be 0-255)");
+	if (tipc_mode == TIPC_NET_MODE)
+		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
+			" (cannot change max remotes once TIPC has joined a network)");
+	tipc_max_remotes = value;
+	return tipc_cfg_reply_none();
+}
+
+static struct sk_buff *cfg_set_netid(void)
+{
+	u32 value;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	if (value == tipc_net_id)
+		return tipc_cfg_reply_none();
+	if (value != delimit(value, 1, 9999))
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+						   " (network id must be 1-9999)");
+	if (tipc_mode == TIPC_NET_MODE)
+		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
+			" (cannot change network id once TIPC has joined a network)");
+	tipc_net_id = value;
+	return tipc_cfg_reply_none();
+}
+
+struct sk_buff *tipc_cfg_do_cmd(u32 orig_node, u16 cmd, const void *request_area,
+				int request_space, int reply_headroom)
+{
+	struct sk_buff *rep_tlv_buf;
+
+	spin_lock_bh(&config_lock);
+
+	/* Save request and reply details in a well-known location */
+
+	req_tlv_area = request_area;
+	req_tlv_space = request_space;
+	rep_headroom = reply_headroom;
+
+	/* Check command authorization */
+
+	if (likely(orig_node == tipc_own_addr)) {
+		/* command is permitted */
+	} else if (cmd >= 0x8000) {
+		rep_tlv_buf = tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
+							  " (cannot be done remotely)");
+		goto exit;
+	} else if (!tipc_remote_management) {
+		rep_tlv_buf = tipc_cfg_reply_error_string(TIPC_CFG_NO_REMOTE);
+		goto exit;
+	}
+	else if (cmd >= 0x4000) {
+		u32 domain = 0;
+
+		if ((tipc_nametbl_translate(TIPC_ZM_SRV, 0, &domain) == 0) ||
+		    (domain != orig_node)) {
+			rep_tlv_buf = tipc_cfg_reply_error_string(TIPC_CFG_NOT_ZONE_MSTR);
+			goto exit;
+		}
+	}
+
+	/* Call appropriate processing routine */
+
+	switch (cmd) {
+	case TIPC_CMD_NOOP:
+		rep_tlv_buf = tipc_cfg_reply_none();
+		break;
+	case TIPC_CMD_GET_NODES:
+		rep_tlv_buf = tipc_node_get_nodes(req_tlv_area, req_tlv_space);
+		break;
+	case TIPC_CMD_GET_ROUTES:
+		rep_tlv_buf = tipc_nametbl_get_routes(req_tlv_area, req_tlv_space);
+		break;
+	case TIPC_CMD_GET_LINKS:
+		rep_tlv_buf = tipc_node_get_links(req_tlv_area, req_tlv_space);
+		break;
+	case TIPC_CMD_SHOW_LINK_STATS:
+		rep_tlv_buf = tipc_link_cmd_show_stats(req_tlv_area, req_tlv_space);
+		break;
+	case TIPC_CMD_RESET_LINK_STATS:
+		rep_tlv_buf = tipc_link_cmd_reset_stats(req_tlv_area, req_tlv_space);
+		break;
+	case TIPC_CMD_SHOW_NAME_TABLE:
+		rep_tlv_buf = tipc_nametbl_get(req_tlv_area, req_tlv_space);
+		break;
+	case TIPC_CMD_GET_BEARER_NAMES:
+		rep_tlv_buf = tipc_bearer_get_names();
+		break;
+	case TIPC_CMD_GET_MEDIA_NAMES:
+		rep_tlv_buf = tipc_media_get_names();
+		break;
+	case TIPC_CMD_SHOW_PORTS:
+		rep_tlv_buf = tipc_port_get_ports();
+		break;
+#if 0
+	case TIPC_CMD_SHOW_PORT_STATS:
+		rep_tlv_buf = port_show_stats(req_tlv_area, req_tlv_space);
+		break;
+	case TIPC_CMD_RESET_PORT_STATS:
+		rep_tlv_buf = tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED);
+		break;
+#endif
+	case TIPC_CMD_SET_LOG_SIZE:
+		rep_tlv_buf = tipc_log_resize_cmd(req_tlv_area, req_tlv_space);
+		break;
+	case TIPC_CMD_DUMP_LOG:
+		rep_tlv_buf = tipc_log_dump();
+		break;
+	case TIPC_CMD_SHOW_STATS:
+		rep_tlv_buf = tipc_show_stats();
+		break;
+	case TIPC_CMD_SET_LINK_TOL:
+	case TIPC_CMD_SET_LINK_PRI:
+	case TIPC_CMD_SET_LINK_WINDOW:
+		rep_tlv_buf = tipc_link_cmd_config(req_tlv_area, req_tlv_space, cmd);
+		break;
+#ifdef PROTO_MULTI_DISCOVERY_OBJECT
+	case TIPC_CMD_CREATE_LINK:
+		rep_tlv_buf = tipc_disc_cmd_create_link(req_tlv_area, req_tlv_space);
+		break;
+	case TIPC_CMD_DELETE_LINK:
+		rep_tlv_buf = tipc_link_cmd_delete(req_tlv_area, req_tlv_space);
+		break;
+#endif
+	case TIPC_CMD_ENABLE_BEARER:
+		rep_tlv_buf = cfg_enable_bearer();
+		break;
+	case TIPC_CMD_DISABLE_BEARER:
+		rep_tlv_buf = cfg_disable_bearer();
+		break;
+	case TIPC_CMD_SET_NODE_ADDR:
+		rep_tlv_buf = cfg_set_own_addr();
+		break;
+	case TIPC_CMD_SET_REMOTE_MNG:
+		rep_tlv_buf = cfg_set_remote_mng();
+		break;
+	case TIPC_CMD_SET_MAX_PORTS:
+		rep_tlv_buf = cfg_set_max_ports();
+		break;
+	case TIPC_CMD_SET_MAX_PUBL:
+		rep_tlv_buf = cfg_set_max_publications();
+		break;
+	case TIPC_CMD_SET_MAX_SUBSCR:
+		rep_tlv_buf = cfg_set_max_subscriptions();
+		break;
+	case TIPC_CMD_SET_MAX_ZONES:
+		rep_tlv_buf = cfg_set_max_zones();
+		break;
+	case TIPC_CMD_SET_MAX_CLUSTERS:
+		rep_tlv_buf = cfg_set_max_clusters();
+		break;
+	case TIPC_CMD_SET_MAX_NODES:
+		rep_tlv_buf = cfg_set_max_nodes();
+		break;
+	case TIPC_CMD_SET_MAX_REMOTES:
+		rep_tlv_buf = cfg_set_max_remotes();
+		break;
+	case TIPC_CMD_SET_NETID:
+		rep_tlv_buf = cfg_set_netid();
+		break;
+	case TIPC_CMD_GET_REMOTE_MNG:
+		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_remote_management);
+		break;
+	case TIPC_CMD_GET_MAX_PORTS:
+		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_ports);
+		break;
+	case TIPC_CMD_GET_MAX_ZONES:
+		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_zones);
+		break;
+	case TIPC_CMD_GET_MAX_CLUSTERS:
+		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_clusters);
+		break;
+	case TIPC_CMD_GET_MAX_NODES:
+		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_nodes);
+		break;
+	case TIPC_CMD_GET_MAX_REMOTES:
+		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_remotes);
+		break;
+	case TIPC_CMD_GET_MAX_PUBL:
+		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_publications);
+		break;
+	case TIPC_CMD_GET_MAX_SUBSCR:
+		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_max_subscriptions);
+		break;
+	case TIPC_CMD_GET_NETID:
+		rep_tlv_buf = tipc_cfg_reply_unsigned(tipc_net_id);
+		break;
+	case TIPC_CMD_NOT_NET_ADMIN:
+		rep_tlv_buf =
+			tipc_cfg_reply_error_string(TIPC_CFG_NOT_NET_ADMIN);
+		break;
+	default:
+		rep_tlv_buf = tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
+							  " (unknown command)");
+		break;
+	}
+
+	/* Return reply buffer */
+exit:
+	spin_unlock_bh(&config_lock);
+	return rep_tlv_buf;
+}
+
+static void cfg_named_msg_event(void *userdata,
+				u32 port_ref,
+				struct sk_buff **buf,
+				const unchar *msg,
+				u32 size,
+				u32 importance,
+				struct tipc_portid const *orig,
+				struct tipc_name_seq const *dest)
+{
+	struct tipc_cfg_msg_hdr *req_hdr;
+	struct tipc_cfg_msg_hdr *rep_hdr;
+	struct sk_buff *rep_buf;
+
+	/* Validate configuration message header (ignore invalid message) */
+
+	req_hdr = (struct tipc_cfg_msg_hdr *)msg;
+	if ((size < sizeof(*req_hdr)) ||
+	    (size != TCM_ALIGN(ntohl(req_hdr->tcm_len))) ||
+	    (ntohs(req_hdr->tcm_flags) != TCM_F_REQUEST)) {
+		warn("Invalid configuration message discarded\n");
+		return;
+	}
+
+	/* Generate reply for request (if can't, return request) */
+
+	rep_buf = tipc_cfg_do_cmd(orig->node,
+				  ntohs(req_hdr->tcm_type),
+				  msg + sizeof(*req_hdr),
+				  size - sizeof(*req_hdr),
+				  BUF_HEADROOM + MAX_H_SIZE + sizeof(*rep_hdr));
+	if (rep_buf) {
+		skb_push(rep_buf, sizeof(*rep_hdr));
+		rep_hdr = (struct tipc_cfg_msg_hdr *)rep_buf->data;
+		memcpy(rep_hdr, req_hdr, sizeof(*rep_hdr));
+		rep_hdr->tcm_len = htonl(rep_buf->len);
+		rep_hdr->tcm_flags &= htons(~TCM_F_REQUEST);
+	} else {
+		rep_buf = *buf;
+		*buf = NULL;
+	}
+
+	/* NEED TO ADD CODE TO HANDLE FAILED SEND (SUCH AS CONGESTION) */
+	tipc_send_buf2port(port_ref, orig, rep_buf, rep_buf->len);
+}
+
+int tipc_cfg_init(void)
+{
+	struct tipc_name_seq seq;
+	int res;
+
+	res = tipc_attach(&mng.user_ref, NULL, NULL);
+	if (res)
+		goto failed;
+
+	res = tipc_createport(mng.user_ref, NULL, TIPC_CRITICAL_IMPORTANCE,
+			      NULL, NULL, NULL, NULL, cfg_named_msg_event, 
+			      NULL, NULL, &mng.port_ref);
+	if (res)
+		goto failed;
+
+	seq.type = TIPC_CFG_SRV;
+	seq.lower = seq.upper = tipc_own_addr;
+	res = tipc_publish_rsv(mng.port_ref, TIPC_CLUSTER_SCOPE, &seq);
+	if (res)
+		goto failed;
+
+	return 0;
+
+failed:
+	err("Unable to create configuration service\n");
+	tipc_detach(mng.user_ref);
+	mng.user_ref = 0;
+	return res;
+}
+
+void tipc_cfg_stop(void)
+{
+	if (mng.user_ref) {
+		tipc_detach(mng.user_ref);
+		mng.user_ref = 0;
+	}
+}
+
+#else
+
+static struct publication *mng_publ = NULL;
+
+int tipc_cfg_init(void)
+{
+	mng_publ = tipc_nametbl_publish_rsv(TIPC_CFG_SRV, tipc_own_addr,
+					    tipc_own_addr, TIPC_CLUSTER_SCOPE,
+					    0, tipc_random);
+	if (mng_publ == NULL)
+		err("Unable to create configuration service identifier\n");
+	return !mng_publ;
+}
+
+void tipc_cfg_stop(void)
+{
+	if (mng_publ) {
+		tipc_nametbl_withdraw(TIPC_CFG_SRV, tipc_own_addr, 0, tipc_random);
+		mng_publ = NULL;
+	}
+}
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_cfgsrv.h android_cluster/linux-2.6.29/net/tipc/tipc_cfgsrv.h
--- linux-2.6.29/net/tipc/tipc_cfgsrv.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_cfgsrv.h	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,78 @@
+/*
+ * net/tipc/tipc_cfgsrv.h: Include file for TIPC configuration service code
+ *
+ * Copyright (c) 2003-2006, Ericsson AB
+ * Copyright (c) 2005, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_CONFIG_H
+#define _TIPC_CONFIG_H
+
+/* ---------------------------------------------------------------------- */
+
+#include "tipc_core.h"
+#include "tipc_link.h"
+
+struct sk_buff *tipc_cfg_reply_alloc(int payload_size);
+int tipc_cfg_append_tlv(struct sk_buff *buf, int tlv_type,
+			void *tlv_data, int tlv_data_size);
+struct sk_buff *tipc_cfg_reply_unsigned_type(u16 tlv_type, u32 value);
+struct sk_buff *tipc_cfg_reply_string_type(u16 tlv_type, char *string);
+
+static inline struct sk_buff *tipc_cfg_reply_none(void)
+{
+	return tipc_cfg_reply_alloc(0);
+}
+
+static inline struct sk_buff *tipc_cfg_reply_unsigned(u32 value)
+{
+	return tipc_cfg_reply_unsigned_type(TIPC_TLV_UNSIGNED, value);
+}
+
+static inline struct sk_buff *tipc_cfg_reply_error_string(char *string)
+{
+	return tipc_cfg_reply_string_type(TIPC_TLV_ERROR_STRING, string);
+}
+
+static inline struct sk_buff *tipc_cfg_reply_ultra_string(char *string)
+{
+	return tipc_cfg_reply_string_type(TIPC_TLV_ULTRA_STRING, string);
+}
+
+struct sk_buff *tipc_cfg_do_cmd(u32 orig_node, u16 cmd,
+				const void *req_tlv_area, int req_tlv_space,
+				int headroom);
+
+int  tipc_cfg_init(void);
+void tipc_cfg_stop(void);
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_core.c android_cluster/linux-2.6.29/net/tipc/tipc_core.c
--- linux-2.6.29/net/tipc/tipc_core.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_core.c	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,294 @@
+/*
+ * net/tipc/tipc_core.c: TIPC module code
+ *
+ * Copyright (c) 2003-2006, Ericsson AB
+ * Copyright (c) 2005-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/random.h>
+
+#include "tipc_core.h"
+#include "tipc_dbg.h"
+#include "tipc_ref.h"
+#include "tipc_net.h"
+#include "tipc_user_reg.h"
+#include "tipc_name_table.h"
+#include "tipc_topsrv.h"
+#include "tipc_cfgsrv.h"
+
+/* configurable TIPC parameters */
+
+#ifndef CONFIG_TIPC_ADVANCED
+#define CONFIG_TIPC_NETID	4711
+#define CONFIG_TIPC_REMOTE_MNG	1
+#define CONFIG_TIPC_PORTS	8191
+#define CONFIG_TIPC_NODES	255
+#define CONFIG_TIPC_CLUSTERS	8
+#define CONFIG_TIPC_ZONES	4
+#define CONFIG_TIPC_REMOTES	8
+#define CONFIG_TIPC_PUBL	10000
+#define CONFIG_TIPC_SUBSCR	2000
+#define CONFIG_TIPC_LOG		0
+#else
+#ifndef CONFIG_TIPC_REMOTE_MNG
+#define CONFIG_TIPC_REMOTE_MNG	0
+#endif
+#endif
+
+u32 tipc_own_addr;
+int tipc_net_id;
+int tipc_remote_management;
+int tipc_max_nodes;
+int tipc_max_clusters;
+int tipc_max_zones;
+int tipc_max_remotes;
+int tipc_max_ports;
+int tipc_max_publications;
+int tipc_max_subscriptions;
+
+/* global variables used by multiple sub-systems within TIPC */
+
+int tipc_mode = TIPC_NOT_RUNNING;
+int tipc_random;
+atomic_t tipc_user_count = ATOMIC_INIT(0);
+
+const char tipc_alphabet[] =
+	"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_.";
+
+
+int tipc_get_mode(void)
+{
+	return tipc_mode;
+}
+
+/**
+ * buf_acquire - creates a TIPC message buffer
+ * @size: message size (including TIPC header)
+ *
+ * Returns a new buffer with data pointers set to the specified size.
+ * 
+ * NOTE: Headroom is reserved to allow prepending of a data link header.
+ *       There may also be unrequested tailroom present at the buffer's end.
+ */
+
+struct sk_buff *buf_acquire(u32 size)
+{
+	struct sk_buff *skb;
+	unsigned int buf_size = (BUF_HEADROOM + size + 3) & ~3u;
+
+	skb = alloc_skb_fclone(buf_size, GFP_ATOMIC);
+	if (skb) {
+		skb_reserve(skb, BUF_HEADROOM);
+		skb_put(skb, size);
+		skb->next = NULL;
+	}
+	dbg_assert(skb != NULL);
+	return skb;
+}
+
+/**
+ * tipc_core_stop_net - shut down TIPC networking sub-systems
+ */
+
+void tipc_core_stop_net(void)
+{
+	tipc_net_stop();
+	tipc_eth_media_stop();
+}
+
+/**
+ * tipc_core_start_net - start TIPC networking sub-systems
+ */
+
+int tipc_core_start_net(unsigned long addr)
+{
+	int res;
+
+	if ((res = tipc_net_start(addr)) ||
+	    (res = tipc_eth_media_start())) {
+		tipc_core_stop_net();
+	}
+	return res;
+}
+
+/**
+ * tipc_core_stop - switch TIPC from SINGLE NODE to NOT RUNNING mode
+ */
+
+void tipc_core_stop(void)
+{
+	if (tipc_mode != TIPC_NODE_MODE)
+		return;
+
+	tipc_mode = TIPC_NOT_RUNNING;
+
+	tipc_socket_stop();
+	tipc_netlink_stop();
+	tipc_cfg_stop();
+	tipc_subscr_stop();
+	tipc_nametbl_stop();
+	tipc_routetbl_stop();
+	tipc_reg_stop();
+	tipc_ref_table_stop();
+	tipc_handler_stop();
+}
+
+/**
+ * tipc_core_start - switch TIPC from NOT RUNNING to SINGLE NODE mode
+ */
+
+int tipc_core_start(void)
+{
+	int res;
+
+	if (tipc_mode != TIPC_NOT_RUNNING)
+		return -ENOPROTOOPT;
+
+	get_random_bytes(&tipc_random, sizeof(tipc_random));
+	tipc_mode = TIPC_NODE_MODE;
+
+	if ((res = tipc_handler_start())
+	    || (res = tipc_ref_table_init(tipc_max_ports, tipc_random))
+	    || (res = tipc_reg_start())
+	    || (res = tipc_routetbl_init())
+	    || (res = tipc_nametbl_init())
+            || (res = tipc_k_signal((Handler)tipc_subscr_start, 0))
+	    || (res = tipc_k_signal((Handler)tipc_cfg_init, 0))
+	    || (res = tipc_netlink_start())
+	    || (res = tipc_socket_init())
+	    ) {
+		tipc_core_stop();
+	}
+	return res;
+}
+
+
+static int __init tipc_init(void)
+{
+	int res;
+
+	tipc_log_resize(CONFIG_TIPC_LOG);
+	info("Activated (version " TIPC_MOD_VER
+	     " compiled " __DATE__ " " __TIME__ ")\n");
+
+	tipc_own_addr = 0;
+	tipc_net_id = delimit(CONFIG_TIPC_NETID, 1, 9999);
+	tipc_remote_management = CONFIG_TIPC_REMOTE_MNG;
+	tipc_max_ports = delimit(CONFIG_TIPC_PORTS, 127, 65536);
+	tipc_max_nodes = delimit(CONFIG_TIPC_NODES, 8, 4095);
+	tipc_max_clusters = delimit(CONFIG_TIPC_CLUSTERS, 1, 4095);
+	tipc_max_zones = delimit(CONFIG_TIPC_ZONES, 1, 255);
+	tipc_max_remotes = delimit(CONFIG_TIPC_REMOTES, 0, 255);
+	tipc_max_publications = delimit(CONFIG_TIPC_PUBL, 1, 65535);
+	tipc_max_subscriptions = delimit(CONFIG_TIPC_SUBSCR, 1, 65535);
+
+	if ((res = tipc_core_start()))
+		err("Unable to start in single node mode\n");
+	else
+		info("Started in single node mode\n");
+	return res;
+}
+
+static void __exit tipc_exit(void)
+{
+	tipc_core_stop_net();
+	tipc_core_stop();
+	info("Deactivated\n");
+	tipc_log_resize(0);
+}
+
+module_init(tipc_init);
+module_exit(tipc_exit);
+
+MODULE_DESCRIPTION("TIPC: Transparent Inter Process Communication");
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_VERSION(TIPC_MOD_VER);
+
+/* Native TIPC API for kernel-space applications (see tipc.h) */
+
+EXPORT_SYMBOL(tipc_attach);
+EXPORT_SYMBOL(tipc_detach);
+EXPORT_SYMBOL(tipc_get_addr);
+EXPORT_SYMBOL(tipc_get_mode);
+EXPORT_SYMBOL(tipc_createport);
+EXPORT_SYMBOL(tipc_deleteport);
+EXPORT_SYMBOL(tipc_ownidentity);
+EXPORT_SYMBOL(tipc_portimportance);
+EXPORT_SYMBOL(tipc_set_portimportance);
+EXPORT_SYMBOL(tipc_portunreliable);
+EXPORT_SYMBOL(tipc_set_portunreliable);
+EXPORT_SYMBOL(tipc_portunreturnable);
+EXPORT_SYMBOL(tipc_set_portunreturnable);
+EXPORT_SYMBOL(tipc_publish);
+EXPORT_SYMBOL(tipc_withdraw);
+EXPORT_SYMBOL(tipc_connect2port);
+EXPORT_SYMBOL(tipc_disconnect);
+EXPORT_SYMBOL(tipc_shutdown);
+EXPORT_SYMBOL(tipc_isconnected);
+EXPORT_SYMBOL(tipc_peer);
+EXPORT_SYMBOL(tipc_ref_valid);
+EXPORT_SYMBOL(tipc_send);
+EXPORT_SYMBOL(tipc_send_buf);
+EXPORT_SYMBOL(tipc_send2name);
+EXPORT_SYMBOL(tipc_forward2name);
+EXPORT_SYMBOL(tipc_send_buf2name);
+EXPORT_SYMBOL(tipc_forward_buf2name);
+EXPORT_SYMBOL(tipc_send2port);
+EXPORT_SYMBOL(tipc_forward2port);
+EXPORT_SYMBOL(tipc_send_buf2port);
+EXPORT_SYMBOL(tipc_forward_buf2port);
+EXPORT_SYMBOL(tipc_multicast);
+/* EXPORT_SYMBOL(tipc_multicast_buf); not available yet */
+EXPORT_SYMBOL(tipc_ispublished);
+EXPORT_SYMBOL(tipc_available_nodes);
+
+/* TIPC API for external bearers (see tipc_bearer.h) */
+
+EXPORT_SYMBOL(tipc_block_bearer);
+EXPORT_SYMBOL(tipc_continue);
+EXPORT_SYMBOL(tipc_disable_bearer);
+EXPORT_SYMBOL(tipc_enable_bearer);
+EXPORT_SYMBOL(tipc_recv_msg);
+EXPORT_SYMBOL(tipc_register_media);
+
+/* TIPC API for external APIs (see tipc_port.h) */
+
+EXPORT_SYMBOL(tipc_createport_raw);
+EXPORT_SYMBOL(tipc_reject_msg);
+EXPORT_SYMBOL(tipc_send_buf_fast);
+EXPORT_SYMBOL(tipc_acknowledge);
+EXPORT_SYMBOL(tipc_get_port);
+EXPORT_SYMBOL(tipc_get_handle);
+
diff -ruN linux-2.6.29/net/tipc/tipc_core.h android_cluster/linux-2.6.29/net/tipc/tipc_core.h
--- linux-2.6.29/net/tipc/tipc_core.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_core.h	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,429 @@
+/*
+ * net/tipc/tipc_core.h: Include file for TIPC global declarations
+ *
+ * Copyright (c) 2005-2006, Ericsson AB
+ * Copyright (c) 2005-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_CORE_H
+#define _TIPC_CORE_H
+
+#include <linux/tipc.h>
+#include <linux/tipc_config.h>
+#include <net/tipc/tipc_plugin_msg.h>
+#include <net/tipc/tipc_plugin_port.h>
+#include <net/tipc/tipc_plugin_if.h>
+#include <net/tipc/tipc.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/mm.h>
+#include <linux/timer.h>
+#include <linux/string.h>
+#include <asm/uaccess.h>
+#include <linux/interrupt.h>
+#include <asm/atomic.h>
+#include <asm/hardirq.h>
+#include <linux/netdevice.h>
+#include <linux/in.h>
+#include <linux/list.h>
+#include <linux/version.h>
+#include <linux/vmalloc.h>
+
+
+#define TIPC_MOD_VER "1.7.6"
+
+/*
+ * Spinlock wrappers (lets TIPC common files run unchanged on other OS's)
+ */
+ 
+#define DECLARE_SPINLOCK(x) extern spinlock_t x
+#define DECLARE_RWLOCK(x)   extern rwlock_t x
+
+static inline void spin_lock_term(spinlock_t *lock) { }
+
+/*
+ * Sanity test macros
+ */
+
+#define assert(i)  BUG_ON(!(i))
+
+#ifdef CONFIG_TIPC_DEBUG
+#define dbg_assert(i) BUG_ON(!(i))
+#else
+#define dbg_assert(i) do {} while (0)
+#endif
+
+/*
+ * TIPC system monitoring code
+ */
+
+/*
+ * TIPC's print buffer subsystem supports the following print buffers:
+ *
+ * TIPC_NULL : null buffer (i.e. print nowhere)
+ * TIPC_CONS : system console
+ * TIPC_LOG  : TIPC log buffer
+ * &buf	     : user-defined buffer (struct print_buf *)
+ *
+ * Note: TIPC_LOG is configured to echo its output to the system console;
+ *       user-defined buffers can be configured to do the same thing.
+ */
+
+extern struct print_buf *const TIPC_NULL;
+extern struct print_buf *const TIPC_CONS;
+extern struct print_buf *const TIPC_LOG;
+
+void tipc_printf(struct print_buf *, const char *fmt, ...);
+
+/*
+ * TIPC_OUTPUT is the destination print buffer for system messages.
+ */
+
+#ifndef TIPC_OUTPUT
+#define TIPC_OUTPUT TIPC_LOG
+#endif
+
+/*
+ * TIPC can be configured to send system messages to TIPC_OUTPUT, the system
+ * console only, or to suppress them entirely.
+ */
+
+#ifdef CONFIG_TIPC_SYSTEM_MSGS
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+#define err(fmt, arg...)  tipc_printf(TIPC_OUTPUT, KERN_ERR "TIPC: " fmt, ## arg)
+#define warn(fmt, arg...) tipc_printf(TIPC_OUTPUT, KERN_WARNING "TIPC: " fmt, ## arg)
+#define info(fmt, arg...) tipc_printf(TIPC_OUTPUT, KERN_NOTICE "TIPC: " fmt, ## arg)
+
+#else
+
+#define err(fmt, arg...)  printk(KERN_ERR "TIPC: " fmt , ## arg)
+#define info(fmt, arg...) printk(KERN_INFO "TIPC: " fmt , ## arg)
+#define warn(fmt, arg...) printk(KERN_WARNING "TIPC: " fmt , ## arg)
+
+#endif
+
+#else
+
+#define err(fmt, arg...)  do {} while (0)
+#define info(fmt, arg...) do {} while (0)
+#define warn(fmt, arg...) do {} while (0)
+
+#endif
+
+/*
+ * DBG_OUTPUT is the destination print buffer for debug messages.
+ * It defaults to the the null print buffer, but can be redefined
+ * (typically in the individual .c files being debugged) to allow
+ * selected debug messages to be generated where needed.
+ */
+
+#ifndef DBG_OUTPUT
+#define DBG_OUTPUT TIPC_NULL
+#endif
+
+/*
+ * TIPC can be configured to send debug messages to the specified print buffer
+ * (typically DBG_OUTPUT) or to suppress them entirely.
+ */
+
+#ifdef CONFIG_TIPC_DEBUG
+
+#define dbg_printf(pb, fmt, arg...) \
+	do {if (pb != TIPC_NULL) tipc_printf(pb, fmt, ## arg);} while (0)
+#define dbg_msg(pb, msg, txt) \
+	do {if (pb != TIPC_NULL) tipc_msg_dbg(pb, msg, txt);} while (0)
+#define dbg_dump(pb, fmt, arg...) \
+	do {if (pb != TIPC_NULL) tipc_dump_dbg(pb, fmt, ##arg);} while (0)
+
+#define dbg(fmt, arg...)	dbg_printf(DBG_OUTPUT, fmt, ##arg)
+#define msg_dbg(msg, txt)	dbg_msg(DBG_OUTPUT, msg, txt)
+#define dump_dbg(fmt, arg...)	dbg_dump(DBG_OUTPUT, fmt, ##arg)
+
+void tipc_msg_dbg(struct print_buf *, struct tipc_msg *, const char *);
+void tipc_dump_dbg(struct print_buf *, const char *fmt, ...);
+
+#else
+
+#define dbg_printf(pb, fmt, arg...)	do {} while (0)
+#define dbg_msg(pb, msg, txt)		do {} while (0)
+#define dbg_dump(pb, fmt, arg...)	do {} while (0)
+
+#define dbg(fmt, arg...)	do {} while (0)
+#define msg_dbg(msg, txt)	do {} while (0)
+#define dump_dbg(fmt, arg...)	do {} while (0)
+
+#define tipc_msg_dbg(...)	do {} while (0)
+#define tipc_dump_dbg(...)	do {} while (0)
+
+#endif
+
+
+/*
+ * TIPC-specific error codes
+ */
+
+#define ELINKCONG EAGAIN	/* link congestion <=> resource unavailable */
+
+/*
+ * Global configuration variables
+ */
+
+extern u32 tipc_own_addr;
+extern int tipc_max_nodes;
+extern int tipc_max_clusters;
+extern int tipc_max_zones;
+extern int tipc_max_remotes;
+extern int tipc_max_ports;
+extern int tipc_max_subscriptions;
+extern int tipc_max_publications;
+extern int tipc_net_id;
+extern int tipc_remote_management;
+
+/*
+ * Other global variables
+ */
+
+extern int tipc_mode;
+extern int tipc_random;
+extern const char tipc_alphabet[];
+extern atomic_t tipc_user_count;
+
+
+/*
+ * Routines available to privileged subsystems
+ */
+
+extern int  tipc_core_start(void);
+extern void tipc_core_stop(void);
+extern int  tipc_core_start_net(unsigned long addr);
+extern void tipc_core_stop_net(void);
+extern int  tipc_handler_start(void);
+extern void tipc_handler_stop(void);
+extern int  tipc_netlink_start(void);
+extern void tipc_netlink_stop(void);
+extern int  tipc_socket_init(void);
+extern void tipc_socket_stop(void);
+
+static inline int delimit(int val, int min, int max)
+{
+	if (val > max)
+		return max;
+	if (val < min)
+		return min;
+	return val;
+}
+
+
+/*
+ * TIPC timer and signal code
+ */
+
+typedef void (*Handler) (unsigned long);
+
+u32 tipc_k_signal(Handler routine, unsigned long argument);
+
+/**
+ * k_init_timer - initialize a timer
+ * @timer: pointer to timer structure
+ * @routine: pointer to routine to invoke when timer expires
+ * @argument: value to pass to routine when timer expires
+ *
+ * Timer must be initialized before use (and terminated when no longer needed).
+ */
+
+static inline void k_init_timer(struct timer_list *timer, Handler routine,
+				unsigned long argument)
+{
+	dbg("initializing timer %p\n", timer);
+	setup_timer(timer, routine, argument);
+}
+
+/**
+ * k_start_timer - start a timer
+ * @timer: pointer to timer structure
+ * @msec: time to delay (in ms)
+ *
+ * Schedules a previously initialized timer for later execution.
+ * If timer is already running, the new timeout overrides the previous request.
+ *
+ * To ensure the timer doesn't expire before the specified delay elapses,
+ * the amount of delay is rounded up when converting to the jiffies
+ * then an additional jiffy is added to account for the fact that
+ * the starting time may be in the middle of the current jiffy.
+ */
+
+static inline void k_start_timer(struct timer_list *timer, unsigned long msec)
+{
+	dbg("starting timer %p for %u\n", timer, msec);
+	mod_timer(timer, jiffies + msecs_to_jiffies(msec) + 1);
+}
+
+/**
+ * k_cancel_timer - cancel a timer
+ * @timer: pointer to timer structure
+ *
+ * Cancels a previously initialized timer.
+ * Can be called safely even if the timer is already inactive.
+ *
+ * WARNING: Must not be called when holding locks required by the timer's
+ *          timeout routine, otherwise deadlock can occur on SMP systems!
+ */
+
+static inline void k_cancel_timer(struct timer_list *timer)
+{
+	dbg("cancelling timer %p\n", timer);
+	del_timer_sync(timer);
+}
+
+/**
+ * k_term_timer - terminate a timer
+ * @timer: pointer to timer structure
+ *
+ * Prevents further use of a previously initialized timer.
+ *
+ * WARNING: Caller must ensure timer isn't currently running.
+ *
+ * (Do not "enhance" this routine to automatically cancel an active timer,
+ * otherwise deadlock can arise when a timeout routine calls k_term_timer.)
+ */
+
+static inline void k_term_timer(struct timer_list *timer)
+{
+	dbg("terminating timer %p\n", timer);
+}
+
+
+/*
+ * TIPC message buffer code
+ *
+ * TIPC message buffer headroom reserves space for the worst-case
+ * link-level device header (in case the message is sent off-node).
+ *
+ * Note: Headroom should be a multiple of 4 to ensure the TIPC header fields
+ *       are word aligned for quicker access
+ */
+
+#define BUF_HEADROOM LL_MAX_HEADER
+
+struct tipc_skb_cb {
+	void *handle;
+};
+
+#define TIPC_SKB_CB(__skb) ((struct tipc_skb_cb *)&((__skb)->cb[0]))
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+static inline unsigned char *skb_tail_pointer(const struct sk_buff *skb)
+{
+	return skb->tail;
+}
+
+static inline void skb_copy_to_linear_data(struct sk_buff *skb,
+					   const void *from,
+					   const unsigned int len)
+{
+	memcpy(skb->data, from, len);
+}
+
+static inline void skb_copy_to_linear_data_offset(struct sk_buff *skb,
+						  const int offset,
+						  const void *from,
+						  const unsigned int len)
+{
+	memcpy(skb->data + offset, from, len);
+}
+#endif
+
+
+static inline struct tipc_msg *buf_msg(struct sk_buff *skb)
+{
+	return (struct tipc_msg *)skb->data;
+}
+
+static inline void *buf_handle(struct sk_buff *skb)
+{
+	return TIPC_SKB_CB(skb)->handle;
+}
+
+static inline void buf_set_handle(struct sk_buff *skb, void *value)
+{
+	TIPC_SKB_CB(skb)->handle = value;
+}
+
+/**
+ * buf_acquire - creates a TIPC message buffer
+ * @size: message size (including TIPC header)
+ *
+ * Returns a new buffer with data pointers set to the specified size.
+ *
+ * NOTE: Headroom is reserved to allow prepending of a data link header.
+ *       There may also be unrequested tailroom present at the buffer's end.
+ */
+
+struct sk_buff *buf_acquire(u32 size);
+
+/**
+ * buf_discard - frees a TIPC message buffer
+ * @skb: message buffer
+ *
+ * Frees a message buffer.  If passed NULL, just returns.
+ */
+
+static inline void buf_discard(struct sk_buff *skb)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,17)
+	kfree_skb(skb);
+#else
+	if (likely(skb != NULL))
+		kfree_skb(skb);
+#endif
+}
+
+/**
+ * buf_linearize - convert a TIPC message buffer into a single contiguous piece
+ * @skb: message buffer
+ *
+ * Returns 0 on success.
+ */
+
+static inline int buf_linearize(struct sk_buff *skb)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,18)
+	return skb_linearize(skb);
+#else
+	return skb_linearize(skb, GFP_ATOMIC);
+#endif
+}
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_dbg.c android_cluster/linux-2.6.29/net/tipc/tipc_dbg.c
--- linux-2.6.29/net/tipc/tipc_dbg.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_dbg.c	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,437 @@
+/*
+ * net/tipc/tipc_dbg.c: TIPC print buffer routines
+ *
+ * Copyright (c) 1996-2006, Ericsson AB
+ * Copyright (c) 2005-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_cfgsrv.h"
+#include "tipc_dbg.h"
+
+#if defined(CONFIG_TIPC_CONFIG_SERVICE) \
+    || defined(CONFIG_TIPC_SYSTEM_MSGS) \
+    || defined(CONFIG_TIPC_DEBUG)
+
+/*
+ * TIPC pre-defines the following print buffers:
+ *
+ * TIPC_NULL : null buffer (i.e. print nowhere)
+ * TIPC_CONS : system console
+ * TIPC_LOG  : TIPC log buffer
+ *
+ * Additional user-defined print buffers are also permitted.
+ */
+
+static struct print_buf null_buf = { NULL, 0, NULL, 0 };
+struct print_buf *const TIPC_NULL = &null_buf;
+
+static struct print_buf cons_buf = { NULL, 0, NULL, 1 };
+struct print_buf *const TIPC_CONS = &cons_buf;
+
+static struct print_buf log_buf = { NULL, 0, NULL, 1 };
+struct print_buf *const TIPC_LOG = &log_buf;
+
+/*
+ * Locking policy when using print buffers.
+ *
+ * 1) tipc_printf() uses 'print_lock' to protect against concurrent access to
+ * 'print_string' when writing to a print buffer. This also protects against
+ * concurrent writes to the print buffer being written to.
+ *
+ * 2) tipc_dump_dbg() and tipc_log_XXX() leverage the aforementioned
+ * use of 'print_lock' to protect against all types of concurrent operations
+ * on their associated print buffer (not just write operations).
+ *
+ * Note: All routines of the form tipc_printbuf_XXX() are lock-free, and rely
+ * on the caller to prevent simultaneous use of the print buffer(s) being
+ * manipulated.
+ */
+
+static char print_string[TIPC_PB_MAX_STR];
+static DEFINE_SPINLOCK(print_lock);
+
+
+#define FORMAT(PTR,LEN,FMT) \
+{\
+       va_list args;\
+       va_start(args, FMT);\
+       LEN = vsprintf(PTR, FMT, args);\
+       va_end(args);\
+       *(PTR + LEN) = '\0';\
+}
+
+/**
+ * tipc_printbuf_init - initialize print buffer to empty
+ * @pb: pointer to print buffer structure
+ * @raw: pointer to character array used by print buffer
+ * @size: size of character array
+ *
+ * Note: If the character array is too small (or absent), the print buffer
+ * becomes a null device that discards anything written to it.
+ */
+
+void tipc_printbuf_init(struct print_buf *pb, char *raw, u32 size)
+{
+	pb->buf = raw;
+	pb->crs = raw;
+	pb->size = size;
+	pb->echo = 0;
+
+	if (size < TIPC_PB_MIN_SIZE) {
+		pb->buf = NULL;
+	} else if (raw) {
+		pb->buf[0] = 0;
+		pb->buf[size - 1] = ~0;
+	}
+}
+
+/**
+ * tipc_printbuf_reset - reinitialize print buffer to empty state
+ * @pb: pointer to print buffer structure
+ */
+
+void tipc_printbuf_reset(struct print_buf *pb)
+{
+	if (pb->buf) {
+		pb->crs = pb->buf;
+		pb->buf[0] = 0;
+		pb->buf[pb->size - 1] = ~0;
+	}
+}
+
+/**
+ * tipc_printbuf_empty - test if print buffer is in empty state
+ * @pb: pointer to print buffer structure
+ *
+ * Returns non-zero if print buffer is empty.
+ */
+
+int tipc_printbuf_empty(struct print_buf *pb)
+{
+	return (!pb->buf || (pb->crs == pb->buf));
+}
+
+/**
+ * tipc_printbuf_validate - check for print buffer overflow
+ * @pb: pointer to print buffer structure
+ *
+ * Verifies that a print buffer has captured all data written to it.
+ * If data has been lost, linearize buffer and prepend an error message
+ *
+ * Returns length of print buffer data string (including trailing NUL)
+ */
+
+int tipc_printbuf_validate(struct print_buf *pb)
+{
+	char *err = "\n\n*** PRINT BUFFER OVERFLOW ***\n\n";
+	char *cp_buf;
+	struct print_buf cb;
+
+	if (!pb->buf)
+		return 0;
+
+	if (pb->buf[pb->size - 1] == 0) {
+		cp_buf = kmalloc(pb->size, GFP_ATOMIC);
+		if (cp_buf) {
+			tipc_printbuf_init(&cb, cp_buf, pb->size);
+			tipc_printbuf_move(&cb, pb);
+			tipc_printbuf_move(pb, &cb);
+			kfree(cp_buf);
+			memcpy(pb->buf, err, strlen(err));
+		} else {
+			tipc_printbuf_reset(pb);
+			tipc_printf(pb, err);
+		}
+	}
+	return (pb->crs - pb->buf + 1);
+}
+
+/**
+ * tipc_printbuf_move - move print buffer contents to another print buffer
+ * @pb_to: pointer to destination print buffer structure
+ * @pb_from: pointer to source print buffer structure
+ *
+ * Current contents of destination print buffer (if any) are discarded.
+ * Source print buffer becomes empty if a successful move occurs.
+ */
+
+void tipc_printbuf_move(struct print_buf *pb_to, struct print_buf *pb_from)
+{
+	int len;
+
+	/* Handle the cases where contents can't be moved */
+
+	if (!pb_to->buf)
+		return;
+
+	if (!pb_from->buf) {
+		tipc_printbuf_reset(pb_to);
+		return;
+	}
+
+	if (pb_to->size < pb_from->size) {
+		strcpy(pb_to->buf, "*** PRINT BUFFER MOVE ERROR ***");
+		pb_to->buf[pb_to->size - 1] = ~0;
+		pb_to->crs = strchr(pb_to->buf, 0);
+		return;
+	}
+
+	/* Copy data from char after cursor to end (if used) */
+
+	len = pb_from->buf + pb_from->size - pb_from->crs - 2;
+	if ((pb_from->buf[pb_from->size - 1] == 0) && (len > 0)) {
+		strcpy(pb_to->buf, pb_from->crs + 1);
+		pb_to->crs = pb_to->buf + len;
+	} else
+		pb_to->crs = pb_to->buf;
+
+	/* Copy data from start to cursor (always) */
+
+	len = pb_from->crs - pb_from->buf;
+	strcpy(pb_to->crs, pb_from->buf);
+	pb_to->crs += len;
+
+	tipc_printbuf_reset(pb_from);
+}
+
+/**
+ * tipc_printf - append formatted output to print buffer
+ * @pb: pointer to print buffer
+ * @fmt: formatted info to be printed
+ */
+
+void tipc_printf(struct print_buf *pb, const char *fmt, ...)
+{
+	int chars_to_add;
+	int chars_left;
+	char save_char;
+
+	spin_lock_bh(&print_lock);
+
+	FORMAT(print_string, chars_to_add, fmt);
+	if (chars_to_add >= TIPC_PB_MAX_STR)
+		strcpy(print_string, "*** PRINT BUFFER STRING TOO LONG ***");
+
+	if (pb->buf) {
+		chars_left = pb->buf + pb->size - pb->crs - 1;
+		if (chars_to_add <= chars_left) {
+			strcpy(pb->crs, print_string);
+			pb->crs += chars_to_add;
+		} else if (chars_to_add >= (pb->size - 1)) {
+			strcpy(pb->buf, print_string + chars_to_add + 1
+			       - pb->size);
+			pb->crs = pb->buf + pb->size - 1;
+		} else {
+			strcpy(pb->buf, print_string + chars_left);
+			save_char = print_string[chars_left];
+			print_string[chars_left] = 0;
+			strcpy(pb->crs, print_string);
+			print_string[chars_left] = save_char;
+			pb->crs = pb->buf + chars_to_add - chars_left;
+		}
+	}
+
+	if (pb->echo)
+		printk(print_string);
+
+	spin_unlock_bh(&print_lock);
+}
+
+#endif
+
+#ifdef CONFIG_TIPC_DEBUG
+
+/**
+ * print_to_console - write string of bytes to console in multiple chunks
+ */
+
+static void print_to_console(char *crs, int len)
+{
+	int rest = len;
+
+	while (rest > 0) {
+		int sz = rest < TIPC_PB_MAX_STR ? rest : TIPC_PB_MAX_STR;
+		char c = crs[sz];
+
+		crs[sz] = 0;
+		printk((const char *)crs);
+		crs[sz] = c;
+		rest -= sz;
+		crs += sz;
+	}
+}
+
+/**
+ * printbuf_dump - write print buffer contents to console
+ */
+
+static void printbuf_dump_dbg(struct print_buf *pb)
+{
+	int len;
+
+	if (!pb->buf) {
+		printk("*** PRINT BUFFER NOT ALLOCATED ***");
+		return;
+	}
+
+	/* Dump print buffer from char after cursor to end (if used) */
+
+	len = pb->buf + pb->size - pb->crs - 2;
+	if ((pb->buf[pb->size - 1] == 0) && (len > 0))
+		print_to_console(pb->crs + 1, len);
+
+	/* Dump print buffer from start to cursor (always) */
+
+	len = pb->crs - pb->buf;
+	print_to_console(pb->buf, len);
+}
+
+/**
+ * tipc_dump_dbg - dump (non-console) print buffer to console
+ * @pb: pointer to print buffer
+ */
+
+void tipc_dump_dbg(struct print_buf *pb, const char *fmt, ...)
+{
+	int len;
+
+	if (pb == TIPC_CONS)
+		return;
+
+	spin_lock_bh(&print_lock);
+
+	FORMAT(print_string, len, fmt);
+	printk(print_string);
+
+	printk("\n---- Start of %s log dump ----\n\n",
+	       (pb == TIPC_LOG) ? "global" : "local");
+	printbuf_dump_dbg(pb);
+	tipc_printbuf_reset(pb);
+	printk("\n---- End of dump ----\n");
+
+	spin_unlock_bh(&print_lock);
+}
+
+#endif
+
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+/**
+ * tipc_log_resize - change the size of the TIPC log buffer
+ * @log_size: print buffer size to use
+ */
+
+int tipc_log_resize(int log_size)
+{
+	int res = 0;
+
+	spin_lock_bh(&print_lock);
+	if (TIPC_LOG->buf) {
+		kfree(TIPC_LOG->buf);
+		TIPC_LOG->buf = NULL;
+	}
+	if (log_size) {
+		if (log_size < TIPC_PB_MIN_SIZE)
+			log_size = TIPC_PB_MIN_SIZE;
+		res = TIPC_LOG->echo;
+		tipc_printbuf_init(TIPC_LOG, kmalloc(log_size, GFP_ATOMIC),
+				   log_size);
+		TIPC_LOG->echo = res;
+		res = !TIPC_LOG->buf;
+	}
+	spin_unlock_bh(&print_lock);
+
+	return res;
+}
+
+/**
+ * tipc_log_resize_cmd - reconfigure size of TIPC log buffer
+ */
+
+struct sk_buff *tipc_log_resize_cmd(const void *req_tlv_area, int req_tlv_space)
+{
+	u32 value;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_UNSIGNED))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	value = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	if (value != delimit(value, 0, 32768))
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+						   " (log size must be 0-32768)");
+	if (tipc_log_resize(value))
+		return tipc_cfg_reply_error_string(
+			"unable to create specified log (log size is now 0)");
+	return tipc_cfg_reply_none();
+}
+
+/**
+ * tipc_log_dump - capture TIPC log buffer contents in configuration message
+ */
+
+struct sk_buff *tipc_log_dump(void)
+{
+	struct sk_buff *reply;
+
+	spin_lock_bh(&print_lock);
+	if (!TIPC_LOG->buf) {
+		spin_unlock_bh(&print_lock);
+		reply = tipc_cfg_reply_ultra_string("log not activated\n");
+	} else if (tipc_printbuf_empty(TIPC_LOG)) {
+		spin_unlock_bh(&print_lock);
+		reply = tipc_cfg_reply_ultra_string("log is empty\n");
+	}
+	else {
+		struct tlv_desc *rep_tlv;
+		struct print_buf pb;
+		int str_len;
+
+		str_len = min(TIPC_LOG->size, 32768u);
+		spin_unlock_bh(&print_lock);
+		reply = tipc_cfg_reply_alloc(TLV_SPACE(str_len));
+		if (reply) {
+			rep_tlv = (struct tlv_desc *)reply->data;
+			tipc_printbuf_init(&pb, TLV_DATA(rep_tlv), str_len);
+			spin_lock_bh(&print_lock);
+			tipc_printbuf_move(&pb, TIPC_LOG);
+			spin_unlock_bh(&print_lock);
+			str_len = strlen(TLV_DATA(rep_tlv)) + 1;
+			skb_put(reply, TLV_SPACE(str_len));
+			TLV_SET(rep_tlv, TIPC_TLV_ULTRA_STRING, NULL, str_len);
+		}
+	}
+	return reply;
+}
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_dbg.h android_cluster/linux-2.6.29/net/tipc/tipc_dbg.h
--- linux-2.6.29/net/tipc/tipc_dbg.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_dbg.h	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,73 @@
+/*
+ * net/tipc/tipc_dbg.h: Include file for TIPC print buffer routines
+ *
+ * Copyright (c) 1997-2006, Ericsson AB
+ * Copyright (c) 2005-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_DBG_H
+#define _TIPC_DBG_H
+
+/**
+ * struct print_buf - TIPC print buffer structure
+ * @buf: pointer to character array containing print buffer contents
+ * @size: size of character array
+ * @crs: pointer to first unused space in character array (i.e. final NUL)
+ * @echo: echo output to system console if non-zero
+ */
+
+struct print_buf {
+	char *buf;
+	u32 size;
+	char *crs;
+	int echo;
+};
+
+#define TIPC_PB_MIN_SIZE 64	/* minimum size for a print buffer's array */
+#define TIPC_PB_MAX_STR 512	/* max printable string (with trailing NUL) */
+
+void tipc_printbuf_init(struct print_buf *pb, char *buf, u32 size);
+void tipc_printbuf_reset(struct print_buf *pb);
+int  tipc_printbuf_empty(struct print_buf *pb);
+int  tipc_printbuf_validate(struct print_buf *pb);
+void tipc_printbuf_move(struct print_buf *pb_to, struct print_buf *pb_from);
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+int tipc_log_resize(int log_size);
+#else
+static inline int tipc_log_resize(int log_size) { return 0; }
+#endif
+struct sk_buff *tipc_log_resize_cmd(const void *req_tlv_area,
+				    int req_tlv_space);
+struct sk_buff *tipc_log_dump(void);
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_discover.c android_cluster/linux-2.6.29/net/tipc/tipc_discover.c
--- linux-2.6.29/net/tipc/tipc_discover.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_discover.c	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,559 @@
+/*
+ * net/tipc/tipc_discover.c: TIPC neighbor discovery code
+ * 
+ * Copyright (c) 2003-2006, Ericsson AB
+ * Copyright (c) 2005-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_dbg.h"
+#include "tipc_link.h"
+#include "tipc_net.h"
+#include "tipc_discover.h"
+#include "tipc_cfgsrv.h"
+#include "tipc_port.h"
+#include "tipc_name_table.h"
+
+#define TIPC_DISC_INIT 125	/* min delay during bearer start up */
+#define TIPC_DISC_FAST 1000	/* normal delay if bearer has no links */
+#define TIPC_DISC_SLOW 60000	/* normal delay if bearer has links */
+#define TIPC_DISC_INACTIVE 0xffffffff	/* There is no timer */
+
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+#ifdef PROTO_MULTI_DISCOVERY_OBJECT
+
+/**
+ * disc_addr_match - determine if node discovery addresses overlap
+ * 
+ * See if address pair [ma2,ta2] matches address pair [ma1,ta1] 
+ * (i.e. the contents of [ma1,ta1] can be overridden by [ma2,ta2]) 
+ *
+ * The following rules apply:
+ *
+ * - Broadcast matches broadcast if ta1 is within scope of ta2 or vice versa
+ * - Unicast matches unicast if ma1 is equal to ma2.
+ * - Always match if ta1 and ta2 are complete and equal.
+ */
+ 
+static int disc_addr_match(struct tipc_media_addr *ma1, u32 ta1,
+                           struct tipc_media_addr *ma2, u32 ta2)
+{
+        if (ma2->broadcast && ma1->broadcast) {
+                if (tipc_in_scope(ta1,ta2) || tipc_in_scope(ta1,ta2))
+                        return 1;
+        } 
+        
+        if (!ma2->broadcast && !ma1->broadcast) {
+                if (!memcmp(ma1,ma2,sizeof(struct tipc_media_addr)))
+                        return 1;
+        }
+
+        if (tipc_node(ta1) && (ta1 == ta2))
+                return 1;
+
+        return 0;
+}
+
+struct sk_buff *tipc_disc_cmd_create_link(const void *disc_tlv_area, 
+					  int disc_tlv_space) 
+{
+        char *cmd_str;
+	char cmd[TIPC_MAX_BEARER_NAME + TIPC_MAX_MEDIA_ADDR + TIPC_MAX_ADDR + 1];
+	char *if_name;
+        char *addr_string;
+        struct bearer *b_ptr;
+	struct discoverer *d_ptr;
+	struct discoverer *temp_d_ptr;
+        struct tipc_media_addr media_addr;
+        u32 domain = 0;
+	u32 zone = 0;
+	u32 cluster = 0;
+	u32 node = 0;
+
+	if (!TLV_CHECK(disc_tlv_area, disc_tlv_space, TIPC_TLV_CREATE_LINK))
+                return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+        cmd_str = (char *)TLV_DATA(disc_tlv_area);
+        strncpy(cmd, cmd_str, sizeof(cmd));
+
+        /* Find TIPC or media address, second parameter */
+
+        addr_string = strchr(cmd, ',');
+        if (addr_string == NULL)
+                return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+        *addr_string = '\0';
+        addr_string++;
+
+        /* Find bearer, first parameter */
+
+        if_name = cmd;
+        write_lock_bh(&tipc_net_lock);
+        b_ptr = tipc_bearer_find(if_name);
+        if (b_ptr == NULL) 
+                goto error;
+
+        /* If translation to media address fails, try if TIPC address */
+
+        if (b_ptr->media->str2addr(&media_addr, addr_string)) {
+
+                if (sscanf(addr_string,"%u.%u.%u", &zone, &cluster, &node) != 3)
+                        goto error;
+
+                domain = tipc_addr(zone, cluster, node);
+
+                if (!tipc_addr_domain_valid(domain))
+                        goto error;
+
+                memcpy(&media_addr, &b_ptr->media->bcast_addr, sizeof(media_addr));
+        } 
+
+
+        if (in_own_cluster(domain) && !is_slave(domain) && !is_slave(tipc_own_addr))
+                goto error;
+
+        /* 
+         * Check if corresponding discoverers already exist, and remove.
+         */
+
+	list_for_each_entry_safe(d_ptr, temp_d_ptr, &b_ptr->disc_list, disc_list) {
+                if (disc_addr_match(&d_ptr->dest, d_ptr->domain,
+				    &media_addr, domain)) {
+                        tipc_disc_deactivate(d_ptr);
+                        tipc_disc_delete(d_ptr);
+                }
+	}
+
+        if (tipc_disc_create(b_ptr, &media_addr, domain)) {
+                write_unlock_bh(&tipc_net_lock);
+                return tipc_cfg_reply_none();		
+        }
+error:
+        write_unlock_bh(&tipc_net_lock);
+        return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+}
+
+#endif
+#endif
+
+/** 
+ * disc_init_msg - initialize a link setup message
+ * @type: message type (request or response)
+ * @dest_domain: network domain of node(s) which should respond to message
+ * @b_ptr: ptr to bearer issuing message
+ */
+
+static struct sk_buff *disc_init_msg(u32 type, u32 dest_domain,
+				     struct bearer *b_ptr)
+{
+	struct sk_buff *buf = buf_acquire(INT_H_SIZE);
+	struct tipc_msg *msg;
+	u32 sig;
+
+	if (buf) {
+		msg = buf_msg(buf);
+		tipc_msg_init(msg, LINK_CONFIG, type, INT_H_SIZE, dest_domain);
+		msg_set_non_seq(msg, 1);
+		sig = tipc_random & 0xffff;
+		msg_set_node_sig(msg, (sig ? sig : 1));
+		msg_set_node_flags(msg, NF_MULTICLUSTER);
+		msg_set_dest_domain(msg, dest_domain);
+		msg_set_bc_netid(msg, tipc_net_id);
+		if (b_ptr->media->addr2msg(&b_ptr->publ.addr, &msg->hdr[5])) {
+			buf_discard(buf);
+			buf = NULL;
+		}
+	}
+	return buf;
+}
+
+/**
+ * disc_dupl_alert - issue node address duplication alert
+ * @b_ptr: pointer to bearer detecting duplication
+ * @node_addr: duplicated node address
+ * @media_addr: media address advertised by duplicated node
+ */
+
+static void disc_dupl_alert(struct bearer *b_ptr, u32 node_addr, 
+			    struct tipc_media_addr *media_addr)
+{
+#ifdef CONFIG_TIPC_SYSTEM_MSGS
+	char node_addr_str[16];
+	char media_addr_str[64];
+	struct print_buf pb;
+
+	tipc_addr_string_fill(node_addr_str, node_addr);
+	tipc_printbuf_init(&pb, media_addr_str, sizeof(media_addr_str));
+	tipc_media_addr_printf(&pb, media_addr);
+	tipc_printbuf_validate(&pb);
+	warn("Duplicate %s using %s seen on <%s>\n",
+	     node_addr_str, media_addr_str, b_ptr->publ.name);
+#endif
+}
+
+/**
+ * tipc_disc_recv_msg - handle incoming link setup message (request or response)
+ * @buf: buffer containing message
+ * @b_ptr: bearer that message arrived on
+ */
+
+void tipc_disc_recv_msg(struct sk_buff *buf, struct bearer *b_ptr)
+{
+	struct link *link;
+	struct tipc_media_addr media_addr;
+        struct sk_buff *rbuf;
+	struct tipc_msg *msg = buf_msg(buf);
+	u32 dest = msg_dest_domain(msg);
+	u32 orig = msg_prevnode(msg);
+	u32 net_id = msg_bc_netid(msg);
+	u32 type = msg_type(msg);
+	u32 signature = msg_node_sig(msg);
+	u32 node_flags = msg_node_flags(msg);
+	struct tipc_node *n_ptr;
+        struct discoverer *d_ptr;
+        int link_fully_up;
+	int found_disc;
+
+	b_ptr->media->msg2addr(&media_addr, &msg->hdr[5]);
+	msg_dbg(msg, "RECV:");
+	buf_discard(buf);
+
+	/* Validate network address of requesting node */
+
+	if (net_id != tipc_net_id)
+		return;
+
+#ifdef CONFIG_TIPC_UNICLUSTER_FRIENDLY
+	if ((node_flags & NF_MULTICLUSTER) == 0 && !in_own_cluster(orig))
+		return;
+#else
+	if ((node_flags & NF_MULTICLUSTER) == 0)
+		return;
+#endif
+
+	if (!tipc_addr_domain_valid(dest))
+		return;
+	if (!tipc_addr_node_valid(orig))
+		return;
+
+	if (orig == tipc_own_addr) {
+		if (memcmp(&media_addr, &b_ptr->publ.addr, sizeof(media_addr)))
+			disc_dupl_alert(b_ptr, tipc_own_addr, &media_addr);
+		return;
+	}
+
+	if (!tipc_in_scope(dest, tipc_own_addr))
+		return;
+	found_disc = 0;
+	list_for_each_entry(d_ptr, &b_ptr->disc_list, disc_list) {
+#if 0
+		if (disc_addr_match(&d_ptr->dest, d_ptr->domain,
+				    &media_addr, orig))
+#endif
+		if (tipc_in_scope(d_ptr->domain, orig)) {
+			found_disc = 1;
+			break;
+		}
+	}
+	if (!found_disc)
+		return;
+#if 0
+	if (is_slave(tipc_own_addr) && is_slave(orig))
+		return;
+	if (is_slave(orig) && !in_own_cluster(orig))
+		return;
+#endif
+
+        /* We can accept discovery messages from requesting node */
+
+        n_ptr = tipc_net_find_node(orig);
+        if (n_ptr == NULL) {
+                n_ptr = tipc_node_create(orig);
+		if (n_ptr == NULL)
+			return;
+        }
+        tipc_node_lock(n_ptr);
+
+	/* Don't talk to neighbor during cleanup after last session */
+
+	if (n_ptr->cleanup_required) {
+		tipc_node_unlock(n_ptr);                
+		return;
+	}
+
+	/*
+	 * Handle cases where we already have a working link on the bearer
+	 *
+	 * If the discovery message's media address doesn't match the link's,
+	 * the duplicate link request is rejected.
+	 *
+	 * If the discovery message's media address matches the link's,
+	 * the message is just a re-request for something we've already done,
+	 * so we can skip ahead.
+	 */
+
+	link = n_ptr->links[b_ptr->identity];
+	if (tipc_link_is_up(link)) {
+		if (memcmp(&link->media_addr, &media_addr, sizeof(media_addr))) {
+			disc_dupl_alert(b_ptr, orig, &media_addr);
+			tipc_node_unlock(n_ptr);                
+			return;
+		}
+		goto link_ok;
+	}
+
+	/*
+	 * Handles cases where there is no working link on this bearer
+	 *
+	 * If there is any working link to the node, ensure the discovery
+	 * message's signature is correct -- if it isn't, reject the request.
+	 *
+	 * If there is no working link to the node, ensure the discovery
+	 * message's signature is correct -- if it isn't, accept the new
+	 * signature but "invalidate" all existing links to the node to
+	 * ensure they can't re-activate without a prior discovery message.
+	 *
+	 * TODO: It might be better to delete these "stale" link endpoints,
+	 * but this could be tricky [see tipc_link_delete()].
+	 */
+
+	if (n_ptr->working_links > 0) {
+		if (signature != n_ptr->signature) {
+			disc_dupl_alert(b_ptr, orig, &media_addr);
+			tipc_node_unlock(n_ptr);                
+			return;
+		}
+	}
+	else {
+		if (signature != n_ptr->signature) {
+			struct link *curr_link;
+			int i;
+
+			for (i = 0; i < TIPC_MAX_BEARERS; i++) {
+				if ((curr_link = n_ptr->links[i]) != NULL) {
+					memset(&curr_link->media_addr, 0, 
+					       sizeof(media_addr));
+					tipc_link_reset(curr_link);
+				}
+			}
+		}
+	}
+
+	/*
+	 * Create link endpoint for this bearer if none currently exists, 
+	 * otherwise reconfigure link endpoint to use specified media address
+	 */
+
+	if (link == NULL) {
+#ifndef CONFIG_TIPC_MULTIPLE_LINKS
+		if (n_ptr->link_cnt > 0) {
+			char node_addr_str[16];
+
+			tipc_addr_string_fill(node_addr_str, orig);
+			warn("Ignoring request for second link to node %s\n",
+			     node_addr_str);
+			tipc_node_unlock(n_ptr);
+			return;
+		}
+#endif
+		link = tipc_link_create(b_ptr, orig, &media_addr);
+		if (link == NULL) {
+			warn("Memory squeeze; Failed to create link\n");
+			tipc_node_unlock(n_ptr);                
+			return;
+		}
+	} else {
+		memcpy(&link->media_addr, &media_addr, sizeof(media_addr));
+		tipc_link_reset(link);
+	}
+
+	/* Accept node info in discovery message */
+
+link_ok:
+	n_ptr->signature = signature;
+	n_ptr->flags = node_flags;
+        link_fully_up = link_working_working(link);
+        tipc_node_unlock(n_ptr);             
+   
+	/* Send response to discovery message, if necessary */
+
+        if ((type == DSC_RESP_MSG) || link_fully_up)
+                return;
+        if (b_ptr->publ.blocked)
+		return;
+        rbuf = disc_init_msg(DSC_RESP_MSG, orig, b_ptr);
+        if (rbuf != NULL) {
+                msg_dbg(buf_msg(rbuf), "SEND:");
+                tipc_bearer_send(b_ptr, rbuf, &media_addr);
+		buf_discard(rbuf);
+	}
+}
+
+/**
+ * tipc_disc_deactivate - deactivate discoverer searching
+ * @d_ptr: ptr to discoverer structure
+ */
+
+void tipc_disc_deactivate(struct discoverer *d_ptr)
+{
+        k_cancel_timer(&d_ptr->timer);
+        d_ptr->timer_intv = TIPC_DISC_INACTIVE;
+} 
+
+/**
+ * tipc_disc_update - update frequency of periodic link setup requests
+ * @d_ptr: ptr to discovery structure
+ * 
+ * Reinitiates discovery process if discoverer has no associated nodes
+ * and is either not currently searching or is searching at the slow rate
+ */
+
+void tipc_disc_update(struct discoverer *d_ptr) 
+{
+        if (d_ptr->num_nodes == 0) {
+		if ((d_ptr->timer_intv == TIPC_DISC_INACTIVE) ||
+		    (d_ptr->timer_intv > TIPC_DISC_FAST)) {
+			d_ptr->timer_intv = TIPC_DISC_INIT;
+			k_start_timer(&d_ptr->timer, d_ptr->timer_intv);
+		}
+	}
+} 
+
+/**
+ * tipc_disc_send_msg - send discovery request message
+ * @d_ptr: ptr to discoverer structure
+ */
+
+void tipc_disc_send_msg(struct discoverer *d_ptr)
+{
+        if (!d_ptr->bearer->publ.blocked) {
+		msg_dbg(buf_msg(d_ptr->buf), "SEND:");
+                tipc_bearer_send(d_ptr->bearer, d_ptr->buf, &d_ptr->dest);
+	}
+}
+
+/**
+ * disc_timeout - send a periodic discovery request
+ * @d_ptr: ptr to discoverer structure
+ * 
+ * Called whenever a link setup request timer associated with a bearer expires.
+ */
+
+static void disc_timeout(struct discoverer *d_ptr) 
+{
+        struct bearer *b_ptr = d_ptr->bearer;
+	int max_delay;
+
+	spin_lock_bh(&b_ptr->publ.lock);
+
+	/* See if discovery object can be deactivated */
+
+	if ((tipc_node(d_ptr->domain) != 0) && (d_ptr->num_nodes != 0)) {
+		d_ptr->timer_intv = TIPC_DISC_INACTIVE;
+		goto exit;
+	}
+
+	/* 
+	 * Send discovery message, then update discovery timer
+	 *
+	 * Keep doubling time between requests until limit is reached;
+	 * hold at fast polling rate if don't have any associated nodes,
+	 * otherwise hold at slow polling rate
+	 */
+
+        tipc_disc_send_msg(d_ptr);
+
+        d_ptr->timer_intv *= 2;
+	max_delay = (d_ptr->num_nodes == 0) ? TIPC_DISC_FAST : TIPC_DISC_SLOW;
+        if (d_ptr->timer_intv > max_delay)
+                d_ptr->timer_intv = max_delay;
+
+	k_start_timer(&d_ptr->timer, d_ptr->timer_intv);
+exit:
+	spin_unlock_bh(&b_ptr->publ.lock);
+}
+
+/**
+ * tipc_disc_create - start sending periodic discovery requests
+ * @b_ptr: ptr to bearer issuing requests
+ * @dest: destination address for discovery message
+ * @domain: network domain of node(s) to be discovered
+ * 
+ * Returns 1 if successful, otherwise 0.
+ *
+ * 'tipc_net_lock' must be write-locked by caller on entry
+ */
+
+int tipc_disc_create(struct bearer *b_ptr, struct tipc_media_addr *dest,
+                     u32 domain)
+{
+	struct discoverer *d_ptr;
+
+	d_ptr = kmalloc(sizeof(*d_ptr), GFP_ATOMIC);
+	if (!d_ptr)
+		return 0;
+
+	d_ptr->buf = disc_init_msg(DSC_REQ_MSG, domain, b_ptr);
+	if (!d_ptr->buf) {
+		kfree(d_ptr);
+		return 0;
+	}
+
+	d_ptr->bearer = b_ptr;
+        list_add(&d_ptr->disc_list, &b_ptr->disc_list);
+	memcpy(&d_ptr->dest, dest, sizeof(*dest));
+        d_ptr->domain = domain;
+	d_ptr->num_nodes = 0;
+	d_ptr->timer_intv = TIPC_DISC_INIT;
+	k_init_timer(&d_ptr->timer, (Handler)disc_timeout, (unsigned long)d_ptr);
+        k_start_timer(&d_ptr->timer, d_ptr->timer_intv);
+	tipc_disc_send_msg(d_ptr);
+	return 1;
+} 
+
+/**
+ * tipc_disc_delete - stop sending periodic link setup requests
+ * @disc: ptr to link request structure
+ * Timer must be cancelled or expired before doing this call
+ */
+
+void tipc_disc_delete(struct discoverer *d_ptr) 
+{
+	if (!d_ptr)
+		return;
+
+	k_term_timer(&d_ptr->timer);
+	buf_discard(d_ptr->buf);
+        list_del_init(&d_ptr->disc_list);
+	kfree(d_ptr);
+}
+
diff -ruN linux-2.6.29/net/tipc/tipc_discover.h android_cluster/linux-2.6.29/net/tipc/tipc_discover.h
--- linux-2.6.29/net/tipc/tipc_discover.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_discover.h	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,75 @@
+/*
+ * net/tipc/tipc_discover.h: Include file for TIPC neighbor discovery code
+ *
+ * Copyright (c) 2003-2006, Ericsson AB
+ * Copyright (c) 2005-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_DISCOVER_H
+#define _TIPC_DISCOVER_H
+
+#include "tipc_core.h"
+
+/**
+ * struct discoverer - information about an ongoing link setup request
+ * @bearer: bearer used for discovery messages
+ * @disc_list: adjacent discoverers belonging to the same bearer
+ * @dest: destination address for discovery messages
+ * @domain: network domain of node(s) which should respond to discovery message
+ * @num_nodes: number of nodes currently discovered
+ * @buf: discovery message to be (repeatedly) sent
+ * @timer: timer governing period between discovery messages
+ * @timer_intv: current interval between requests (in ms)
+ */
+ 
+struct discoverer {
+	struct bearer *bearer;
+	struct list_head disc_list;
+	struct tipc_media_addr dest;
+        u32 domain;
+	int num_nodes;
+	struct sk_buff *buf;
+	struct timer_list timer;
+	unsigned int timer_intv;
+};
+
+int tipc_disc_create(struct bearer *b_ptr, struct tipc_media_addr *dest,
+		     u32 domain);
+void tipc_disc_update(struct discoverer *d_ptr);
+void tipc_disc_delete(struct discoverer *d_ptr);
+void tipc_disc_deactivate(struct discoverer *d_ptr);
+void tipc_disc_recv_msg(struct sk_buff *buf, struct bearer *b_ptr);
+void tipc_disc_send_msg(struct discoverer *d_ptr);
+struct sk_buff *tipc_disc_cmd_create_link(const void *disc_tlv_area, 
+					  int disc_tlv_space);
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_eth_media.c android_cluster/linux-2.6.29/net/tipc/tipc_eth_media.c
--- linux-2.6.29/net/tipc/tipc_eth_media.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_eth_media.c	2014-05-27 23:04:10.634023603 -0700
@@ -0,0 +1,469 @@
+/*
+ * net/tipc/tipc_eth_media.c: Ethernet bearer support for TIPC
+ *
+ * Copyright (c) 2001-2007, Ericsson AB
+ * Copyright (c) 2005-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <net/tipc/tipc.h>
+#include <net/tipc/tipc_plugin_if.h>
+#include <net/tipc/tipc_plugin_msg.h>
+#include <linux/netdevice.h>
+#include <linux/workqueue.h>
+#include <linux/version.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+#include <net/net_namespace.h>
+#endif
+
+#define MAX_ETH_BEARERS		TIPC_MAX_BEARERS
+#define ETH_LINK_PRIORITY	TIPC_DEF_LINK_PRI
+#define ETH_LINK_TOLERANCE	TIPC_DEF_LINK_TOL
+
+/**
+ * struct eth_bearer - Ethernet bearer data structure
+ * @bearer: ptr to associated "generic" bearer structure
+ * @dev: ptr to associated Ethernet network device
+ * @tipc_packet_type: used in binding TIPC to Ethernet driver
+ * @cleanup: work item used when disabling bearer
+ */
+
+struct eth_bearer {
+	struct tipc_bearer *bearer;
+	struct net_device *dev;
+	struct packet_type tipc_packet_type;
+	struct work_struct cleanup;
+};
+
+static struct tipc_media eth_media_info;
+static struct eth_bearer eth_bearers[MAX_ETH_BEARERS];
+static int eth_started = 0;
+
+static struct notifier_block notifier;
+static struct work_struct reg_notifier;
+
+/**
+ * eth_media_addr_init - initialize Ethernet media address structure
+ * 
+ * Structure's "value" field stores address info in the following format:
+ * - Ethernet media type identifier [4 bytes, in network byte order]
+ * - MAC address [6 bytes]
+ * - unused [10 bytes of zeroes]
+ * 
+ * Note: This is the same format as the TIPC neighbour discovery message uses
+ * to designate an Ethernet address, which simplies the job of getting the
+ * media address into/out of the message header.
+ */
+
+static void eth_media_addr_init(struct tipc_media_addr *a, char *mac)
+{
+	memset(a->value, 0, sizeof(a->value));
+	a->value[3] = TIPC_MEDIA_ID_ETH;
+	memcpy(&a->value[4], mac, ETH_ALEN);
+
+	a->media_id = TIPC_MEDIA_ID_ETH;
+        a->broadcast = !memcmp(mac, &eth_media_info.bcast_addr.value[4], ETH_ALEN);
+}
+
+/**
+ * send_msg - send a TIPC message out over an Ethernet interface
+ */
+
+static int send_msg(struct sk_buff *buf, struct tipc_bearer *tb_ptr,
+		    struct tipc_media_addr *dest)
+{
+	struct sk_buff *clone;
+	struct net_device *dev;
+	int delta;
+
+	clone = skb_clone(buf, GFP_ATOMIC);
+	if (!clone)
+		return 0;
+	
+	dev = ((struct eth_bearer *)(tb_ptr->usr_handle))->dev;
+	delta = dev->hard_header_len - skb_headroom(buf);
+
+	if ((delta > 0) && 
+	    pskb_expand_head(clone, SKB_DATA_ALIGN(delta), 0, GFP_ATOMIC)) {
+		kfree_skb(clone);
+		return 0;
+	}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+	skb_reset_network_header(clone);
+#else
+	clone->nh.raw = clone->data;
+#endif
+	clone->dev = dev;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+	dev_hard_header(clone, dev, ETH_P_TIPC, &dest->value[4],
+			dev->dev_addr, clone->len);
+#else
+	dev->hard_header(clone, dev, ETH_P_TIPC, &dest->value[4],
+			 dev->dev_addr, clone->len);
+#endif
+	dev_queue_xmit(clone);
+	return 0;
+}
+
+/**
+ * recv_msg - handle incoming TIPC message from an Ethernet interface
+ *
+ * Accept only packets explicitly sent to this node, or broadcast packets;
+ * ignores packets sent using Ethernet multicast, and traffic sent to other
+ * nodes (which can happen if interface is running in promiscuous mode).
+ */
+
+static int recv_msg(struct sk_buff *buf, struct net_device *dev,
+		    struct packet_type *pt, struct net_device *orig_dev)
+{
+	struct eth_bearer *eb_ptr = (struct eth_bearer *)pt->af_packet_priv;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,27)
+	if (!net_eq(dev_net(dev), &init_net)) {
+		kfree_skb(buf);
+		return 0;
+	}
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+	if (dev_net(dev) != &init_net) {
+		kfree_skb(buf);
+		return 0;
+	}
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+	if (dev->nd_net != &init_net) {
+		kfree_skb(buf);
+		return 0;
+	}
+#endif
+
+	if (likely(eb_ptr->bearer)) {
+		if (likely(buf->pkt_type <= PACKET_BROADCAST)) {
+			buf->next = NULL;
+			tipc_recv_msg(buf, eb_ptr->bearer);
+			return 0;
+		}
+	}
+	kfree_skb(buf);
+	return 0;
+}
+
+/**
+ * enable_bearer - attach TIPC bearer to an Ethernet interface
+ */
+
+static int enable_bearer(struct tipc_bearer *tb_ptr)
+{
+	struct net_device *dev = NULL;
+	struct net_device *pdev;
+	struct eth_bearer *eb_ptr = &eth_bearers[0];
+	struct eth_bearer *stop = &eth_bearers[MAX_ETH_BEARERS];
+	char *driver_name = strchr((const char *)tb_ptr->name, ':') + 1;
+	int pending_dev = 0;
+
+	/* Find unused Ethernet bearer structure */
+
+	while (eb_ptr->dev) {
+		if (!eb_ptr->bearer)
+			pending_dev++;
+		if (++eb_ptr == stop)
+			return pending_dev ? -EAGAIN : -EDQUOT;
+	}
+
+	/* Find device with specified name */
+
+	read_lock(&dev_base_lock);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+	for_each_netdev(&init_net, pdev) {
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+	for_each_netdev(pdev) {
+#else
+	for (pdev = dev_base; pdev; pdev = pdev->next) {
+#endif
+		if (!strncmp(pdev->name, driver_name, IFNAMSIZ)) {
+			dev = pdev;
+			dev_hold(dev);
+			break;
+		}
+	}
+	read_unlock(&dev_base_lock);
+	if (!dev)
+		return -ENODEV;
+
+	/* Create Ethernet bearer for device */
+
+	eb_ptr->dev = dev;
+	eb_ptr->tipc_packet_type.type = htons(ETH_P_TIPC);
+	eb_ptr->tipc_packet_type.dev = dev;
+	eb_ptr->tipc_packet_type.func = recv_msg;
+	eb_ptr->tipc_packet_type.af_packet_priv = eb_ptr;
+	INIT_LIST_HEAD(&(eb_ptr->tipc_packet_type.list));
+	dev_add_pack(&eb_ptr->tipc_packet_type);
+
+	/* Associate TIPC bearer with Ethernet bearer */
+
+	eb_ptr->bearer = tb_ptr;
+	tb_ptr->usr_handle = (void *)eb_ptr;
+	tb_ptr->mtu = dev->mtu;
+	tb_ptr->blocked = 0;
+	eth_media_addr_init(&tb_ptr->addr, (char *)&dev->dev_addr);
+
+	return 0;
+}
+
+/**
+ * cleanup_bearer - break association between Ethernet bearer and interface 
+ * 
+ * This routine must be invoked from a work queue because it can sleep. 
+ */
+
+static void cleanup_bearer(struct work_struct *work)
+{
+	struct eth_bearer *eb_ptr =
+		container_of(work, struct eth_bearer, cleanup);
+
+	dev_remove_pack(&eb_ptr->tipc_packet_type);
+	dev_put(eb_ptr->dev);
+	eb_ptr->dev = NULL;
+}
+
+/**
+ * disable_bearer - detach TIPC bearer from an Ethernet interface
+ *
+ * Mark Ethernet bearer as inactive so that incoming buffers are thrown away,
+ * then get worker thread to complete bearer cleanup.  (Can't do cleanup
+ * here because cleanup code needs to sleep and caller holds spinlocks.)
+ */
+
+static void disable_bearer(struct tipc_bearer *tb_ptr)
+{
+	struct eth_bearer *eb_ptr = (struct eth_bearer *)tb_ptr->usr_handle;
+
+	eb_ptr->bearer = NULL;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,20)
+	INIT_WORK(&eb_ptr->cleanup, cleanup_bearer);
+#else
+	INIT_WORK(&eb_ptr->cleanup, (void (*)(void *))cleanup_bearer,
+		  &eb_ptr->cleanup);
+#endif
+	schedule_work(&eb_ptr->cleanup);
+}
+
+/**
+ * recv_notification - handle device updates from OS
+ *
+ * Change the state of the Ethernet bearer (if any) associated with the
+ * specified device.
+ */
+
+static int recv_notification(struct notifier_block *nb, unsigned long evt,
+			     void *dv)
+{
+	struct net_device *dev = (struct net_device *)dv;
+	struct eth_bearer *eb_ptr = &eth_bearers[0];
+	struct eth_bearer *stop = &eth_bearers[MAX_ETH_BEARERS];
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,27)
+	if (!net_eq(dev_net(dev), &init_net))
+		return NOTIFY_DONE;
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+	if (dev_net(dev) != &init_net)
+		return NOTIFY_DONE;
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+	if (dev->nd_net != &init_net)
+		return NOTIFY_DONE;
+#endif
+
+	while ((eb_ptr->dev != dev)) {
+		if (++eb_ptr == stop)
+			return NOTIFY_DONE;	/* couldn't find device */
+	}
+	if (!eb_ptr->bearer)
+		return NOTIFY_DONE;		/* bearer had been disabled */
+
+	eb_ptr->bearer->mtu = dev->mtu;
+
+	switch (evt) {
+	case NETDEV_CHANGE:
+		if (netif_carrier_ok(dev))
+			tipc_continue(eb_ptr->bearer);
+		else
+			tipc_block_bearer(eb_ptr->bearer->name);
+		break;
+	case NETDEV_UP:
+		tipc_continue(eb_ptr->bearer);
+		break;
+	case NETDEV_DOWN:
+		tipc_block_bearer(eb_ptr->bearer->name);
+		break;
+	case NETDEV_CHANGEMTU:
+	case NETDEV_CHANGEADDR:
+		tipc_block_bearer(eb_ptr->bearer->name);
+		tipc_continue(eb_ptr->bearer);
+		break;
+	case NETDEV_UNREGISTER:
+	case NETDEV_CHANGENAME:
+		tipc_disable_bearer(eb_ptr->bearer->name);
+		break;
+	}
+	return NOTIFY_OK;
+}
+
+static int eth_msg2addr(struct tipc_media_addr *a, u32 *msg_area)
+{
+	if (msg_area[0] != htonl(TIPC_MEDIA_ID_ETH))
+		return 1;
+
+	eth_media_addr_init(a, (char *)&msg_area[1]);
+	return 0;
+}
+
+static int eth_addr2msg(struct tipc_media_addr *a, u32 *msg_area)
+{
+	if (a->media_id != TIPC_MEDIA_ID_ETH)
+		return 1;
+
+	memcpy(msg_area, a->value, sizeof(a->value));
+	return 0;
+}
+
+/**
+ * eth_addr2str - convert Ethernet address to string
+ */
+
+static int eth_addr2str(struct tipc_media_addr *a, char *str_buf, int str_size)
+{                       
+	unsigned char *mac;
+
+	if ((a->media_id != TIPC_MEDIA_ID_ETH) || (str_size < 18))
+		return 1;
+		
+	mac = (unsigned char *)&a->value[4];
+	sprintf(str_buf, "%02x:%02x:%02x:%02x:%02x:%02x",
+		mac[0], mac[1], mac[2], mac[3], mac[4], mac[5]);
+	return 0;
+}
+
+
+/**
+ * eth_str2addr - convert string to Ethernet address
+ */
+
+static int eth_str2addr(struct tipc_media_addr *a, char *str_buf)
+{                     
+	char mac[6];
+
+        if (ETH_ALEN != sscanf(str_buf, "%02x:%02x:%02x:%02x:%02x:%02x",
+                               (u32 *)&mac[0], (u32 *)&mac[1], (u32 *)&mac[2],
+                               (u32 *)&mac[3], (u32 *)&mac[4], (u32 *)&mac[5]))
+            return 1;
+
+	eth_media_addr_init(a, mac);
+        return 0;
+}
+
+/*
+ * Ethernet media registration info required by TIPC
+ */
+
+static struct tipc_media eth_media_info = {
+	TIPC_MEDIA_ID_ETH,
+	"eth",
+	ETH_LINK_PRIORITY,
+	ETH_LINK_TOLERANCE,
+	TIPC_DEF_LINK_WIN,
+	send_msg,
+	enable_bearer,
+	disable_bearer,
+	eth_addr2str,
+	eth_str2addr,
+        eth_msg2addr,
+        eth_addr2msg,
+	{{0, 0, 0, TIPC_MEDIA_ID_ETH, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
+	  0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
+	TIPC_MEDIA_ID_ETH, 1}
+};
+
+/**
+ * do_registration - register TIPC to receive device notifications
+ * 
+ * This routine must be invoked from a work queue because it can sleep. 
+ */
+
+static void do_registration(struct work_struct *dummy)
+{
+	notifier.notifier_call = &recv_notification;
+	notifier.priority = 0;
+	register_netdevice_notifier(&notifier);
+}
+
+/**
+ * tipc_eth_media_start - activate Ethernet bearer support
+ *
+ * Register Ethernet media type with TIPC bearer code.
+ * Also register with OS for notifications about device state changes.
+ */
+
+int tipc_eth_media_start(void)
+{                       
+	int res;
+
+	if (eth_started)
+		return -EINVAL;
+
+	memset(eth_bearers, 0, sizeof(eth_bearers));
+
+	res = tipc_register_media(&eth_media_info);
+	if (res)
+		return res;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,20)
+	INIT_WORK(&reg_notifier, do_registration);
+#else
+	INIT_WORK(&reg_notifier, (void (*)(void *))do_registration, NULL);
+#endif
+	schedule_work(&reg_notifier);
+	eth_started = 1;
+	return res;
+}
+
+/**
+ * tipc_eth_media_stop - deactivate Ethernet bearer support
+ */
+
+void tipc_eth_media_stop(void)
+{
+	if (!eth_started)
+		return;
+
+	flush_scheduled_work();
+	unregister_netdevice_notifier(&notifier);
+	eth_started = 0;
+}
diff -ruN linux-2.6.29/net/tipc/tipc_handler.c android_cluster/linux-2.6.29/net/tipc/tipc_handler.c
--- linux-2.6.29/net/tipc/tipc_handler.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_handler.c	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,142 @@
+/*
+ * net/tipc/tipc_handler.c: TIPC signal handling
+ *
+ * Copyright (c) 2000-2006, Ericsson AB
+ * Copyright (c) 2005-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include <linux/version.h>
+
+struct queue_item {
+	struct list_head next_signal;
+	void (*handler) (unsigned long);
+	unsigned long data;
+};
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,15)
+static struct kmem_cache *tipc_queue_item_cache;
+#else
+static struct kmem_cache_s *tipc_queue_item_cache;
+#endif
+static struct list_head signal_queue_head;
+static DEFINE_SPINLOCK(qitem_lock);
+static int handler_enabled = 0;
+
+static void process_signal_queue(unsigned long dummy);
+
+static DECLARE_TASKLET_DISABLED(tipc_tasklet, process_signal_queue, 0);
+
+
+unsigned int tipc_k_signal(Handler routine, unsigned long argument)
+{
+	struct queue_item *item;
+
+	if (!handler_enabled) {
+		err("Signal request ignored by handler\n");
+		return -ENOPROTOOPT;
+	}
+
+	spin_lock_bh(&qitem_lock);
+	item = kmem_cache_alloc(tipc_queue_item_cache, GFP_ATOMIC);
+	if (!item) {
+		err("Signal queue out of memory\n");
+		spin_unlock_bh(&qitem_lock);
+		return -ENOMEM;
+	}
+	item->handler = routine;
+	item->data = argument;
+	list_add_tail(&item->next_signal, &signal_queue_head);
+	spin_unlock_bh(&qitem_lock);
+	tasklet_schedule(&tipc_tasklet);
+	return 0;
+}
+
+static void process_signal_queue(unsigned long dummy)
+{
+	struct queue_item *__volatile__ item;
+	struct list_head *l, *n;
+
+	spin_lock_bh(&qitem_lock);
+	list_for_each_safe(l, n, &signal_queue_head) {
+		item = list_entry(l, struct queue_item, next_signal);
+		list_del(&item->next_signal);
+		spin_unlock_bh(&qitem_lock);
+		item->handler(item->data);
+		spin_lock_bh(&qitem_lock);
+		kmem_cache_free(tipc_queue_item_cache, item);
+	}
+	spin_unlock_bh(&qitem_lock);
+}
+
+int tipc_handler_start(void)
+{
+	tipc_queue_item_cache =
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,23)
+		kmem_cache_create("tipc_queue_items", sizeof(struct queue_item),
+				  0, SLAB_HWCACHE_ALIGN, NULL);
+#else
+		kmem_cache_create("tipc_queue_items", sizeof(struct queue_item),
+				  0, SLAB_HWCACHE_ALIGN, NULL, NULL);
+#endif
+	if (!tipc_queue_item_cache)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&signal_queue_head);
+	tasklet_enable(&tipc_tasklet);
+	handler_enabled = 1;
+	return 0;
+}
+
+void tipc_handler_stop(void)
+{
+	struct list_head *l, *n;
+	struct queue_item *item;
+
+	if (!handler_enabled)
+		return;
+
+	handler_enabled = 0;
+	tasklet_disable(&tipc_tasklet);
+	tasklet_kill(&tipc_tasklet);
+
+	spin_lock_bh(&qitem_lock);
+	list_for_each_safe(l, n, &signal_queue_head) {
+		item = list_entry(l, struct queue_item, next_signal);
+		list_del(&item->next_signal);
+		kmem_cache_free(tipc_queue_item_cache, item);
+	}
+	spin_unlock_bh(&qitem_lock);
+
+	kmem_cache_destroy(tipc_queue_item_cache);
+}
+
diff -ruN linux-2.6.29/net/tipc/tipc_link.c android_cluster/linux-2.6.29/net/tipc/tipc_link.c
--- linux-2.6.29/net/tipc/tipc_link.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_link.c	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,3581 @@
+/*
+ * net/tipc/tipc_link.c: TIPC link code
+ *
+ * Copyright (c) 1996-2007, Ericsson AB
+ * Copyright (c) 2004-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_dbg.h"
+#include "tipc_link.h"
+#include "tipc_net.h"
+#include "tipc_node.h"
+#include "tipc_port.h"
+#include "tipc_addr.h"
+#include "tipc_name_distr.h"
+#include "tipc_bearer.h"
+#include "tipc_name_table.h"
+#include "tipc_discover.h"
+#include "tipc_cfgsrv.h"
+#include "tipc_bcast.h"
+
+
+/*
+ * Out-of-range value for link session numbers
+ */
+
+#define INVALID_SESSION 0x10000
+
+/*
+ * Limit for deferred reception queue:
+ */
+
+#define DEF_QUEUE_LIMIT 256u
+
+/*
+ * Link state events:
+ */
+
+#define  STARTING_EVT    856384768	/* link processing trigger */
+#define  TRAFFIC_MSG_EVT 560815u	/* rx'd ??? */
+#define  TIMEOUT_EVT     560817u	/* link timer expired */
+
+/*
+ * The following two 'message types' is really just implementation
+ * data conveniently stored in the message header.
+ * They must not be considered part of the protocol
+ */
+#define OPEN_MSG   0
+#define CLOSED_MSG 1
+
+/*
+ * State value stored in 'exp_msg_count'
+ */
+
+#define START_CHANGEOVER 100000u
+
+/**
+ * struct link_name - deconstructed link name
+ * @addr_local: network address of node at this end
+ * @if_local: name of interface at this end
+ * @addr_peer: network address of node at far end
+ * @if_peer: name of interface at far end
+ */
+
+struct link_name {
+	u32 addr_local;
+	char if_local[TIPC_MAX_IF_NAME];
+	u32 addr_peer;
+	char if_peer[TIPC_MAX_IF_NAME];
+};
+
+/*
+ * Global counter of fragmented messages issued by node
+ */
+
+static atomic_t link_fragm_msg_no = ATOMIC_INIT(0);
+
+
+static void link_handle_out_of_seq_msg(struct link *l_ptr,
+				       struct sk_buff *buf);
+static void link_recv_proto_msg(struct link *l_ptr, struct sk_buff *buf);
+static int  link_recv_changeover_msg(struct link **l_ptr, struct sk_buff **buf);
+static void link_set_supervision_props(struct link *l_ptr, u32 tolerance);
+static int  link_send_sections_long(struct port *sender,
+				    struct iovec const *msg_sect,
+				    u32 num_sect, u32 destnode);
+static void link_check_defragm_bufs(struct link *l_ptr);
+static void link_state_event(struct link *l_ptr, u32 event);
+static void link_reset_statistics(struct link *l_ptr);
+
+
+/*
+ * Debugging code used by link routines only
+ *
+ * When debugging link problems on a system that has multiple links,
+ * the standard TIPC debugging routines may not be useful since they
+ * allow the output from multiple links to be intermixed.  For this reason
+ * routines of the form "dbg_link_XXX()" have been created that will capture
+ * debug info into a link's personal print buffer, which can then be dumped
+ * to the system console upon request.
+ *
+ * To utilize the dbg_link_XXX() routines:
+ * - set LINK_LOG_BUF_SIZE to the size of a link's print buffer 
+ *   (must be at least TIPC_PB_MIN_SIZE)
+ * - set DBG_OUTPUT_LINK where needed to indicate where the debug output
+ *   should be directed (see example below)
+ *
+ * Notes:
+ * - "l_ptr" must be valid when using dbg_link_XXX() routines
+ * - when debugging a system that has only one link, it may be easier to set
+ *   LINK_LOG_BUF_SIZE to 0 and simply point DBG_OUTPUT_LINK to the system
+ *   console or TIPC's log buffer (see example below)
+ * - it may also be sufficient in some situations to use TIPC's standard
+ *   debugging routines and control the debugging output using DBG_OUTPUT
+ */
+
+#define LINK_LOG_BUF_SIZE 0
+
+/*
+ * DBG_OUTPUT_LINK is the destination print buffer chain for per-link debug
+ * messages.  It defaults to the the null print buffer, but can be enabled
+ * where needed to allow debug messages to be selectively generated.
+ */
+
+#define DBG_OUTPUT_LINK TIPC_NULL
+#if 0
+#define DBG_OUTPUT_LINK (&l_ptr->print_buf)
+#define DBG_OUTPUT_LINK TIPC_LOG
+#define DBG_OUTPUT_LINK TIPC_CONS
+#endif
+
+#ifdef CONFIG_TIPC_DEBUG
+
+#define dbg_link(fmt, arg...)	   dbg_printf(DBG_OUTPUT_LINK, fmt, ##arg)
+#define dbg_link_msg(msg, txt)	   dbg_msg(DBG_OUTPUT_LINK, msg, txt)
+#define dbg_link_dump(fmt, arg...) dbg_dump(DBG_OUTPUT_LINK, fmt, ##arg)
+#define dbg_link_state(txt)	\
+	do {if (DBG_OUTPUT_LINK != TIPC_NULL) \
+		{tipc_printf(DBG_OUTPUT_LINK, txt); \
+		 dbg_print_link_state(DBG_OUTPUT_LINK, l_ptr);} \
+	} while(0)
+
+static void dbg_print_link_state(struct print_buf *buf, struct link *l_ptr);
+static void dbg_print_link(struct link *l_ptr, const char *str);
+static void dbg_print_buf_chain(struct sk_buff *root_buf);
+
+#else
+
+#define dbg_link(fmt, arg...)	       	do {} while (0)
+#define dbg_link_msg(msg, txt)	       	do {} while (0)
+#define dbg_link_dump(fmt, arg...)	do {} while (0)
+#define dbg_link_state(txt)		do {} while (0)
+
+#define dbg_print_link(...)		do {} while (0)
+#define dbg_print_buf_chain(...)	do {} while (0)
+#define dbg_print_link_state(...)	do {} while (0)
+
+#endif
+
+/*
+ *  Simple link routines
+ */
+
+static unsigned int align(unsigned int i)
+{
+	return (i + 3) & ~3u;
+}
+
+static void link_init_max_pkt(struct link *l_ptr)
+{
+	u32 max_pkt;
+
+	max_pkt = (l_ptr->b_ptr->publ.mtu & ~3);
+	if (max_pkt > MAX_MSG_SIZE)
+		max_pkt = MAX_MSG_SIZE;
+
+	l_ptr->max_pkt_target = max_pkt;
+	if (l_ptr->max_pkt_target < MAX_PKT_DEFAULT)
+		l_ptr->max_pkt = l_ptr->max_pkt_target;
+	else
+		l_ptr->max_pkt = MAX_PKT_DEFAULT;
+
+	l_ptr->max_pkt_probes = 0;
+}
+
+static u32 link_next_sent(struct link *l_ptr)
+{
+	if (l_ptr->next_out)
+		return buf_seqno(l_ptr->next_out);
+	return mod(l_ptr->next_out_no);
+}
+
+static u32 link_last_sent(struct link *l_ptr)
+{
+	return mod(link_next_sent(l_ptr) - 1);
+}
+
+/*
+ *  Simple non-static link routines (i.e. referenced outside this file)
+ */
+
+int tipc_link_is_up(struct link *l_ptr)
+{
+	if (!l_ptr)
+		return 0;
+	return (link_working_working(l_ptr) || link_working_unknown(l_ptr));
+}
+
+int tipc_link_is_active(struct link *l_ptr)
+{
+	return ((l_ptr->owner->active_links[0] == l_ptr) ||
+		(l_ptr->owner->active_links[1] == l_ptr));
+}
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+/**
+ * link_name_validate - validate & (optionally) deconstruct link name
+ * @name - ptr to link name string
+ * @name_parts - ptr to area for link name components (or NULL if not needed)
+ *
+ * Returns 1 if link name is valid, otherwise 0.
+ */
+
+static int link_name_validate(const char *name, struct link_name *name_parts)
+{
+	char name_copy[TIPC_MAX_LINK_NAME];
+	char *addr_local;
+	char *if_local;
+	char *addr_peer;
+	char *if_peer;
+	char dummy;
+	u32 z_local, c_local, n_local;
+	u32 z_peer, c_peer, n_peer;
+	u32 if_local_len;
+	u32 if_peer_len;
+
+	/* copy link name & ensure length is OK */
+
+	name_copy[TIPC_MAX_LINK_NAME - 1] = 0;
+	/* need above in case non-Posix strncpy() doesn't pad with nulls */
+	strncpy(name_copy, name, TIPC_MAX_LINK_NAME);
+	if (name_copy[TIPC_MAX_LINK_NAME - 1] != 0)
+		return 0;
+
+	/* ensure all component parts of link name are present */
+
+	addr_local = name_copy;
+	if ((if_local = strchr(addr_local, ':')) == NULL)
+		return 0;
+	*(if_local++) = 0;
+	if ((addr_peer = strchr(if_local, '-')) == NULL)
+		return 0;
+	*(addr_peer++) = 0;
+	if_local_len = addr_peer - if_local;
+	if ((if_peer = strchr(addr_peer, ':')) == NULL)
+		return 0;
+	*(if_peer++) = 0;
+	if_peer_len = strlen(if_peer) + 1;
+
+	/* validate component parts of link name */
+
+	if ((sscanf(addr_local, "%u.%u.%u%c",
+		    &z_local, &c_local, &n_local, &dummy) != 3) ||
+	    (sscanf(addr_peer, "%u.%u.%u%c",
+		    &z_peer, &c_peer, &n_peer, &dummy) != 3) ||
+	    (z_local > 255) || (c_local > 4095) || (n_local > 4095) ||
+	    (z_peer  > 255) || (c_peer  > 4095) || (n_peer  > 4095) ||
+	    (if_local_len <= 1) || (if_local_len > TIPC_MAX_IF_NAME) ||
+	    (if_peer_len  <= 1) || (if_peer_len  > TIPC_MAX_IF_NAME) ||
+	    (strspn(if_local, tipc_alphabet) != (if_local_len - 1)) ||
+	    (strspn(if_peer, tipc_alphabet) != (if_peer_len - 1)))
+		return 0;
+
+	/* return link name components, if necessary */
+
+	if (name_parts) {
+		name_parts->addr_local = tipc_addr(z_local, c_local, n_local);
+		strcpy(name_parts->if_local, if_local);
+		name_parts->addr_peer = tipc_addr(z_peer, c_peer, n_peer);
+		strcpy(name_parts->if_peer, if_peer);
+	}
+	return 1;
+}
+
+#endif
+
+/**
+ * link_timeout - handle expiration of link timer
+ * @l_ptr: pointer to link
+ *
+ * This routine must not grab "tipc_net_lock" to avoid a potential deadlock conflict
+ * with tipc_link_delete().  (There is no risk that the node will be deleted by
+ * another thread because tipc_link_delete() always cancels the link timer before
+ * tipc_node_delete() is called.)
+ */
+
+static void link_timeout(struct link *l_ptr)
+{
+	tipc_node_lock(l_ptr->owner);
+
+	/* update counters used in statistical profiling of send traffic */
+
+	l_ptr->stats.accu_queue_sz += l_ptr->out_queue_size;
+	l_ptr->stats.queue_sz_counts++;
+
+	if (l_ptr->out_queue_size > l_ptr->stats.max_queue_sz)
+		l_ptr->stats.max_queue_sz = l_ptr->out_queue_size;
+
+	if (l_ptr->first_out) {
+		struct tipc_msg *msg = buf_msg(l_ptr->first_out);
+		u32 length = msg_size(msg);
+
+		if ((msg_user(msg) == MSG_FRAGMENTER)
+		    && (msg_type(msg) == FIRST_FRAGMENT)) {
+			length = msg_size(msg_get_wrapped(msg));
+		}
+		if (length) {
+			l_ptr->stats.msg_lengths_total += length;
+			l_ptr->stats.msg_length_counts++;
+			if (length <= 64)
+				l_ptr->stats.msg_length_profile[0]++;
+			else if (length <= 256)
+				l_ptr->stats.msg_length_profile[1]++;
+			else if (length <= 1024)
+				l_ptr->stats.msg_length_profile[2]++;
+			else if (length <= 4096)
+				l_ptr->stats.msg_length_profile[3]++;
+			else if (length <= 16384)
+				l_ptr->stats.msg_length_profile[4]++;
+			else if (length <= 32768)
+				l_ptr->stats.msg_length_profile[5]++;
+			else
+				l_ptr->stats.msg_length_profile[6]++;
+		}
+	}
+
+	/* do all other link processing performed on a periodic basis */
+
+	link_check_defragm_bufs(l_ptr);
+
+	link_state_event(l_ptr, TIMEOUT_EVT);
+
+	if (l_ptr->next_out)
+		tipc_link_push_queue(l_ptr);
+
+	tipc_node_unlock(l_ptr->owner);
+}
+
+static void link_set_timer(struct link *l_ptr, u32 time)
+{
+	k_start_timer(&l_ptr->timer, time);
+}
+
+/**
+ * tipc_link_create - create a new link
+ * @b_ptr: pointer to associated bearer
+ * @peer: network address of node at other end of link
+ * @media_addr: media address to use when sending messages over link
+ *
+ * Returns pointer to link.
+ */
+
+struct link *tipc_link_create(struct bearer *b_ptr, const u32 peer,
+			      const struct tipc_media_addr *media_addr)
+{
+	struct link *l_ptr;
+	struct tipc_msg *msg;
+	char *if_name;
+
+	l_ptr = kzalloc(sizeof(*l_ptr), GFP_ATOMIC);
+	if (!l_ptr) {
+		warn("Link creation failed, no memory\n");
+		return NULL;
+	}
+
+	if (LINK_LOG_BUF_SIZE) {
+		char *pb = kmalloc(LINK_LOG_BUF_SIZE, GFP_ATOMIC);
+
+		if (!pb) {
+			kfree(l_ptr);
+			warn("Link creation failed, no memory for print buffer\n");
+			return NULL;
+		}
+		tipc_printbuf_init(&l_ptr->print_buf, pb, LINK_LOG_BUF_SIZE);
+	}
+
+	l_ptr->addr = peer;
+	if_name = strchr(b_ptr->publ.name, ':') + 1;
+	sprintf(l_ptr->name, "%u.%u.%u:%s-%u.%u.%u:",
+		tipc_zone(tipc_own_addr), tipc_cluster(tipc_own_addr),
+		tipc_node(tipc_own_addr),
+		if_name,
+		tipc_zone(peer), tipc_cluster(peer), tipc_node(peer));
+		/* note: peer i/f is appended to link name by reset/activate */
+	memcpy(&l_ptr->media_addr, media_addr, sizeof(*media_addr));
+	l_ptr->checkpoint = 1;
+	l_ptr->b_ptr = b_ptr;
+	link_set_supervision_props(l_ptr, b_ptr->tolerance);
+	l_ptr->state = RESET_UNKNOWN;
+	l_ptr->pmsg = (struct tipc_msg *)&l_ptr->proto_msg;
+	msg = l_ptr->pmsg;
+	tipc_msg_init(msg, LINK_PROTOCOL, RESET_MSG, INT_H_SIZE, l_ptr->addr);
+	msg_set_size(msg, sizeof(l_ptr->proto_msg));
+	msg_set_session(msg, (tipc_random & 0xffff));
+	msg_set_bearer_id(msg, b_ptr->identity);
+	strcpy((char *)msg_data(msg), if_name);
+	l_ptr->priority = b_ptr->priority;
+	tipc_link_set_queue_limits(l_ptr, b_ptr->window);
+	link_init_max_pkt(l_ptr);
+	l_ptr->next_out_no = 1;
+	INIT_LIST_HEAD(&l_ptr->waiting_ports);
+	link_reset_statistics(l_ptr);
+
+	l_ptr->owner = tipc_node_attach_link(l_ptr);
+	if (!l_ptr->owner) {
+		if (LINK_LOG_BUF_SIZE)
+			kfree(l_ptr->print_buf.buf);
+		kfree(l_ptr);
+		return NULL;
+	}
+
+	k_init_timer(&l_ptr->timer, (Handler)link_timeout, (unsigned long)l_ptr);
+	list_add_tail(&l_ptr->link_list, &b_ptr->links);
+
+	tipc_k_signal((Handler)tipc_link_start, (unsigned long)l_ptr);
+
+	dbg("tipc_link_create(): tolerance = %u,cont intv = %u, abort_limit = %u\n",
+	    l_ptr->tolerance, l_ptr->continuity_interval, l_ptr->abort_limit);
+
+	return l_ptr;
+}
+
+/**
+ * tipc_link_delete - delete a link
+ * @l_ptr: pointer to link
+ *
+ * Note: 'tipc_net_lock' is write_locked, bearer is locked.
+ * This routine must not grab the node lock until after link timer cancellation
+ * to avoid a potential deadlock situation.
+ */
+
+void tipc_link_delete(struct link *l_ptr)
+{
+	if (!l_ptr) {
+		err("Attempt to delete non-existent link\n");
+		return;
+	}
+
+	k_cancel_timer(&l_ptr->timer);
+	tipc_node_lock(l_ptr->owner);
+	tipc_link_reset(l_ptr);
+	tipc_node_detach_link(l_ptr->owner, l_ptr);
+	tipc_link_stop(l_ptr);
+	list_del_init(&l_ptr->link_list);
+	if (LINK_LOG_BUF_SIZE)
+		kfree(l_ptr->print_buf.buf);
+	tipc_node_unlock(l_ptr->owner);
+	k_term_timer(&l_ptr->timer);
+	kfree(l_ptr);
+}
+
+/** 
+ * link_remote_delete - delete a link on command from other end
+ * @l_ptr: pointer to link
+ * 
+ * Note: The call comes via a tipc_k_signal. No locks are set at this moment.
+ */
+
+static void link_remote_delete(struct link *l_ptr)
+{
+        struct bearer *b_ptr = l_ptr->b_ptr;
+
+	write_lock_bh(&tipc_net_lock);
+        spin_lock_bh(&b_ptr->publ.lock);
+        tipc_bearer_remove_discoverer(b_ptr,l_ptr->addr);
+        tipc_link_delete(l_ptr);
+        spin_unlock_bh(&b_ptr->publ.lock);
+	write_unlock_bh(&tipc_net_lock);
+}
+
+void tipc_link_start(struct link *l_ptr)
+{
+	dbg("tipc_link_start %x\n", l_ptr);
+	link_state_event(l_ptr, STARTING_EVT);
+}
+
+/**
+ * link_schedule_port - schedule port for deferred sending
+ * @l_ptr: pointer to link
+ * @origport: reference to sending port
+ * @sz: amount of data to be sent
+ *
+ * Schedules port for renewed sending of messages after link congestion
+ * has abated.
+ */
+
+static int link_schedule_port(struct link *l_ptr, u32 origport, u32 sz)
+{
+	struct port *p_ptr;
+
+	spin_lock_bh(&tipc_port_list_lock);
+	p_ptr = tipc_port_lock(origport);
+	if (p_ptr) {
+		if (!p_ptr->wakeup)
+			goto exit;
+		if (!list_empty(&p_ptr->wait_list))
+			goto exit;
+		p_ptr->publ.congested = 1;
+		p_ptr->waiting_pkts = 1 + ((sz - 1) / l_ptr->max_pkt);
+		list_add_tail(&p_ptr->wait_list, &l_ptr->waiting_ports);
+		l_ptr->stats.link_congs++;
+exit:
+		tipc_port_unlock(p_ptr);
+	}
+	spin_unlock_bh(&tipc_port_list_lock);
+	return -ELINKCONG;
+}
+
+void tipc_link_wakeup_ports(struct link *l_ptr, int all)
+{
+	struct port *p_ptr;
+	struct port *temp_p_ptr;
+	int win = l_ptr->queue_limit[0] - l_ptr->out_queue_size;
+
+	if (all)
+		win = 100000;
+	if (win <= 0)
+		return;
+	if (!spin_trylock_bh(&tipc_port_list_lock))
+		return;
+	if (link_congested(l_ptr))
+		goto exit;
+	list_for_each_entry_safe(p_ptr, temp_p_ptr, &l_ptr->waiting_ports,
+				 wait_list) {
+		if (win <= 0)
+			break;
+		list_del_init(&p_ptr->wait_list);
+		spin_lock_bh(p_ptr->publ.lock);
+		p_ptr->publ.congested = 0;
+		p_ptr->wakeup(&p_ptr->publ);
+		win -= p_ptr->waiting_pkts;
+		spin_unlock_bh(p_ptr->publ.lock);
+	}
+
+exit:
+	spin_unlock_bh(&tipc_port_list_lock);
+}
+
+/**
+ * link_release_outqueue - purge link's outbound message queue
+ * @l_ptr: pointer to link
+ */
+
+static void link_release_outqueue(struct link *l_ptr)
+{
+	struct sk_buff *buf = l_ptr->first_out;
+	struct sk_buff *next;
+
+	while (buf) {
+		next = buf->next;
+		buf_discard(buf);
+		buf = next;
+	}
+	l_ptr->first_out = NULL;
+	l_ptr->out_queue_size = 0;
+}
+
+/**
+ * tipc_link_reset_fragments - purge link's inbound message fragments queue
+ * @l_ptr: pointer to link
+ */
+
+void tipc_link_reset_fragments(struct link *l_ptr)
+{
+	struct sk_buff *buf = l_ptr->defragm_buf;
+	struct sk_buff *next;
+
+	while (buf) {
+		next = buf->next;
+		buf_discard(buf);
+		buf = next;
+	}
+	l_ptr->defragm_buf = NULL;
+}
+
+/**
+ * tipc_link_stop - purge all inbound and outbound messages associated with link
+ * @l_ptr: pointer to link
+ */
+
+void tipc_link_stop(struct link *l_ptr)
+{
+	struct sk_buff *buf;
+	struct sk_buff *next;
+
+	buf = l_ptr->oldest_deferred_in;
+	while (buf) {
+		next = buf->next;
+		buf_discard(buf);
+		buf = next;
+	}
+
+	buf = l_ptr->first_out;
+	while (buf) {
+		next = buf->next;
+		buf_discard(buf);
+		buf = next;
+	}
+
+	tipc_link_reset_fragments(l_ptr);
+
+	buf_discard(l_ptr->proto_msg_queue);
+	l_ptr->proto_msg_queue = NULL;
+}
+
+
+void tipc_link_reset(struct link *l_ptr)
+{
+	struct sk_buff *buf;
+	u32 prev_state = l_ptr->state;
+	u32 checkpoint = l_ptr->next_in_no;
+	int was_active_link = tipc_link_is_active(l_ptr);
+
+	msg_set_session(l_ptr->pmsg, ((msg_session(l_ptr->pmsg) + 1) & 0xffff));
+
+	/* Link is down, accept any session */
+	l_ptr->peer_session = INVALID_SESSION;
+
+	/* Prepare for max packet size negotiation */
+	link_init_max_pkt(l_ptr);
+
+	l_ptr->state = RESET_UNKNOWN;
+	dbg_link_state("Resetting Link\n");
+
+	if ((prev_state == RESET_UNKNOWN) || (prev_state == RESET_RESET))
+		return;
+
+	tipc_node_link_down(l_ptr->owner, l_ptr);
+	tipc_bearer_remove_dest(l_ptr->b_ptr, l_ptr->addr, &l_ptr->media_addr);
+#if 0
+	printk("\nReset link <%s>\n", l_ptr->name);
+	dbg_link_dump("\n\nDumping link <%s>:\n", l_ptr->name);
+#endif
+	if (was_active_link && tipc_node_is_up(l_ptr->owner) &&
+	    l_ptr->owner->permit_changeover) {
+		l_ptr->reset_checkpoint = checkpoint;
+		l_ptr->exp_msg_count = START_CHANGEOVER;
+	}
+
+	/* Clean up all queues: */
+
+	link_release_outqueue(l_ptr);
+	buf_discard(l_ptr->proto_msg_queue);
+	l_ptr->proto_msg_queue = NULL;
+	buf = l_ptr->oldest_deferred_in;
+	while (buf) {
+		struct sk_buff *next = buf->next;
+		buf_discard(buf);
+		buf = next;
+	}
+	if (!list_empty(&l_ptr->waiting_ports))
+		tipc_link_wakeup_ports(l_ptr, 1);
+
+	l_ptr->retransm_queue_head = 0;
+	l_ptr->retransm_queue_size = 0;
+	l_ptr->last_out = NULL;
+	l_ptr->first_out = NULL;
+	l_ptr->next_out = NULL;
+	l_ptr->unacked_window = 0;
+	l_ptr->checkpoint = 1;
+	l_ptr->next_out_no = 1;
+	l_ptr->deferred_inqueue_sz = 0;
+	l_ptr->oldest_deferred_in = NULL;
+	l_ptr->newest_deferred_in = NULL;
+	l_ptr->fsm_msg_cnt = 0;
+	l_ptr->stale_count = 0;
+	link_reset_statistics(l_ptr);
+}
+
+
+static void link_activate(struct link *l_ptr)
+{
+	l_ptr->next_in_no = l_ptr->stats.recv_info = 1;
+	tipc_node_link_up(l_ptr->owner, l_ptr);
+	tipc_bearer_add_dest(l_ptr->b_ptr, l_ptr->addr, &l_ptr->media_addr);
+}
+
+/**
+ * link_state_event - link finite state machine
+ * @l_ptr: pointer to link
+ * @event: state machine event to process
+ */
+
+static void link_state_event(struct link *l_ptr, unsigned event)
+{
+	struct link *other;
+	u32 cont_intv = l_ptr->continuity_interval;
+
+	if (!l_ptr->started && (event != STARTING_EVT))
+		return;		/* Not yet. */
+
+	if (link_blocked(l_ptr)) {
+		if (event == TIMEOUT_EVT) {
+			link_set_timer(l_ptr, cont_intv);
+		}
+		return;	  /* Changeover going on */
+	}
+	dbg_link("STATE_EV: <%s> ", l_ptr->name);
+
+	switch (l_ptr->state) {
+	case WORKING_WORKING:
+		dbg_link("WW/");
+		switch (event) {
+		case TRAFFIC_MSG_EVT:
+			dbg_link("TRF-");
+			/* fall through */
+		case ACTIVATE_MSG:
+			dbg_link("ACT\n");
+			break;
+		case TIMEOUT_EVT:
+			dbg_link("TIM ");
+			if (l_ptr->next_in_no != l_ptr->checkpoint) {
+				l_ptr->checkpoint = l_ptr->next_in_no;
+				if (tipc_bclink_acks_missing(l_ptr->owner)) {
+					tipc_link_send_proto_msg(l_ptr, STATE_MSG,
+							    0, 0, 0, 0, 0, 0);
+					l_ptr->fsm_msg_cnt++;
+				} else if (l_ptr->max_pkt < l_ptr->max_pkt_target) {
+					tipc_link_send_proto_msg(l_ptr, STATE_MSG,
+							    1, 0, 0, 0, 0, 0);
+					l_ptr->fsm_msg_cnt++;
+				}
+				link_set_timer(l_ptr, cont_intv);
+				break;
+			}
+			dbg_link(" -> WU\n");
+			l_ptr->state = WORKING_UNKNOWN;
+			l_ptr->fsm_msg_cnt = 0;
+			tipc_link_send_proto_msg(l_ptr, STATE_MSG, 1, 0, 0, 0, 0, 0);
+			l_ptr->fsm_msg_cnt++;
+			link_set_timer(l_ptr, cont_intv / 4);
+			break;
+		case RESET_MSG:
+			dbg_link("RES -> RR\n");
+			info("Resetting link <%s>, requested by peer\n",
+			     l_ptr->name);
+			tipc_link_reset(l_ptr);
+			l_ptr->state = RESET_RESET;
+			l_ptr->fsm_msg_cnt = 0;
+			tipc_link_send_proto_msg(l_ptr, ACTIVATE_MSG, 0, 0, 0, 0, 0, 0);
+			l_ptr->fsm_msg_cnt++;
+			link_set_timer(l_ptr, cont_intv);
+			break;
+		default:
+			err("Unknown link event %u in WW state\n", event);
+		}
+		break;
+	case WORKING_UNKNOWN:
+		dbg_link("WU/");
+		switch (event) {
+		case TRAFFIC_MSG_EVT:
+			dbg_link("TRF-");
+		case ACTIVATE_MSG:
+			dbg_link("ACT -> WW\n");
+			l_ptr->state = WORKING_WORKING;
+			l_ptr->fsm_msg_cnt = 0;
+			link_set_timer(l_ptr, cont_intv);
+			break;
+		case RESET_MSG:
+			dbg_link("RES -> RR\n");
+			info("Resetting link <%s>, requested by peer "
+			     "while probing\n", l_ptr->name);
+			tipc_link_reset(l_ptr);
+			l_ptr->state = RESET_RESET;
+			l_ptr->fsm_msg_cnt = 0;
+			tipc_link_send_proto_msg(l_ptr, ACTIVATE_MSG, 0, 0, 0, 0, 0, 0);
+			l_ptr->fsm_msg_cnt++;
+			link_set_timer(l_ptr, cont_intv);
+			break;
+		case TIMEOUT_EVT:
+			dbg_link("TIM ");
+			if (l_ptr->next_in_no != l_ptr->checkpoint) {
+				dbg_link("-> WW \n");
+				l_ptr->state = WORKING_WORKING;
+				l_ptr->fsm_msg_cnt = 0;
+				l_ptr->checkpoint = l_ptr->next_in_no;
+				if (tipc_bclink_acks_missing(l_ptr->owner)) {
+					tipc_link_send_proto_msg(l_ptr, STATE_MSG,
+							    0, 0, 0, 0, 0, 0);
+					l_ptr->fsm_msg_cnt++;
+				}
+				link_set_timer(l_ptr, cont_intv);
+			} else if (l_ptr->fsm_msg_cnt < l_ptr->abort_limit) {
+				dbg_link("Probing %u/%u,timer = %u ms)\n",
+					 l_ptr->fsm_msg_cnt, l_ptr->abort_limit,
+					 cont_intv / 4);
+				tipc_link_send_proto_msg(l_ptr, STATE_MSG, 
+						    1, 0, 0, 0, 0, 0);
+				l_ptr->fsm_msg_cnt++;
+				link_set_timer(l_ptr, cont_intv / 4);
+			} else {	/* Link has failed */
+				dbg_link("-> RU (%u probes unanswered)\n",
+					 l_ptr->fsm_msg_cnt);
+				warn("Resetting link <%s>, peer not responding\n",
+				     l_ptr->name);
+				tipc_link_reset(l_ptr);
+				l_ptr->state = RESET_UNKNOWN;
+				l_ptr->fsm_msg_cnt = 0;
+				tipc_link_send_proto_msg(l_ptr, RESET_MSG,
+						    0, 0, 0, 0, 0, 0);
+				l_ptr->fsm_msg_cnt++;
+				link_set_timer(l_ptr, cont_intv);
+			}
+			break;
+		default:
+			err("Unknown link event %u in WU state\n", event);
+		}
+		break;
+	case RESET_UNKNOWN:
+		dbg_link("RU/");
+		switch (event) {
+		case TRAFFIC_MSG_EVT:
+			dbg_link("TRF-\n");
+			break;
+		case ACTIVATE_MSG:
+			other = l_ptr->owner->active_links[0];
+			if (other && link_working_unknown(other)) {
+				dbg_link("ACT\n");
+				break;
+			}
+			dbg_link("ACT -> WW\n");
+			l_ptr->state = WORKING_WORKING;
+			l_ptr->fsm_msg_cnt = 0;
+			link_activate(l_ptr);
+			tipc_link_send_proto_msg(l_ptr, STATE_MSG, 1, 0, 0, 0, 0, 0);
+			l_ptr->fsm_msg_cnt++;
+			link_set_timer(l_ptr, cont_intv);
+			break;
+		case RESET_MSG:
+			dbg_link("RES \n");
+			dbg_link(" -> RR\n");
+			l_ptr->state = RESET_RESET;
+			l_ptr->fsm_msg_cnt = 0;
+			tipc_link_send_proto_msg(l_ptr, ACTIVATE_MSG, 1, 0, 0, 0, 0, 0);
+			l_ptr->fsm_msg_cnt++;
+			link_set_timer(l_ptr, cont_intv);
+			break;
+		case STARTING_EVT:
+			dbg_link("START-");
+			l_ptr->started = 1;
+			/* fall through */
+		case TIMEOUT_EVT:
+			dbg_link("TIM \n");
+                        tipc_bearer_send_discover(l_ptr->b_ptr,l_ptr->addr);
+			tipc_link_send_proto_msg(l_ptr, RESET_MSG, 0, 0, 0, 0, 0, 0);
+			l_ptr->fsm_msg_cnt++;
+			link_set_timer(l_ptr, cont_intv);
+			break;
+		default:
+			err("Unknown link event %u in RU state\n", event);
+		}
+		break;
+	case RESET_RESET:
+		dbg_link("RR/ ");
+		switch (event) {
+		case TRAFFIC_MSG_EVT:
+			dbg_link("TRF-");
+			/* fall through */
+		case ACTIVATE_MSG:
+			other = l_ptr->owner->active_links[0];
+			if (other && link_working_unknown(other)) {
+				dbg_link("ACT\n");
+				break;
+			}
+			dbg_link("ACT -> WW\n");
+			l_ptr->state = WORKING_WORKING;
+			l_ptr->fsm_msg_cnt = 0;
+			link_activate(l_ptr);
+			tipc_link_send_proto_msg(l_ptr, STATE_MSG, 1, 0, 0, 0, 0, 0);
+			l_ptr->fsm_msg_cnt++;
+			link_set_timer(l_ptr, cont_intv);
+			break;
+		case RESET_MSG:
+			dbg_link("RES\n");
+			break;
+		case TIMEOUT_EVT:
+			dbg_link("TIM\n");
+			tipc_link_send_proto_msg(l_ptr, ACTIVATE_MSG, 0, 0, 0, 0, 0, 0);
+			l_ptr->fsm_msg_cnt++;
+			link_set_timer(l_ptr, cont_intv);
+			dbg_link("fsm_msg_cnt %u\n", l_ptr->fsm_msg_cnt);
+			break;
+		default:
+			err("Unknown link event %u in RR state\n", event);
+		}
+		break;
+	default:
+		err("Unknown link state %u/%u\n", l_ptr->state, event);
+	}
+}
+
+/*
+ * link_bundle_buf(): Append contents of a buffer to
+ * the tail of an existing one.
+ */
+
+static int link_bundle_buf(struct link *l_ptr,
+			   struct sk_buff *bundler,
+			   struct sk_buff *buf)
+{
+	struct tipc_msg *bundler_msg = buf_msg(bundler);
+	struct tipc_msg *msg = buf_msg(buf);
+	u32 size = msg_size(msg);
+	u32 bundle_size = msg_size(bundler_msg);
+	u32 to_pos = align(bundle_size);
+	u32 pad = to_pos - bundle_size;
+
+	if (msg_user(bundler_msg) != MSG_BUNDLER)
+		return 0;
+	if (msg_type(bundler_msg) != OPEN_MSG)
+		return 0;
+	if (skb_tailroom(bundler) < (pad + size))
+		return 0;
+	if (l_ptr->max_pkt < (to_pos + size))
+		return 0;
+
+	skb_put(bundler, pad + size);
+	skb_copy_to_linear_data_offset(bundler, to_pos, buf->data, size);
+	msg_set_size(bundler_msg, to_pos + size);
+	msg_set_msgcnt(bundler_msg, msg_msgcnt(bundler_msg) + 1);
+	dbg("Packed msg # %u(%u octets) into pos %u in buf(#%u)\n",
+	    msg_msgcnt(bundler_msg), size, to_pos, msg_seqno(bundler_msg));
+	msg_dbg(msg, "PACKD:");
+	buf_discard(buf);
+	l_ptr->stats.sent_bundled++;
+	return 1;
+}
+
+static void link_add_to_outqueue(struct link *l_ptr,
+				 struct sk_buff *buf,
+				 struct tipc_msg *msg)
+{
+	u32 ack = mod(l_ptr->next_in_no - 1);
+	u32 seqno = mod(l_ptr->next_out_no++);
+
+	msg_set_word(msg, 2, ((ack << 16) | seqno));
+	msg_set_bcast_ack(msg, l_ptr->owner->bclink.last_in);
+	buf->next = NULL;
+	if (l_ptr->first_out) {
+		l_ptr->last_out->next = buf;
+		l_ptr->last_out = buf;
+	} else
+		l_ptr->first_out = l_ptr->last_out = buf;
+	l_ptr->out_queue_size++;
+}
+
+/*
+ * tipc_link_send_buf() is the 'full path' for messages, called from
+ * inside TIPC when the 'fast path' in tipc_send_buf
+ * has failed, and from link_send()
+ */
+
+int tipc_link_send_buf(struct link *l_ptr, struct sk_buff *buf)
+{
+	struct tipc_msg *msg = buf_msg(buf);
+	u32 size = msg_size(msg);
+	u32 dsz = msg_data_sz(msg);
+	u32 queue_size = l_ptr->out_queue_size;
+	u32 imp = tipc_msg_tot_importance(msg);
+	u32 queue_limit = l_ptr->queue_limit[imp];
+	u32 max_packet = l_ptr->max_pkt;
+
+	msg_set_prevnode(msg, tipc_own_addr);	/* If routed message */
+
+	/* Match msg importance against queue limits: */
+
+	if (unlikely(queue_size >= queue_limit)) {
+		if (imp <= TIPC_CRITICAL_IMPORTANCE) {
+			return link_schedule_port(l_ptr, msg_origport(msg),
+						  size);
+		}
+		msg_dbg(msg, "TIPC: Congestion, throwing away\n");
+		buf_discard(buf);
+		if (imp > CONN_MANAGER) {
+			warn("Resetting link <%s>, send queue full", l_ptr->name);
+			tipc_link_reset(l_ptr);
+		}
+		return dsz;
+	}
+
+	/* Fragmentation needed ? */
+
+	if (size > max_packet)
+		return tipc_link_send_long_buf(l_ptr, buf);
+
+	/* Packet can be queued or sent: */
+
+	if (queue_size > l_ptr->stats.max_queue_sz)
+		l_ptr->stats.max_queue_sz = queue_size;
+
+	if (likely(!tipc_bearer_congested(l_ptr->b_ptr, l_ptr) &&
+		   !link_congested(l_ptr))) {
+		link_add_to_outqueue(l_ptr, buf, msg);
+
+		if (likely(tipc_bearer_send(l_ptr->b_ptr, buf, &l_ptr->media_addr))) {
+			l_ptr->unacked_window = 0;
+		} else {
+			tipc_bearer_schedule(l_ptr->b_ptr, l_ptr);
+			l_ptr->stats.bearer_congs++;
+			l_ptr->next_out = buf;
+		}
+		return dsz;
+	}
+	/* Congestion: can message be bundled ?: */
+
+	if ((msg_user(msg) != CHANGEOVER_PROTOCOL) &&
+	    (msg_user(msg) != MSG_FRAGMENTER)) {
+
+		/* Try adding message to an existing bundle */
+
+		if (l_ptr->next_out &&
+		    link_bundle_buf(l_ptr, l_ptr->last_out, buf)) {
+			tipc_bearer_resolve_congestion(l_ptr->b_ptr, l_ptr);
+			return dsz;
+		}
+
+		/* Try creating a new bundle */
+
+		if (size <= max_packet * 2 / 3) {
+			struct sk_buff *bundler = buf_acquire(max_packet);
+			struct tipc_msg bundler_hdr;
+
+			if (bundler) {
+				tipc_msg_init(&bundler_hdr, MSG_BUNDLER, OPEN_MSG,
+					      INT_H_SIZE, l_ptr->addr);
+				skb_copy_to_linear_data(bundler, &bundler_hdr,
+							INT_H_SIZE);
+				skb_trim(bundler, INT_H_SIZE);
+				link_bundle_buf(l_ptr, bundler, buf);
+				buf = bundler;
+				msg = buf_msg(buf);
+				l_ptr->stats.sent_bundles++;
+			}
+		}
+	}
+	if (!l_ptr->next_out)
+		l_ptr->next_out = buf;
+	link_add_to_outqueue(l_ptr, buf, msg);
+	tipc_bearer_resolve_congestion(l_ptr->b_ptr, l_ptr);
+	return dsz;
+}
+
+/*
+ * tipc_link_send(): same as tipc_link_send_buf(), but the link to use has
+ * not been selected yet, and the the owner node is not locked
+ * Called by TIPC internal users, e.g. the name distributor
+ */
+
+int tipc_link_send(struct sk_buff *buf, u32 dest, u32 selector)
+{
+	struct link *l_ptr;
+	struct tipc_node *n_ptr;
+	int res = -ELINKCONG;
+
+	read_lock_bh(&tipc_net_lock);
+	n_ptr = tipc_net_select_node(dest);
+	if (n_ptr) {
+		tipc_node_lock(n_ptr);
+		l_ptr = n_ptr->active_links[selector & 1];
+		if (l_ptr) {
+			dbg("tipc_link_send: found link %x for dest %x\n", l_ptr, dest);
+			res = tipc_link_send_buf(l_ptr, buf);
+		} else {
+			dbg("Attempt to send msg to unreachable node:\n");
+			msg_dbg(buf_msg(buf),">>>");
+			buf_discard(buf);
+		}
+		tipc_node_unlock(n_ptr);
+	} else {
+		dbg("Attempt to send msg to unknown node:\n");
+		msg_dbg(buf_msg(buf),">>>");
+		buf_discard(buf);
+	}
+	read_unlock_bh(&tipc_net_lock);
+	return res;
+}
+
+/*
+ * link_send_buf_fast: Entry for data messages where the
+ * destination link is known and the header is complete,
+ * inclusive total message length. Very time critical.
+ * Link is locked. Returns user data length.
+ */
+
+static int link_send_buf_fast(struct link *l_ptr, struct sk_buff *buf,
+			      u32 *used_max_pkt)
+{
+	struct tipc_msg *msg = buf_msg(buf);
+	int res = msg_data_sz(msg);
+
+	if (likely(!link_congested(l_ptr))) {
+		if (likely(msg_size(msg) <= l_ptr->max_pkt)) {
+			if (likely(list_empty(&l_ptr->b_ptr->cong_links))) {
+				link_add_to_outqueue(l_ptr, buf, msg);
+				if (likely(tipc_bearer_send(l_ptr->b_ptr, buf,
+							    &l_ptr->media_addr))) {
+					l_ptr->unacked_window = 0;
+					msg_dbg(msg,"SENT_FAST:");
+					return res;
+				}
+				dbg("failed sent fast...\n");
+				tipc_bearer_schedule(l_ptr->b_ptr, l_ptr);
+				l_ptr->stats.bearer_congs++;
+				l_ptr->next_out = buf;
+				return res;
+			}
+		}
+		else
+			*used_max_pkt = l_ptr->max_pkt;
+	}
+	return tipc_link_send_buf(l_ptr, buf);  /* All other cases */
+}
+
+/*
+ * tipc_send_buf_fast: Entry for data messages where the
+ * destination node is known to be off-node and the header is complete,
+ * inclusive total message length.
+ * Returns user data length.
+ */
+int tipc_send_buf_fast(struct sk_buff *buf, u32 destnode)
+{
+	struct link *l_ptr;
+	struct tipc_node *n_ptr;
+	int res;
+	u32 selector = msg_origport(buf_msg(buf)) & 1;
+	u32 dummy;
+
+	assert(!addr_in_node(destnode));
+
+	read_lock_bh(&tipc_net_lock);
+	n_ptr = tipc_net_select_node(destnode);
+
+	if (likely(n_ptr)) {
+		tipc_node_lock(n_ptr);
+		l_ptr = n_ptr->active_links[selector];
+		dbg("send_fast: buf %x selected %x, destnode = %x\n",
+		    buf, l_ptr, destnode);
+		if (likely(l_ptr)) {
+			res = link_send_buf_fast(l_ptr, buf, &dummy);
+			tipc_node_unlock(n_ptr);
+			read_unlock_bh(&tipc_net_lock);
+			return res;
+		}
+		tipc_node_unlock(n_ptr);
+	}
+	read_unlock_bh(&tipc_net_lock);
+	res = msg_data_sz(buf_msg(buf));
+	tipc_reject_msg(buf, TIPC_ERR_NO_NODE);
+	return res;
+}
+
+
+/*
+ * tipc_link_send_sections_fast: Entry for messages where the
+ * destination processor is known and the header is complete,
+ * except for total message length.
+ * Returns user data length or errno.
+ */
+int tipc_link_send_sections_fast(struct port *sender,
+				 struct iovec const *msg_sect,
+				 const u32 num_sect,
+				 u32 destaddr)
+{
+	struct tipc_msg *hdr = &sender->publ.phdr;
+	struct link *l_ptr;
+	struct sk_buff *buf;
+	struct tipc_node *node;
+	int res;
+	u32 selector = msg_origport(hdr) & 1;
+
+again:
+	/*
+	 * Try building message using port's max_pkt hint.
+	 * (Must not hold any locks while building message.)
+	 */
+
+	res = tipc_msg_build(hdr, msg_sect, num_sect, sender->publ.max_pkt,
+			     !sender->user_port, &buf);
+
+	read_lock_bh(&tipc_net_lock);
+	node = tipc_net_select_node(destaddr);
+
+	if (likely(node)) {
+		tipc_node_lock(node);
+		l_ptr = node->active_links[selector];
+		if (likely(l_ptr)) {
+			if (likely(buf)) {
+				res = link_send_buf_fast(l_ptr, buf,
+							 &sender->publ.max_pkt);
+				if (unlikely(res < 0))
+					buf_discard(buf);
+exit:
+				tipc_node_unlock(node);
+				read_unlock_bh(&tipc_net_lock);
+				return res;
+			}
+
+			/* Exit if build request was invalid */
+
+			if (unlikely(res < 0))
+				goto exit;
+
+			/* Exit if link (or bearer) is congested */
+
+			if (link_congested(l_ptr) || 
+			    !list_empty(&l_ptr->b_ptr->cong_links)) {
+				res = link_schedule_port(l_ptr,
+							 sender->publ.ref, res);
+				goto exit;
+			}
+
+			/* 
+			 * Message size exceeds max_pkt hint; update hint,
+			 * then re-try fast path or fragment the message
+			 */
+
+			sender->publ.max_pkt = l_ptr->max_pkt;
+			tipc_node_unlock(node);
+			read_unlock_bh(&tipc_net_lock);
+
+
+			if ((msg_hdr_sz(hdr) + res) <= sender->publ.max_pkt)
+				goto again;
+
+			return link_send_sections_long(sender, msg_sect,
+						       num_sect, destaddr);
+		}
+		tipc_node_unlock(node);
+	}
+	read_unlock_bh(&tipc_net_lock);
+
+	/* Couldn't find a link to the destination node */
+
+	if (buf)
+		return tipc_reject_msg(buf, TIPC_ERR_NO_NODE);
+	if (res >= 0)
+		return tipc_port_reject_sections(sender, hdr, msg_sect, num_sect,
+						 TIPC_ERR_NO_NODE);
+	return res;
+}
+
+/*
+ * link_send_sections_long(): Entry for long messages where the
+ * destination node is known and the header is complete,
+ * inclusive total message length.
+ * Link and bearer congestion status have been checked to be ok,
+ * and are ignored if they change.
+ *
+ * Note that fragments do not use the full link MTU so that they won't have
+ * to undergo refragmentation if link changeover causes them to be sent
+ * over another link with an additional tunnel header added as prefix.
+ * (Refragmentation will still occur if the other link has a smaller MTU.)
+ *
+ * Returns user data length or errno.
+ */
+static int link_send_sections_long(struct port *sender,
+				   struct iovec const *msg_sect,
+				   u32 num_sect,
+				   u32 destaddr)
+{
+	struct link *l_ptr;
+	struct tipc_node *node;
+	struct tipc_msg *hdr = &sender->publ.phdr;
+	u32 dsz = msg_data_sz(hdr);
+	u32 max_pkt,fragm_sz,rest;
+	struct tipc_msg fragm_hdr;
+	struct sk_buff *buf,*buf_chain,*prev;
+	u32 fragm_crs,fragm_rest,hsz,sect_rest;
+	const unchar *sect_crs;
+	int curr_sect;
+	u32 fragm_no;
+
+again:
+	fragm_no = 1;
+	max_pkt = sender->publ.max_pkt - INT_H_SIZE;
+		/* leave room for tunnel header in case of link changeover */
+	fragm_sz = max_pkt - INT_H_SIZE;
+		/* leave room for fragmentation header in each fragment */
+	rest = dsz;
+	fragm_crs = 0;
+	fragm_rest = 0;
+	sect_rest = 0;
+	sect_crs = NULL;
+	curr_sect = -1;
+
+	/* Prepare reusable fragment header: */
+
+	msg_dbg(hdr, ">FRAGMENTING>");
+	tipc_msg_init(&fragm_hdr, MSG_FRAGMENTER, FIRST_FRAGMENT,
+		      INT_H_SIZE, msg_destnode(hdr));
+	msg_set_link_selector(&fragm_hdr, (sender->publ.ref & 1));
+	msg_set_fragm_msg_no(&fragm_hdr, 
+			     atomic_inc_return(&link_fragm_msg_no) & 0xffff);
+	msg_set_fragm_no(&fragm_hdr, 1);
+
+	/* Prepare header of first fragment: */
+
+	msg_set_size(&fragm_hdr, max_pkt);
+	buf_chain = buf = buf_acquire(max_pkt);
+	if (!buf)
+		return -ENOMEM;
+	buf->next = NULL;
+	skb_copy_to_linear_data(buf, &fragm_hdr, INT_H_SIZE);
+	hsz = msg_hdr_sz(hdr);
+	skb_copy_to_linear_data_offset(buf, INT_H_SIZE, hdr, hsz);
+	msg_dbg(buf_msg(buf), ">BUILD>");
+
+	/* Chop up message: */
+
+	fragm_crs = INT_H_SIZE + hsz;
+	fragm_rest = fragm_sz - hsz;
+
+	do {		/* For all sections */
+		u32 sz;
+
+		if (!sect_rest) {
+			sect_rest = msg_sect[++curr_sect].iov_len;
+			sect_crs = (const unchar *)msg_sect[curr_sect].iov_base;
+		}
+
+		if (sect_rest < fragm_rest)
+			sz = sect_rest;
+		else
+			sz = fragm_rest;
+
+		if (likely(!sender->user_port)) {
+			if (copy_from_user(buf->data + fragm_crs, sect_crs, sz)) {
+error:
+				for (; buf_chain; buf_chain = buf) {
+					buf = buf_chain->next;
+					buf_discard(buf_chain);
+				}
+				return -EFAULT;
+			}
+		} else
+			skb_copy_to_linear_data_offset(buf, fragm_crs,
+						       sect_crs, sz);
+		sect_crs += sz;
+		sect_rest -= sz;
+		fragm_crs += sz;
+		fragm_rest -= sz;
+		rest -= sz;
+
+		if (!fragm_rest && rest) {
+
+			/* Initiate new fragment: */
+			if (rest <= fragm_sz) {
+				fragm_sz = rest;
+				msg_set_type(&fragm_hdr,LAST_FRAGMENT);
+			} else {
+				msg_set_type(&fragm_hdr, FRAGMENT);
+			}
+			msg_set_size(&fragm_hdr, fragm_sz + INT_H_SIZE);
+			msg_set_fragm_no(&fragm_hdr, ++fragm_no);
+			prev = buf;
+			buf = buf_acquire(fragm_sz + INT_H_SIZE);
+			if (!buf)
+				goto error;
+
+			buf->next = NULL;
+			prev->next = buf;
+			skb_copy_to_linear_data(buf, &fragm_hdr, INT_H_SIZE);
+			fragm_crs = INT_H_SIZE;
+			fragm_rest = fragm_sz;
+			msg_dbg(buf_msg(buf),"  >BUILD>");
+		}
+	}
+	while (rest > 0);
+
+	/*
+	 * Now we have a buffer chain. Select a link and check
+	 * that packet size is still OK
+	 */
+
+	node = tipc_net_select_node(destaddr);
+
+	if (likely(node)) {
+		tipc_node_lock(node);
+		l_ptr = node->active_links[sender->publ.ref & 1];
+		if (!l_ptr) {
+			tipc_node_unlock(node);
+			goto reject;
+		}
+		if (l_ptr->max_pkt < max_pkt) {
+			sender->publ.max_pkt = l_ptr->max_pkt;
+			tipc_node_unlock(node);
+			for (; buf_chain; buf_chain = buf) {
+				buf = buf_chain->next;
+				buf_discard(buf_chain);
+			}
+			goto again;
+		}
+	} else {
+reject:
+		for (; buf_chain; buf_chain = buf) {
+			buf = buf_chain->next;
+			buf_discard(buf_chain);
+		}
+		return tipc_port_reject_sections(sender, hdr, msg_sect, num_sect,
+						 TIPC_ERR_NO_NODE);
+	}
+
+	/* Append whole chain to send queue: */
+
+	buf = buf_chain;
+	if (!l_ptr->next_out)
+		l_ptr->next_out = buf_chain;
+	l_ptr->stats.sent_fragmented++;
+	while (buf) {
+		struct sk_buff *next = buf->next;
+		struct tipc_msg *msg = buf_msg(buf);
+
+		l_ptr->stats.sent_fragments++;
+		link_add_to_outqueue(l_ptr, buf, msg);
+		msg_dbg(msg, ">ADD>");
+		buf = next;
+	}
+
+	/* Send it, if possible: */
+
+	tipc_link_push_queue(l_ptr);
+	tipc_node_unlock(node);
+	return dsz;
+}
+
+/*
+ * tipc_link_push_packet: Push one unsent packet to the media
+ */
+u32 tipc_link_push_packet(struct link *l_ptr)
+{
+	struct sk_buff *buf = l_ptr->first_out;
+	u32 r_q_size = l_ptr->retransm_queue_size;
+	u32 r_q_head = l_ptr->retransm_queue_head;
+
+	/* Step to position where retransmission failed, if any,    */
+	/* consider that buffers may have been released in meantime */
+
+	if (r_q_size && buf) {
+		u32 last = lesser(mod(r_q_head + r_q_size),
+				  link_last_sent(l_ptr));
+		u32 first = buf_seqno(buf);
+
+		while (buf && less(first, r_q_head)) {
+			first = mod(first + 1);
+			buf = buf->next;
+		}
+		l_ptr->retransm_queue_head = r_q_head = first;
+		l_ptr->retransm_queue_size = r_q_size = mod(last - first);
+	}
+
+	/* Continue retransmission now, if there is anything: */
+
+	if (r_q_size && buf) {
+		msg_set_ack(buf_msg(buf), mod(l_ptr->next_in_no - 1));
+		msg_set_bcast_ack(buf_msg(buf), l_ptr->owner->bclink.last_in);
+		if (tipc_bearer_send(l_ptr->b_ptr, buf, &l_ptr->media_addr)) {
+			msg_dbg(buf_msg(buf), ">DEF-RETR>");
+			l_ptr->retransm_queue_head = mod(++r_q_head);
+			l_ptr->retransm_queue_size = --r_q_size;
+			l_ptr->stats.retransmitted++;
+			return 0;
+		} else {
+			l_ptr->stats.bearer_congs++;
+			msg_dbg(buf_msg(buf), "|>DEF-RETR>");
+			return PUSH_FAILED;
+		}
+	}
+
+	/* Send deferred protocol message, if any: */
+
+	buf = l_ptr->proto_msg_queue;
+	if (buf) {
+		msg_set_ack(buf_msg(buf), mod(l_ptr->next_in_no - 1));
+		msg_set_bcast_ack(buf_msg(buf),l_ptr->owner->bclink.last_in);
+		if (tipc_bearer_send(l_ptr->b_ptr, buf, &l_ptr->media_addr)) {
+			msg_dbg(buf_msg(buf), ">DEF-PROT>");
+			l_ptr->unacked_window = 0;
+			buf_discard(buf);
+			l_ptr->proto_msg_queue = NULL;
+			return 0;
+		} else {
+			msg_dbg(buf_msg(buf), "|>DEF-PROT>");
+			l_ptr->stats.bearer_congs++;
+			return PUSH_FAILED;
+		}
+	}
+
+	/* Send one deferred data message, if send window not full: */
+
+	buf = l_ptr->next_out;
+	if (buf) {
+		struct tipc_msg *msg = buf_msg(buf);
+		u32 next = msg_seqno(msg);
+		u32 first = buf_seqno(l_ptr->first_out);
+
+		if (mod(next - first) < l_ptr->queue_limit[0]) {
+			msg_set_ack(msg, mod(l_ptr->next_in_no - 1));
+			msg_set_bcast_ack(msg, l_ptr->owner->bclink.last_in);
+			if (tipc_bearer_send(l_ptr->b_ptr, buf, &l_ptr->media_addr)) {
+				if (msg_user(msg) == MSG_BUNDLER)
+					msg_set_type(msg, CLOSED_MSG);
+				msg_dbg(msg, ">PUSH-DATA>");
+				l_ptr->next_out = buf->next;
+				return 0;
+			} else {
+				msg_dbg(msg, "|PUSH-DATA|");
+				l_ptr->stats.bearer_congs++;
+				return PUSH_FAILED;
+			}
+		}
+	}
+	return PUSH_FINISHED;
+}
+
+/*
+ * push_queue(): push out the unsent messages of a link where
+ *               congestion has abated. Node is locked
+ */
+void tipc_link_push_queue(struct link *l_ptr)
+{
+	u32 res;
+
+	if (tipc_bearer_congested(l_ptr->b_ptr, l_ptr))
+		return;
+
+	do {
+		res = tipc_link_push_packet(l_ptr);
+	}
+	while (!res);
+	if (res == PUSH_FAILED)
+		tipc_bearer_schedule(l_ptr->b_ptr, l_ptr);
+}
+
+static void link_reset_all(unsigned long addr)
+{
+	struct tipc_node *n_ptr;
+	char addr_string[16];
+	u32 i;
+
+	read_lock_bh(&tipc_net_lock);
+	n_ptr = tipc_net_find_node((u32)addr);
+	if (!n_ptr) {
+		read_unlock_bh(&tipc_net_lock);
+		return;	/* node no longer exists */
+	}
+
+	tipc_node_lock(n_ptr);
+
+	tipc_addr_string_fill(addr_string, addr);
+	warn("Resetting all links to %s\n", addr_string);
+
+	for (i = 0; i < TIPC_MAX_BEARERS; i++) {
+		if (n_ptr->links[i]) {
+			warn("Resetting link <%s>\n", n_ptr->links[i]->name);
+			dbg_print_link_state(TIPC_OUTPUT, n_ptr->links[i]);
+			tipc_link_reset(n_ptr->links[i]);
+		}
+	}
+
+	tipc_node_unlock(n_ptr);
+	read_unlock_bh(&tipc_net_lock);
+}
+
+static void link_retransmit_failure(struct link *l_ptr, struct sk_buff *buf)
+{
+	warn("Retransmission failure on link <%s>\n", l_ptr->name);
+	tipc_msg_dbg(TIPC_OUTPUT, buf_msg(buf), ">RETR-FAIL>");
+
+	if (l_ptr->addr) {
+
+		/* Handle failure on standard link */
+
+		warn("Resetting link <%s>\n", l_ptr->name);
+		dbg_print_link_state(TIPC_OUTPUT, l_ptr);
+		tipc_link_reset(l_ptr);
+
+	} else {
+
+		/* Handle failure on broadcast link */
+
+		struct tipc_node *n_ptr;
+		char addr_string[16];
+
+		dbg_printf(TIPC_OUTPUT, "Msg seq number: %u,  ", buf_seqno(buf));
+		dbg_printf(TIPC_OUTPUT, "Outstanding acks: %u\n", (u32)buf_handle(buf));
+		
+		/* recover retransmit requester */
+		n_ptr = (struct tipc_node *)l_ptr->owner->node_list.next;
+		tipc_node_lock(n_ptr);
+
+		tipc_addr_string_fill(addr_string, n_ptr->elm.addr);
+		dbg_printf(TIPC_OUTPUT, "Broadcast link info for %s\n", addr_string);
+		dbg_printf(TIPC_OUTPUT, "Supported: %d,  ", n_ptr->bclink.supported);
+		dbg_printf(TIPC_OUTPUT, "Acked: %u\n", n_ptr->bclink.acked);
+		dbg_printf(TIPC_OUTPUT, "Last in: %u,  ", n_ptr->bclink.last_in);
+		dbg_printf(TIPC_OUTPUT, "Oos state: %u,  ", n_ptr->bclink.oos_state);
+		dbg_printf(TIPC_OUTPUT, "Last sent: %u\n", n_ptr->bclink.last_sent);
+
+		tipc_k_signal((Handler)link_reset_all, (unsigned long)n_ptr->elm.addr);
+
+		tipc_node_unlock(n_ptr);
+
+		l_ptr->stale_count = 0;
+	}
+}
+
+void tipc_link_retransmit(struct link *l_ptr, struct sk_buff *buf,
+			  u32 retransmits)
+{
+	struct tipc_msg *msg;
+
+	if (!buf)
+		return;
+
+	msg = buf_msg(buf);
+
+	dbg("Retransmitting %u in link %x\n", retransmits, l_ptr);
+
+	if (tipc_bearer_congested(l_ptr->b_ptr, l_ptr)) {
+		if (l_ptr->retransm_queue_size == 0) {
+			msg_dbg(msg, ">NO_RETR->BCONG>");
+			dbg_print_link(l_ptr, "   ");
+			l_ptr->retransm_queue_head = msg_seqno(msg);
+			l_ptr->retransm_queue_size = retransmits;
+		} else {
+			err("Unexpected retransmit on link %s (qsize=%d)\n",
+			    l_ptr->name, l_ptr->retransm_queue_size);
+		}
+		return;
+	} else {
+		/* Detect repeated retransmit failures on uncongested bearer */
+
+		if (l_ptr->last_retransmitted == msg_seqno(msg)) {
+			if (++l_ptr->stale_count > 100) {
+				link_retransmit_failure(l_ptr, buf);
+				return;
+			}
+		} else {
+			l_ptr->last_retransmitted = msg_seqno(msg);
+			l_ptr->stale_count = 1;
+		}
+	}
+
+	while (retransmits && (buf != l_ptr->next_out) && buf) {
+		msg = buf_msg(buf);
+		msg_set_ack(msg, mod(l_ptr->next_in_no - 1));
+		msg_set_bcast_ack(msg, l_ptr->owner->bclink.last_in);
+		if (tipc_bearer_send(l_ptr->b_ptr, buf, &l_ptr->media_addr)) {
+			msg_dbg(buf_msg(buf), ">RETR>");
+			buf = buf->next;
+			retransmits--;
+			l_ptr->stats.retransmitted++;
+		} else {
+			tipc_bearer_schedule(l_ptr->b_ptr, l_ptr);
+			l_ptr->stats.bearer_congs++;
+			l_ptr->retransm_queue_head = buf_seqno(buf);
+			l_ptr->retransm_queue_size = retransmits;
+			return;
+		}
+	}
+
+	l_ptr->retransm_queue_head = l_ptr->retransm_queue_size = 0;
+}
+
+/**
+ * link_insert_deferred_queue - insert deferred messages back into receive chain
+ */
+
+static struct sk_buff *link_insert_deferred_queue(struct link *l_ptr,
+						  struct sk_buff *buf)
+{
+	u32 seq_no;
+
+	if (l_ptr->oldest_deferred_in == NULL)
+		return buf;
+
+	seq_no = buf_seqno(l_ptr->oldest_deferred_in);
+	if (seq_no == mod(l_ptr->next_in_no)) {
+		l_ptr->newest_deferred_in->next = buf;
+		buf = l_ptr->oldest_deferred_in;
+		l_ptr->oldest_deferred_in = NULL;
+		l_ptr->deferred_inqueue_sz = 0;
+	}
+	return buf;
+}
+
+/**
+ * link_recv_buf_validate - validate basic format of received message
+ *
+ * This routine ensures a TIPC message has an acceptable header, and at least
+ * as much data as the header indicates it should.  The routine also ensures
+ * that the entire message header is stored in the main fragment of the message
+ * buffer, to simplify future access to message header fields.
+ *
+ * Note: Having extra info present in the message header or data areas is OK.
+ * TIPC will ignore the excess, under the assumption that it is optional info
+ * introduced by a later release of the protocol.
+ */
+
+static int link_recv_buf_validate(struct sk_buff *buf)
+{
+	static u32 min_data_hdr_size[8] = {
+		SHORT_H_SIZE, MCAST_H_SIZE, LONG_H_SIZE, DIR_MSG_H_SIZE,
+		MAX_H_SIZE, MAX_H_SIZE, MAX_H_SIZE, MAX_H_SIZE
+		};
+
+	struct tipc_msg *msg;
+	u32 tipc_hdr[2];
+	u32 size;
+	u32 hdr_size;
+	u32 min_hdr_size;
+
+	if (unlikely(buf->len < MIN_H_SIZE))
+		return 0;
+
+	msg = skb_header_pointer(buf, 0, sizeof(tipc_hdr), tipc_hdr);
+	if (msg == NULL)
+		return 0;
+
+	if (unlikely(msg_version(msg) != TIPC_VERSION))
+		return 0;
+
+	size = msg_size(msg);
+	hdr_size = msg_hdr_sz(msg);
+	min_hdr_size = msg_isdata(msg) ?
+		min_data_hdr_size[msg_type(msg)] : INT_H_SIZE;
+
+	if (unlikely((hdr_size < min_hdr_size) ||
+		     (size < hdr_size) ||
+		     (buf->len < size) ||
+		     (size - hdr_size > TIPC_MAX_USER_MSG_SIZE)))
+		return 0;
+
+	return pskb_may_pull(buf, hdr_size);
+}
+
+/**
+ * tipc_recv_msg - process TIPC messages arriving from off-node
+ * @head: pointer to message buffer chain
+ * @tb_ptr: pointer to bearer message arrived on
+ * 
+ * Invoked with no locks held.  Bearer pointer must point to a valid bearer
+ * structure (i.e. cannot be NULL), but bearer can be inactive.
+ */
+
+void tipc_recv_msg(struct sk_buff *head, struct tipc_bearer *tb_ptr)
+{
+	read_lock_bh(&tipc_net_lock);
+	while (head) {
+		struct bearer *b_ptr = (struct bearer *)tb_ptr;
+		struct tipc_node *n_ptr;
+		struct link *l_ptr;
+		struct sk_buff *crs;
+		struct sk_buff *buf;
+		struct tipc_msg *msg;
+		u32 type;
+		u32 seq_no;
+		u32 ackd;
+		u32 released;
+
+		buf = head;
+		head = head->next;
+
+		/* Ensure bearer is still enabled */
+
+		if (unlikely(!b_ptr->active))
+			goto cont;
+       		
+		/* Ensure message is well-formed */
+
+		if (unlikely(!link_recv_buf_validate(buf)))
+			goto cont;
+
+		/* 
+		 * Ensure message is stored as a single contiguous unit;
+		 * support for non-linear sk_buffs is under development ...
+		 */
+
+		if (unlikely(buf_linearize(buf))) {
+			goto cont;
+		}
+
+		/* Handle arrival of a non-unicast link message */
+
+		msg = buf_msg(buf);
+
+		if (unlikely(msg_non_seq(msg))) {
+			if (msg_user(msg) == LINK_CONFIG)
+				tipc_disc_recv_msg(buf, b_ptr);
+			else
+				tipc_bclink_recv_pkt(buf);
+			continue;
+		}
+
+		/* Discard non-routeable messages destined for another node */
+
+		if (unlikely(!msg_isdata(msg) && 
+			     (msg_destnode(msg) != tipc_own_addr))) {
+			if ((msg_user(msg) != CONN_MANAGER) &&
+			    (msg_user(msg) != MSG_FRAGMENTER))
+				goto cont;
+		}
+
+		/* Locate neighboring node that sent message */
+
+		n_ptr = tipc_net_find_node(msg_prevnode(msg));
+		if (unlikely(!n_ptr))
+			goto cont;
+		tipc_node_lock(n_ptr);
+
+		/* Don't talk to neighbor during cleanup after last session */
+
+		if (n_ptr->cleanup_required) {
+			tipc_node_unlock(n_ptr);                
+			goto cont;
+		}
+
+		/* Locate unicast link endpoint that should handle message */
+
+		l_ptr = n_ptr->links[b_ptr->identity];
+		if (unlikely(!l_ptr)) {
+			tipc_node_unlock(n_ptr);
+			goto cont;
+		}
+
+		/* Validate message sequence number info */
+
+		seq_no = msg_seqno(msg);
+		ackd = msg_ack(msg);
+
+		/* TODO: Implement stronger sequence # checking someday ... */
+
+		/* Release acked messages */
+
+		if (less(n_ptr->bclink.acked, msg_bcast_ack(msg)) &&
+		    tipc_node_is_up(n_ptr) && n_ptr->bclink.supported) {
+			tipc_bclink_acknowledge(n_ptr, msg_bcast_ack(msg));
+		}
+
+		released = 0;
+		crs = l_ptr->first_out;
+		while ((crs != l_ptr->next_out) &&
+		       less_eq(buf_seqno(crs), ackd)) {
+			struct sk_buff *next = crs->next;
+
+			buf_discard(crs);
+			crs = next;
+			released++;
+		}
+		if (released) {
+			l_ptr->first_out = crs;
+			l_ptr->out_queue_size -= released;
+		}
+
+		/* Try sending any messages link endpoint has pending */
+
+		if (unlikely(l_ptr->next_out))
+			tipc_link_push_queue(l_ptr);
+		if (unlikely(!list_empty(&l_ptr->waiting_ports)))
+			tipc_link_wakeup_ports(l_ptr, 0);
+		if (unlikely(++l_ptr->unacked_window >= TIPC_MIN_LINK_WIN)) {
+			l_ptr->stats.sent_acks++;
+			tipc_link_send_proto_msg(l_ptr, STATE_MSG, 0, 0, 0, 0, 0, 0);
+		}
+
+		/* Now (finally!) process the incoming message */
+
+#ifdef CONFIG_TIPC_MULTIPLE_LINKS
+protocol_check:
+#endif
+		if (likely(link_working_working(l_ptr))) {
+			if (likely(seq_no == mod(l_ptr->next_in_no))) {
+				l_ptr->next_in_no++;
+				if (unlikely(l_ptr->oldest_deferred_in))
+					head = link_insert_deferred_queue(l_ptr,
+									  head);
+deliver:
+                                if (likely(msg_isdata(msg))) {
+                                        tipc_node_unlock(n_ptr);
+                                        if (likely(msg_short(msg) ||
+						   (msg_destnode(msg) == tipc_own_addr))) 
+                                                tipc_port_recv_msg(buf);
+                                        else
+                                                tipc_net_route_msg(buf);
+                                        continue;
+                                } 
+
+				if (unlikely(msg_destnode(msg) != tipc_own_addr)) {
+                                        tipc_node_unlock(n_ptr);
+					if ((msg_user(msg) != MSG_FRAGMENTER) &&
+					    (msg_user(msg) != CONN_MANAGER))
+						goto cont;
+					tipc_net_route_msg(buf);
+					continue;
+				}
+
+                                switch (msg_user(msg)) {
+                                case MSG_BUNDLER:
+                                        l_ptr->stats.recv_bundles++;
+                                        l_ptr->stats.recv_bundled += 
+                                                msg_msgcnt(msg);
+                                        tipc_node_unlock(n_ptr);
+                                        tipc_link_recv_bundle(buf);
+                                        continue;
+				case NAME_DISTRIBUTOR:
+                                        tipc_node_unlock(n_ptr);
+					tipc_named_recv(buf);
+                                        continue;
+				case ROUTE_DISTRIBUTOR:
+                                        tipc_node_unlock(n_ptr);
+					tipc_route_recv(buf);
+                                        continue;
+                                case CONN_MANAGER:
+                                        /* route message normally */
+                                        break;
+                                case MSG_FRAGMENTER:
+                                        l_ptr->stats.recv_fragments++;
+                                        if (tipc_link_recv_fragment(&l_ptr->defragm_buf, 
+                                                                    &buf, &msg)) {
+                                                l_ptr->stats.recv_fragmented++;
+                                                goto deliver;
+                                        }
+                                        break;
+#ifdef CONFIG_TIPC_MULTIPLE_LINKS
+                                case CHANGEOVER_PROTOCOL:
+                                        type = msg_type(msg);
+                                        if (link_recv_changeover_msg(&l_ptr, &buf)) {
+                                                msg = buf_msg(buf);
+                                                seq_no = msg_seqno(msg);
+                                                if (type == ORIGINAL_MSG)
+                                                        goto deliver;
+                                                goto protocol_check;
+                                        }
+                                        break;
+#endif
+				default:
+					dbg("Unsupported message discarded (user=%d)\n",
+					    msg_user(msg));
+					buf_discard(buf);
+					buf = NULL;
+					break;
+                                }
+				tipc_node_unlock(n_ptr);
+				tipc_net_route_msg(buf);
+				continue;
+			}
+			link_handle_out_of_seq_msg(l_ptr, buf);
+			head = link_insert_deferred_queue(l_ptr, head);
+			tipc_node_unlock(n_ptr);
+			continue;
+		}
+
+		if (msg_user(msg) == LINK_PROTOCOL) {
+			link_recv_proto_msg(l_ptr, buf);
+			head = link_insert_deferred_queue(l_ptr, head);
+			tipc_node_unlock(n_ptr);
+			continue;
+		}
+		msg_dbg(msg,"NSEQ<REC<");
+		link_state_event(l_ptr, TRAFFIC_MSG_EVT);
+
+		if (link_working_working(l_ptr)) {
+			/* Re-insert in front of queue */
+			msg_dbg(msg,"RECV-REINS:");
+			buf->next = head;
+			head = buf;
+			tipc_node_unlock(n_ptr);
+			continue;
+		}
+		tipc_node_unlock(n_ptr);
+cont:
+		buf_discard(buf);
+	}
+	read_unlock_bh(&tipc_net_lock);
+}
+
+/**
+ * tipc_link_defer_pkt - Add out-of-sequence message to deferred reception queue
+ *
+ * Returns increase in queue length (i.e. 0 or 1)
+ */
+
+u32 tipc_link_defer_pkt(struct sk_buff **head, struct sk_buff **tail,
+			struct sk_buff *buf, u32 buf_seq_no)
+{
+	struct sk_buff *curr;
+	struct sk_buff **prev_link;
+
+	buf->next = NULL;
+
+	/* Handle most likely cases (add to empty queue, or at end of queue) */
+
+	if (!(*head)) {
+		*head = *tail = buf;
+		return 1;
+	}
+
+	if (less(buf_seqno(*tail), buf_seq_no)) {
+		(*tail)->next = buf;
+		*tail = buf;
+		return 1;
+	}
+
+	/* Locate insertion point in queue, then insert; discard if duplicate */
+
+	for (prev_link = head, curr = *head; ;
+	     prev_link = &curr->next, curr = curr->next) {
+		u32 curr_seq_no = buf_seqno(curr);
+
+		if (buf_seq_no == curr_seq_no) {
+			buf_discard(buf);
+			return 0;
+		}
+		
+		/* Note: here less_eq() is equivalent to less(), but faster */
+
+		if (less_eq(buf_seq_no, curr_seq_no))
+			break;
+	}
+
+	buf->next = curr;
+	*prev_link = buf;
+	return 1;
+}
+
+/**
+ * link_handle_out_of_seq_msg - handle arrival of out-of-sequence packet
+ */
+
+static void link_handle_out_of_seq_msg(struct link *l_ptr,
+				       struct sk_buff *buf)
+{
+	u32 seq_no = buf_seqno(buf);
+
+	if (likely(msg_user(buf_msg(buf)) == LINK_PROTOCOL)) {
+		link_recv_proto_msg(l_ptr, buf);
+		return;
+	}
+
+	dbg("rx OOS msg: seq_no %u, expecting %u (%u)\n",
+	    seq_no, mod(l_ptr->next_in_no), l_ptr->next_in_no);
+
+	/* Record OOS packet arrival (force mismatch on next timeout) */
+
+	l_ptr->checkpoint--;
+
+	/*
+	 * Discard packet if a duplicate; otherwise add it to deferred queue
+	 * and notify peer of gap as per protocol specification
+	 */
+
+	if (less(seq_no, mod(l_ptr->next_in_no))) {
+		l_ptr->stats.duplicates++;
+		buf_discard(buf);
+		return;
+	}
+
+	if (tipc_link_defer_pkt(&l_ptr->oldest_deferred_in,
+				&l_ptr->newest_deferred_in,
+				buf, seq_no)) {
+		l_ptr->deferred_inqueue_sz++;
+		l_ptr->stats.deferred_recv++;
+		if ((l_ptr->deferred_inqueue_sz % 16) == 1)
+			tipc_link_send_proto_msg(l_ptr, STATE_MSG, 0, 0, 0, 0, 0, 0);
+	} else
+		l_ptr->stats.duplicates++;
+}
+
+/*
+ * Send protocol message to the other endpoint.
+ */
+void tipc_link_send_proto_msg(struct link *l_ptr, u32 msg_typ, int probe_msg,
+                              u32 gap, u32 tolerance, u32 priority, u32 ack_mtu,
+                              int stop)
+{
+	struct sk_buff *buf = NULL;
+	struct tipc_msg *msg = l_ptr->pmsg;
+	u32 msg_size = sizeof(l_ptr->proto_msg);
+
+	if (link_blocked(l_ptr))
+		return;
+	msg_set_type(msg, msg_typ);
+	msg_set_net_plane(msg, l_ptr->b_ptr->net_plane);
+	msg_set_bcast_ack(msg, l_ptr->owner->bclink.last_in);
+	msg_set_last_bcast(msg, tipc_bclink_get_last_sent());
+
+	if (msg_typ == STATE_MSG) {
+		u32 next_sent = mod(l_ptr->next_out_no);
+
+		if (!tipc_link_is_up(l_ptr))
+			return;
+		if (l_ptr->next_out)
+			next_sent = buf_seqno(l_ptr->next_out);
+		msg_set_next_sent(msg, next_sent);
+		if (l_ptr->oldest_deferred_in) {
+			u32 rec = buf_seqno(l_ptr->oldest_deferred_in);
+			gap = mod(rec - mod(l_ptr->next_in_no));
+		}
+		msg_set_seq_gap(msg, gap);
+		if (gap)
+			l_ptr->stats.sent_nacks++;
+		msg_set_link_tolerance(msg, tolerance);
+		msg_set_linkprio(msg, priority);
+		msg_set_max_pkt(msg, ack_mtu);
+		msg_set_ack(msg, mod(l_ptr->next_in_no - 1));
+		msg_set_probe(msg, probe_msg != 0);
+		if (probe_msg) {
+			u32 mtu = l_ptr->max_pkt;
+
+			if ((mtu < l_ptr->max_pkt_target) &&
+			    link_working_working(l_ptr) &&
+			    l_ptr->fsm_msg_cnt) {
+				msg_size = (mtu + (l_ptr->max_pkt_target - mtu)/2 + 2) & ~3;
+				if (l_ptr->max_pkt_probes == 10) {
+					l_ptr->max_pkt_target = (msg_size - 4);
+					l_ptr->max_pkt_probes = 0;
+					msg_size = (mtu + (l_ptr->max_pkt_target - mtu)/2 + 2) & ~3;
+				}
+				l_ptr->max_pkt_probes++;
+			}
+
+			l_ptr->stats.sent_probes++;
+		}
+		l_ptr->stats.sent_states++;
+	} else {		/* RESET_MSG or ACTIVATE_MSG */
+		msg_set_ack(msg, mod(l_ptr->reset_checkpoint - 1));
+		msg_set_seq_gap(msg, 0);
+		msg_set_next_sent(msg, 1);
+		msg_set_stop(msg, stop);
+		msg_set_link_tolerance(msg, l_ptr->tolerance);
+		msg_set_linkprio(msg, l_ptr->priority);
+		msg_set_max_pkt(msg, l_ptr->max_pkt_target);
+	}
+
+	msg_set_redundant_link(msg, tipc_node_has_redundant_links(l_ptr->owner));
+	msg_set_linkprio(msg, l_ptr->priority);
+
+	/* Ensure sequence number will not fit : */
+
+	msg_set_seqno(msg, mod(l_ptr->next_out_no + (0xffff/2)));
+
+	/* Congestion? */
+
+	if (tipc_bearer_congested(l_ptr->b_ptr, l_ptr)) {
+		if (!l_ptr->proto_msg_queue) {
+			l_ptr->proto_msg_queue =
+				buf_acquire(sizeof(l_ptr->proto_msg));
+		}
+		buf = l_ptr->proto_msg_queue;
+		if (!buf)
+			return;
+		skb_copy_to_linear_data(buf, msg, sizeof(l_ptr->proto_msg));
+		return;
+	}
+	msg_set_timestamp(msg, jiffies_to_msecs(jiffies));
+
+	/* Message can be sent */
+
+	msg_dbg(msg, ">>");
+
+	buf = buf_acquire(msg_size);
+	if (!buf)
+		return;
+
+	skb_copy_to_linear_data(buf, msg, sizeof(l_ptr->proto_msg));
+	msg_set_size(buf_msg(buf), msg_size);
+
+	if (tipc_bearer_send(l_ptr->b_ptr, buf, &l_ptr->media_addr)) {
+		l_ptr->unacked_window = 0;
+		buf_discard(buf);
+		return;
+	}
+
+	/* New congestion */
+	tipc_bearer_schedule(l_ptr->b_ptr, l_ptr);
+	l_ptr->proto_msg_queue = buf;
+	l_ptr->stats.bearer_congs++;
+}
+
+/*
+ * Receive protocol message :
+ * Note that network plane id propagates through the network, and may
+ * change at any time. The node with lowest address rules
+ */
+
+static void link_recv_proto_msg(struct link *l_ptr, struct sk_buff *buf)
+{
+	u32 rec_gap = 0;
+	u32 max_pkt_info;
+	u32 max_pkt_ack;
+	u32 msg_tol;
+	struct tipc_msg *msg = buf_msg(buf);
+
+	dbg("AT(%u):", jiffies_to_msecs(jiffies));
+	msg_dbg(msg, "<<");
+	if (link_blocked(l_ptr))
+		goto exit;
+
+	/* record unnumbered packet arrival (force mismatch on next timeout) */
+
+	l_ptr->checkpoint--;
+
+	if (l_ptr->b_ptr->net_plane != msg_net_plane(msg))
+		if (tipc_own_addr > msg_prevnode(msg))
+			l_ptr->b_ptr->net_plane = msg_net_plane(msg);
+
+	l_ptr->owner->permit_changeover = msg_redundant_link(msg);
+
+	switch (msg_type(msg)) {
+
+	case RESET_MSG:
+		if (!link_working_unknown(l_ptr) &&
+		    (l_ptr->peer_session != INVALID_SESSION)) {
+			if (msg_session(msg) == l_ptr->peer_session) {
+				dbg("Duplicate RESET: %u<->%u\n",
+				    msg_session(msg), l_ptr->peer_session);
+				break; /* duplicate: ignore */
+			}
+		}
+                if (msg_stop(msg)) {
+                        tipc_link_reset(l_ptr);
+                        l_ptr->blocked = 1;
+                        tipc_k_signal((Handler)link_remote_delete,(unsigned long)l_ptr);
+                        break;
+                }
+
+		/* fall thru' */
+	case ACTIVATE_MSG:
+		/* Update link settings according other endpoint's values */
+
+		strcpy((strrchr(l_ptr->name, ':') + 1), (char *)msg_data(msg));
+
+		if ((msg_tol = msg_link_tolerance(msg)) &&
+		    (msg_tol > l_ptr->tolerance))
+			link_set_supervision_props(l_ptr, msg_tol);
+
+		if (msg_linkprio(msg) > l_ptr->priority)
+			l_ptr->priority = msg_linkprio(msg);
+
+		max_pkt_info = msg_max_pkt(msg);
+		if (max_pkt_info) {
+			if (max_pkt_info < l_ptr->max_pkt_target)
+				l_ptr->max_pkt_target = max_pkt_info;
+			if (l_ptr->max_pkt > l_ptr->max_pkt_target)
+				l_ptr->max_pkt = l_ptr->max_pkt_target;
+		} else {
+			l_ptr->max_pkt = l_ptr->max_pkt_target;
+		}
+		l_ptr->owner->bclink.supported = 
+			in_own_cluster(l_ptr->owner->elm.addr) &&
+			(max_pkt_info != 0);
+
+		link_state_event(l_ptr, msg_type(msg));
+
+		l_ptr->peer_session = msg_session(msg);
+		l_ptr->peer_bearer_id = msg_bearer_id(msg);
+
+		/* Synchronize broadcast link information */
+
+		if (!tipc_node_has_redundant_links(l_ptr->owner)) {
+			l_ptr->owner->bclink.last_sent =
+				l_ptr->owner->bclink.last_in =
+				msg_last_bcast(msg);
+			l_ptr->owner->bclink.oos_state = 0;
+		}
+		break;
+	case STATE_MSG:
+
+		if ((msg_tol = msg_link_tolerance(msg)))
+			link_set_supervision_props(l_ptr, msg_tol);
+
+		if (msg_linkprio(msg) &&
+		    (msg_linkprio(msg) != l_ptr->priority)) {
+			warn("Resetting link <%s>, priority change %u->%u\n",
+			     l_ptr->name, l_ptr->priority, msg_linkprio(msg));
+			l_ptr->priority = msg_linkprio(msg);
+			tipc_link_reset(l_ptr); /* Enforce change to take effect */
+			break;
+		}
+		link_state_event(l_ptr, TRAFFIC_MSG_EVT);
+		l_ptr->stats.recv_states++;
+		if (link_reset_unknown(l_ptr))
+			break;
+
+		if (less_eq(mod(l_ptr->next_in_no), msg_next_sent(msg))) {
+			rec_gap = mod(msg_next_sent(msg) -
+				      mod(l_ptr->next_in_no));
+		}
+
+		max_pkt_ack = msg_max_pkt(msg);
+		if (max_pkt_ack > l_ptr->max_pkt) {
+			dbg("Link <%s> updated MTU %u -> %u\n",
+			    l_ptr->name, l_ptr->max_pkt, max_pkt_ack);
+			l_ptr->max_pkt = max_pkt_ack;
+			l_ptr->max_pkt_probes = 0;
+		}
+
+		max_pkt_ack = 0;
+		if (msg_probe(msg)) {
+			l_ptr->stats.recv_probes++;
+			if (msg_size(msg) > sizeof(l_ptr->proto_msg)) {
+				max_pkt_ack = msg_size(msg);
+			}
+		}
+
+		/* Protocol message before retransmits, reduce loss risk */
+
+		if (l_ptr->owner->bclink.supported)
+			tipc_bclink_update_link_state(l_ptr->owner,
+						      msg_last_bcast(msg));
+
+		if (rec_gap || (msg_probe(msg))) {
+			tipc_link_send_proto_msg(l_ptr, STATE_MSG,
+					    0, rec_gap, 0, 0, max_pkt_ack, 0);
+		}
+		if (msg_seq_gap(msg)) {
+			msg_dbg(msg, "With Gap:");
+			l_ptr->stats.recv_nacks++;
+			tipc_link_retransmit(l_ptr, l_ptr->first_out,
+					     msg_seq_gap(msg));
+		}
+		break;
+	default:
+		msg_dbg(buf_msg(buf), "<DISCARDING UNKNOWN<");
+	}
+exit:
+	buf_discard(buf);
+}
+
+/**
+ * buf_extract - extracts embedded TIPC message from another message
+ * @skb: encapsulating message buffer
+ * @from_pos: offset to extract from
+ *
+ * Returns a new message buffer containing an embedded message.  The 
+ * encapsulating message itself is left unchanged.
+ */
+
+static struct sk_buff *buf_extract(struct sk_buff *skb, u32 from_pos)
+{
+	struct tipc_msg *msg = (struct tipc_msg *)(skb->data + from_pos);
+	u32 size = msg_size(msg);
+	struct sk_buff *eb;
+
+	eb = buf_acquire(size);
+	if (eb)
+		skb_copy_to_linear_data(eb, msg, size);
+	return eb;
+}
+
+#ifdef CONFIG_TIPC_MULTIPLE_LINKS
+
+/*
+ * link_tunnel(): Send one message via a link belonging to another bearer.
+ * Owner node is locked.
+ */
+static void link_tunnel(struct link *l_ptr, struct tipc_msg *tunnel_hdr, 
+			struct tipc_msg  *msg, u32 selector)
+{
+	struct link *tunnel;
+	struct sk_buff *buf;
+	u32 length = msg_size(msg);
+
+	tunnel = l_ptr->owner->active_links[selector & 1];
+	if (!tipc_link_is_up(tunnel)) {
+		warn("Link changeover error, "
+		     "tunnel link no longer available\n");
+		return;
+	}
+	msg_set_size(tunnel_hdr, length + INT_H_SIZE);
+	buf = buf_acquire(length + INT_H_SIZE);
+	if (!buf) {
+		warn("Link changeover error, "
+		     "unable to send tunnel msg\n");
+		return;
+	}
+	skb_copy_to_linear_data(buf, tunnel_hdr, INT_H_SIZE);
+	skb_copy_to_linear_data_offset(buf, INT_H_SIZE, msg, length);
+	dbg("%c->%c:", l_ptr->b_ptr->net_plane, tunnel->b_ptr->net_plane);
+	msg_dbg(buf_msg(buf), ">SEND>");
+	tipc_link_send_buf(tunnel, buf);
+}
+
+
+
+/*
+ * changeover(): Send whole message queue via the remaining link
+ *               Owner node is locked.
+ */
+
+void tipc_link_changeover(struct link *l_ptr)
+{
+	u32 msgcount = l_ptr->out_queue_size;
+	struct sk_buff *crs = l_ptr->first_out;
+	struct link *tunnel = l_ptr->owner->active_links[0];
+	struct tipc_msg tunnel_hdr;
+	int split_bundles;
+
+	if (!tunnel)
+		return;
+
+	if (!l_ptr->owner->permit_changeover) {
+		warn("Link changeover error, "
+		     "peer did not permit changeover\n");
+		return;
+	}
+
+	tipc_msg_init(&tunnel_hdr, CHANGEOVER_PROTOCOL, ORIGINAL_MSG,
+		      INT_H_SIZE, l_ptr->addr);
+	msg_set_bearer_id(&tunnel_hdr, l_ptr->peer_bearer_id);
+	msg_set_msgcnt(&tunnel_hdr, msgcount);
+	dbg("Link changeover requires %u tunnel messages\n", msgcount);
+
+	if (!l_ptr->first_out) {
+		struct sk_buff *buf;
+
+		buf = buf_acquire(INT_H_SIZE);
+		if (buf) {
+			skb_copy_to_linear_data(buf, &tunnel_hdr, INT_H_SIZE);
+			msg_set_size(&tunnel_hdr, INT_H_SIZE);
+			dbg("%c->%c:", l_ptr->b_ptr->net_plane,
+			    tunnel->b_ptr->net_plane);
+			msg_dbg(&tunnel_hdr, "EMPTY>SEND>");
+			tipc_link_send_buf(tunnel, buf);
+		} else {
+			warn("Link changeover error, "
+			     "unable to send changeover msg\n");
+		}
+		return;
+	}
+
+	split_bundles = (l_ptr->owner->active_links[0] !=
+			 l_ptr->owner->active_links[1]);
+
+	while (crs) {
+		struct tipc_msg *msg = buf_msg(crs);
+
+		if ((msg_user(msg) == MSG_BUNDLER) && split_bundles) {
+			u32 bundle_size = msg_msgcnt(msg);
+			struct tipc_msg *m = msg_get_wrapped(msg);
+			unchar* pos = (unchar*)m;
+
+			while (bundle_size--) {
+				msg_set_seqno(m,msg_seqno(msg));
+				link_tunnel(l_ptr, &tunnel_hdr, m,
+					    msg_link_selector(m));
+				pos += align(msg_size(m));
+				m = (struct tipc_msg *)pos;
+			}
+		} else {
+			link_tunnel(l_ptr, &tunnel_hdr, msg,
+				    msg_link_selector(msg));
+		}
+		crs = crs->next;
+	}
+}
+
+void tipc_link_send_duplicate(struct link *l_ptr, struct link *tunnel)
+{
+	struct sk_buff *iter;
+	struct tipc_msg tunnel_hdr;
+
+	tipc_msg_init(&tunnel_hdr, CHANGEOVER_PROTOCOL, DUPLICATE_MSG,
+		      INT_H_SIZE, l_ptr->addr);
+	msg_set_msgcnt(&tunnel_hdr, l_ptr->out_queue_size);
+	msg_set_bearer_id(&tunnel_hdr, l_ptr->peer_bearer_id);
+	iter = l_ptr->first_out;
+	while (iter) {
+		struct sk_buff *outbuf;
+		struct tipc_msg *msg = buf_msg(iter);
+		u32 length = msg_size(msg);
+
+		if (msg_user(msg) == MSG_BUNDLER)
+			msg_set_type(msg, CLOSED_MSG);
+		msg_set_ack(msg, mod(l_ptr->next_in_no - 1));	/* Update */
+		msg_set_bcast_ack(msg, l_ptr->owner->bclink.last_in);
+		msg_set_size(&tunnel_hdr, length + INT_H_SIZE);
+		outbuf = buf_acquire(length + INT_H_SIZE);
+		if (outbuf == NULL) {
+			warn("Link changeover error, "
+			     "unable to send duplicate msg\n");
+			return;
+		}
+		skb_copy_to_linear_data(outbuf, &tunnel_hdr, INT_H_SIZE);
+		skb_copy_to_linear_data_offset(outbuf, INT_H_SIZE, iter->data,
+					       length);
+		dbg("%c->%c:", l_ptr->b_ptr->net_plane,
+		    tunnel->b_ptr->net_plane);
+		msg_dbg(buf_msg(outbuf), ">SEND>");
+		tipc_link_send_buf(tunnel, outbuf);
+		if (!tipc_link_is_up(l_ptr))
+			return;
+		iter = iter->next;
+	}
+}
+
+/*
+ *  link_recv_changeover_msg(): Receive tunneled packet sent
+ *  via other link. Node is locked. Return extracted buffer.
+ */
+
+static int link_recv_changeover_msg(struct link **l_ptr,
+				    struct sk_buff **buf)
+{
+	struct sk_buff *tunnel_buf = *buf;
+	struct link *dest_link;
+	struct tipc_msg *msg;
+	struct tipc_msg *tunnel_msg = buf_msg(tunnel_buf);
+	u32 msg_typ = msg_type(tunnel_msg);
+	u32 msg_count = msg_msgcnt(tunnel_msg);
+
+	dest_link = (*l_ptr)->owner->links[msg_bearer_id(tunnel_msg)];
+	if (!dest_link) {
+		msg_dbg(tunnel_msg, "NOLINK/<REC<");
+		goto exit;
+	}
+	if (dest_link == *l_ptr) {
+		err("Unexpected changeover message on link <%s>\n",
+		    (*l_ptr)->name);
+		goto exit;
+	}
+	dbg("%c<-%c:", dest_link->b_ptr->net_plane,
+	    (*l_ptr)->b_ptr->net_plane);
+	*l_ptr = dest_link;
+	msg = msg_get_wrapped(tunnel_msg);
+
+	if (msg_typ == DUPLICATE_MSG) {
+		if (less(msg_seqno(msg), mod(dest_link->next_in_no))) {
+			msg_dbg(tunnel_msg, "DROP/<REC<");
+			goto exit;
+		}
+		*buf = buf_extract(tunnel_buf,INT_H_SIZE);
+		if (*buf == NULL) {
+			warn("Link changeover error, duplicate msg dropped\n");
+			goto exit;
+		}
+		msg_dbg(tunnel_msg, "TNL<REC<");
+		buf_discard(tunnel_buf);
+		return 1;
+	}
+
+	/* First original message ?: */
+
+	if (tipc_link_is_up(dest_link)) {
+		msg_dbg(tunnel_msg, "UP/FIRST/<REC<");
+		info("Resetting link <%s>, changeover initiated by peer\n",
+		     dest_link->name);
+		tipc_link_reset(dest_link);
+		dest_link->exp_msg_count = msg_count;
+		dbg("Expecting %u tunnelled messages\n", msg_count);
+		if (!msg_count)
+			goto exit;
+	} else if (dest_link->exp_msg_count == START_CHANGEOVER) {
+		msg_dbg(tunnel_msg, "BLK/FIRST/<REC<");
+		dest_link->exp_msg_count = msg_count;
+		dbg("Expecting %u tunnelled messages\n", msg_count);
+		if (!msg_count)
+			goto exit;
+	}
+
+	/* Receive original message */
+
+	if (dest_link->exp_msg_count == 0) {
+		warn("Link switchover error, "
+		     "got too many tunnelled messages\n");
+		msg_dbg(tunnel_msg, "OVERDUE/DROP/<REC<");
+		dbg_print_link(dest_link, "LINK:");
+		goto exit;
+	}
+	dest_link->exp_msg_count--;
+	if (less(msg_seqno(msg), dest_link->reset_checkpoint)) {
+		msg_dbg(tunnel_msg, "DROP/DUPL/<REC<");
+		goto exit;
+	} else {
+		*buf = buf_extract(tunnel_buf, INT_H_SIZE);
+		if (*buf != NULL) {
+			msg_dbg(tunnel_msg, "TNL<REC<");
+			buf_discard(tunnel_buf);
+			return 1;
+		} else {
+			warn("Link changeover error, original msg dropped\n");
+		}
+	}
+exit:
+	*buf = NULL;
+	buf_discard(tunnel_buf);
+	return 0;
+}
+
+#endif
+
+/*
+ *  Bundler functionality:
+ */
+void tipc_link_recv_bundle(struct sk_buff *buf)
+{
+	u32 msgcount = msg_msgcnt(buf_msg(buf));
+	u32 pos = INT_H_SIZE;
+	struct sk_buff *obuf;
+	struct tipc_msg *omsg;
+
+	msg_dbg(buf_msg(buf), "<BNDL<: ");
+	while (msgcount--) {
+		obuf = buf_extract(buf, pos);
+		if (obuf == NULL) {
+			warn("Link unable to unbundle message(s)\n");
+			break;
+		}
+		omsg = buf_msg(obuf);
+		pos += align(msg_size(omsg));
+		msg_dbg(omsg, "     /");
+		msg_set_destnode_cache(omsg, tipc_own_addr);
+		tipc_net_route_msg(obuf);
+	}
+	buf_discard(buf);
+}
+
+/*
+ *  Fragmentation/defragmentation:
+ */
+
+
+/*
+ * tipc_link_send_long_buf: Entry for buffers needing fragmentation.
+ * The buffer is complete, inclusive total message length.
+ * Returns user data length.
+ */
+int tipc_link_send_long_buf(struct link *l_ptr, struct sk_buff *buf)
+{
+	struct tipc_msg *inmsg = buf_msg(buf);
+	struct tipc_msg fragm_hdr;
+	u32 insize = msg_size(inmsg);
+	u32 dsz = msg_data_sz(inmsg);
+	unchar *crs = buf->data;
+	u32 rest = insize;
+	u32 pack_sz = l_ptr->max_pkt;
+	u32 fragm_sz = pack_sz - INT_H_SIZE;
+	u32 fragm_no = 1;
+	u32 destaddr;
+
+	if (msg_short(inmsg))
+		destaddr = l_ptr->addr;
+	else
+		destaddr = msg_destnode(inmsg);
+
+	if (msg_routed(inmsg))
+		msg_set_prevnode(inmsg, tipc_own_addr);
+
+	/* Prepare reusable fragment header: */
+
+	tipc_msg_init(&fragm_hdr, MSG_FRAGMENTER, FIRST_FRAGMENT,
+		      INT_H_SIZE, destaddr);
+	msg_set_link_selector(&fragm_hdr, msg_link_selector(inmsg));
+	msg_set_fragm_msg_no(&fragm_hdr, 
+			     atomic_inc_return(&link_fragm_msg_no) & 0xffff);
+	msg_set_fragm_no(&fragm_hdr, fragm_no);
+	l_ptr->stats.sent_fragmented++;
+
+	/* Chop up message: */
+
+	while (rest > 0) {
+		struct sk_buff *fragm;
+
+		if (rest <= fragm_sz) {
+			fragm_sz = rest;
+			msg_set_type(&fragm_hdr, LAST_FRAGMENT);
+		}
+		fragm = buf_acquire(fragm_sz + INT_H_SIZE);
+		if (fragm == NULL) {
+			warn("Link unable to fragment message\n");
+			dsz = -ENOMEM;
+			goto exit;
+		}
+		msg_set_size(&fragm_hdr, fragm_sz + INT_H_SIZE);
+		skb_copy_to_linear_data(fragm, &fragm_hdr, INT_H_SIZE);
+		skb_copy_to_linear_data_offset(fragm, INT_H_SIZE, crs,
+					       fragm_sz);
+
+		/*  Send queued messages first, if any: */
+
+		l_ptr->stats.sent_fragments++;
+		tipc_link_send_buf(l_ptr, fragm);
+		if (!tipc_link_is_up(l_ptr))
+			return dsz;
+		msg_set_fragm_no(&fragm_hdr, ++fragm_no);
+		rest -= fragm_sz;
+		crs += fragm_sz;
+		msg_set_type(&fragm_hdr, FRAGMENT);
+	}
+exit:
+	buf_discard(buf);
+	return dsz;
+}
+
+/* 
+ * A partially reassembled message must store certain values so that subsequent 
+ * fragments can be incorporated correctly.  The following routines store these
+ * values in temporarily available fields in the partially reassembled message,
+ * thereby making dynamic memory allocation unecessary.
+ */
+
+static inline u32 get_long_msg_orig(struct sk_buff *buf)
+{
+	return (u32)(unsigned long)buf_handle(buf);
+}
+
+static inline void set_long_msg_orig(struct sk_buff *buf, u32 orig)
+{
+	 buf_set_handle(buf, (void *)(unsigned long)orig);
+}
+
+static inline u32 get_long_msg_seqno(struct sk_buff *buf)
+{
+	return buf_seqno(buf);
+}
+
+static inline void set_long_msg_seqno(struct sk_buff *buf, u32 seqno)
+{
+	msg_set_seqno(buf_msg(buf), seqno);
+}
+
+static inline u32 get_fragm_size(struct sk_buff *buf)
+{
+	return msg_ack(buf_msg(buf));
+}
+
+static inline void set_fragm_size(struct sk_buff *buf, u32 sz)
+{
+	msg_set_ack(buf_msg(buf), sz);
+}
+
+static inline u32 get_expected_frags(struct sk_buff *buf)
+{
+	return msg_bcast_ack(buf_msg(buf));
+}
+
+static inline void set_expected_frags(struct sk_buff *buf, u32 exp)
+{
+	msg_set_bcast_ack(buf_msg(buf), exp);
+}
+
+static inline u32 get_timer_cnt(struct sk_buff *buf)
+{
+	return msg_reroute_cnt(buf_msg(buf));
+}
+
+static inline void reset_timer_cnt(struct sk_buff *buf)
+{
+	msg_reset_reroute_cnt(buf_msg(buf));
+}
+
+static inline void incr_timer_cnt(struct sk_buff *buf)
+{
+	msg_incr_reroute_cnt(buf_msg(buf));
+}
+
+/*
+ * tipc_link_recv_fragment(): Called with node lock on. Returns
+ * the reassembled buffer if message is complete.
+ */
+int tipc_link_recv_fragment(struct sk_buff **pending, struct sk_buff **fb,
+			    struct tipc_msg **m)
+{
+	struct sk_buff *pbuf = *pending;
+	struct sk_buff *fbuf = *fb;
+	struct sk_buff *prev = NULL;
+	struct tipc_msg *fragm = buf_msg(fbuf);
+	u32 long_msg_orig = msg_orignode(fragm);
+	u32 long_msg_seq_no = msg_fragm_msg_no(fragm);
+
+	*fb = NULL;
+	msg_dbg(fragm,"FRG<REC<");
+
+	/* Is there an incomplete message waiting for this fragment? */
+
+	while (pbuf && ((get_long_msg_orig(pbuf) != long_msg_orig)
+			|| (get_long_msg_seqno(pbuf) != long_msg_seq_no))) {
+		prev = pbuf;
+		pbuf = pbuf->next;
+	}
+
+	if (!pbuf && (msg_type(fragm) == FIRST_FRAGMENT)) {
+		struct tipc_msg *imsg = (struct tipc_msg *)msg_data(fragm);
+		u32 msg_sz = msg_size(imsg);
+		u32 fragm_sz = msg_data_sz(fragm);
+		u32 exp_fragm_cnt = msg_sz/fragm_sz + !!(msg_sz % fragm_sz);
+		u32 max =  TIPC_MAX_USER_MSG_SIZE + LONG_H_SIZE;
+
+		if (msg_type(imsg) == TIPC_MCAST_MSG)
+			max = TIPC_MAX_USER_MSG_SIZE + MCAST_H_SIZE;
+		if (msg_size(imsg) > max) {
+			msg_dbg(fragm,"<REC<Oversized: ");
+			buf_discard(fbuf);
+			return 0;
+		}
+		pbuf = buf_acquire(msg_size(imsg));
+		if (pbuf != NULL) {
+			pbuf->next = *pending;
+			*pending = pbuf;
+			skb_copy_to_linear_data(pbuf, imsg,
+						msg_data_sz(fragm));
+
+			/*  Prepare buffer for subsequent fragments. */
+
+			set_long_msg_orig(pbuf, long_msg_orig); 
+			set_long_msg_seqno(pbuf, long_msg_seq_no); 
+			set_fragm_size(pbuf,fragm_sz); 
+			set_expected_frags(pbuf,exp_fragm_cnt - 1); 
+			reset_timer_cnt(pbuf);
+		} else {
+			warn("Link unable to reassemble fragmented message\n");
+		}
+		buf_discard(fbuf);
+		return 0;
+	} else if (pbuf && (msg_type(fragm) != FIRST_FRAGMENT)) {
+		u32 dsz = msg_data_sz(fragm);
+		u32 fsz = get_fragm_size(pbuf);
+		u32 crs = ((msg_fragm_no(fragm) - 1) * fsz);
+		u32 exp_frags = get_expected_frags(pbuf) - 1;
+		skb_copy_to_linear_data_offset(pbuf, crs,
+					       msg_data(fragm), dsz);
+		buf_discard(fbuf);
+		reset_timer_cnt(pbuf);
+
+		/* Is message complete? */
+
+		if (exp_frags == 0) {
+			if (prev)
+				prev->next = pbuf->next;
+			else
+				*pending = pbuf->next;
+			*fb = pbuf;
+			*m = buf_msg(pbuf);
+			return 1;
+		}
+		set_expected_frags(pbuf,exp_frags);
+		return 0;
+	}
+	dbg(" Discarding orphan fragment %x\n",fbuf);
+	msg_dbg(fragm,"ORPHAN:");
+	dbg("Pending long buffers:\n");
+	dbg_print_buf_chain(*pending);
+	buf_discard(fbuf);
+	return 0;
+}
+
+/**
+ * link_check_defragm_bufs - flush stale incoming message fragments
+ * @l_ptr: pointer to link
+ */
+
+static void link_check_defragm_bufs(struct link *l_ptr)
+{
+	struct sk_buff *prev = NULL;
+	struct sk_buff *next = NULL;
+	struct sk_buff *buf = l_ptr->defragm_buf;
+
+	if (!buf)
+		return;
+	if (!link_working_working(l_ptr))
+		return;
+	while (buf) {
+		u32 cnt = get_timer_cnt(buf);
+
+		next = buf->next;
+		if (cnt < 4) {
+			incr_timer_cnt(buf);
+			prev = buf;
+		} else {
+			dbg(" Discarding incomplete long buffer\n");
+			msg_dbg(buf_msg(buf), "LONG:");
+			dbg_print_link(l_ptr, "curr:");
+			dbg("Pending long buffers:\n");
+			dbg_print_buf_chain(l_ptr->defragm_buf);
+			if (prev)
+				prev->next = buf->next;
+			else
+				l_ptr->defragm_buf = buf->next;
+			buf_discard(buf);
+		}
+		buf = next;
+	}
+}
+
+
+
+static void link_set_supervision_props(struct link *l_ptr, u32 tolerance)
+{
+	l_ptr->tolerance = tolerance;
+	l_ptr->continuity_interval =
+		((tolerance / 4) > 500) ? 500 : tolerance / 4;
+	l_ptr->abort_limit = tolerance / (l_ptr->continuity_interval / 4);
+}
+
+
+void tipc_link_set_queue_limits(struct link *l_ptr, u32 window)
+{
+	/* Data messages from this node, inclusive FIRST_FRAGM */
+	l_ptr->queue_limit[TIPC_LOW_IMPORTANCE] = window;
+	l_ptr->queue_limit[TIPC_MEDIUM_IMPORTANCE] = (window / 3) * 4;
+	l_ptr->queue_limit[TIPC_HIGH_IMPORTANCE] = (window / 3) * 5;
+	l_ptr->queue_limit[TIPC_CRITICAL_IMPORTANCE] = (window / 3) * 6;
+	/* Transiting data messages,inclusive FIRST_FRAGM */
+	l_ptr->queue_limit[TIPC_LOW_IMPORTANCE + 4] = 300;
+	l_ptr->queue_limit[TIPC_MEDIUM_IMPORTANCE + 4] = 600;
+	l_ptr->queue_limit[TIPC_HIGH_IMPORTANCE + 4] = 900;
+	l_ptr->queue_limit[TIPC_CRITICAL_IMPORTANCE + 4] = 1200;
+	l_ptr->queue_limit[CONN_MANAGER] = 1200;
+	l_ptr->queue_limit[ROUTE_DISTRIBUTOR] = 1200;
+	l_ptr->queue_limit[CHANGEOVER_PROTOCOL] = 2500;
+	l_ptr->queue_limit[NAME_DISTRIBUTOR] = 3000;
+	/* FRAGMENT and LAST_FRAGMENT packets */
+	l_ptr->queue_limit[MSG_FRAGMENTER] = 4000;
+}
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+/**
+ * link_find_link - locate link by name
+ * @name - ptr to link name string
+ * @node - ptr to area to be filled with ptr to associated node
+ *
+ * Caller must hold 'tipc_net_lock' to ensure node and bearer are not deleted;
+ * this also prevents link deletion.
+ *
+ * Returns pointer to link (or 0 if invalid link name).
+ */
+
+static struct link *link_find_link(const char *name, struct tipc_node **node)
+{
+	struct link_name link_name_parts;
+	struct bearer *b_ptr;
+	struct link *l_ptr;
+
+	if (!link_name_validate(name, &link_name_parts))
+		return NULL;
+
+	b_ptr = tipc_bearer_find(link_name_parts.if_local);
+	if (!b_ptr)
+		return NULL;
+
+	*node = tipc_net_find_node(link_name_parts.addr_peer); 
+	if (!*node)
+		return NULL;
+
+	l_ptr = (*node)->links[b_ptr->identity];
+	if (!l_ptr || strcmp(l_ptr->name, name))
+		return NULL;
+
+	return l_ptr;
+}
+
+
+/**
+ * value_is_valid -- check if priority/link tolerance/window is within range
+ *
+ * @cmd - value type (TIPC_CMD_SET_LINK_*)
+ * @new_value - the new value
+ *
+ * Returns 1 if value is within range, 0 if not.
+ */
+static int value_is_valid(u16 cmd, u32 new_value)
+{
+	switch (cmd) {
+	case TIPC_CMD_SET_LINK_TOL:
+		return (new_value >= TIPC_MIN_LINK_TOL) &&
+			(new_value <= TIPC_MAX_LINK_TOL);
+	case TIPC_CMD_SET_LINK_PRI:
+		return (new_value >= TIPC_MIN_LINK_PRI) &&
+			(new_value <= TIPC_MAX_LINK_PRI);
+	case TIPC_CMD_SET_LINK_WINDOW:
+		return (new_value >= TIPC_MIN_LINK_WIN) &&
+			(new_value <= TIPC_MAX_LINK_WIN);
+	}
+	return 0;
+}
+
+
+/**
+ * cmd_set_link_value - change priority/tolerance/window size of link, bearer or media
+ * @name - ptr to link, bearer or media name string
+ * @new_value - the new link priority or new bearer default link priority
+ * @cmd - which link/bearer property to set (TIPC_CMD_SET_LINK_*)
+ *
+ * Caller must hold 'tipc_net_lock' to ensure link/bearer are not deleted.
+ *
+ * Returns 0 if value updated and negative value on error.
+ */
+static int cmd_set_link_value(const char *name, u32 new_value, u16 cmd)
+{
+	struct tipc_node *node;
+	struct link *l_ptr;
+	struct bearer *b_ptr;
+	struct tipc_media *m_ptr;
+
+	l_ptr = link_find_link(name, &node);
+	if (l_ptr) {
+		/*
+		 * acquire node lock for tipc_link_send_proto_msg().
+		 * see "TIPC locking policy" in tipc_net.c.
+		 */
+		tipc_node_lock(node);
+		switch (cmd) {
+		case TIPC_CMD_SET_LINK_TOL:
+			link_set_supervision_props(l_ptr, new_value);
+			tipc_link_send_proto_msg(l_ptr, STATE_MSG,
+				0, 0, new_value, 0, 0, 0);
+			break;
+		case TIPC_CMD_SET_LINK_PRI:
+			l_ptr->priority = new_value;
+			tipc_link_send_proto_msg(l_ptr, STATE_MSG,
+				0, 0, 0, new_value, 0, 0);
+			break;
+		case TIPC_CMD_SET_LINK_WINDOW:
+			tipc_link_set_queue_limits(l_ptr, new_value);
+			break;
+		}
+		tipc_node_unlock(node);
+		return 0;
+	}
+
+	b_ptr = tipc_bearer_find(name);
+	if (b_ptr) {
+		switch (cmd) {
+		case TIPC_CMD_SET_LINK_TOL:
+			b_ptr->tolerance = new_value;
+			return 0;
+		case TIPC_CMD_SET_LINK_PRI:
+			b_ptr->priority = new_value;
+			return 0;
+		case TIPC_CMD_SET_LINK_WINDOW:
+			b_ptr->window = new_value;
+			return 0;
+		}
+		return -EINVAL;
+	}
+
+	m_ptr = tipc_media_find_name(name);
+	if (!m_ptr)
+		return -ENODEV;
+	switch (cmd) {
+	case TIPC_CMD_SET_LINK_TOL:
+		m_ptr->tolerance = new_value;
+		return 0;
+	case TIPC_CMD_SET_LINK_PRI:
+		m_ptr->priority = new_value;
+		return 0;
+	case TIPC_CMD_SET_LINK_WINDOW:
+		m_ptr->window = new_value;
+		return 0;
+	}
+	return -EINVAL;
+}
+
+
+struct sk_buff *tipc_link_cmd_config(const void *req_tlv_area, int req_tlv_space,
+				     u16 cmd)
+{
+	struct tipc_link_config *args;
+	u32 new_value;
+	int res;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_LINK_CONFIG))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	args = (struct tipc_link_config *)TLV_DATA(req_tlv_area);
+	new_value = ntohl(args->value);
+
+	if (!value_is_valid(cmd, new_value))
+		return tipc_cfg_reply_error_string("cannot change, value invalid");
+
+	if (!strcmp(args->name, tipc_bclink_name)) {
+		if ((cmd == TIPC_CMD_SET_LINK_WINDOW) &&
+		    (tipc_bclink_set_queue_limits(new_value) == 0))
+			return tipc_cfg_reply_none();
+		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
+						   " (cannot change setting on broadcast link)");
+	}
+
+	read_lock_bh(&tipc_net_lock);
+	res = cmd_set_link_value(args->name, new_value, cmd);
+	read_unlock_bh(&tipc_net_lock);
+	if (res)
+		return tipc_cfg_reply_error_string("cannot change link setting");
+
+	return tipc_cfg_reply_none();
+}
+
+#endif
+
+/**
+ * link_reset_statistics - reset link statistics
+ * @l_ptr: pointer to link
+ */
+
+static void link_reset_statistics(struct link *l_ptr)
+{
+	memset(&l_ptr->stats, 0, sizeof(l_ptr->stats));
+	l_ptr->stats.sent_info = l_ptr->next_out_no;
+	l_ptr->stats.recv_info = l_ptr->next_in_no;
+}
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+struct sk_buff *tipc_link_cmd_reset_stats(const void *req_tlv_area, int req_tlv_space)
+{
+	char *link_name;
+	struct link *l_ptr;
+	struct tipc_node *node;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_LINK_NAME))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	link_name = (char *)TLV_DATA(req_tlv_area);
+	if (!strcmp(link_name, tipc_bclink_name)) {
+		if (tipc_bclink_reset_stats())
+			return tipc_cfg_reply_error_string("link not found");
+		return tipc_cfg_reply_none();
+	}
+
+	read_lock_bh(&tipc_net_lock);
+	l_ptr = link_find_link(link_name, &node);
+	if (!l_ptr) {
+		read_unlock_bh(&tipc_net_lock);
+		return tipc_cfg_reply_error_string("link not found");
+	}
+
+	tipc_node_lock(node);
+	link_reset_statistics(l_ptr);
+	tipc_node_unlock(node);
+	read_unlock_bh(&tipc_net_lock);
+	return tipc_cfg_reply_none();
+}
+
+#ifdef PROTO_MULTI_DISCOVERY_OBJECT
+struct sk_buff *tipc_link_cmd_delete(const void *req_tlv_area, int req_tlv_space)
+{
+        char *cmd_str;
+	char *link_name;
+	struct link *l_ptr,*temp_l_ptr;
+	struct tipc_node *n_ptr;
+        struct bearer *b_ptr;
+	char *if_name,*domain_str;
+	char  cmd[TIPC_MAX_LINK_NAME + 1];
+        u32 domain,zone,cluster,node;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_LINK_NAME))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+        cmd_str = (char*) TLV_DATA(req_tlv_area);
+        strncpy(cmd,cmd_str,sizeof(cmd));
+
+	link_name = cmd_str;
+	if (!strcmp(link_name, tipc_bclink_name)) {
+		if (tipc_bclink_reset_stats())
+			return tipc_cfg_reply_error_string("link not found");
+		return tipc_cfg_reply_none();
+	}
+
+        write_lock_bh(&tipc_net_lock);
+
+        /* Scope comprising several links ? */
+
+        if (strchr(link_name,'/') != NULL)
+                goto error;
+
+        if (strchr(link_name,'-') == NULL) {
+
+                if_name = cmd_str;
+
+                domain_str = strchr(if_name,',');
+                if (domain_str == NULL)
+                        goto error;
+                *domain_str = 0;
+                domain_str++;
+
+                if (sscanf(domain_str,"%u.%u.%u",&zone,&cluster,&node) != 3)
+                        goto error;
+
+                domain = tipc_addr(zone,cluster,node);
+
+                if (!tipc_addr_domain_valid(domain))
+                        goto error;
+
+                b_ptr = tipc_bearer_find(if_name);
+
+                if (b_ptr == NULL) 
+                        goto error;
+        } else {
+                l_ptr = link_find_link(link_name, &n_ptr); 
+                if (!l_ptr) 
+                        goto error;
+                domain = l_ptr->addr;
+                b_ptr = l_ptr->b_ptr;
+        }
+
+        if (in_own_cluster(domain))
+                goto error;
+
+	spin_lock_bh(&b_ptr->publ.lock);
+
+        tipc_bearer_remove_discoverer(b_ptr,domain);
+
+	list_for_each_entry_safe(l_ptr, temp_l_ptr, &b_ptr->links, link_list) {
+
+                if (tipc_in_scope(domain,l_ptr->addr)) {
+                        if (in_own_cluster(l_ptr->addr))
+                                continue;
+                        n_ptr = l_ptr->owner;
+                        tipc_node_lock(n_ptr);
+                        tipc_link_reset(l_ptr);
+
+                        /* Tell other end to not re-establish */
+
+                        tipc_link_send_proto_msg(l_ptr,RESET_MSG, 
+                                                 0, 0, 0, 0, 0, 1);
+                        l_ptr->blocked = 1;
+                        tipc_node_unlock(n_ptr);
+                        tipc_link_delete(l_ptr);
+                }
+	}
+	spin_unlock_bh(&b_ptr->publ.lock);
+	write_unlock_bh(&tipc_net_lock);
+	return tipc_cfg_reply_none();
+
+ error:
+	write_unlock_bh(&tipc_net_lock);
+        return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+}
+#endif
+
+/**
+ * percent - convert count to a percentage of total (rounding up or down)
+ */
+
+static u32 percent(u32 count, u32 total)
+{
+	return (count * 100 + (total / 2)) / total;
+}
+
+/**
+ * tipc_link_stats - print link statistics
+ * @name: link name
+ * @buf: print buffer area
+ * @buf_size: size of print buffer area
+ *
+ * Returns length of print buffer data string (or 0 if error)
+ */
+
+static int tipc_link_stats(const char *name, char *buf, const u32 buf_size)
+{
+	struct print_buf pb;
+	struct link *l_ptr;
+	struct tipc_node *node;
+	char *status;
+	u32 profile_total = 0;
+
+	if (!strcmp(name, tipc_bclink_name))
+		return tipc_bclink_stats(buf, buf_size);
+
+	tipc_printbuf_init(&pb, buf, buf_size);
+
+	read_lock_bh(&tipc_net_lock);
+	l_ptr = link_find_link(name, &node);
+	if (!l_ptr) {
+		read_unlock_bh(&tipc_net_lock);
+		return 0;
+	}
+	tipc_node_lock(node);
+
+	if (tipc_link_is_active(l_ptr))
+		status = "ACTIVE";
+	else if (tipc_link_is_up(l_ptr))
+		status = "STANDBY";
+	else
+		status = "DEFUNCT";
+	tipc_printf(&pb, "Link <%s>\n"
+			 "  %s  MTU:%u  Priority:%u  Tolerance:%u ms"
+			 "  Window:%u packets\n",
+		    l_ptr->name, status, l_ptr->max_pkt, 
+		    l_ptr->priority, l_ptr->tolerance, l_ptr->queue_limit[0]);
+	tipc_printf(&pb, "  RX packets:%u fragments:%u/%u bundles:%u/%u\n",
+		    l_ptr->next_in_no - l_ptr->stats.recv_info,
+		    l_ptr->stats.recv_fragments,
+		    l_ptr->stats.recv_fragmented,
+		    l_ptr->stats.recv_bundles,
+		    l_ptr->stats.recv_bundled);
+	tipc_printf(&pb, "  TX packets:%u fragments:%u/%u bundles:%u/%u\n",
+		    l_ptr->next_out_no - l_ptr->stats.sent_info,
+		    l_ptr->stats.sent_fragments,
+		    l_ptr->stats.sent_fragmented,
+		    l_ptr->stats.sent_bundles,
+		    l_ptr->stats.sent_bundled);
+	profile_total = l_ptr->stats.msg_length_counts;
+	if (!profile_total)
+		profile_total = 1;
+	tipc_printf(&pb, "  TX profile sample:%u packets  average:%u octets\n"
+			 "  0-64:%u%% -256:%u%% -1024:%u%% -4096:%u%% "
+			 "-16354:%u%% -32768:%u%% -66000:%u%%\n",
+		    l_ptr->stats.msg_length_counts,
+		    l_ptr->stats.msg_lengths_total / profile_total,
+		    percent(l_ptr->stats.msg_length_profile[0], profile_total),
+		    percent(l_ptr->stats.msg_length_profile[1], profile_total),
+		    percent(l_ptr->stats.msg_length_profile[2], profile_total),
+		    percent(l_ptr->stats.msg_length_profile[3], profile_total),
+		    percent(l_ptr->stats.msg_length_profile[4], profile_total),
+		    percent(l_ptr->stats.msg_length_profile[5], profile_total),
+		    percent(l_ptr->stats.msg_length_profile[6], profile_total));
+	tipc_printf(&pb, "  RX states:%u probes:%u naks:%u defs:%u dups:%u\n",
+		    l_ptr->stats.recv_states,
+		    l_ptr->stats.recv_probes,
+		    l_ptr->stats.recv_nacks,
+		    l_ptr->stats.deferred_recv,
+		    l_ptr->stats.duplicates);
+	tipc_printf(&pb, "  TX states:%u probes:%u naks:%u acks:%u dups:%u\n",
+		    l_ptr->stats.sent_states,
+		    l_ptr->stats.sent_probes,
+		    l_ptr->stats.sent_nacks,
+		    l_ptr->stats.sent_acks,
+		    l_ptr->stats.retransmitted);
+	tipc_printf(&pb, "  Congestion bearer:%u link:%u  Send queue max:%u avg:%u\n",
+		    l_ptr->stats.bearer_congs,
+		    l_ptr->stats.link_congs,
+		    l_ptr->stats.max_queue_sz,
+		    l_ptr->stats.queue_sz_counts
+		    ? (l_ptr->stats.accu_queue_sz / l_ptr->stats.queue_sz_counts)
+		    : 0);
+
+	tipc_node_unlock(node);
+	read_unlock_bh(&tipc_net_lock);
+	return tipc_printbuf_validate(&pb);
+}
+
+#define MAX_LINK_STATS_INFO 2000
+
+struct sk_buff *tipc_link_cmd_show_stats(const void *req_tlv_area, int req_tlv_space)
+{
+	struct sk_buff *buf;
+	struct tlv_desc *rep_tlv;
+	int str_len;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_LINK_NAME))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	buf = tipc_cfg_reply_alloc(TLV_SPACE(MAX_LINK_STATS_INFO));
+	if (!buf)
+		return NULL;
+
+	rep_tlv = (struct tlv_desc *)buf->data;
+
+	str_len = tipc_link_stats((char *)TLV_DATA(req_tlv_area),
+				  (char *)TLV_DATA(rep_tlv), MAX_LINK_STATS_INFO);
+	if (!str_len) {
+		buf_discard(buf);
+		return tipc_cfg_reply_error_string("link not found");
+	}
+
+	skb_put(buf, TLV_SPACE(str_len));
+	TLV_SET(rep_tlv, TIPC_TLV_ULTRA_STRING, NULL, str_len);
+
+	return buf;
+}
+
+#endif
+
+#if 0
+int link_control(const char *name, u32 op, u32 val)
+{
+	int res = -EINVAL;
+	struct link *l_ptr;
+	u32 bearer_id;
+	struct tipc_node *node;
+	u32 a;
+
+	a = link_name2addr(name, &bearer_id);
+	read_lock_bh(&tipc_net_lock);
+	node = tipc_net_find_node(a);
+
+	if (node) {
+		tipc_node_lock(node);
+		l_ptr = node->links[bearer_id];
+		if (l_ptr) {
+			if (op == TIPC_REMOVE_LINK) {
+				struct bearer *b_ptr = l_ptr->b_ptr;
+				spin_lock_bh(&b_ptr->publ.lock);
+				tipc_link_delete(l_ptr);
+				spin_unlock_bh(&b_ptr->publ.lock);
+			}
+			if (op == TIPC_CMD_BLOCK_LINK) {
+				tipc_link_reset(l_ptr);
+				l_ptr->blocked = 1;
+			}
+			if (op == TIPC_CMD_UNBLOCK_LINK) {
+				l_ptr->blocked = 0;
+			}
+			res = 0;
+		}
+		tipc_node_unlock(node);
+	}
+	read_unlock_bh(&tipc_net_lock);
+	return res;
+}
+#endif
+
+/**
+ * tipc_link_get_max_pkt - get maximum packet size to use when sending to destination
+ * @dest: network address of destination node
+ * @selector: used to select from set of active links
+ *
+ * If no active link can be found, uses default maximum packet size.
+ */
+
+u32 tipc_link_get_max_pkt(u32 dest, u32 selector)
+{
+	struct tipc_node *n_ptr;
+	struct link *l_ptr;
+	u32 res = MAX_PKT_DEFAULT;
+
+	if (dest == tipc_own_addr)
+		return MAX_MSG_SIZE;
+
+	read_lock_bh(&tipc_net_lock);
+	n_ptr = tipc_net_select_node(dest);
+	if (n_ptr) {
+		tipc_node_lock(n_ptr);
+		l_ptr = n_ptr->active_links[selector & 1];
+		if (l_ptr)
+			res = l_ptr->max_pkt;
+		tipc_node_unlock(n_ptr);
+	}
+	read_unlock_bh(&tipc_net_lock);
+	return res;
+}
+
+#if 0
+static void link_dump_rec_queue(struct link *l_ptr)
+{
+	struct sk_buff *crs;
+
+	if (!l_ptr->oldest_deferred_in) {
+		info("Reception queue empty\n");
+		return;
+	}
+	info("Contents of Reception queue:\n");
+	crs = l_ptr->oldest_deferred_in;
+	while (crs) {
+		if (crs->data == (void *)0x0000a3a3) {
+			info("buffer %x invalid\n", crs);
+			return;
+		}
+		msg_dbg(buf_msg(crs), "In rec queue: \n");
+		crs = crs->next;
+	}
+}
+#endif
+
+#ifdef CONFIG_TIPC_DEBUG
+
+static void link_dump_send_queue(struct link *l_ptr)
+{
+	if (l_ptr->next_out) {
+		info("\nContents of unsent queue:\n");
+		dbg_print_buf_chain(l_ptr->next_out);
+	}
+	info("\nContents of send queue:\n");
+	if (l_ptr->first_out) {
+		dbg_print_buf_chain(l_ptr->first_out);
+	}
+	info("Empty send queue\n");
+}
+
+static void dbg_print_link_state(struct print_buf *buf, struct link *l_ptr)
+{
+	if (link_reset_reset(l_ptr) || link_reset_unknown(l_ptr)) {
+		tipc_printf(buf, "Link already reset\n");
+		return;
+	}
+
+	tipc_printf(buf, "Link %x<%s>:", l_ptr->addr, l_ptr->b_ptr->publ.name);
+	tipc_printf(buf, ": NXO(%u):", mod(l_ptr->next_out_no));
+	tipc_printf(buf, "NXI(%u):", mod(l_ptr->next_in_no));
+	tipc_printf(buf, "SQUE");
+	if (l_ptr->first_out) {
+		tipc_printf(buf, "[%u..", buf_seqno(l_ptr->first_out));
+		if (l_ptr->next_out)
+			tipc_printf(buf, "%u..", buf_seqno(l_ptr->next_out));
+		tipc_printf(buf, "%u]", buf_seqno(l_ptr->last_out));
+		if ((mod(buf_seqno(l_ptr->last_out) -
+			 buf_seqno(l_ptr->first_out))
+		     != (l_ptr->out_queue_size - 1))
+		    || (l_ptr->last_out->next != 0)) {
+			tipc_printf(buf, "\nSend queue inconsistency\n");
+			tipc_printf(buf, "first_out= %x ", l_ptr->first_out);
+			tipc_printf(buf, "next_out= %x ", l_ptr->next_out);
+			tipc_printf(buf, "last_out= %x ", l_ptr->last_out);
+			link_dump_send_queue(l_ptr);
+		}
+	} else
+		tipc_printf(buf, "[]");
+	tipc_printf(buf, "SQSIZ(%u)", l_ptr->out_queue_size);
+	if (l_ptr->oldest_deferred_in) {
+		u32 o = buf_seqno(l_ptr->oldest_deferred_in);
+		u32 n = buf_seqno(l_ptr->newest_deferred_in);
+		tipc_printf(buf, ":RQUE[%u..%u]", o, n);
+		if (l_ptr->deferred_inqueue_sz != mod((n + 1) - o)) {
+			tipc_printf(buf, ":RQSIZ(%u)",
+				    l_ptr->deferred_inqueue_sz);
+		}
+	}
+	if (link_working_unknown(l_ptr))
+		tipc_printf(buf, ":WU");
+	if (link_reset_reset(l_ptr))
+		tipc_printf(buf, ":RR");
+	if (link_reset_unknown(l_ptr))
+		tipc_printf(buf, ":RU");
+	if (link_working_working(l_ptr))
+		tipc_printf(buf, ":WW");
+	tipc_printf(buf, "\n");
+}
+
+static void dbg_print_link(struct link *l_ptr, const char *str)
+{
+	if (DBG_OUTPUT != TIPC_NULL) {
+		tipc_printf(DBG_OUTPUT, str);
+		dbg_print_link_state(DBG_OUTPUT, l_ptr);
+	}
+}
+
+static void dbg_print_buf_chain(struct sk_buff *root_buf)
+{
+	if (DBG_OUTPUT != TIPC_NULL) {
+		struct sk_buff *buf = root_buf;
+
+		while (buf) {
+			msg_dbg(buf_msg(buf), "In chain: ");
+			buf = buf->next;
+		}
+	}
+}
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_link.h android_cluster/linux-2.6.29/net/tipc/tipc_link.h
--- linux-2.6.29/net/tipc/tipc_link.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_link.h	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,337 @@
+/*
+ * net/tipc/tipc_link.h: Include file for TIPC link code
+ *
+ * Copyright (c) 1995-2006, Ericsson AB
+ * Copyright (c) 2004-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_LINK_H
+#define _TIPC_LINK_H
+
+#include "tipc_dbg.h"
+#include "tipc_msg.h"
+#include "tipc_bearer.h"
+#include "tipc_node.h"
+
+#define PUSH_FAILED   1
+#define PUSH_FINISHED 2
+
+/*
+ * Link states
+ */
+
+#define WORKING_WORKING 560810u
+#define WORKING_UNKNOWN 560811u
+#define RESET_UNKNOWN   560812u
+#define RESET_RESET     560813u
+
+/*
+ * Starting value for maximum packet size negotiation on unicast links
+ * (unless bearer MTU is less)
+ */
+
+#define MAX_PKT_DEFAULT 1500
+
+/**
+ * struct link - TIPC link data structure
+ * @addr: network address of link's peer node
+ * @name: link name character string
+ * @media_addr: media address to use when sending messages over link
+ * @timer: link timer
+ * @owner: pointer to peer node
+ * @link_list: adjacent links in bearer's list of links
+ * @started: indicates if link has been started
+ * @checkpoint: reference point for triggering link continuity checking
+ * @peer_session: link session # being used by peer end of link
+ * @peer_bearer_id: bearer id used by link's peer endpoint
+ * @b_ptr: pointer to bearer used by link
+ * @tolerance: minimum link continuity loss needed to reset link [in ms]
+ * @continuity_interval: link continuity testing interval [in ms]
+ * @abort_limit: # of unacknowledged continuity probes needed to reset link
+ * @state: current state of link FSM
+ * @blocked: indicates if link has been administratively blocked
+ * @fsm_msg_cnt: # of protocol messages link FSM has sent in current state
+ * @proto_msg: template for control messages generated by link
+ * @pmsg: convenience pointer to "proto_msg" field
+ * @priority: current link priority
+ * @queue_limit: outbound message queue congestion thresholds (indexed by user)
+ * @exp_msg_count: # of tunnelled messages expected during link changeover
+ * @reset_checkpoint: seq # of last acknowledged message at time of link reset
+ * @max_pkt: current maximum packet size for this link
+ * @max_pkt_target: desired maximum packet size for this link
+ * @max_pkt_probes: # of probes based on current (max_pkt, max_pkt_target)
+ * @out_queue_size: # of messages in outbound message queue
+ * @first_out: ptr to first outbound message in queue
+ * @last_out: ptr to last outbound message in queue
+ * @next_out_no: next sequence number to use for outbound messages
+ * @last_retransmitted: sequence number of most recently retransmitted message
+ * @stale_count: # of identical retransmit requests made by peer
+ * @next_in_no: next sequence number to expect for inbound messages
+ * @deferred_inqueue_sz: # of messages in inbound message queue
+ * @oldest_deferred_in: ptr to first inbound message in queue
+ * @newest_deferred_in: ptr to last inbound message in queue
+ * @unacked_window: # of inbound messages rx'd without ack'ing back to peer
+ * @proto_msg_queue: ptr to (single) outbound control message
+ * @retransm_queue_size: number of messages to retransmit
+ * @retransm_queue_head: sequence number of first message to retransmit
+ * @next_out: ptr to first unsent outbound message in queue
+ * @waiting_ports: linked list of ports waiting for link congestion to abate
+ * @long_msg_seq_no: next identifier to use for outbound fragmented messages
+ * @defragm_buf: list of partially reassembled inbound message fragments
+ * @stats: collects statistics regarding link activity
+ * @print_buf: print buffer used to log link activity
+ */
+
+struct link {
+	u32 addr;
+	char name[TIPC_MAX_LINK_NAME];
+	struct tipc_media_addr media_addr;
+	struct timer_list timer;
+	struct tipc_node *owner;
+	struct list_head link_list;
+
+	/* Management and link supervision data */
+	int started;
+	u32 checkpoint;
+	u32 peer_session;
+	u32 peer_bearer_id;
+	struct bearer *b_ptr;
+	u32 tolerance;
+	u32 continuity_interval;
+	u32 abort_limit;
+	int state;
+	int blocked;
+	u32 fsm_msg_cnt;
+	struct {
+		unchar hdr[INT_H_SIZE];
+		unchar body[TIPC_MAX_IF_NAME];
+	} proto_msg;
+	struct tipc_msg *pmsg;
+	u32 priority;
+	u32 queue_limit[15];	/* queue_limit[0]==window limit */
+
+	/* Changeover */
+	u32 exp_msg_count;
+	u32 reset_checkpoint;
+
+	/* Max packet negotiation */
+	u32 max_pkt;
+	u32 max_pkt_target;
+	u32 max_pkt_probes;
+
+	/* Sending */
+	u32 out_queue_size;
+	struct sk_buff *first_out;
+	struct sk_buff *last_out;
+	u32 next_out_no;
+	u32 last_retransmitted;
+	u32 stale_count;
+
+	/* Reception */
+	u32 next_in_no;
+	u32 deferred_inqueue_sz;
+	struct sk_buff *oldest_deferred_in;
+	struct sk_buff *newest_deferred_in;
+	u32 unacked_window;
+
+	/* Congestion handling */
+	struct sk_buff *proto_msg_queue;
+	u32 retransm_queue_size;
+	u32 retransm_queue_head;
+	struct sk_buff *next_out;
+	struct list_head waiting_ports;
+
+	/* Fragmentation/defragmentation */
+	struct sk_buff *defragm_buf;
+
+	/* Statistics */
+	struct {
+		u32 sent_info;		/* used in counting # sent packets */
+		u32 recv_info;		/* used in counting # recv'd packets */
+		u32 sent_states;
+		u32 recv_states;
+		u32 sent_probes;
+		u32 recv_probes;
+		u32 sent_nacks;
+		u32 recv_nacks;
+		u32 sent_acks;
+		u32 sent_bundled;
+		u32 sent_bundles;
+		u32 recv_bundled;
+		u32 recv_bundles;
+		u32 retransmitted;
+		u32 sent_fragmented;
+		u32 sent_fragments;
+		u32 recv_fragmented;
+		u32 recv_fragments;
+		u32 link_congs;		/* # port sends blocked by congestion */
+		u32 bearer_congs;
+		u32 deferred_recv;
+		u32 duplicates;
+
+		/* for statistical profiling of send queue size */
+
+		u32 max_queue_sz;
+		u32 accu_queue_sz;
+		u32 queue_sz_counts;
+
+		/* for statistical profiling of message lengths */
+
+		u32 msg_length_counts;
+		u32 msg_lengths_total;
+		u32 msg_length_profile[7];
+#if 0
+		u32 sent_tunneled;
+		u32 recv_tunneled;
+#endif
+	} stats;
+
+	struct print_buf print_buf;
+};
+
+struct port;
+
+struct link *tipc_link_create(struct bearer *b_ptr, const u32 peer,
+			      const struct tipc_media_addr *media_addr);
+void tipc_link_delete(struct link *l_ptr);
+void tipc_link_changeover(struct link *l_ptr);
+void tipc_link_send_duplicate(struct link *l_ptr, struct link *dest);
+void tipc_link_reset_fragments(struct link *l_ptr);
+int tipc_link_is_up(struct link *l_ptr);
+int tipc_link_is_active(struct link *l_ptr);
+void tipc_link_start(struct link *l_ptr);
+u32 tipc_link_push_packet(struct link *l_ptr);
+void tipc_link_stop(struct link *l_ptr);
+struct sk_buff *tipc_link_cmd_delete(const void *req_tlv_area, int req_tlv_space);
+struct sk_buff *tipc_link_cmd_config(const void *req_tlv_area, int req_tlv_space, u16 cmd);
+struct sk_buff *tipc_link_cmd_show_stats(const void *req_tlv_area, int req_tlv_space);
+struct sk_buff *tipc_link_cmd_reset_stats(const void *req_tlv_area, int req_tlv_space);
+void tipc_link_reset(struct link *l_ptr);
+int tipc_link_send(struct sk_buff *buf, u32 dest, u32 selector);
+int tipc_link_send_buf(struct link *l_ptr, struct sk_buff *buf);
+u32 tipc_link_get_max_pkt(u32 dest,u32 selector);
+int tipc_link_send_sections_fast(struct port* sender,
+				 struct iovec const *msg_sect,
+				 const u32 num_sect,
+				 u32 destnode);
+int tipc_link_send_long_buf(struct link *l_ptr, struct sk_buff *buf);
+void tipc_link_recv_bundle(struct sk_buff *buf);
+int  tipc_link_recv_fragment(struct sk_buff **pending,
+			     struct sk_buff **fb,
+			     struct tipc_msg **msg);
+void tipc_link_send_proto_msg(struct link *l_ptr, u32 msg_typ, int prob, u32 gap,
+			      u32 tolerance, u32 priority, u32 acked_mtu, int stop);
+void tipc_link_push_queue(struct link *l_ptr);
+u32 tipc_link_defer_pkt(struct sk_buff **head, struct sk_buff **tail,
+			struct sk_buff *buf, u32 buf_seq_no);
+void tipc_link_wakeup_ports(struct link *l_ptr, int all);
+void tipc_link_set_queue_limits(struct link *l_ptr, u32 window);
+void tipc_link_retransmit(struct link *l_ptr, struct sk_buff *start, u32 retransmits);
+
+static inline u32 buf_seqno(struct sk_buff *buf)
+{
+	return msg_seqno(buf_msg(buf));
+}
+
+/*
+ * Link sequence number manipulation routines (uses modulo 2**16 arithmetic)
+ */
+
+static inline u32 mod(u32 x)
+{
+	return x & 0xffffu;
+}
+
+static inline int between(u32 lower, u32 upper, u32 n)
+{
+	if ((lower < n) && (n < upper))
+		return 1;
+	if ((upper < lower) && ((n > lower) || (n < upper)))
+		return 1;
+	return 0;
+}
+
+static inline int less_eq(u32 left, u32 right)
+{
+	return (mod(right - left) < 32768u);
+}
+
+static inline int less(u32 left, u32 right)
+{
+	return (less_eq(left, right) && (mod(right) != mod(left)));
+}
+
+static inline u32 lesser(u32 left, u32 right)
+{
+	return less_eq(left, right) ? left : right;
+}
+
+static inline u32 greater(u32 left, u32 right)
+{
+	return less_eq(left, right) ? right : left;
+}
+
+/*
+ * Link status checking routines
+ */
+
+static inline int link_working_working(struct link *l_ptr)
+{
+	return (l_ptr->state == WORKING_WORKING);
+}
+
+static inline int link_working_unknown(struct link *l_ptr)
+{
+	return (l_ptr->state == WORKING_UNKNOWN);
+}
+
+static inline int link_reset_unknown(struct link *l_ptr)
+{
+	return (l_ptr->state == RESET_UNKNOWN);
+}
+
+static inline int link_reset_reset(struct link *l_ptr)
+{
+	return (l_ptr->state == RESET_RESET);
+}
+
+static inline int link_blocked(struct link *l_ptr)
+{
+	return (l_ptr->exp_msg_count || l_ptr->blocked);
+}
+
+static inline int link_congested(struct link *l_ptr)
+{
+	return (l_ptr->out_queue_size >= l_ptr->queue_limit[0]);
+}
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_msg.c android_cluster/linux-2.6.29/net/tipc/tipc_msg.c
--- linux-2.6.29/net/tipc/tipc_msg.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_msg.c	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,389 @@
+/*
+ * net/tipc/msg.c: TIPC message header routines
+ *
+ * Copyright (c) 2000-2006, Ericsson AB
+ * Copyright (c) 2005-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_addr.h"
+#include "tipc_dbg.h"
+#include "tipc_msg.h"
+#include "tipc_bearer.h"
+
+
+/** 
+ * tipc_msg_build - create message using specified header and data
+ * 
+ * Note: Caller must not hold any locks in case copy_from_user() is interrupted!
+ * 
+ * Returns message data size or errno
+ */
+
+int tipc_msg_build(struct tipc_msg *hdr, 
+		   struct iovec const *msg_sect, u32 num_sect,
+		   int max_size, int usrmem, struct sk_buff** buf)
+{
+	int dsz, sz, hsz, pos, res, cnt;
+
+	dsz = tipc_msg_calc_data_size(msg_sect, num_sect);
+	if (unlikely(dsz > TIPC_MAX_USER_MSG_SIZE)) {
+		*buf = NULL;
+		return -EINVAL;
+	}
+
+	pos = hsz = msg_hdr_sz(hdr);
+	sz = hsz + dsz;
+	msg_set_size(hdr, sz);
+	if (unlikely(sz > max_size)) {
+		*buf = NULL;
+		return dsz;
+	}
+
+	*buf = buf_acquire(sz);
+	if (!(*buf))
+		return -ENOMEM;
+	skb_copy_to_linear_data(*buf, hdr, hsz);
+	for (res = 1, cnt = 0; res && (cnt < num_sect); cnt++) {
+		if (likely(usrmem))
+			res = !copy_from_user((*buf)->data + pos, 
+					      msg_sect[cnt].iov_base, 
+					      msg_sect[cnt].iov_len);
+		else
+			skb_copy_to_linear_data_offset(*buf, pos,
+						       msg_sect[cnt].iov_base,
+						       msg_sect[cnt].iov_len);
+		pos += msg_sect[cnt].iov_len;
+	}
+	if (likely(res))
+		return dsz;
+
+	buf_discard(*buf);
+	*buf = NULL;
+	return -EFAULT;
+}
+
+void tipc_msg_init(struct tipc_msg *m, u32 user, u32 type, 
+		   u32 hsize, u32 destnode)
+{
+	memset(m, 0, hsize);
+	msg_set_version(m);
+	msg_set_user(m, user);
+	msg_set_hdr_sz(m, hsize);
+	msg_set_size(m, hsize);
+	msg_set_type(m, type);
+	msg_set_prevnode(m, tipc_own_addr);
+	if (!msg_short(m)) {
+		msg_set_orignode(m, tipc_own_addr);
+		msg_set_destnode(m, destnode);
+	}
+}
+
+/** 
+ * tipc_msg_calc_data_size - determine total data size for message
+ */
+
+int tipc_msg_calc_data_size(struct iovec const *msg_sect, u32 num_sect)
+{
+	int dsz = 0;
+	int i;
+
+	for (i = 0; i < num_sect; i++)
+		dsz += msg_sect[i].iov_len;
+	return dsz;
+}
+
+u32 tipc_msg_tot_importance(struct tipc_msg *m)
+{
+	if (likely(msg_isdata(m))) {
+		if (likely(msg_orignode(m) == tipc_own_addr))
+			return msg_importance(m);
+		return msg_importance(m) + 4;
+	}
+	if ((msg_user(m) == MSG_FRAGMENTER)  &&
+	    (msg_type(m) == FIRST_FRAGMENT))
+		return msg_importance(msg_get_wrapped(m));
+	return msg_importance(m);
+}
+
+
+#ifdef CONFIG_TIPC_DEBUG
+
+void tipc_msg_dbg(struct print_buf *buf, struct tipc_msg *msg, const char *str)
+{
+	u32 usr = msg_user(msg);
+	tipc_printf(buf, str);
+
+	switch (usr) {
+	case MSG_BUNDLER:
+		tipc_printf(buf, "BNDL::");
+		tipc_printf(buf, "MSGS(%u):", msg_msgcnt(msg));
+		break;
+	case BCAST_PROTOCOL:
+		tipc_printf(buf, "BCASTP::");
+		break;
+	case MSG_FRAGMENTER:
+		tipc_printf(buf, "FRAGM::");
+		switch (msg_type(msg)) {
+		case FIRST_FRAGMENT:
+			tipc_printf(buf, "FIRST:");
+			break;
+		case FRAGMENT:
+			tipc_printf(buf, "BODY:");
+			break;
+		case LAST_FRAGMENT:
+			tipc_printf(buf, "LAST:");
+			break;
+		default:
+			tipc_printf(buf, "UNKNOWN:%x",msg_type(msg));
+
+		}
+		tipc_printf(buf, "NO(%u/%u):", msg_fragm_msg_no(msg),
+			    msg_fragm_no(msg));
+		break;
+	case TIPC_LOW_IMPORTANCE:
+	case TIPC_MEDIUM_IMPORTANCE:
+	case TIPC_HIGH_IMPORTANCE:
+	case TIPC_CRITICAL_IMPORTANCE:
+		tipc_printf(buf, "DAT%u:", msg_user(msg));
+		if (msg_short(msg)) {
+			tipc_printf(buf, "CON:");
+			break;
+		}
+		switch (msg_type(msg)) {
+		case TIPC_CONN_MSG:
+			tipc_printf(buf, "CON:");
+			break;
+		case TIPC_MCAST_MSG:
+			tipc_printf(buf, "MCST:");
+			break;
+		case TIPC_NAMED_MSG:
+			tipc_printf(buf, "NAM:");
+			break;
+		case TIPC_DIRECT_MSG:
+			tipc_printf(buf, "DIR:");
+			break;
+		default:
+			tipc_printf(buf, "UNKNOWN TYPE %u",msg_type(msg));
+		}
+		if (msg_routed(msg) && !msg_non_seq(msg))
+			tipc_printf(buf, "ROUT:");
+		if (msg_reroute_cnt(msg))
+			tipc_printf(buf, "REROUTED(%u):",
+				    msg_reroute_cnt(msg));
+		break;
+	case NAME_DISTRIBUTOR:
+		tipc_printf(buf, "NMD::");
+		switch (msg_type(msg)) {
+		case DIST_PUBLISH:
+			tipc_printf(buf, "PUBL(%u):", (msg_size(msg) - msg_hdr_sz(msg)) / 20);	/* Items */
+			break;
+		case DIST_WITHDRAW:
+			tipc_printf(buf, "WDRW:");
+			break;
+		default:
+			tipc_printf(buf, "UNKNOWN:%x",msg_type(msg));
+		}
+		if (msg_routed(msg))
+			tipc_printf(buf, "ROUT:");
+		if (msg_reroute_cnt(msg))
+			tipc_printf(buf, "REROUTED(%u):",
+				    msg_reroute_cnt(msg));
+		break;
+	case CONN_MANAGER:
+		tipc_printf(buf, "CONN_MNG:");
+		switch (msg_type(msg)) {
+		case CONN_PROBE:
+			tipc_printf(buf, "PROBE:");
+			break;
+		case CONN_PROBE_REPLY:
+			tipc_printf(buf, "PROBE_REPLY:");
+			break;
+		case CONN_ACK:
+			tipc_printf(buf, "CONN_ACK:");
+			tipc_printf(buf, "ACK(%u):",msg_msgcnt(msg));
+			break;
+		default:
+			tipc_printf(buf, "UNKNOWN TYPE:%x",msg_type(msg));
+		}
+		if (msg_routed(msg))
+			tipc_printf(buf, "ROUT:");
+		if (msg_reroute_cnt(msg))
+			tipc_printf(buf, "REROUTED(%u):",msg_reroute_cnt(msg));
+		break;
+	case LINK_PROTOCOL:
+		tipc_printf(buf, "PROT:TIM(%u):",msg_timestamp(msg));
+		switch (msg_type(msg)) {
+		case STATE_MSG:
+			tipc_printf(buf, "STATE:");
+			tipc_printf(buf, "%s:",msg_probe(msg) ? "PRB" :"");
+			tipc_printf(buf, "NXS(%u):",msg_next_sent(msg));
+			tipc_printf(buf, "GAP(%u):",msg_seq_gap(msg));
+			tipc_printf(buf, "LSTBC(%u):",msg_last_bcast(msg));
+			break;
+		case RESET_MSG:
+			tipc_printf(buf, "RESET:");
+			if (msg_size(msg) != msg_hdr_sz(msg))
+				tipc_printf(buf, "BEAR:%s:",msg_data(msg));
+			break;
+		case ACTIVATE_MSG:
+			tipc_printf(buf, "ACTIVATE:");
+			break;
+		default:
+			tipc_printf(buf, "UNKNOWN TYPE:%x",msg_type(msg));
+		}
+		tipc_printf(buf, "PLANE(%c):",msg_net_plane(msg));
+		tipc_printf(buf, "SESS(%u):",msg_session(msg));
+		break;
+	case CHANGEOVER_PROTOCOL:
+		tipc_printf(buf, "TUNL:");
+		switch (msg_type(msg)) {
+		case DUPLICATE_MSG:
+			tipc_printf(buf, "DUPL:");
+			break;
+		case ORIGINAL_MSG:
+			tipc_printf(buf, "ORIG:");
+			tipc_printf(buf, "EXP(%u)",msg_msgcnt(msg));
+			break;
+		default:
+			tipc_printf(buf, "UNKNOWN TYPE:%x",msg_type(msg));
+		}
+		break;
+	case LINK_CONFIG:
+		tipc_printf(buf, "CFG:");
+		switch (msg_type(msg)) {
+		case DSC_REQ_MSG:
+			tipc_printf(buf, "DSC_REQ:");
+			break;
+		case DSC_RESP_MSG:
+			tipc_printf(buf, "DSC_RESP:");
+			break;
+		default:
+			tipc_printf(buf, "UNKNOWN TYPE:%x:",msg_type(msg));
+			break;
+		}
+		break;
+	default:
+		tipc_printf(buf, "UNKNOWN USER:");
+	}
+
+	switch (usr) {
+	case CONN_MANAGER:
+	case TIPC_LOW_IMPORTANCE:
+	case TIPC_MEDIUM_IMPORTANCE:
+	case TIPC_HIGH_IMPORTANCE:
+	case TIPC_CRITICAL_IMPORTANCE:
+		switch (msg_errcode(msg)) {
+		case TIPC_OK:
+			break;
+		case TIPC_ERR_NO_NAME:
+			tipc_printf(buf, "NO_NAME:");
+			break;
+		case TIPC_ERR_NO_PORT:
+			tipc_printf(buf, "NO_PORT:");
+			break;
+		case TIPC_ERR_NO_NODE:
+			tipc_printf(buf, "NO_PROC:");
+			break;
+		case TIPC_ERR_OVERLOAD:
+			tipc_printf(buf, "OVERLOAD:");
+			break;
+		case TIPC_CONN_SHUTDOWN:
+			tipc_printf(buf, "SHUTDOWN:");
+			break;
+		default:
+			tipc_printf(buf, "UNKNOWN ERROR(%x):",
+				    msg_errcode(msg));
+		}
+	default:{}
+	}
+
+	tipc_printf(buf, "HZ(%u):", msg_hdr_sz(msg));
+	tipc_printf(buf, "SZ(%u):", msg_size(msg));
+	tipc_printf(buf, "SQNO(%u):", msg_seqno(msg));
+
+	if (msg_non_seq(msg))
+		tipc_printf(buf, "NOSEQ:");
+	else {
+		tipc_printf(buf, "ACK(%u):", msg_ack(msg));
+	}
+	tipc_printf(buf, "BACK(%u):", msg_bcast_ack(msg));
+	tipc_printf(buf, "PRND(%x)", msg_prevnode(msg));
+
+	if (msg_isdata(msg)) {
+		if (msg_named(msg)) {
+			tipc_printf(buf, "NTYP(%u):", msg_nametype(msg));
+			tipc_printf(buf, "NINST(%u)", msg_nameinst(msg));
+		}
+	}
+
+	if ((usr != LINK_PROTOCOL) && (usr != LINK_CONFIG) &&
+	    (usr != MSG_BUNDLER)) {
+		if (!msg_short(msg)) {
+			tipc_printf(buf, ":ORIG(%x:%u):",
+				    msg_orignode(msg), msg_origport(msg));
+			tipc_printf(buf, ":DEST(%x:%u):",
+				    msg_destnode(msg), msg_destport(msg));
+		} else {
+			tipc_printf(buf, ":OPRT(%u):", msg_origport(msg));
+			tipc_printf(buf, ":DPRT(%u):", msg_destport(msg));
+		}
+	}
+	if (msg_user(msg) == NAME_DISTRIBUTOR) {
+		tipc_printf(buf, ":ONOD(%x):", msg_orignode(msg));
+		tipc_printf(buf, ":DNOD(%x):", msg_destnode(msg));
+	}
+
+	if (msg_user(msg) ==  LINK_CONFIG) {
+		struct tipc_media_addr orig;
+
+		tipc_printf(buf, ":DDOM(%x):", msg_dest_domain(msg));
+		tipc_printf(buf, ":NETID(%u):", msg_bc_netid(msg));
+		memcpy(orig.value, &msg->hdr[5], sizeof(orig.value));
+		orig.media_id = 0;
+		orig.broadcast = 0;
+		tipc_media_addr_printf(buf, &orig);
+	}
+	if (msg_user(msg) == BCAST_PROTOCOL) {
+		tipc_printf(buf, "BCNACK:AFTER(%u):", msg_bcgap_after(msg));
+		tipc_printf(buf, "TO(%u):", msg_bcgap_to(msg));
+	}
+	tipc_printf(buf, "\n");
+	if ((usr == CHANGEOVER_PROTOCOL) && (msg_msgcnt(msg))) {
+		tipc_msg_dbg(buf, msg_get_wrapped(msg), "      /");
+	}
+	if ((usr == MSG_FRAGMENTER) && (msg_type(msg) == FIRST_FRAGMENT)) {
+		tipc_msg_dbg(buf, msg_get_wrapped(msg), "      /");
+	}
+}
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_msg.h android_cluster/linux-2.6.29/net/tipc/tipc_msg.h
--- linux-2.6.29/net/tipc/tipc_msg.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_msg.h	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,689 @@
+/*
+ * net/tipc/tipc_msg.h: Include file for TIPC message header routines
+ *
+ * Copyright (c) 2000-2007, Ericsson AB
+ * Copyright (c) 2005-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_MSG_H
+#define _TIPC_MSG_H
+
+#include "tipc_core.h"
+#include "tipc_addr.h"
+
+/*
+ * Private TIPC message header definitions
+ *
+ * The following definitions are used internally by TIPC itself.
+ * They supplement the public definitions found in tipc_plugin_msg.h.
+ */
+
+
+#define TIPC_VERSION              2
+
+#define SHORT_H_SIZE              24	/* Connected, in-cluster messages */
+#define DIR_MSG_H_SIZE            32	/* Directly addressed messages */
+#define LONG_H_SIZE               40	/* Named messages */
+#define MCAST_H_SIZE              44	/* Multicast messages */
+#define INT_H_SIZE                40	/* Internal messages */
+#define MIN_H_SIZE                24	/* Smallest legal TIPC header size */
+#define MAX_H_SIZE                60	/* Largest possible TIPC header size */
+
+#define MAX_MSG_SIZE (MAX_H_SIZE + TIPC_MAX_USER_MSG_SIZE)
+
+
+/*
+ * The following definitions are used with TIPC data message headers
+ *
+ * Note: Some of these definitions are generic, and also used with 
+ * TIPC internal message headers
+ */   
+
+static inline void msg_set_word(struct tipc_msg *m, u32 w, u32 val)
+{
+	m->hdr[w] = htonl(val);
+}
+
+static inline void msg_set_bits(struct tipc_msg *m, u32 w,
+				u32 pos, u32 mask, u32 val)
+{
+	val = (val & mask) << pos;
+	mask = mask << pos;
+	m->hdr[w] &= ~htonl(mask);
+	m->hdr[w] |= htonl(val);
+}
+
+static inline void msg_swap_words(struct tipc_msg *msg, u32 a, u32 b)
+{
+	u32 temp = msg->hdr[a];
+
+	msg->hdr[a] = msg->hdr[b];
+	msg->hdr[b] = temp;
+}
+
+/*
+ * Word 0
+ */
+
+static inline u32 msg_version(struct tipc_msg *m)
+{
+	return msg_bits(m, 0, 29, 7);
+}
+
+static inline void msg_set_version(struct tipc_msg *m)
+{
+	msg_set_bits(m, 0, 29, 7, TIPC_VERSION);
+}
+
+static inline u32 msg_user(struct tipc_msg *m)
+{
+	return msg_bits(m, 0, 25, 0xf);
+}
+
+static inline u32 msg_isdata(struct tipc_msg *m)
+{
+	return (msg_user(m) <= TIPC_CRITICAL_IMPORTANCE);
+}
+
+static inline void msg_set_user(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 0, 25, 0xf, n);
+}
+
+static inline void msg_set_importance(struct tipc_msg *m, u32 i)
+{
+	msg_set_user(m, i);
+}
+
+static inline void msg_set_hdr_sz(struct tipc_msg *m,u32 n)
+{
+	msg_set_bits(m, 0, 21, 0xf, n>>2);
+}
+
+static inline int msg_non_seq(struct tipc_msg *m)
+{
+	return msg_bits(m, 0, 20, 1);
+}
+
+static inline void msg_set_non_seq(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 0, 20, 1, n);
+}
+
+static inline int msg_dest_droppable(struct tipc_msg *m)
+{
+	return msg_bits(m, 0, 19, 1);
+}
+
+static inline void msg_set_dest_droppable(struct tipc_msg *m, u32 d)
+{
+	msg_set_bits(m, 0, 19, 1, d);
+}
+
+static inline int msg_src_droppable(struct tipc_msg *m)
+{
+	return msg_bits(m, 0, 18, 1);
+}
+
+static inline void msg_set_src_droppable(struct tipc_msg *m, u32 d)
+{
+	msg_set_bits(m, 0, 18, 1, d);
+}
+
+static inline void msg_set_size(struct tipc_msg *m, u32 sz)
+{
+	msg_set_bits(m, 0, 0, 0x1ffff, sz);
+}
+
+
+/*
+ * Word 1
+ */
+
+static inline void msg_set_type(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 1, 29, 0x7, n);
+}
+
+static inline void msg_set_errcode(struct tipc_msg *m, u32 err)
+{
+	msg_set_bits(m, 1, 25, 0xf, err);
+}
+
+static inline u32 msg_reroute_cnt(struct tipc_msg *m)
+{
+	return msg_bits(m, 1, 21, 0xf);
+}
+
+static inline void msg_incr_reroute_cnt(struct tipc_msg *m)
+{
+	msg_set_bits(m, 1, 21, 0xf, msg_reroute_cnt(m) + 1);
+}
+
+static inline void msg_reset_reroute_cnt(struct tipc_msg *m)
+{
+	msg_set_bits(m, 1, 21, 0xf, 0);
+}
+
+static inline u32 msg_lookup_scope(struct tipc_msg *m)
+{
+	return msg_bits(m, 1, 19, 0x3);
+}
+
+static inline void msg_set_lookup_scope(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 1, 19, 0x3, n);
+}
+
+static inline u32 msg_bcast_ack(struct tipc_msg *m)
+{
+	return msg_bits(m, 1, 0, 0xffff);
+}
+
+static inline void msg_set_bcast_ack(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 1, 0, 0xffff, n);
+}
+
+/*
+ * Word 2
+ */
+
+static inline u32 msg_ack(struct tipc_msg *m)
+{
+	return msg_bits(m, 2, 16, 0xffff);
+}
+
+static inline void msg_set_ack(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 2, 16, 0xffff, n);
+}
+
+static inline u32 msg_seqno(struct tipc_msg *m)
+{
+	return msg_bits(m, 2, 0, 0xffff);
+}
+
+static inline void msg_set_seqno(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 2, 0, 0xffff, n);
+}
+
+/*
+ * TIPC may utilize the "link ack #" and "link seq #" fields of a short
+ * message header to hold the destination node for the message, since the
+ * normal "dest node" field isn't present.  This cache is only referenced
+ * when required, so populating the cache of a longer message header is
+ * harmless (as long as the header has the two link sequence fields present).
+ *
+ * Note: Host byte order is OK here, since the info never goes off-card.
+ */
+
+static inline u32 msg_destnode_cache(struct tipc_msg *m)
+{
+	return m->hdr[2];
+}
+
+static inline void msg_set_destnode_cache(struct tipc_msg *m, u32 dnode)
+{
+	m->hdr[2] = dnode;
+}
+
+/*
+ * Words 3-10
+ */
+
+
+static inline void msg_set_prevnode(struct tipc_msg *m, u32 a)
+{
+	msg_set_word(m, 3, a);
+}
+
+static inline void msg_set_origport(struct tipc_msg *m, u32 p)
+{
+	msg_set_word(m, 4, p);
+}
+
+static inline void msg_set_destport(struct tipc_msg *m, u32 p)
+{
+	msg_set_word(m, 5, p);
+}
+
+static inline void msg_set_mc_netid(struct tipc_msg *m, u32 p)
+{
+	msg_set_word(m, 5, p);
+}
+
+static inline void msg_set_orignode(struct tipc_msg *m, u32 a)
+{
+	msg_set_word(m, 6, a);
+}
+
+static inline void msg_set_destnode(struct tipc_msg *m, u32 a)
+{
+	msg_set_word(m, 7, a);
+}
+
+static inline u32 msg_routed(struct tipc_msg *m)
+{
+	if (likely(msg_short(m)))
+		return 0;
+	return(msg_destnode(m) ^ msg_orignode(m)) >> 11;
+}
+
+static inline void msg_set_nametype(struct tipc_msg *m, u32 n)
+{
+	msg_set_word(m, 8, n);
+}
+
+static inline u32 msg_timestamp(struct tipc_msg *m)
+{
+	return msg_word(m, 8);
+}
+
+static inline void msg_set_timestamp(struct tipc_msg *m, u32 n)
+{
+	msg_set_word(m, 8, n);
+}
+
+static inline void msg_set_namelower(struct tipc_msg *m, u32 n)
+{
+	msg_set_word(m, 9, n);
+}
+
+static inline void msg_set_nameinst(struct tipc_msg *m, u32 n)
+{
+	msg_set_namelower(m, n);
+}
+
+static inline void msg_set_nameupper(struct tipc_msg *m, u32 n)
+{
+	msg_set_word(m, 10, n);
+}
+
+static inline struct tipc_msg *msg_get_wrapped(struct tipc_msg *m)
+{
+	return (struct tipc_msg *)msg_data(m);
+}
+
+
+/*
+ * The following definitions are used with TIPC internal message headers only
+ */   
+
+/* 
+ * Internal message users (types 0-3 are for application messages)
+ *
+ * Note: Pre-TIPC 1.7 releases treat incoming USER_TYPE_9 messages as route
+ * distribution messages.
+ */
+
+#define  USER_TYPE_4          4		/* NOT USED */
+#define  BCAST_PROTOCOL       5
+#define  MSG_BUNDLER          6
+#define  LINK_PROTOCOL        7
+#define  CONN_MANAGER         8
+#define  USER_TYPE_9          9		/* NOT USED (see Note above) */
+#define  CHANGEOVER_PROTOCOL  10
+#define  NAME_DISTRIBUTOR     11
+#define  MSG_FRAGMENTER       12
+#define  LINK_CONFIG          13
+#define  ROUTE_DISTRIBUTOR    14
+#define  USER_TYPE_15         15	/* NOT USED */
+
+/*
+ *  Connection management protocol messages
+ */
+
+#define CONN_PROBE        0
+#define CONN_PROBE_REPLY  1
+#define CONN_ACK          2
+
+/* 
+ * Name/route distributor messages
+ */
+
+#define DIST_PUBLISH	0
+#define DIST_WITHDRAW	1
+#define DIST_PURGE	2
+
+/*
+ * Link configuration message node-capability indicator flags
+ */
+
+#define NF_DONT_USE_1	0x0001		/* uni-cluster nodes may set this */
+#define NF_DONT_USE_2	0x0002		/* uni-cluster nodes may set this */
+#define NF_MULTICLUSTER	0x1000		/* node is multi-cluster capable */
+
+/*
+ * Word 1
+ */
+
+static inline u32 msg_seq_gap(struct tipc_msg *m)
+{
+	return msg_bits(m, 1, 16, 0x1fff);
+}
+
+static inline void msg_set_seq_gap(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 1, 16, 0x1fff, n);
+}
+
+static inline u32 msg_node_flags(struct tipc_msg *m)
+{
+	return msg_bits(m, 1, 16, 0x1fff);
+}
+
+static inline void msg_set_node_flags(struct tipc_msg *m, u32 n) 
+{
+	msg_set_bits(m, 1, 16, 0x1fff, n);
+}
+
+static inline u32 msg_node_sig(struct tipc_msg *m)
+{
+	return msg_bits(m, 1, 0, 0xffff);
+}
+
+static inline void msg_set_node_sig(struct tipc_msg *m, u32 n) 
+{
+	msg_set_bits(m, 1, 0, 0xffff, n);
+}
+
+
+/*
+ * Word 2
+ */
+
+static inline u32 msg_dest_domain(struct tipc_msg *m)
+{
+	return msg_word(m, 2);
+}
+
+static inline void msg_set_dest_domain(struct tipc_msg *m, u32 n)
+{
+	msg_set_word(m, 2, n);
+}
+
+static inline u32 msg_bcgap_after(struct tipc_msg *m)
+{
+	return msg_bits(m, 2, 16, 0xffff);
+}
+
+static inline void msg_set_bcgap_after(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 2, 16, 0xffff, n);
+}
+
+static inline u32 msg_bcgap_to(struct tipc_msg *m)
+{
+	return msg_bits(m, 2, 0, 0xffff);
+}
+
+static inline void msg_set_bcgap_to(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 2, 0, 0xffff, n);
+}
+
+
+/*
+ * Word 4
+ */
+
+static inline u32 msg_last_bcast(struct tipc_msg *m)
+{
+	return msg_bits(m, 4, 16, 0xffff);
+}
+
+static inline void msg_set_last_bcast(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 4, 16, 0xffff, n);
+}
+
+static inline u32 msg_fragm_no(struct tipc_msg *m)
+{
+	return msg_bits(m, 4, 16, 0xffff);
+}
+
+static inline void msg_set_fragm_no(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 4, 16, 0xffff, n);
+}
+
+static inline u32 msg_next_sent(struct tipc_msg *m)
+{
+	return msg_bits(m, 4, 0, 0xffff);
+}
+
+static inline void msg_set_next_sent(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 4, 0, 0xffff, n);
+}
+
+static inline u32 msg_fragm_msg_no(struct tipc_msg *m)
+{
+	return msg_bits(m, 4, 0, 0xffff);
+}
+
+static inline void msg_set_fragm_msg_no(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 4, 0, 0xffff, n);
+}
+
+static inline u32 msg_bc_netid(struct tipc_msg *m)
+{
+	return msg_word(m, 4);
+}
+
+static inline void msg_set_bc_netid(struct tipc_msg *m, u32 id)
+{
+	msg_set_word(m, 4, id);
+}
+
+static inline u32 msg_link_selector(struct tipc_msg *m)
+{
+	return msg_bits(m, 4, 0, 1);
+}
+
+static inline void msg_set_link_selector(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 4, 0, 1, n);
+}
+
+/*
+ * Word 5
+ */
+
+static inline u32 msg_session(struct tipc_msg *m)
+{
+	return msg_bits(m, 5, 16, 0xffff);
+}
+
+static inline void msg_set_session(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 5, 16, 0xffff, n);
+}
+
+static inline u32 msg_probe(struct tipc_msg *m)
+{
+	return msg_bits(m, 5, 0, 1);
+}
+
+static inline void msg_set_probe(struct tipc_msg *m, u32 val)
+{
+	msg_set_bits(m, 5, 0, 1, (val & 1));
+}
+
+static inline char msg_net_plane(struct tipc_msg *m)
+{
+	return msg_bits(m, 5, 1, 7) + 'A';
+}
+
+static inline void msg_set_net_plane(struct tipc_msg *m, char n)
+{
+	msg_set_bits(m, 5, 1, 7, (n - 'A'));
+}
+
+static inline u32 msg_linkprio(struct tipc_msg *m)
+{
+	return msg_bits(m, 5, 4, 0x1f);
+}
+
+static inline void msg_set_linkprio(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 5, 4, 0x1f, n);
+}
+
+static inline u32 msg_bearer_id(struct tipc_msg *m)
+{
+	return msg_bits(m, 5, 9, 0x7);
+}
+
+static inline void msg_set_bearer_id(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 5, 9, 0x7, n);
+}
+
+static inline u32 msg_stop(struct tipc_msg *m)
+{
+	return msg_bits(m, 5, 13, 0x1);
+}
+
+static inline void msg_set_stop(struct tipc_msg *m, u32 s)
+{
+	msg_set_bits(m, 5, 13, 0x1, s);
+}
+
+static inline u32 msg_redundant_link(struct tipc_msg *m)
+{
+	return msg_bits(m, 5, 12, 0x1);
+}
+
+static inline void msg_set_redundant_link(struct tipc_msg *m, u32 r)
+{
+	msg_set_bits(m, 5, 12, 0x1, r);
+}
+
+
+/*
+ * Word 9
+ */
+
+static inline u32 msg_item_size(struct tipc_msg *m)
+{
+	return msg_bits(m, 9, 24, 0xff);
+}
+
+static inline void msg_set_item_size(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 9, 24, 0xff, n);
+}
+
+static inline u32 msg_msgcnt(struct tipc_msg *m)
+{
+	return msg_bits(m, 9, 16, 0xffff);
+}
+
+static inline void msg_set_msgcnt(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 9, 16, 0xffff, n);
+}
+
+static inline u32 msg_bcast_tag(struct tipc_msg *m)
+{
+	return msg_bits(m, 9, 16, 0xffff);
+}
+
+static inline void msg_set_bcast_tag(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 9, 16, 0xffff, n);
+}
+
+static inline u32 msg_max_pkt(struct tipc_msg *m)
+{
+	return (msg_bits(m, 9, 16, 0xffff) * 4);
+}
+
+static inline void msg_set_max_pkt(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 9, 16, 0xffff, (n / 4));
+}
+
+static inline u32 msg_link_tolerance(struct tipc_msg *m)
+{
+	return msg_bits(m, 9, 0, 0xffff);
+}
+
+static inline void msg_set_link_tolerance(struct tipc_msg *m, u32 n)
+{
+	msg_set_bits(m, 9, 0, 0xffff, n);
+}
+
+/*
+ * Segmentation message types
+ */
+
+#define FIRST_FRAGMENT     0
+#define FRAGMENT           1
+#define LAST_FRAGMENT      2
+
+/*
+ * Link management protocol message types
+ */
+
+#define STATE_MSG       0
+#define RESET_MSG       1
+#define ACTIVATE_MSG    2
+
+/*
+ * Changeover tunnel message types
+ */
+#define DUPLICATE_MSG    0
+#define ORIGINAL_MSG     1
+
+
+/*
+ * Config protocol message types
+ */
+
+#define DSC_REQ_MSG          0
+#define DSC_RESP_MSG         1
+
+
+int  tipc_msg_build(struct tipc_msg *hdr, 
+		    struct iovec const *msg_sect, u32 num_sect,
+		    int max_size, int usrmem, struct sk_buff** buf);
+void tipc_msg_init(struct tipc_msg *m, u32 user, u32 type, 
+		   u32 hsize, u32 destnode);
+int  tipc_msg_calc_data_size(struct iovec const *msg_sect, u32 num_sect);
+u32  tipc_msg_tot_importance(struct tipc_msg *m);
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_name_distr.c android_cluster/linux-2.6.29/net/tipc/tipc_name_distr.c
--- linux-2.6.29/net/tipc/tipc_name_distr.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_name_distr.c	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,1165 @@
+/*
+ * net/tipc/tipc_name_distr.c: TIPC name distribution code
+ * 
+ * Copyright (c) 2000-2006, Ericsson AB
+ * Copyright (c) 2005-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_dbg.h"
+#include "tipc_link.h"
+#include "tipc_msg.h"
+#include "tipc_name_distr.h"
+
+
+/*
+ * Distribution list definitions
+ */
+
+#define NODE_TO_NODE_LIST	0	/* Item from this node with NODE scope */
+#define NODE_TO_CLUSTER_LIST	1	/* Item from this node with CLUSTER scope */
+#define NODE_TO_ZONE_LIST	2	/* Item from this node with ZONE scope */
+#define NODE_TO_NETWORK_LIST	3	/* Item from this node with NETWORK scope */
+#define CLUSTER_TO_CLUSTER_LIST	4	/* Item from rest of cluster with CLUSTER scope */
+#define CLUSTER_TO_ZONE_LIST	5	/* Item from rest of cluster with ZONE scope */
+#define CLUSTER_TO_NETWORK_LIST	6	/* Item from rest of cluster with NETWORK scope */
+#define ZONE_TO_ZONE_LIST	7	/* Item from rest of zone with ZONE scope */
+#define ZONE_TO_NETWORK_LIST	8	/* Item from rest of zone with ZONE scope */
+#define NETWORK_TO_NETWORK_LIST	9	/* Item from rest of network with NETWORK scope */
+
+#define NUM_DIST_LISTS 10
+
+/*
+ * Maximum amount of data in a single bulk item distribution message;
+ * helps to avoid fragmentation overhead which might otherwise occur
+ */
+
+#define MAX_DIST_MSG_DATA (1500 - LONG_H_SIZE)
+
+
+typedef struct 
+{
+	struct list_head list;
+	int list_size;
+} dist_list_t;
+ 
+
+/**
+ * dist_list_select - determine relevant distribution list for published item
+ * @addr: network address of node that published item
+ * @scope: scope of publication
+ */
+
+static int dist_list_select(u32 addr, u32 scope)
+{
+	int dist_list_id;
+
+	if (addr_in_node(addr)) {
+		if (scope == TIPC_CLUSTER_SCOPE)
+			dist_list_id = NODE_TO_CLUSTER_LIST;
+		else if (scope == TIPC_ZONE_SCOPE)
+			dist_list_id = NODE_TO_ZONE_LIST;
+		else if (scope == TIPC_NODE_SCOPE)
+			dist_list_id = NODE_TO_NODE_LIST;
+		else 
+			dist_list_id = NODE_TO_NETWORK_LIST;
+	} else if (in_own_cluster(addr)) {
+		if (scope == TIPC_CLUSTER_SCOPE)
+			dist_list_id = CLUSTER_TO_CLUSTER_LIST;
+		else if (scope == TIPC_ZONE_SCOPE)
+			dist_list_id = CLUSTER_TO_ZONE_LIST;
+		else
+			dist_list_id = CLUSTER_TO_NETWORK_LIST;
+	} else if (in_own_zone(addr)) {
+		if (scope == TIPC_ZONE_SCOPE)
+			dist_list_id = ZONE_TO_ZONE_LIST;
+		else
+			dist_list_id = ZONE_TO_NETWORK_LIST;
+	} else {
+		dist_list_id = NETWORK_TO_NETWORK_LIST;
+	}
+
+	return dist_list_id;
+}
+
+/*
+ * NAME TABLE CODE
+ */
+
+/**
+ * struct distr_item - publication info exchanged by TIPC nodes
+ * @type: name sequence type
+ * @lower: name sequence lower bound
+ * @upper: name sequence upper bound
+ * @ref: publishing port reference
+ * @key: publication key
+ * @node: network address of publishing port's node
+ * @dist_info: distribution info for name publication
+ * 
+ * ===> NAME_DISTRIBUTOR message stores fields in network byte order <===
+ */
+
+struct name_item {
+	u32 type;
+	u32 lower;
+	u32 upper;
+	u32 ref;
+	u32 key;
+	u32 node;		/* optional */
+	u32 dist_info;		/* optional */
+};
+
+
+#define NAME_ITEM_SIZE_UNI 5	/* # words/item for uni-cluster TIPC nodes */
+#define NAME_ITEM_SIZE     7	/* # words/item for multi-cluster TIPC nodes */
+
+#define NAME_ITEM_BYTES    (NAME_ITEM_SIZE * sizeof(u32))
+#define NAME_ITEMS_MAX     (MAX_DIST_MSG_DATA / NAME_ITEM_BYTES)
+
+/*
+ * Name distribution lists
+ */
+
+static dist_list_t dist_name_list[NUM_DIST_LISTS] = { 
+	{ LIST_HEAD_INIT(dist_name_list[0].list), 0 },
+	{ LIST_HEAD_INIT(dist_name_list[1].list), 0 },
+	{ LIST_HEAD_INIT(dist_name_list[2].list), 0 },
+	{ LIST_HEAD_INIT(dist_name_list[3].list), 0 },
+	{ LIST_HEAD_INIT(dist_name_list[4].list), 0 },
+	{ LIST_HEAD_INIT(dist_name_list[5].list), 0 },
+	{ LIST_HEAD_INIT(dist_name_list[6].list), 0 },
+	{ LIST_HEAD_INIT(dist_name_list[7].list), 0 },
+	{ LIST_HEAD_INIT(dist_name_list[8].list), 0 },
+	{ LIST_HEAD_INIT(dist_name_list[9].list), 0 },
+	};
+
+/**
+ * name_to_item - convert distributed publication to name item
+ */
+
+static void name_to_item(struct publication *publ, int dist_mask,
+			 unchar *item, int item_size)
+{
+	struct name_item *i = (struct name_item *)item;
+
+	i->type = htonl(publ->type);
+	i->lower = htonl(publ->lower);
+	i->upper = htonl(publ->upper);
+	i->ref = htonl(publ->ref);
+	i->key = htonl(publ->key);
+
+	if (item_size >= NAME_ITEM_SIZE) {
+		i->dist_info = htonl(publ->scope | (dist_mask << 4));
+		i->node = htonl(publ->node);
+	}
+}
+
+/**
+ * item_to_name - convert name item to distributed publication
+ */
+
+static void item_to_name(struct publication *publ, int *dist_mask,
+			 unchar *item, int item_size)
+{
+	struct name_item *i = (struct name_item *)item;
+
+	publ->type = ntohl(i->type);
+	publ->lower = ntohl(i->lower);
+	publ->upper = ntohl(i->upper);
+	publ->ref = ntohl(i->ref);
+	publ->key = ntohl(i->key);
+
+	if (item_size >= NAME_ITEM_SIZE) {
+		publ->node = ntohl(i->node);
+		publ->scope = ntohl(i->dist_info);
+		*dist_mask = (publ->scope >> 4) & 0xF;
+		publ->scope &= 0xF;
+	}
+}
+
+/**
+ * named_prepare_buf - allocate & initialize a name info message
+ */
+
+static struct sk_buff *named_prepare_buf(u32 type, u32 num_items, int item_size, 
+					 u32 dest)
+{
+	u32 size = LONG_H_SIZE + num_items * (item_size * sizeof(u32));
+	struct sk_buff *buf = buf_acquire(size);  
+	struct tipc_msg *msg;
+
+	if (buf != NULL) {
+		msg = buf_msg(buf);
+		tipc_msg_init(msg, NAME_DISTRIBUTOR, type, LONG_H_SIZE, dest);
+		msg_set_size(msg, size);
+		msg_set_item_size(msg, item_size);
+	}
+	return buf;
+}
+
+/**
+ * tipc_named_insert_publ - add name to appropriate distribution list
+ */
+
+void tipc_named_insert_publ(struct publication *publ)
+{
+	int dist_list_id;
+
+	dist_list_id = dist_list_select(publ->node, publ->scope);
+
+	list_add_tail(&publ->distr_list, &dist_name_list[dist_list_id].list);
+	dist_name_list[dist_list_id].list_size++;
+}
+
+/**
+ * tipc_named_remove_publ - remove name from its distribution list
+ */
+
+void tipc_named_remove_publ(struct publication *publ)
+{
+	int dist_list_id;
+
+	dist_list_id = dist_list_select(publ->node, publ->scope);
+
+	list_del(&publ->distr_list);
+	dist_name_list[dist_list_id].list_size--;
+}
+
+/**
+ * named_distribute - prepare name info for distribution to another node
+ */
+
+static void named_distribute(struct list_head *delivery_list, u32 dest_node,
+			     int dist_list_id, int dist_mask, int item_size)
+{
+	struct publication *publ;
+	struct sk_buff *buf = NULL;
+	unchar *item = NULL;
+	u32 buf_todo_items = 0;
+	u32 name_items_max = MAX_DIST_MSG_DATA / (item_size * sizeof(u32));
+	int list_cnt;
+
+	list_cnt = dist_name_list[dist_list_id].list_size;
+
+	list_for_each_entry(publ, &dist_name_list[dist_list_id].list,
+			    distr_list) {
+		if (buf == NULL) {
+			buf_todo_items = (list_cnt <= name_items_max) ?
+				list_cnt : name_items_max;
+			buf = named_prepare_buf(DIST_PUBLISH, buf_todo_items, 
+						item_size, dest_node);       
+			if (!buf) {
+				warn("Bulk publication failure\n");
+				return;
+			}
+			list_cnt -= buf_todo_items;
+			item = msg_data(buf_msg(buf));
+		}
+		name_to_item(publ, dist_mask, item, item_size);
+		item += (item_size * sizeof(u32));
+		if (--buf_todo_items == 0) {
+			msg_set_link_selector(buf_msg(buf), (dest_node & 1));
+			list_add_tail((struct list_head *)buf, delivery_list);
+			buf = NULL;
+		}
+	}
+	dbg_assert(buf == NULL);
+}
+
+/**
+ * tipc_named_node_up - tell specified node about relevant name info
+ */
+
+void tipc_named_node_up(unsigned long node)
+{
+	struct sk_buff *buf;
+	struct sk_buff *temp_buf;
+	struct list_head delivery_list;
+
+	INIT_LIST_HEAD(&delivery_list);
+
+	read_lock_bh(&tipc_nametbl_lock); 
+
+	if (in_own_cluster(node)) {
+		named_distribute(&delivery_list, node, NODE_TO_CLUSTER_LIST,
+				 0, NAME_ITEM_SIZE);
+		named_distribute(&delivery_list, node, NODE_TO_ZONE_LIST,
+				 TIPC_DIST_TO_ZONE, NAME_ITEM_SIZE);
+		named_distribute(&delivery_list, node, NODE_TO_NETWORK_LIST,
+				 TIPC_DIST_TO_NETWORK | TIPC_DIST_TO_ZONE,
+				 NAME_ITEM_SIZE);
+		if (tipc_own_routes > 0) {
+			named_distribute(&delivery_list, node, ZONE_TO_ZONE_LIST,
+					 0, NAME_ITEM_SIZE);
+			named_distribute(&delivery_list, node, ZONE_TO_NETWORK_LIST,
+					 TIPC_DIST_TO_NETWORK, NAME_ITEM_SIZE);
+			named_distribute(&delivery_list, node, NETWORK_TO_NETWORK_LIST,
+					 TIPC_DIST_TO_ZONE, NAME_ITEM_SIZE);
+		}
+	} else if (in_own_zone(node)) {
+		named_distribute(&delivery_list, node, NODE_TO_ZONE_LIST,
+				 TIPC_DIST_TO_CLUSTER, NAME_ITEM_SIZE);
+		named_distribute(&delivery_list, node, NODE_TO_NETWORK_LIST,
+				 TIPC_DIST_TO_NETWORK | TIPC_DIST_TO_CLUSTER,
+				 NAME_ITEM_SIZE);
+		named_distribute(&delivery_list, node, CLUSTER_TO_ZONE_LIST,
+				 TIPC_DIST_TO_CLUSTER, NAME_ITEM_SIZE);
+		named_distribute(&delivery_list, node, CLUSTER_TO_NETWORK_LIST,
+				 TIPC_DIST_TO_NETWORK | TIPC_DIST_TO_CLUSTER,
+				 NAME_ITEM_SIZE);
+		named_distribute(&delivery_list, node, NETWORK_TO_NETWORK_LIST,
+				 TIPC_DIST_TO_CLUSTER, NAME_ITEM_SIZE);
+	} else /* node is in another zone */ {
+		named_distribute(&delivery_list, node, NODE_TO_NETWORK_LIST,
+				 TIPC_DIST_TO_ZONE | TIPC_DIST_TO_CLUSTER,
+				 NAME_ITEM_SIZE);
+		named_distribute(&delivery_list, node, CLUSTER_TO_NETWORK_LIST,
+				 TIPC_DIST_TO_ZONE | TIPC_DIST_TO_CLUSTER,
+				 NAME_ITEM_SIZE);
+		named_distribute(&delivery_list, node, ZONE_TO_NETWORK_LIST,
+				 TIPC_DIST_TO_ZONE | TIPC_DIST_TO_CLUSTER,
+				 NAME_ITEM_SIZE);
+	}
+
+	read_unlock_bh(&tipc_nametbl_lock); 
+
+	list_for_each_safe(buf, temp_buf, ((struct sk_buff *)&delivery_list)) {
+		list_del((struct list_head *)buf);
+		if (tipc_link_send(buf, node, node) < 0) {
+			warn("Bulk publication not sent\n");
+		}
+	}
+}
+
+/**
+ * tipc_named_node_up_uni - tell uni-cluster node about relevant name info
+ */
+
+void tipc_named_node_up_uni(unsigned long node)
+{
+#ifdef CONFIG_TIPC_UNICLUSTER_FRIENDLY
+	struct sk_buff *buf;
+	struct sk_buff *temp_buf;
+	struct list_head delivery_list;
+
+	INIT_LIST_HEAD(&delivery_list);
+
+	read_lock_bh(&tipc_nametbl_lock); 
+
+	named_distribute(&delivery_list, node, NODE_TO_CLUSTER_LIST,
+			 0, NAME_ITEM_SIZE_UNI);
+	named_distribute(&delivery_list, node, NODE_TO_ZONE_LIST,
+			 0, NAME_ITEM_SIZE_UNI);
+	named_distribute(&delivery_list, node, NODE_TO_NETWORK_LIST,
+			 0, NAME_ITEM_SIZE_UNI);
+
+	read_unlock_bh(&tipc_nametbl_lock); 
+
+	list_for_each_safe(buf, temp_buf, ((struct sk_buff *)&delivery_list)) {
+		list_del((struct list_head *)buf);
+		if (tipc_link_send(buf, node, node) < 0) {
+			warn("Bulk publication not sent\n");
+		}
+	}
+#endif
+}
+
+/**
+ * named_cluster_distribute - send name to all adjacent cluster nodes
+ */
+
+static void named_cluster_distribute(struct publication *publ, int msg_type,
+				     int dist_mask)
+{
+	struct sk_buff *buf;
+
+	dist_mask &= ~TIPC_DIST_TO_CLUSTER;
+
+	buf = named_prepare_buf(msg_type, 1, NAME_ITEM_SIZE, tipc_addr(0, 0, 0));
+	if (!buf) {
+		warn("Memory squeeze; failed to distribute publication\n");
+		return;
+	}
+
+	name_to_item(publ, dist_mask, msg_data(buf_msg(buf)), NAME_ITEM_SIZE);
+
+#ifdef CONFIG_TIPC_UNICLUSTER_FRIENDLY
+	/*
+	 * Hide name from to cluster's uni-cluster nodes if it was
+	 * issued by another node, since they are unable to deal with
+	 * name messages originating outside their own cluster (and would
+	 * think this node is the originator!) 
+	 */
+
+	if (publ->node != tipc_own_addr) {
+		struct sk_buff *buf_copy;
+		struct tipc_node *n_ptr;
+		int i;
+
+		for (i = 0; i < tipc_local_nodes.first_free; i++) {
+			n_ptr = (struct tipc_node *)tipc_local_nodes.element[i];
+			if (!tipc_node_is_up(n_ptr) || 
+			    (n_ptr->flags & NF_MULTICLUSTER) == 0)
+				continue;
+
+			buf_copy = skb_copy(buf, GFP_ATOMIC);
+			if (buf_copy == NULL) {
+				warn("Publication distribution to cluster failed\n");
+				break;
+			}
+			msg_set_destnode(buf_msg(buf_copy),
+					 n_ptr->elm.addr);
+			if (tipc_link_send(buf_copy, n_ptr->elm.addr, 
+					   n_ptr->elm.addr) < 0) {
+				warn("Publication distribution to cluster failed\n");
+			}
+		}
+
+		buf_discard(buf);
+		return;
+	}
+#endif
+
+	/*
+	 * Broadcast name to all nodes in own cluster
+	 *
+	 * Note: Uni-cluster nodes will ignore the extra fields at the end
+	 * of the lone name item, so the "new style" form is OK here
+	 */
+
+	if (tipc_bclink_send_msg(buf) < 0) {
+		warn("Publication distribution to cluster failed\n");
+	}
+}
+
+/**
+ * named_zone_distribute - send name to all adjacent clusters in zone
+ */
+
+static void named_zone_distribute(struct publication *publ, int msg_type,
+				  int dist_mask)
+{
+	struct sk_buff *buf;
+	u32 router;
+	int i;
+
+	dist_mask &= ~TIPC_DIST_TO_ZONE;
+	dist_mask |= TIPC_DIST_TO_CLUSTER;
+
+	for (i = 0; i < tipc_remote_nodes.first_free; i++) {
+		router = tipc_remote_nodes.element[i]->addr;
+
+		if (!in_own_zone(router))
+			continue;
+		if (!tipc_node_is_up(
+			(struct tipc_node *)tipc_remote_nodes.element[i]))
+			continue;
+
+		buf = named_prepare_buf(msg_type, 1, NAME_ITEM_SIZE, router);
+		if (!buf) {
+			warn("Memory squeeze; failed to distribute name table msg\n");
+			break;
+		}
+		name_to_item(publ, dist_mask, msg_data(buf_msg(buf)), NAME_ITEM_SIZE);
+		if (tipc_link_send(buf, router, router) < 0) {
+			warn("Failed to distribute name table msg\n");
+		}
+	}
+}
+
+/**
+ * named_network_distribute - send name to all neighboring zones in network
+ */
+
+static void named_network_distribute(struct publication *publ, int msg_type,
+				     int dist_mask)
+{
+	struct sk_buff *buf;
+	u32 router;
+	int i;
+
+	dist_mask &= ~TIPC_DIST_TO_NETWORK;
+	dist_mask |= (TIPC_DIST_TO_ZONE | TIPC_DIST_TO_CLUSTER);
+
+	for (i = 0; i < tipc_remote_nodes.first_free; i++) {
+		router = tipc_remote_nodes.element[i]->addr;
+
+		if (in_own_zone(router))
+			continue;
+		if (!tipc_node_is_up(
+			(struct tipc_node *)tipc_remote_nodes.element[i]))
+			continue;
+
+		buf = named_prepare_buf(msg_type, 1, NAME_ITEM_SIZE, router);
+		if (!buf) {
+			warn("Memory squeeze; failed to distribute name table msg\n");
+			break;
+		}
+		name_to_item(publ, dist_mask, msg_data(buf_msg(buf)), NAME_ITEM_SIZE);
+		if (tipc_link_send(buf, router, router) < 0) {
+			warn("Failed to distribute name table msg\n");
+		}
+	}
+}
+
+
+/**
+ * tipc_named_distribute - send name info to relevant nodes
+ */
+
+void tipc_named_distribute(struct publication *publ, int msg_type,
+			   int dist_mask)
+{
+	if (tipc_mode != TIPC_NET_MODE)
+		return;
+
+	if (dist_mask & TIPC_DIST_TO_CLUSTER) {
+		named_cluster_distribute(publ, msg_type, dist_mask);
+	}
+	if (dist_mask & TIPC_DIST_TO_ZONE) {
+		named_zone_distribute(publ, msg_type, dist_mask);
+	}
+	if (dist_mask & TIPC_DIST_TO_NETWORK) {
+		named_network_distribute(publ, msg_type, dist_mask);
+	}
+}
+
+/**
+ * named_purge_publ - delete name associated with a failed node/region
+ * 
+ * Invoked for each name that can no longer be reached.  
+ * Removes publication structure from name table & deletes it.
+ * In rare cases the link may have come back up again when this
+ * function is called, and we have two items representing the same
+ * publication. Nudge this item's key to distinguish it from the other.
+ *
+ * Publication's network element subscription is already unsubscribed, 
+ * so we don't have to do that here ...
+ */
+
+static void named_purge_publ(struct publication *publ)
+{
+	write_lock_bh(&tipc_nametbl_lock);
+
+	publ->key += 1222345;
+	publ = tipc_nametbl_remove_publ(publ->type, publ->lower, 
+					publ->node, publ->ref, publ->key);
+	if (publ != NULL) {
+		tipc_named_remove_publ(publ);
+	}
+
+	write_unlock_bh(&tipc_nametbl_lock);
+
+	kfree(publ);
+}
+
+/**
+ * tipc_named_recv - process name table update message sent by another node
+ */
+
+void tipc_named_recv(struct sk_buff *buf)
+{
+	struct publication publ_info;
+	struct publication *publ;
+	struct tipc_msg *msg = buf_msg(buf);
+	u32 type = msg_type(msg);
+	unchar *item = msg_data(msg);
+	int item_size = msg_item_size(msg);
+	int item_size_min = NAME_ITEM_SIZE;
+	u32 item_count;
+	int dist_mask;
+
+#ifdef CONFIG_TIPC_UNICLUSTER_FRIENDLY
+	if (item_size == 0) {
+		item_size = NAME_ITEM_SIZE_UNI;
+		item_size_min = NAME_ITEM_SIZE_UNI;
+		publ_info.node = msg_orignode(msg);
+		publ_info.scope = TIPC_CLUSTER_SCOPE;
+		dist_mask = 0;
+	}
+#endif
+
+	if (item_size < item_size_min) {
+		warn("Invalid name table item received\n");
+		item_count = 0;
+	} else {
+		item_count = msg_data_sz(msg) / (item_size * sizeof(u32));
+	}
+
+	while (item_count--) {
+
+		item_to_name(&publ_info, &dist_mask, item, item_size);
+
+		if (type == DIST_PUBLISH) {
+			write_lock_bh(&tipc_nametbl_lock); 
+			publ = tipc_nametbl_insert_publ(publ_info.type, 
+							publ_info.lower,
+							publ_info.upper,
+							publ_info.scope,
+							publ_info.node,
+							publ_info.ref,
+							publ_info.key);
+			if (publ) {
+				tipc_netsub_bind(&publ->subscr, publ->node,
+						 (net_ev_handler)named_purge_publ,
+						 publ);
+				tipc_named_insert_publ(publ);
+			}
+			write_unlock_bh(&tipc_nametbl_lock);
+
+			/* TODO: Is there a slight risk that, on an SMP system,
+			   another CPU could remove the publication (due to a
+			   route withdrawl, triggered by a link timeout) and 
+			   send a withdraw message for it before we send the
+			   associated publish message? (The risk seems small,
+			   but could it happen???) To lessen the impact of
+			   such a situation, we use a copy of the publication
+			   info, but this still means that there is a chance
+			   of messages being delivered in the wrong order.
+			   We probably need to ensure that distribution of
+			   name (& route) messages is single threaded to
+			   eliminate this risk ... */
+
+			if (publ) {
+				if (dist_mask)
+					tipc_named_distribute(&publ_info,
+							      DIST_PUBLISH,
+							      dist_mask);
+			}
+		}
+		else if (type == DIST_WITHDRAW) {
+			write_lock_bh(&tipc_nametbl_lock); 
+			publ = tipc_nametbl_remove_publ(publ_info.type,
+							publ_info.lower,
+							publ_info.node,
+							publ_info.ref,
+							publ_info.key);
+
+			if (publ) {
+				tipc_netsub_unbind(&publ->subscr);
+				tipc_named_remove_publ(publ);
+			}
+			write_unlock_bh(&tipc_nametbl_lock); 
+			if (publ) {
+				if (dist_mask)
+					tipc_named_distribute(publ,
+							      DIST_WITHDRAW,
+							      dist_mask);
+				kfree(publ);
+			}
+		}
+		else {
+			dbg("Unrecognized name table message received\n");
+		}
+
+		item += (item_size * sizeof(u32));
+	}
+	buf_discard(buf);
+}
+
+/**
+ * tipc_named_reinit - update name table entries to reflect new node address
+ * 
+ * This routine is called when TIPC enters networking mode.
+ * All names currently published by this node are updated to reflect
+ * the node's new network address.
+ */
+
+void tipc_named_reinit(void)
+{
+	struct publication *publ;
+
+	write_lock_bh(&tipc_nametbl_lock); 
+	list_for_each_entry(publ,
+			    &dist_name_list[NODE_TO_NODE_LIST].list,
+			    distr_list) {
+		publ->node = tipc_own_addr;
+	}
+	list_for_each_entry(publ,
+			    &dist_name_list[NODE_TO_CLUSTER_LIST].list,
+			    distr_list) {
+		publ->node = tipc_own_addr;
+	}
+	list_for_each_entry(publ,
+			    &dist_name_list[NODE_TO_ZONE_LIST].list,
+			    distr_list) {
+		publ->node = tipc_own_addr;
+	}
+	write_unlock_bh(&tipc_nametbl_lock); 
+}
+
+/*
+ * ROUTING TABLE CODE
+ */
+
+/**
+ * struct route_item - route info exchanged by TIPC nodes
+ * @remote_region: network address of region reachable by route
+ * @local_router: network address of node publishing route
+ * @remote_router: network address of node route connects to
+ * @dist_info: distribution info for route publication
+ * 
+ * ===> ROUTE_DISTRIBUTOR message stores fields in network byte order <===
+ */
+
+struct route_item {
+	u32 remote_region;
+	u32 local_router;
+	u32 remote_router;
+	u32 dist_info;
+};
+
+#define ROUTE_ITEM_SIZE     4
+#define ROUTE_ITEM_BYTES    (ROUTE_ITEM_SIZE * sizeof(u32))
+#define ROUTE_ITEMS_MAX     (MAX_DIST_MSG_DATA / ROUTE_ITEM_BYTES)
+
+/*
+ * Route distribution lists
+ *
+ * Note: some lists are currently unused, since routes are only published
+ *       with "cluster" or "zone" scope at the moment
+ */
+
+static dist_list_t dist_route_list[NUM_DIST_LISTS] = { 
+	{ LIST_HEAD_INIT(dist_route_list[0].list), 0 },
+	{ LIST_HEAD_INIT(dist_route_list[1].list), 0 },
+	{ LIST_HEAD_INIT(dist_route_list[2].list), 0 },
+	{ LIST_HEAD_INIT(dist_route_list[3].list), 0 },
+	{ LIST_HEAD_INIT(dist_route_list[4].list), 0 },
+	{ LIST_HEAD_INIT(dist_route_list[5].list), 0 },
+	{ LIST_HEAD_INIT(dist_route_list[6].list), 0 },
+	{ LIST_HEAD_INIT(dist_route_list[7].list), 0 },
+	{ LIST_HEAD_INIT(dist_route_list[8].list), 0 },
+	{ LIST_HEAD_INIT(dist_route_list[9].list), 0 },
+	};
+
+
+/**
+ * route_to_item - convert distributed publication to route item
+ */
+
+static void route_to_item(struct publication *publ, int dist_mask, unchar *item)
+{
+	struct route_item *i = (struct route_item *)item;
+
+	i->remote_region = htonl(publ->lower);
+	i->local_router = htonl(publ->node);
+	i->remote_router = htonl(publ->ref);
+	i->dist_info = htonl(publ->scope | (dist_mask << 4));
+}
+
+/**
+ * item_to_route - convert route item to distributed publication
+ */
+
+static void item_to_route(struct publication *publ, int *dist_mask,
+			  unchar *item, int item_size)
+{
+	struct route_item *i = (struct route_item *)item;
+
+	publ->type = TIPC_ROUTE;
+	publ->lower = ntohl(i->remote_region);
+	publ->upper = publ->lower;
+	publ->node = ntohl(i->local_router);
+	publ->ref = ntohl(i->remote_router);
+	publ->key = 0;
+	publ->scope = ntohl(i->dist_info);
+	*dist_mask = (publ->scope >> 4) & 0xF;
+	publ->scope &= 0xF;
+}
+
+/**
+ * route_prepare_buf - allocate & initialize a route info message
+ */
+
+static struct sk_buff *route_prepare_buf(u32 type, u32 num_items, u32 dest)
+{
+	u32 size = LONG_H_SIZE + (num_items * ROUTE_ITEM_BYTES);
+	struct sk_buff *buf = buf_acquire(size);  
+	struct tipc_msg *msg;
+
+	if (buf != NULL) {
+		msg = buf_msg(buf);
+		tipc_msg_init(msg, ROUTE_DISTRIBUTOR, type, LONG_H_SIZE, dest);
+		msg_set_size(msg, size);
+		msg_set_item_size(msg, ROUTE_ITEM_SIZE);
+	}
+	return buf;
+}
+
+
+/**
+ * tipc_route_insert_publ - add route to appropriate distribution list
+ */
+
+void tipc_route_insert_publ(struct publication *publ)
+{
+	int dist_list_id;
+
+	dist_list_id = dist_list_select(publ->node, publ->scope);
+	
+	list_add_tail(&publ->distr_list, &dist_route_list[dist_list_id].list);
+	dist_route_list[dist_list_id].list_size++;
+}
+
+/**
+ * tipc_route_remove_publ - remove route from its distribution list
+ */
+
+void tipc_route_remove_publ(struct publication *publ)
+{
+	int dist_list_id;
+
+	dist_list_id = dist_list_select(publ->node, publ->scope);
+	
+	list_del(&publ->distr_list);
+	dist_route_list[dist_list_id].list_size--;
+}
+
+/**
+ * route_distribute - prepare route info for distribution to another node
+ */
+
+static void route_distribute(struct list_head *delivery_list, u32 dest_node,
+			     int dist_list_id, int dist_mask)
+{
+	struct publication *publ;
+	struct sk_buff *buf = NULL;
+	unchar *item = NULL;
+	u32 buf_todo_items = 0;
+	int list_cnt = dist_route_list[dist_list_id].list_size;
+
+	list_for_each_entry(publ, &dist_route_list[dist_list_id].list,
+			    distr_list) {
+		if (buf == NULL) {
+			buf_todo_items = (list_cnt <= ROUTE_ITEMS_MAX) ?
+				list_cnt : ROUTE_ITEMS_MAX;
+			buf = route_prepare_buf(DIST_PUBLISH, buf_todo_items, 
+						dest_node);       
+			if (!buf) {
+				warn("Bulk route publication failure\n");
+				return;
+			}
+			list_cnt -= buf_todo_items;
+			item = msg_data(buf_msg(buf));
+		}
+		route_to_item(publ, dist_mask, item);
+		item += ROUTE_ITEM_BYTES;
+		if (--buf_todo_items == 0) {
+			msg_set_link_selector(buf_msg(buf), (dest_node & 1));
+			list_add_tail((struct list_head *)buf, delivery_list);
+			buf = NULL;
+		}
+	}
+	dbg_assert(buf == NULL);
+}
+
+/**
+ * tipc_route_node_up - tell specified node about relevant routes
+ */
+
+void tipc_route_node_up(unsigned long node)
+{
+	struct sk_buff *buf;
+	struct sk_buff *temp_buf;
+	struct list_head delivery_list;
+
+	INIT_LIST_HEAD(&delivery_list);
+
+	read_lock_bh(&tipc_routetbl_lock); 
+
+	if (in_own_cluster(node)) {
+		route_distribute(&delivery_list, node, NODE_TO_CLUSTER_LIST,
+				 0);
+		route_distribute(&delivery_list, node, NODE_TO_ZONE_LIST,
+				 TIPC_DIST_TO_ZONE);
+		if (tipc_own_routes > 0) {
+			route_distribute(&delivery_list, node, ZONE_TO_ZONE_LIST,
+					 0);
+		}
+	} else { /* in_own_zone(node) */
+		route_distribute(&delivery_list, node, NODE_TO_ZONE_LIST,
+				 TIPC_DIST_TO_CLUSTER);
+		route_distribute(&delivery_list, node, CLUSTER_TO_ZONE_LIST,
+				 TIPC_DIST_TO_CLUSTER);
+	}
+
+	read_unlock_bh(&tipc_routetbl_lock);
+
+	list_for_each_safe(buf, temp_buf, ((struct sk_buff *)&delivery_list)) {
+		list_del((struct list_head *)buf);
+		if (tipc_link_send(buf, node, node) < 0) {
+			warn("Bulk route update not sent\n");
+		}
+	}
+}
+
+/**
+ * route_cluster_distribute - send route to all neighboring cluster nodes
+ * 
+ * Note: Pre-TIPC 1.7 nodes will ignore routing info messages
+ */
+
+static void route_cluster_distribute(struct publication *publ, int msg_type,
+				     int dist_mask)
+{
+	struct sk_buff *buf;
+
+	dist_mask &= ~TIPC_DIST_TO_CLUSTER;
+
+	buf = route_prepare_buf(msg_type, 1, tipc_addr(0, 0, 0));
+	if (!buf) {
+		warn("Memory squeeze; failed to distribute route\n");
+		return;
+	}
+
+	route_to_item(publ, dist_mask, msg_data(buf_msg(buf)));
+
+	if (tipc_bclink_send_msg(buf) < 0) {
+		warn("Route distribution to cluster failed\n");
+	}
+}
+
+/**
+ * route_zone_distribute - send route to all neighboring clusters in zone
+ */
+
+static void route_zone_distribute(struct publication *publ, int msg_type,
+				  int dist_mask)
+{
+	struct sk_buff *buf;
+	u32 router;
+	int i;
+
+	dist_mask &= ~TIPC_DIST_TO_ZONE;
+	dist_mask |= TIPC_DIST_TO_CLUSTER;
+
+	for (i = 0; i < tipc_remote_nodes.first_free; i++) {
+		router = tipc_remote_nodes.element[i]->addr;
+
+		if (!in_own_zone(router))
+			continue;
+		if (!tipc_node_is_up(
+			(struct tipc_node *)tipc_remote_nodes.element[i]))
+			continue;
+
+		buf = route_prepare_buf(msg_type, 1, router);
+		if (!buf) {
+			warn("Memory squeeze; failed to distribute route msg\n");
+			break;
+		}
+		route_to_item(publ, dist_mask, msg_data(buf_msg(buf)));
+		if (tipc_link_send(buf, router, router) < 0) {
+			warn("Failed to distribute route msg\n");
+		}
+	}
+}
+
+/**
+ * route_network_distribute - send route to all neighboring zones in network
+ */
+
+static void route_network_distribute(struct publication *publ, int msg_type,
+				     int dist_mask)
+{
+	struct sk_buff *buf;
+	u32 router;
+	int i;
+
+	dist_mask &= ~TIPC_DIST_TO_NETWORK;
+	dist_mask |= (TIPC_DIST_TO_ZONE | TIPC_DIST_TO_CLUSTER);
+
+	for (i = 0; i < tipc_remote_nodes.first_free; i++) {
+		router = tipc_remote_nodes.element[i]->addr;
+
+		if (in_own_zone(router))
+			continue;
+		if (!tipc_node_is_up(
+			(struct tipc_node *)tipc_remote_nodes.element[i]))
+			continue;
+
+		buf = route_prepare_buf(msg_type, 1, router);
+		if (!buf) {
+			warn("Memory squeeze; failed to distribute route msg\n");
+			break;
+		}
+		route_to_item(publ, dist_mask, msg_data(buf_msg(buf)));
+		if (tipc_link_send(buf, router, router) < 0) {
+			warn("Failed to distribute route msg\n");
+		}
+	}
+}
+
+/**
+ * tipc_route_distribute - send route info to relevant nodes
+ */
+
+void tipc_route_distribute(struct publication *publ, int msg_type,
+			   int dist_mask)
+{
+	if (dist_mask & TIPC_DIST_TO_CLUSTER) {
+		route_cluster_distribute(publ, msg_type, dist_mask);
+	}
+	if (dist_mask & TIPC_DIST_TO_ZONE) {
+		route_zone_distribute(publ, msg_type, dist_mask);
+	}
+	if (dist_mask & TIPC_DIST_TO_NETWORK) {
+		route_network_distribute(publ, msg_type, dist_mask);
+	}
+}
+
+/**
+ * route_purge_publ - delete route associated with a failed node/region
+ * 
+ * Invoked for each route that can no longer be reached.  
+ * Removes publication structure from route table & deletes it.
+ * In rare cases the link may have come back up again when this
+ * function is called, and we have two items representing the same
+ * publication. Nudge this item's key to distinguish it from the other.
+ *
+ * Publication's network element subscription is already unsubscribed, 
+ * so we don't have to do that here ...
+ */
+
+static void route_purge_publ(struct publication *publ)
+{
+	write_lock_bh(&tipc_routetbl_lock);
+
+	publ->key += 1222345;
+	publ = tipc_nameseq_remove_publ(route_table,publ->lower, 
+					publ->node, publ->ref, publ->key);
+	if (publ != NULL) {
+		tipc_all_routes--;
+		tipc_route_remove_publ(publ);
+	}
+
+	write_unlock_bh(&tipc_routetbl_lock);
+
+	kfree(publ);
+}
+
+/**
+ * tipc_route_recv - process routing table message sent by another node
+ */
+
+void tipc_route_recv(struct sk_buff *buf)
+{
+	struct publication publ_info;
+	struct publication *publ;
+	struct tipc_msg *msg = buf_msg(buf);
+	u32 type = msg_type(msg);
+	unchar *item = msg_data(msg);
+	int item_size = msg_item_size(msg);
+	u32 item_count;
+	int dist_mask;
+
+	if (item_size < ROUTE_ITEM_SIZE) {
+		warn("Invalid routing table item received\n");
+		item_count = 0;
+	} else {
+		item_count = msg_data_sz(msg) / (item_size * sizeof(u32));
+	}
+
+	while (item_count--) {
+
+		item_to_route(&publ_info, &dist_mask, item, item_size);
+
+		if (type == DIST_PUBLISH) {
+			write_lock_bh(&tipc_routetbl_lock);
+			publ = tipc_nameseq_insert_publ(route_table, 
+							TIPC_ROUTE, 
+							publ_info.lower, 
+							publ_info.upper,
+							publ_info.scope,
+							publ_info.node, 
+							publ_info.ref, 
+							publ_info.key);
+			if (publ) {
+				tipc_netsub_bind(&publ->subscr, publ->node,
+						 (net_ev_handler)route_purge_publ,
+						 publ);
+				tipc_all_routes++;
+				tipc_route_insert_publ(publ);
+			}
+			write_unlock_bh(&tipc_routetbl_lock);
+			/* TODO: See comment in corresponding place in
+			         tipc_named_recv(); it applies here too */
+			if (publ) {
+				if (dist_mask) {
+					tipc_route_distribute(&publ_info,
+							      DIST_PUBLISH,
+							      dist_mask);
+				}
+			}
+		}
+		else if (type == DIST_WITHDRAW) {
+			write_lock_bh(&tipc_routetbl_lock); 
+			publ = tipc_nameseq_remove_publ(route_table,
+							publ_info.lower,
+							publ_info.node, 
+							publ_info.ref, 
+							publ_info.key);
+			if (publ) {
+				tipc_netsub_unbind(&publ->subscr);
+				tipc_all_routes--;
+				tipc_route_remove_publ(publ);
+			}
+			write_unlock_bh(&tipc_routetbl_lock); 
+			if (publ) {
+				if (dist_mask) {
+					tipc_route_distribute(publ,
+							      DIST_WITHDRAW,
+							      dist_mask);
+				}
+				kfree(publ);
+			}
+		}
+		else if (type == DIST_PURGE) {
+			write_lock_bh(&tipc_routetbl_lock); 
+			tipc_routetbl_purge(publ_info.lower);
+			write_unlock_bh(&tipc_routetbl_lock);
+
+			if (dist_mask) {
+				tipc_route_distribute(&publ_info,
+						      DIST_PURGE,
+						      dist_mask);
+			}
+		} 
+		else {
+			dbg("Unrecognized routing table message received\n");
+		}
+
+		item += (item_size * sizeof(u32));
+	}
+	buf_discard(buf);
+}
+
diff -ruN linux-2.6.29/net/tipc/tipc_name_distr.h android_cluster/linux-2.6.29/net/tipc/tipc_name_distr.h
--- linux-2.6.29/net/tipc/tipc_name_distr.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_name_distr.h	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,64 @@
+/*
+ * net/tipc/tipc_name_distr.h: Include file for TIPC name distribution code
+ * 
+ * Copyright (c) 2000-2006, Ericsson AB
+ * Copyright (c) 2005-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_NAME_DISTR_H
+#define _TIPC_NAME_DISTR_H
+
+#include "tipc_name_table.h"
+
+/* Publication/route distribution masks */
+
+#define TIPC_DIST_TO_CLUSTER	0x01
+#define TIPC_DIST_TO_ZONE	0x02
+#define TIPC_DIST_TO_NETWORK	0x04
+
+void tipc_named_insert_publ(struct publication *publ);
+void tipc_named_remove_publ(struct publication *publ);
+void tipc_named_distribute(struct publication *publ, int msg_type,
+			   int dist_mask);
+void tipc_named_node_up(unsigned long node);
+void tipc_named_node_up_uni(unsigned long node);
+void tipc_named_recv(struct sk_buff *buf);
+void tipc_named_reinit(void);
+
+void tipc_route_insert_publ(struct publication *publ);
+void tipc_route_remove_publ(struct publication *publ);
+void tipc_route_distribute(struct publication *publ, int msg_type,
+			   int dist_mask);
+void tipc_route_node_up(unsigned long node);
+void tipc_route_recv(struct sk_buff *buf);
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_name_table.c android_cluster/linux-2.6.29/net/tipc/tipc_name_table.c
--- linux-2.6.29/net/tipc/tipc_name_table.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_name_table.c	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,1603 @@
+/*
+ * net/tipc/tipc_name_table.c: TIPC name table code
+ *
+ * Copyright (c) 2000-2006, Ericsson AB
+ * Copyright (c) 2004-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_cfgsrv.h"
+#include "tipc_dbg.h"
+#include "tipc_name_table.h"
+#include "tipc_name_distr.h"
+#include "tipc_addr.h"
+#include "tipc_net.h"
+#include "tipc_topsrv.h"
+#include "tipc_port.h"
+#include "tipc_bcast.h"
+
+/*
+ * NAME TABLE CODE
+ */
+
+static int tipc_nametbl_size = 1024;		/* must be a power of 2 */
+
+/**
+ * struct sub_seq - container for all published instances of a name sequence
+ * @lower: name sequence lower bound
+ * @upper: name sequence upper bound
+ * @node_list: circular list of publications made by own node
+ * @cluster_list: circular list of publications made by own cluster
+ * @zone_list: circular list of publications made by own zone
+ * @node_list_size: number of entries in "node_list"
+ * @cluster_list_size: number of entries in "cluster_list"
+ * @zone_list_size: number of entries in "zone_list"
+ * 
+ * Note: The zone list always contains at least one entry, since all
+ *       publications of the associated name sequence belong to it.
+ *       (The cluster and node lists may be empty.)
+ */
+
+struct sub_seq {
+	u32 lower;
+	u32 upper;
+	struct publication *node_list;
+	struct publication *cluster_list;
+	struct publication *zone_list;
+	u32 node_list_size;
+	u32 cluster_list_size;
+	u32 zone_list_size;
+};
+
+/**
+ * struct name_seq - container for all published instances of a name type
+ * @type: 32 bit 'type' value for name sequence
+ * @sseq: pointer to dynamically-sized array of sub-sequences of this 'type';
+ *        sub-sequences are sorted in ascending order
+ * @alloc: number of sub-sequences currently in array
+ * @first_free: array index of first unused sub-sequence entry
+ * @ns_list: links to adjacent name sequences in hash chain
+ * @subscriptions: list of subscriptions for this 'type'
+ * @lock: spinlock controlling access to publication lists of all sub-sequences
+ */
+
+struct name_seq {
+	u32 type;
+	struct sub_seq *sseqs;
+	u32 alloc;
+	u32 first_free;
+	struct hlist_node ns_list;
+	struct list_head subscriptions;
+	spinlock_t lock;
+};
+
+/**
+ * struct name_table - table containing all existing port name publications
+ * @types: pointer to fixed-sized array of name sequence lists,
+ *         accessed via hashing on 'type'; name sequence lists are *not* sorted
+ * @local_publ_count: number of publications issued by this node
+ */
+
+struct name_table {
+	struct hlist_head *types;
+	u32 local_publ_count;
+};
+
+static struct name_table table = { NULL } ;
+static atomic_t rsv_publ_ok = ATOMIC_INIT(0);
+DEFINE_RWLOCK(tipc_nametbl_lock);
+
+/*
+ * distribution mask array, subscripted by scope of associated publication
+ * (eg. TIPC_NODE_SCOPE); note that first array entry is unused
+ */
+
+static u32 dist_mask_for_scope[5] = {
+	0,
+	TIPC_DIST_TO_ZONE | TIPC_DIST_TO_CLUSTER,
+	TIPC_DIST_TO_CLUSTER,
+	0,
+	TIPC_DIST_TO_NETWORK | TIPC_DIST_TO_ZONE | TIPC_DIST_TO_CLUSTER
+	};
+
+static int hash(int x)
+{
+	return(x & (tipc_nametbl_size - 1));
+}
+
+/**
+ * publ_create - create a publication structure
+ */
+
+static struct publication *publ_create(u32 type, u32 lower, u32 upper,
+				       u32 scope, u32 node, u32 port_ref,
+				       u32 key)
+{
+	struct publication *publ = kzalloc(sizeof(*publ), GFP_ATOMIC);
+	if (publ == NULL) {
+		warn("Publication creation failure, no memory\n");
+		return NULL;
+	}
+
+	publ->type = type;
+	publ->lower = lower;
+	publ->upper = upper;
+	publ->scope = scope;
+	publ->node = node;
+	publ->ref = port_ref;
+	publ->key = key;
+	INIT_LIST_HEAD(&publ->distr_list);
+	INIT_LIST_HEAD(&publ->pport_list);
+	INIT_LIST_HEAD(&publ->subscr.sub_list);
+	return publ;
+}
+
+/**
+ * tipc_subseq_alloc - allocate a specified number of sub-sequence structures
+ */
+
+static struct sub_seq *tipc_subseq_alloc(u32 cnt)
+{
+	struct sub_seq *sseq = kcalloc(cnt, sizeof(struct sub_seq), GFP_ATOMIC);
+	return sseq;
+}
+
+/**
+ * tipc_nameseq_create - create a name sequence structure for the specified 'type'
+ *
+ * Allocates a single sub-sequence structure and sets it to all 0's.
+ */
+
+static struct name_seq *tipc_nameseq_create(u32 type, struct hlist_head *seq_head)
+{
+	struct name_seq *nseq = kzalloc(sizeof(*nseq), GFP_ATOMIC);
+	struct sub_seq *sseq = tipc_subseq_alloc(1);
+
+	if (!nseq || !sseq) {
+		warn("Name sequence creation failed, no memory\n");
+		kfree(nseq);
+		kfree(sseq);
+		return NULL;
+	}
+
+	spin_lock_init(&nseq->lock);
+	nseq->type = type;
+	nseq->sseqs = sseq;
+	dbg("tipc_nameseq_create(): nseq = %p, type %u, ssseqs %p, ff: %u\n",
+	    nseq, type, nseq->sseqs, nseq->first_free);
+	nseq->alloc = 1;
+	INIT_LIST_HEAD(&nseq->subscriptions);
+	hlist_add_head(&nseq->ns_list, seq_head);
+	return nseq;
+}
+
+/**
+ * nameseq_delete_check - deletes a name sequence structure if now unused
+ */
+
+static void nameseq_delete_check(struct name_seq *seq)
+{
+	if ((seq->first_free == 0) && list_empty(&seq->subscriptions)) {
+		hlist_del_init(&seq->ns_list);
+		kfree(seq->sseqs);
+		spin_lock_term(&seq->lock);
+		kfree(seq);
+	}
+}
+
+/**
+ * nameseq_find_subseq - find sub-sequence (if any) matching a name instance
+ *
+ * Very time-critical, so binary searches through sub-sequence array.
+ */
+
+static struct sub_seq *nameseq_find_subseq(struct name_seq *nseq,
+					   u32 instance)
+{
+	struct sub_seq *sseqs = nseq->sseqs;
+	int low = 0;
+	int high = nseq->first_free - 1;
+	int mid;
+
+	while (low <= high) {
+		mid = (low + high) / 2;
+		if (instance < sseqs[mid].lower)
+			high = mid - 1;
+		else if (instance > sseqs[mid].upper)
+			low = mid + 1;
+		else
+			return &sseqs[mid];
+	}
+	return NULL;
+}
+
+/**
+ * nameseq_locate_subseq - determine position of name instance in sub-sequence
+ *
+ * Returns index in sub-sequence array of the entry that contains the specified
+ * instance value; if no entry contains that value, returns the position
+ * where a new entry for it would be inserted in the array.
+ *
+ * Note: Similar to binary search code for locating a sub-sequence.
+ */
+
+static u32 nameseq_locate_subseq(struct name_seq *nseq, u32 instance)
+{
+	struct sub_seq *sseqs = nseq->sseqs;
+	int low = 0;
+	int high = nseq->first_free - 1;
+	int mid;
+
+	while (low <= high) {
+		mid = (low + high) / 2;
+		if (instance < sseqs[mid].lower)
+			high = mid - 1;
+		else if (instance > sseqs[mid].upper)
+			low = mid + 1;
+		else
+			return mid;
+	}
+	return low;
+}
+
+/**
+ * tipc_nameseq_insert_publ -
+ */
+
+struct publication *tipc_nameseq_insert_publ(struct name_seq *nseq,
+					     u32 type, u32 lower, u32 upper,
+					     u32 scope, u32 node, u32 port, u32 key)
+{
+	struct subscription *s;
+	struct subscription *st;
+	struct publication *publ;
+	struct sub_seq *sseq;
+	int created_subseq = 0;
+
+	sseq = nameseq_find_subseq(nseq, lower);
+	dbg("nameseq_ins: for seq %p, {%u,%u}, found sseq %p\n",
+	    nseq, type, lower, sseq);
+	if (sseq) {
+
+		/* Lower end overlaps existing entry => need an exact match */
+
+		if ((sseq->lower != lower) || (sseq->upper != upper)) {
+			warn("Cannot publish {%u,%u,%u}, overlap error\n",
+			     type, lower, upper);
+			return NULL;
+		}
+	} else {
+		u32 inspos;
+		struct sub_seq *freesseq;
+
+		/* Find where lower end should be inserted */
+
+		inspos = nameseq_locate_subseq(nseq, lower);
+
+		/* Fail if upper end overlaps into an existing entry */
+
+		if ((inspos < nseq->first_free) &&
+		    (upper >= nseq->sseqs[inspos].lower)) {
+			warn("Cannot publish {%u,%u,%u}, overlap error\n",
+			     type, lower, upper);
+			return NULL;
+		}
+
+		/* Ensure there is space for new sub-sequence */
+
+		if (nseq->first_free == nseq->alloc) {
+			struct sub_seq *sseqs = tipc_subseq_alloc(nseq->alloc * 2);
+
+			if (!sseqs) {
+				warn("Cannot publish {%u,%u,%u}, no memory\n",
+				     type, lower, upper);
+				return NULL;
+			}
+			dbg("Allocated %u more sseqs\n", nseq->alloc);
+			memcpy(sseqs, nseq->sseqs,
+			       nseq->alloc * sizeof(struct sub_seq));
+			kfree(nseq->sseqs);
+			nseq->sseqs = sseqs;
+			nseq->alloc *= 2;
+		}
+		dbg("Have %u sseqs for type %u\n", nseq->alloc, type);
+
+		/* Insert new sub-sequence */
+
+		dbg("ins in pos %u, ff = %u\n", inspos, nseq->first_free);
+		sseq = &nseq->sseqs[inspos];
+		freesseq = &nseq->sseqs[nseq->first_free];
+		memmove(sseq + 1, sseq, ((char *)freesseq - (char *)sseq));
+		memset(sseq, 0, sizeof (*sseq));
+		nseq->first_free++;
+		sseq->lower = lower;
+		sseq->upper = upper;
+		created_subseq = 1;
+	}
+	dbg("inserting {%u,%u,%u} from <0x%x:%u> into sseq %p(%u,%u) of seq %p\n",
+	    type, lower, upper, node, port, sseq,
+	    sseq->lower, sseq->upper, nseq);
+
+	/* Check if there already is an identical publication : */
+
+	publ = sseq->zone_list;
+	if (publ != NULL) do {
+
+		if ((publ->key == key) && (publ->ref == port) &&
+		    ((publ->node == node) || !publ->node))
+			return NULL;
+
+		publ = publ->zone_list_next;
+
+	} while (publ != sseq->zone_list);
+
+	/* Insert a publication: */
+
+	publ = publ_create(type, lower, upper, scope, node, port, key);
+	if (!publ)
+		return NULL;
+
+	sseq->zone_list_size++;
+	if (!sseq->zone_list)
+		sseq->zone_list = publ->zone_list_next = publ;
+	else {
+		publ->zone_list_next = sseq->zone_list->zone_list_next;
+		sseq->zone_list->zone_list_next = publ;
+	}
+
+	if (addr_in_cluster(node)) {
+		sseq->cluster_list_size++;
+		if (!sseq->cluster_list)
+			sseq->cluster_list = publ->cluster_list_next = publ;
+		else {
+			publ->cluster_list_next =
+			sseq->cluster_list->cluster_list_next;
+			sseq->cluster_list->cluster_list_next = publ;
+		}
+	}
+
+	if (addr_in_node(node)) {
+		sseq->node_list_size++;
+		if (!sseq->node_list)
+			sseq->node_list = publ->node_list_next = publ;
+		else {
+			publ->node_list_next = sseq->node_list->node_list_next;
+			sseq->node_list->node_list_next = publ;
+		}
+	}
+
+	/*
+	 * Any subscriptions waiting for notification?
+	 */
+	list_for_each_entry_safe(s, st, &nseq->subscriptions, nameseq_list) {
+		tipc_subscr_report_overlap(s,
+					   publ->lower,
+					   publ->upper,
+					   TIPC_PUBLISHED,
+					   publ->ref,
+					   publ->node,
+					   created_subseq);
+	}
+	return publ;
+}
+
+/**
+ * tipc_nameseq_remove_publ -
+ *
+ * NOTE: There may be cases where TIPC is asked to remove a publication
+ * that is not in the name table.  For example, if another node issues a
+ * publication for a name sequence that overlaps an existing name sequence
+ * the publication will not be recorded, which means the publication won't
+ * be found when the name sequence is later withdrawn by that node.
+ * A failed withdraw request simply returns a failure indication and lets the
+ * caller issue any error or warning messages associated with such a problem.
+ */
+
+struct publication *tipc_nameseq_remove_publ(struct name_seq *nseq, u32 inst,
+					     u32 node, u32 ref, u32 key)
+{
+	struct publication *publ;
+	struct publication *curr;
+	struct publication *prev;
+	struct sub_seq *sseq;
+	struct sub_seq *free;
+	struct subscription *s, *st;
+	int removed_subseq = 0;
+
+	sseq = nameseq_find_subseq(nseq, inst);
+	if (!sseq)
+		return NULL;
+
+	dbg("tipc_nameseq_remove_publ: seq: %p, sseq %p, {%u,%u}, key %u\n",
+	    nseq, sseq, nseq->type, inst, key);
+
+	/* Remove publication from zone scope list */
+
+	prev = sseq->zone_list;
+	if (prev == NULL)
+		return NULL;
+
+	publ = sseq->zone_list->zone_list_next;
+	while ((publ->key != key) || (publ->ref != ref) ||
+	       (publ->node && (publ->node != node))) {
+		prev = publ;
+		publ = publ->zone_list_next;
+		if (prev == sseq->zone_list) {
+
+			/* Prevent endless loop if publication not found */
+
+			return NULL;
+		}
+	}
+	if (publ != sseq->zone_list)
+		prev->zone_list_next = publ->zone_list_next;
+	else if (publ->zone_list_next != publ) {
+		prev->zone_list_next = publ->zone_list_next;
+		sseq->zone_list = publ->zone_list_next;
+	} else {
+		sseq->zone_list = NULL;
+	}
+	sseq->zone_list_size--;
+
+	/* Remove publication from cluster scope list, if present */
+
+	if (addr_in_cluster(node)) {
+		prev = sseq->cluster_list;
+		curr = sseq->cluster_list->cluster_list_next;
+		while (curr != publ) {
+			prev = curr;
+			curr = curr->cluster_list_next;
+			if (prev == sseq->cluster_list) {
+
+				/* Prevent endless loop for malformed list */
+
+				err("Unable to de-list cluster publication\n"
+				    "{%u%u}, node=0x%x, ref=%u, key=%u)\n",
+				    publ->type, publ->lower, publ->node,
+				    publ->ref, publ->key);
+				goto end_cluster;
+			}
+		}
+		if (publ != sseq->cluster_list)
+			prev->cluster_list_next = publ->cluster_list_next;
+		else if (publ->cluster_list_next != publ) {
+			prev->cluster_list_next = publ->cluster_list_next;
+			sseq->cluster_list = publ->cluster_list_next;
+		} else {
+			sseq->cluster_list = NULL;
+		}
+		sseq->cluster_list_size--;
+	}
+end_cluster:
+
+	/* Remove publication from node scope list, if present */
+
+	if (addr_in_node(node)) {
+		prev = sseq->node_list;
+		curr = sseq->node_list->node_list_next;
+		while (curr != publ) {
+			prev = curr;
+			curr = curr->node_list_next;
+			if (prev == sseq->node_list) {
+
+				/* Prevent endless loop for malformed list */
+
+				err("Unable to de-list node publication\n"
+				    "{%u%u}, node=0x%x, ref=%u, key=%u)\n",
+				    publ->type, publ->lower, publ->node,
+				    publ->ref, publ->key);
+				goto end_node;
+			}
+		}
+		if (publ != sseq->node_list)
+			prev->node_list_next = publ->node_list_next;
+		else if (publ->node_list_next != publ) {
+			prev->node_list_next = publ->node_list_next;
+			sseq->node_list = publ->node_list_next;
+		} else {
+			sseq->node_list = NULL;
+		}
+		sseq->node_list_size--;
+	}
+end_node:
+
+	/* Contract subseq list if no more publications for that subseq */
+
+	if (!sseq->zone_list) {
+		free = &nseq->sseqs[nseq->first_free--];
+		memmove(sseq, sseq + 1, ((char *)free - (char *)(sseq + 1)));
+		removed_subseq = 1;
+	}
+
+	/* Notify any waiting subscriptions */
+
+	list_for_each_entry_safe(s, st, &nseq->subscriptions, nameseq_list) {
+		tipc_subscr_report_overlap(s,
+					   publ->lower,
+					   publ->upper,
+					   TIPC_WITHDRAWN,
+					   publ->ref,
+					   publ->node,
+					   removed_subseq);
+	}
+
+	return publ;
+}
+
+/**
+ * nameseq_subscribe: attach a subscription, and issue
+ * the prescribed number of events if there is any sub-
+ * sequence overlapping with the requested sequence
+ */
+
+static void nameseq_subscribe(struct name_seq *nseq, struct subscription *s)
+{
+	struct sub_seq *sseq = nseq->sseqs;
+
+	list_add(&s->nameseq_list, &nseq->subscriptions);
+
+	if (!sseq)
+		return;
+
+	while (sseq != &nseq->sseqs[nseq->first_free]) {
+		struct publication *zl = sseq->zone_list;
+		if (zl && tipc_subscr_overlap(s,sseq->lower,sseq->upper)) {
+			struct publication *crs = zl;
+			int must_report = 1;
+
+			do {
+				tipc_subscr_report_overlap(s,
+							   sseq->lower,
+							   sseq->upper,
+							   TIPC_PUBLISHED,
+							   crs->ref,
+							   crs->node,
+							   must_report);
+				must_report = 0;
+				crs = crs->zone_list_next;
+			} while (crs != zl);
+		}
+		sseq++;
+	}
+}
+
+static struct name_seq *nametbl_find_seq(u32 type)
+{
+	struct hlist_head *seq_head;
+	struct hlist_node *seq_node;
+	struct name_seq *ns;
+
+	dbg("find_seq %u,(%u,0x%x) table = %p, hash[type] = %u\n",
+	    type, ntohl(type), type, table.types, hash(type));
+
+	seq_head = &table.types[hash(type)];
+	hlist_for_each_entry(ns, seq_node, seq_head, ns_list) {
+		if (ns->type == type) {
+			dbg("found %p\n", ns);
+			return ns;
+		}
+	}
+
+	return NULL;
+};
+
+struct publication *tipc_nametbl_insert_publ(u32 type, u32 lower, u32 upper,
+					     u32 scope, u32 node, u32 port, 
+					     u32 key)
+{
+	struct name_seq *seq = nametbl_find_seq(type);
+
+	dbg("tipc_nametbl_insert_publ: {%u,%u,%u} found %p\n", type, lower, upper, seq);
+	if (lower > upper) {
+		warn("Failed to publish illegal {%u,%u,%u}\n",
+		     type, lower, upper);
+		return NULL;
+	}
+
+	dbg("Publishing {%u,%u,%u} from 0x%x\n", type, lower, upper, node);
+	if (!seq) {
+		seq = tipc_nameseq_create(type, &table.types[hash(type)]);
+		dbg("tipc_nametbl_insert_publ: created %p\n", seq);
+	}
+	if (!seq)
+		return NULL;
+
+	return tipc_nameseq_insert_publ(seq, type, lower, upper,
+					scope, node, port, key);
+}
+
+struct publication *tipc_nametbl_remove_publ(u32 type, u32 lower,
+					     u32 node, u32 ref, u32 key)
+{
+	struct publication *publ;
+	struct name_seq *seq = nametbl_find_seq(type);
+
+	if (!seq)
+		return NULL;
+
+	dbg("Withdrawing {%u,%u} from 0x%x\n", type, lower, node);
+	publ = tipc_nameseq_remove_publ(seq, lower, node, ref, key);
+	nameseq_delete_check(seq);
+	return publ;
+}
+
+/**
+ * tipc_nametbl_translate - perform name translation
+ *
+ * On entry, 'destnode' is the search domain used during translation.
+ *
+ * On exit:
+ * - if name translation is deferred to another node/cluster/zone,
+ *   leaves 'destnode' unchanged (will be non-zero) and returns 0
+ * - if name translation is attempted and succeeds, sets 'destnode'
+ *   to publishing node and returns port reference (will be non-zero)
+ * - if name translation is attempted and fails, sets 'destnode' to 0
+ *   and returns 0
+ */
+
+u32 tipc_nametbl_translate(u32 type, u32 instance, u32 *destnode)
+{
+	struct sub_seq *sseq;
+	struct publication *publ;
+	struct name_seq *seq;
+	u32 ref;
+
+	if (!tipc_in_scope(*destnode, tipc_own_addr))
+		return 0;
+
+	read_lock_bh(&tipc_nametbl_lock);
+	seq = nametbl_find_seq(type);
+	if (unlikely(!seq))
+		goto not_found;
+	sseq = nameseq_find_subseq(seq, instance);
+	if (unlikely(!sseq))
+		goto not_found;
+	spin_lock_bh(&seq->lock);
+
+	/* Closest-First Algorithm */
+
+	if (likely(*destnode == 0)) {
+		publ = sseq->node_list;
+		if (publ) {
+			sseq->node_list = publ->node_list_next;
+found:
+			ref = publ->ref;
+			*destnode = publ->node;
+			spin_unlock_bh(&seq->lock);
+			read_unlock_bh(&tipc_nametbl_lock);
+			return ref;
+		}
+		publ = sseq->cluster_list;
+		if (publ) {
+			sseq->cluster_list = publ->cluster_list_next;
+			goto found;
+		}
+		publ = sseq->zone_list;
+		sseq->zone_list = publ->zone_list_next;
+		goto found;
+	}
+
+	/* Round-Robin Algorithm */
+
+	else if (*destnode == tipc_own_addr) {
+		publ = sseq->node_list;
+		if (publ) {
+			sseq->node_list = publ->node_list_next;
+			goto found;
+		}
+	} else if (in_own_cluster(*destnode)) {
+		publ = sseq->cluster_list;
+		if (publ) {
+			sseq->cluster_list = publ->cluster_list_next;
+			goto found;
+		}
+	} else {
+		publ = sseq->zone_list;
+		sseq->zone_list = publ->zone_list_next;
+		goto found;
+	}
+	spin_unlock_bh(&seq->lock);
+not_found:
+	read_unlock_bh(&tipc_nametbl_lock);
+	*destnode = 0;
+	return 0;
+}
+
+/**
+ * tipc_nametbl_mc_translate - find multicast destinations
+ *
+ * Creates list of all local ports that overlap the given multicast address;
+ * also determines if any off-node ports overlap.
+ *
+ * Note: Publications with a scope narrower than 'limit' are ignored.
+ * (i.e. local node-scope publications mustn't receive messages arriving
+ * from another node, even if the multcast link brought it here)
+ *
+ * Returns non-zero if any off-node ports overlap
+ */
+
+int tipc_nametbl_mc_translate(u32 type, u32 lower, u32 upper, u32 limit,
+			      struct port_list *dports)
+{
+	struct name_seq *seq;
+	struct sub_seq *sseq;
+	struct sub_seq *sseq_stop;
+	int res = 0;
+
+	read_lock_bh(&tipc_nametbl_lock);
+	seq = nametbl_find_seq(type);
+	if (!seq)
+		goto exit;
+
+	spin_lock_bh(&seq->lock);
+
+	sseq = seq->sseqs + nameseq_locate_subseq(seq, lower);
+	sseq_stop = seq->sseqs + seq->first_free;
+	for (; sseq != sseq_stop; sseq++) {
+		struct publication *publ;
+
+		if (sseq->lower > upper)
+			break;
+
+		publ = sseq->node_list;
+		if (publ) {
+			do {
+				if (publ->scope <= limit)
+					tipc_port_list_add(dports, publ->ref);
+				publ = publ->node_list_next;
+			} while (publ != sseq->node_list);
+		}
+
+		if (sseq->cluster_list_size != sseq->node_list_size)
+			res = 1;
+	}
+
+	spin_unlock_bh(&seq->lock);
+exit:
+	read_unlock_bh(&tipc_nametbl_lock);
+	return res;
+}
+
+/**
+ * tipc_publish_rsv - publish port name using a reserved name type
+ */
+
+int tipc_publish_rsv(u32 ref, unsigned int scope, 
+		     struct tipc_name_seq const *seq)
+{
+	int res;
+
+	atomic_inc(&rsv_publ_ok);
+	res = tipc_publish(ref, scope, seq);
+	atomic_dec(&rsv_publ_ok);
+	return res;
+}
+
+
+/**
+ * tipc_nametbl_publish - add name publication to network name tables, 
+ *                        but first check permissions
+ */
+
+struct publication *tipc_nametbl_publish(u32 type, u32 lower, u32 upper, 
+					 u32 scope, u32 port_ref, u32 key)
+{
+	if ((type < TIPC_RESERVED_TYPES) && !atomic_read(&rsv_publ_ok)) {
+		warn("Failed to publish reserved name <%u,%u,%u>\n",
+		     type, lower, upper);
+		return NULL;
+	}
+	return tipc_nametbl_publish_rsv(type, lower, upper, scope, port_ref, key);
+}
+
+/**
+ * tipc_nametbl_publish_rsv - add name publication to network name tables,
+ *                            without checking for permissions
+ */
+
+struct publication *tipc_nametbl_publish_rsv(u32 type, u32 lower, u32 upper, 
+					     u32 scope, u32 port_ref, u32 key)
+{
+	struct publication *publ;
+
+	if (table.local_publ_count >= tipc_max_publications) {
+		warn("Publication failed, local publication limit reached (%u)\n", 
+		     tipc_max_publications);
+		return NULL;
+	}
+
+	write_lock_bh(&tipc_nametbl_lock);
+	publ = tipc_nametbl_insert_publ(type, lower, upper, scope,
+					tipc_own_addr, port_ref, key);
+	if (likely(publ)) {
+		table.local_publ_count++;
+		tipc_named_insert_publ(publ);
+	}
+	write_unlock_bh(&tipc_nametbl_lock);
+
+	if (likely(publ)) {
+		tipc_named_distribute(publ, DIST_PUBLISH,
+				      dist_mask_for_scope[publ->scope]);
+	}
+	return publ;
+}
+
+/**
+ * tipc_nametbl_withdraw - withdraw name publication from network name tables
+ */
+
+void tipc_nametbl_withdraw(u32 type, u32 lower, u32 ref, u32 key)
+{
+	struct publication *publ;
+
+	write_lock_bh(&tipc_nametbl_lock);
+	publ = tipc_nametbl_remove_publ(type, lower, tipc_own_addr, ref, key);
+	if (likely(publ)) {
+		table.local_publ_count--;
+		tipc_named_remove_publ(publ);
+	}
+	write_unlock_bh(&tipc_nametbl_lock);
+
+	if (likely(publ)) {
+		list_del_init(&publ->pport_list);
+		tipc_named_distribute(publ, DIST_WITHDRAW,
+				      dist_mask_for_scope[publ->scope]);
+		kfree(publ);
+	} else {
+		err("Unable to remove local publication\n"
+		    "(type=%u, lower=%u, ref=%u, key=%u)\n",
+		    type, lower, ref, key);
+	}
+}
+
+/**
+ * tipc_nametbl_subscribe - add a subscription object to the name table
+ */
+
+void tipc_nametbl_subscribe(struct subscription *s)
+{
+	u32 type = s->seq.type;
+	struct name_seq *seq;
+
+	write_lock_bh(&tipc_nametbl_lock);
+	seq = nametbl_find_seq(type);
+	if (!seq) {
+		seq = tipc_nameseq_create(type, &table.types[hash(type)]);
+	}
+	if (seq) {
+		spin_lock_bh(&seq->lock);
+		dbg_assert(seq->type == type);
+		nameseq_subscribe(seq, s);
+		spin_unlock_bh(&seq->lock);
+	} else {
+		warn("Failed to create subscription for {%u,%u,%u}\n",
+		     s->seq.type, s->seq.lower, s->seq.upper);
+	}
+	write_unlock_bh(&tipc_nametbl_lock);
+}
+
+/**
+ * tipc_nametbl_unsubscribe - remove a subscription object from name table
+ */
+
+void tipc_nametbl_unsubscribe(struct subscription *s)
+{
+	struct name_seq *seq;
+
+	write_lock_bh(&tipc_nametbl_lock);
+	seq = nametbl_find_seq(s->seq.type);
+	if (seq != NULL) {
+		spin_lock_bh(&seq->lock);
+		list_del_init(&s->nameseq_list);
+		spin_unlock_bh(&seq->lock);
+		nameseq_delete_check(seq);
+	}
+	write_unlock_bh(&tipc_nametbl_lock);
+}
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+/**
+ * nametbl_sseq_list: print specified sub-sequence contents into the given buffer
+ */
+
+static void nametbl_sseq_list(struct sub_seq *sseq, struct print_buf *buf,
+			      u32 depth, u32 index)
+{
+	static char *scope_str[] =
+		{ "", " zone", " cluster", " node", " network" };
+
+	char port_id_str[27];
+	struct publication *publ = sseq->zone_list;
+
+	tipc_printf(buf, "%-10u %-10u ", sseq->lower, sseq->upper);
+
+	if (depth == 2 || !publ) {
+		tipc_printf(buf, "\n");
+		return;
+	}
+
+	do {
+		sprintf(port_id_str, "<%u.%u.%u:%u>",
+			tipc_zone(publ->node), tipc_cluster(publ->node),
+			tipc_node(publ->node), publ->ref);
+		tipc_printf(buf, "%-26s ", port_id_str);
+		if (depth > 3) {
+			tipc_printf(buf, "%-10u %s", publ->key, 
+				    scope_str[publ->scope]);
+		}
+
+		publ = publ->zone_list_next;
+		if (publ == sseq->zone_list)
+			break;
+
+		tipc_printf(buf, "\n%33s", " ");
+	} while (1);
+
+	tipc_printf(buf, "\n");
+}
+
+/**
+ * nametbl_seq_list: print specified name sequence contents into the given buffer
+ */
+
+static void nametbl_seq_list(struct name_seq *seq, struct print_buf *buf,
+			     u32 depth, u32 type, u32 lowbound, u32 upbound,
+			     u32 index)
+{
+	struct sub_seq *sseq;
+	char typearea[11];
+
+	if (seq->first_free == 0)
+		return;
+
+	sprintf(typearea, "%-10u", seq->type);
+
+	if (depth == 1) {
+		tipc_printf(buf, "%s\n", typearea);
+		return;
+	}
+
+	for (sseq = seq->sseqs; sseq != &seq->sseqs[seq->first_free]; sseq++) {
+		if ((lowbound <= sseq->upper) && (upbound >= sseq->lower)) {
+			tipc_printf(buf, "%s ", typearea);
+			spin_lock_bh(&seq->lock);
+			nametbl_sseq_list(sseq, buf, depth, index);
+			spin_unlock_bh(&seq->lock);
+			sprintf(typearea, "%10s", " ");
+		}
+	}
+}
+
+/**
+ * nametbl_header - print name table header into the given buffer
+ */
+
+static void nametbl_header(struct print_buf *buf, u32 depth)
+{
+	static char *header[] = {
+		"Type       ",
+		"Lower      Upper      ",
+		"Port Identity              ",
+		"Publication Scope"
+	};
+
+	int i;
+
+	if (depth > 4)
+		depth = 4;
+	for (i = 0; i < depth; i++)
+		tipc_printf(buf, header[i]);
+	tipc_printf(buf, "\n");
+}
+
+/**
+ * nametbl_list - print specified name table contents into the given buffer
+ */
+
+static void nametbl_list(struct print_buf *buf, u32 depth_info,
+			 u32 type, u32 lowbound, u32 upbound)
+{
+	struct hlist_head *seq_head;
+	struct hlist_node *seq_node;
+	struct name_seq *seq;
+	int all_types;
+	u32 depth;
+	u32 i;
+
+	all_types = (depth_info & TIPC_NTQ_ALLTYPES);
+	depth = (depth_info & ~TIPC_NTQ_ALLTYPES);
+
+	if (depth == 0)
+		return;
+
+	if (all_types) {
+		/* display all entries in name table to specified depth */
+		nametbl_header(buf, depth);
+		lowbound = 0;
+		upbound = ~0;
+		for (i = 0; i < tipc_nametbl_size; i++) {
+			seq_head = &table.types[i];
+			hlist_for_each_entry(seq, seq_node, seq_head, ns_list) {
+				nametbl_seq_list(seq, buf, depth, seq->type, 
+						 lowbound, upbound, i);
+			}
+		}
+	} else {
+		/* display only the sequence that matches the specified type */
+		if (upbound < lowbound) {
+			tipc_printf(buf, "invalid name sequence specified\n");
+			return;
+		}
+		nametbl_header(buf, depth);
+		i = hash(type);
+		seq_head = &table.types[i];
+		hlist_for_each_entry(seq, seq_node, seq_head, ns_list) {
+			if (seq->type == type) {
+				nametbl_seq_list(seq, buf, depth, type, 
+						 lowbound, upbound, i);
+				break;
+			}
+		}
+	}
+}
+
+#if 0
+void tipc_nametbl_print(struct print_buf *buf, const char *str)
+{
+	tipc_printf(buf, str);
+	read_lock_bh(&tipc_nametbl_lock);
+	nametbl_list(buf, 0, 0, 0, 0);
+	read_unlock_bh(&tipc_nametbl_lock);
+}
+#endif
+
+#define MAX_NAME_TBL_QUERY 32768
+
+struct sk_buff *tipc_nametbl_get(const void *req_tlv_area, int req_tlv_space)
+{
+	struct sk_buff *buf;
+	struct tipc_name_table_query *argv;
+	struct tlv_desc *rep_tlv;
+	struct print_buf b;
+	int str_len;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_NAME_TBL_QUERY))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	buf = tipc_cfg_reply_alloc(TLV_SPACE(MAX_NAME_TBL_QUERY));
+	if (!buf)
+		return NULL;
+
+	rep_tlv = (struct tlv_desc *)buf->data;
+	tipc_printbuf_init(&b, TLV_DATA(rep_tlv), MAX_NAME_TBL_QUERY);
+	argv = (struct tipc_name_table_query *)TLV_DATA(req_tlv_area);
+	read_lock_bh(&tipc_nametbl_lock);
+	nametbl_list(&b, ntohl(argv->depth), ntohl(argv->type),
+		     ntohl(argv->lowbound), ntohl(argv->upbound));
+	read_unlock_bh(&tipc_nametbl_lock);
+	str_len = tipc_printbuf_validate(&b);
+
+	skb_put(buf, TLV_SPACE(str_len));
+	TLV_SET(rep_tlv, TIPC_TLV_ULTRA_STRING, NULL, str_len);
+
+	return buf;
+}
+
+#if 0
+void tipc_nametbl_dump(void)
+{
+	nametbl_list(TIPC_CONS, 0, 0, 0, 0);
+}
+#endif
+
+#endif
+
+int tipc_nametbl_init(void)
+{
+	table.types = kcalloc(tipc_nametbl_size, sizeof(struct hlist_head),
+			      GFP_ATOMIC);
+	if (!table.types)
+		return -ENOMEM;
+
+	table.local_publ_count = 0;
+	return 0;
+}
+
+void tipc_nametbl_stop(void)
+{
+	u32 i;
+
+	if (!table.types)
+		return;
+
+	/* Verify name table is empty, then release it */
+
+	write_lock_bh(&tipc_nametbl_lock);
+	for (i = 0; i < tipc_nametbl_size; i++) {
+		if (!hlist_empty(&table.types[i]))
+			err("tipc_nametbl_stop(): hash chain %u is non-null\n", i);
+	}
+	kfree(table.types);
+	table.types = NULL;
+	write_unlock_bh(&tipc_nametbl_lock);
+}
+
+
+/*
+ * ROUTING TABLE CODE
+ */
+
+struct name_seq *route_table = NULL;
+int tipc_own_routes = 0;
+int tipc_all_routes = 0;
+
+static struct subscription region_subscr;
+
+DEFINE_RWLOCK(tipc_routetbl_lock);
+
+/**
+ * net_region_event - handle cleanup when access to a region is lost
+ */
+
+static void net_region_event(struct subscription *sub, 
+			     u32 found_lower, u32 found_upper,
+			     u32 event, u32 port_ref, u32 node)
+{
+	struct net_element *region;
+
+	if (event == TIPC_WITHDRAWN) {
+
+		/* 
+		 * Purge name table entries/connections associated with region,
+		 * if any
+		 */
+
+		region = tipc_net_lookup_element(found_lower, &tipc_regions);
+		if (region != NULL) {
+			net_element_lock(region);
+			tipc_netsub_notify(region, found_lower);
+			net_element_unlock(region);
+		}
+
+		/*
+		 * Notify other zones if connectivity to another cluster
+		 * in our own zone is lost 
+		 */
+
+		if (in_own_zone(found_lower)) {
+			struct publication publ;
+
+			publ.type = TIPC_ROUTE;
+			publ.lower = found_lower;
+			publ.upper = found_lower;
+			publ.scope = TIPC_NETWORK_SCOPE;
+			publ.node = tipc_own_addr;
+			publ.ref = 0;
+			publ.key = 0;
+			tipc_route_distribute(&publ, DIST_PURGE,
+					      TIPC_DIST_TO_NETWORK);
+		}
+	}
+}
+
+int tipc_routetbl_init(void)
+{
+	struct sub_seq *sseq;
+
+	route_table = kzalloc(sizeof(struct name_seq), GFP_ATOMIC);
+	sseq = tipc_subseq_alloc(1);
+	if ((route_table == NULL) || (sseq == NULL)) {
+		kfree(route_table);
+		kfree(sseq);
+		return -ENOMEM;
+	}
+
+	route_table->sseqs = sseq;
+	route_table->alloc = 1;
+	INIT_LIST_HEAD(&route_table->subscriptions);
+	spin_lock_init(&route_table->lock);
+
+	/* TODO: Is there a good reason why we need tipc_routetbl_lock and
+	   the lock that is part of the seq entry used by the routing table? */
+
+	region_subscr.seq.type = TIPC_ROUTE;
+	region_subscr.seq.lower = tipc_addr(1, 1, 0);
+	region_subscr.seq.upper = tipc_addr(255, 4095, 0);
+	region_subscr.timeout = TIPC_WAIT_FOREVER;
+	region_subscr.filter = TIPC_SUB_SERVICE;
+	region_subscr.event_cb = net_region_event;
+
+	nameseq_subscribe(route_table, &region_subscr);
+	return 0;
+}
+
+void tipc_routetbl_stop(void)
+{
+	if (route_table == NULL)
+		return;
+
+	/* Verify routing table is empty, then release it */
+
+	write_lock_bh(&tipc_routetbl_lock);
+	if (route_table->first_free != 0)
+		err("tipc_routetbl_stop(): routing table has %u entries\n",
+		    route_table->first_free);
+	kfree(route_table->sseqs);
+	spin_lock_term(&route_table->lock);
+	kfree(route_table);
+	route_table = NULL;
+	write_unlock_bh(&tipc_routetbl_lock);
+}
+
+/**
+ * tipc_routetbl_translate - determine best route to out-of-cluster target
+ * @target: <Z.C.N> of destination (may be a node, cluster, or zone)
+ *
+ * Returns <Z.C.N> of next hop in route (or 0 if unable to find a route)
+ */
+
+u32 tipc_routetbl_translate(u32 target)
+{
+	struct name_seq *seq;
+	struct sub_seq *sseq;
+	struct publication *publ;
+	struct publication *publ_start;
+	u32 target_region;
+	u32 target_cluster;
+	u32 router;
+	int best_dist;
+	int curr_dist;
+
+	dbg_assert(tipc_addr_domain_valid(target));
+
+	/* 
+	 * Locate name table entry associated with target region;
+	 * for a target within this node's zone, target region is its cluster,
+	 * while for a target in another zone, target region is its zone
+	 */
+
+	read_lock_bh(&tipc_routetbl_lock);
+
+	seq = route_table;
+
+	if (likely(in_own_zone(target)))
+		target_region = addr_cluster(target);
+	else
+		target_region = addr_zone(target);
+restart:
+	sseq = nameseq_find_subseq(seq, target_region);
+	if (unlikely(!sseq)) {
+		read_unlock_bh(&tipc_routetbl_lock);
+		return 0;
+	}
+
+	/*
+	 * Note: Don't need to spin_lock_bh(&seq->lock) since routes
+	 * only change when tipc_routetbl_lock is write-locked
+	 */
+
+	target_cluster = addr_cluster(target);
+	best_dist = 4;
+	router = 0;
+
+	/*
+	 * If own node has a direct link to target region,
+	 * pick the route that gets us closest to the target itself
+	 */
+
+	if (sseq->node_list) {
+		publ_start = sseq->node_list;
+		publ = publ_start->node_list_next;
+		do {
+			if (publ->ref == target) {
+				router = publ->ref;
+				goto found;
+			} else if (addr_cluster(publ->ref) == target_cluster)
+				curr_dist = 2;
+			else
+				curr_dist = 3;
+
+			if (curr_dist < best_dist) {
+				best_dist = curr_dist;
+				router = publ->ref;
+			}
+			/* TODO: ADD LOAD SHARING IF curr_dist == best_dist */
+
+			publ = publ->node_list_next;
+		} while (publ != publ_start->node_list_next);
+		goto found;
+	}
+
+	/*
+	 * If any cluster node has a direct link to target region,
+	 * pick the route that gets us closest to the target itself
+	 */
+
+	if (sseq->cluster_list) {
+		publ_start = sseq->cluster_list;
+		publ = publ_start->cluster_list_next;
+		do {
+			if (publ->ref == target)
+				curr_dist = 1;
+			else if (addr_cluster(publ->ref) == target_cluster)
+				curr_dist = 2;
+			else
+				curr_dist = 3;
+
+			if (curr_dist < best_dist) {
+				best_dist = curr_dist;
+				router = publ->node;
+			}
+			/* TODO: ADD LOAD SHARING IF curr_dist == best_dist */
+
+			publ = publ->cluster_list_next;
+		} while (publ != publ_start->cluster_list_next);
+		goto found;
+	}
+
+	/*
+	 * Look at all non-cluster nodes having a direct link to target region
+	 * (there must be one), find the one that gets us closest to the target,
+	 * then find the best route to that non-cluster node
+	 */
+
+	publ_start = sseq->zone_list;
+	publ = publ_start->zone_list_next;
+	do {
+		if (publ->ref == target)
+			curr_dist = 1;
+		else if (addr_cluster(publ->ref) == target_cluster)
+			curr_dist = 2;
+		else
+			curr_dist = 3;
+
+		if (curr_dist < best_dist) {
+			best_dist = curr_dist;
+			router = publ->node;
+		}
+		/* TODO: ADD LOAD SHARING IF curr_dist == best_dist */
+
+		publ = publ->zone_list_next;
+	} while (publ != publ_start->zone_list_next);
+
+	target = router;
+	target_region = addr_cluster(target);
+	goto restart;
+	
+found:
+	read_unlock_bh(&tipc_routetbl_lock);
+	return router;
+}
+
+/**
+ * tipc_routetbl_publish - publish route to neighboring node  
+ */
+
+void tipc_routetbl_publish(unsigned long node_addr)
+{
+	struct publication *publ;
+	u32 elm_addr;
+	int scope;
+	int dist_mask;
+
+	if (in_own_zone(node_addr)) {
+		elm_addr = addr_cluster(node_addr);
+		scope = TIPC_CLUSTER_SCOPE;
+		dist_mask = TIPC_DIST_TO_CLUSTER;
+	} else {
+		elm_addr = addr_zone(node_addr);
+		scope = TIPC_ZONE_SCOPE;
+		dist_mask = (TIPC_DIST_TO_ZONE | TIPC_DIST_TO_CLUSTER);
+	}
+
+	write_lock_bh(&tipc_routetbl_lock);
+	publ = tipc_nameseq_insert_publ(route_table, TIPC_ROUTE, 
+					elm_addr, elm_addr, scope,
+					tipc_own_addr, node_addr, 0);
+	if (likely(publ)) {
+		tipc_own_routes++;
+		tipc_all_routes++;
+		tipc_route_insert_publ(publ);
+	}
+	write_unlock_bh(&tipc_routetbl_lock);
+
+	if (likely(publ)) {
+		tipc_route_distribute(publ, DIST_PUBLISH, dist_mask);
+	}
+}
+
+/**
+ * tipc_routetbl_withdraw - withdraw route to neighboring node  
+ */
+
+void tipc_routetbl_withdraw(unsigned long node_addr)
+{
+	struct publication *publ;
+	u32 elm_addr;
+	int dist_mask;
+
+	if (in_own_zone(node_addr)) {
+		elm_addr = addr_cluster(node_addr);
+		dist_mask = TIPC_DIST_TO_CLUSTER;
+	} else {
+		elm_addr = addr_zone(node_addr);
+		dist_mask = (TIPC_DIST_TO_CLUSTER | TIPC_DIST_TO_ZONE);
+	}
+
+	write_lock_bh(&tipc_routetbl_lock);
+	publ = tipc_nameseq_remove_publ(route_table, elm_addr, tipc_own_addr,
+					node_addr, 0);
+	if (likely(publ)) {
+		tipc_own_routes--;
+		tipc_all_routes--;
+		tipc_route_remove_publ(publ);
+	}
+	write_unlock_bh(&tipc_routetbl_lock);
+
+	if (likely(publ)) {
+		tipc_route_distribute(publ, DIST_WITHDRAW, dist_mask);
+		kfree(publ);
+	} else {
+		err("Unable to remove local route\n"
+		    "(region=0x%08x, local router=0x%08x, remote router=0x%08x)\n",
+		    elm_addr, tipc_own_addr, node_addr);
+	}
+}
+
+/**
+ * tipc_routetbl_withdraw_node - trigger implied withdrawal
+ */
+
+void tipc_routetbl_withdraw_node(unsigned long node_addr)
+{
+	struct publication publ;
+
+	publ.type = TIPC_ROUTE;
+	publ.lower = node_addr;
+	publ.upper = node_addr;
+	publ.scope = TIPC_NETWORK_SCOPE;
+	publ.node = tipc_own_addr;
+	publ.ref = 0;
+	publ.key = 0;
+	tipc_route_distribute(&publ, DIST_PURGE,
+			      (TIPC_DIST_TO_ZONE | TIPC_DIST_TO_NETWORK));
+}
+
+/**
+ * tipc_routetbl_purge - notify subscribers of lost region that it is gone
+ * 
+ * Unlike tipc_routetbl_withdraw(), this routine does not actually remove
+ * the associated routing table entry.
+ */
+
+void tipc_routetbl_purge(u32 region_addr)
+{
+	struct net_element *region;
+	u32 elm_addr;
+
+	if (in_own_zone(region_addr)) {
+		elm_addr = addr_cluster(region_addr);
+	} else {
+		elm_addr = addr_zone(region_addr);
+	}
+
+	region = tipc_net_lookup_element(elm_addr, &tipc_regions);
+	if (region != NULL) {
+		net_element_lock(region);
+		tipc_netsub_notify(region, region_addr);
+		net_element_unlock(region);
+	}
+}
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+/**
+ * tipc_nametbl_get_routes - return info on available routes to target
+ */
+
+struct sk_buff *tipc_nametbl_get_routes(const void *req_tlv_area,
+					int req_tlv_space)
+{
+	u32 target;
+	u32 payload_size;
+	struct sk_buff *buf;
+	struct sub_seq *sseq;
+	struct publication *publ;
+	struct publication *publ_start;
+	struct tipc_route_info route_info;
+	int i;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_NET_ADDR))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	target = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	if (!tipc_addr_domain_valid(target))
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+						   " (network address)");
+
+	if (!in_own_zone(target))
+		target = addr_zone(target);
+	else if (!in_own_cluster(target))
+		target = addr_cluster(target);
+	else
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+			 " (network address must be outside own cluster)");
+
+	read_lock_bh(&tipc_routetbl_lock);
+
+	/* Allocate space for all known routes */
+
+	payload_size = TLV_SPACE(sizeof(route_info)) * tipc_all_routes;
+	if (payload_size > 32768u) {
+		read_unlock_bh(&tipc_routetbl_lock);
+		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
+						   " (too many routes)");
+	}
+	buf = tipc_cfg_reply_alloc(payload_size);
+	if (!buf) {
+		read_unlock_bh(&tipc_routetbl_lock);
+		return NULL;
+	}
+
+	/* Add TLVs for each route to specified target domain */
+
+	for (i = 0; i < route_table->first_free; i++) {
+		sseq = &route_table->sseqs[i];
+
+		if (!tipc_in_scope(target, sseq->lower))
+			continue;
+
+		/*
+		 * No need to take spinlock on zone list, since the structure
+		 * of the circular list can't change (only the starting point),
+		 * & a change to its start is an [atomic] pointer update ...
+		 */
+
+		route_info.remote_addr = htonl(sseq->lower);
+		publ_start = sseq->zone_list;
+		publ = publ_start->zone_list_next;
+		do {
+			route_info.local_router = htonl(publ->node);
+			route_info.remote_router = htonl(publ->ref);
+			tipc_cfg_append_tlv(buf, TIPC_TLV_ROUTE_INFO, 
+					    &route_info, sizeof(route_info));
+			publ = publ->zone_list_next;
+		} while (publ != publ_start->zone_list_next);
+	}
+
+	read_unlock_bh(&tipc_routetbl_lock);
+	return buf;
+}
+
+#endif
+
diff -ruN linux-2.6.29/net/tipc/tipc_name_table.h android_cluster/linux-2.6.29/net/tipc/tipc_name_table.h
--- linux-2.6.29/net/tipc/tipc_name_table.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_name_table.h	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,134 @@
+/*
+ * net/tipc/tipc_name_table.h: Include file for TIPC name table code
+ *
+ * Copyright (c) 2000-2006, Ericsson AB
+ * Copyright (c) 2004-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_NAME_TABLE_H
+#define _TIPC_NAME_TABLE_H
+
+#include "tipc_net.h"
+#include "tipc_bcast.h"
+#include "tipc_topsrv.h"
+
+/*
+ * TIPC name types reserved for internal TIPC use (both current and planned)
+ */
+
+#define TIPC_ZM_SRV 3  		/* zone master service name type */
+
+
+/**
+ * struct publication - info about a published (name or) name sequence
+ * @type: name sequence type
+ * @lower: name sequence lower bound
+ * @upper: name sequence upper bound
+ * @scope: scope of publication
+ * @node: network address of publishing port's node
+ * @ref: publishing port
+ * @key: publication key
+ * @subscr: network element subscription (used to withdraw unreachable names)
+ * @distr_list: adjacent entries in list of publications with same distribution needs
+ * @pport_list: adjacent entries in list of publications made by this port
+ * @node_list_next: next matching name seq publication with >= node scope
+ * @cluster_list_next: next matching name seq publication with >= cluster scope
+ * @zone_list_next: next matching name seq publication with >= zone scope
+ * 
+ * Note that the node list, cluster list, and zone list are circular lists.
+ */
+
+struct publication {
+	u32 type;
+	u32 lower;
+	u32 upper;
+	u32 scope;
+	u32 node;
+	u32 ref;
+	u32 key;
+	struct net_subscr subscr;
+	struct list_head distr_list;
+	struct list_head pport_list;
+	struct publication *node_list_next;
+	struct publication *cluster_list_next;
+	struct publication *zone_list_next;
+};
+
+
+DECLARE_RWLOCK(tipc_nametbl_lock);
+DECLARE_RWLOCK(tipc_routetbl_lock);
+
+extern struct name_seq *route_table;
+extern int tipc_own_routes;
+extern int tipc_all_routes;
+
+
+struct sk_buff *tipc_nametbl_get(const void *req_tlv_area, int req_tlv_space);
+u32 tipc_nametbl_translate(u32 type, u32 instance, u32 *node);
+int tipc_nametbl_mc_translate(u32 type, u32 lower, u32 upper, u32 limit,
+			 struct port_list *dports);
+int tipc_publish_rsv(u32 ref, unsigned int scope, 
+                     struct tipc_name_seq const *seq);
+struct publication *tipc_nametbl_publish(u32 type, u32 lower, u32 upper,
+					 u32 scope, u32 port_ref, u32 key);
+
+struct publication *tipc_nametbl_publish_rsv(u32 type, u32 lower, u32 upper,
+					     u32 scope, u32 port_ref, u32 key);
+
+void tipc_nametbl_withdraw(u32 type, u32 lower, u32 ref, u32 key);
+
+struct publication *tipc_nametbl_insert_publ(u32 type, u32 lower, u32 upper,
+					u32 scope, u32 node, u32 ref, u32 key);
+struct publication *tipc_nameseq_insert_publ(struct name_seq *nseq,
+					     u32 type, u32 lower, u32 upper,
+					     u32 scope, u32 node, u32 port, u32 key);
+struct publication *tipc_nametbl_remove_publ(u32 type, u32 lower, 
+					u32 node, u32 ref, u32 key);
+struct publication *tipc_nameseq_remove_publ(struct name_seq *nseq, u32 inst,
+					     u32 node, u32 ref, u32 key);
+
+void tipc_nametbl_subscribe(struct subscription *s);
+void tipc_nametbl_unsubscribe(struct subscription *s);
+int tipc_nametbl_init(void);
+void tipc_nametbl_stop(void);
+
+int tipc_routetbl_init(void);
+void tipc_routetbl_stop(void);
+u32 tipc_routetbl_translate(u32 target);
+void tipc_routetbl_publish(unsigned long node_addr);
+void tipc_routetbl_withdraw(unsigned long node_addr);
+void tipc_routetbl_withdraw_node(unsigned long node_addr);
+void tipc_routetbl_purge(u32 region_addr);
+struct sk_buff *tipc_nametbl_get_routes(const void *req_tlv_area,
+					int req_tlv_space);
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_net.c android_cluster/linux-2.6.29/net/tipc/tipc_net.c
--- linux-2.6.29/net/tipc/tipc_net.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_net.c	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,496 @@
+/*
+ * net/tipc/tipc_net.c: TIPC network routing code
+ * 
+ * Copyright (c) 1995-2006, Ericsson AB
+ * Copyright (c) 2005-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_bearer.h"
+#include "tipc_net.h"
+#include "tipc_addr.h"
+#include "tipc_name_table.h"
+#include "tipc_name_distr.h"
+#include "tipc_topsrv.h"
+#include "tipc_link.h"
+#include "tipc_msg.h"
+#include "tipc_port.h"
+#include "tipc_bcast.h"
+#include "tipc_discover.h"
+#include "tipc_cfgsrv.h"
+
+/*
+ * The TIPC locking policy is designed to ensure a very fine locking
+ * granularity, permitting complete parallel access to individual
+ * port and node/link instances. The code consists of three major
+ * locking domains, each protected with their own disjunct set of locks.
+ *
+ * 1: The routing hierarchy.
+ *    Comprises the structures 'net', 'node', 'link' 
+ *    and 'bearer'. The whole hierarchy is protected by a big
+ *    read/write lock, tipc_net_lock, to enssure that nothing is added
+ *    or removed while code is accessing any of these structures.
+ *    This layer must not be called from the two others while they
+ *    hold any of their own locks.
+ *    Neither must it itself do any upcalls to the other two before
+ *    it has released tipc_net_lock and other protective locks.
+ *
+ *   Within the tipc_net_lock domain there are two sub-domains;'node' and
+ *   'bearer', where local write operations are permitted,
+ *   provided that those are protected by individual spin_locks
+ *   per instance. Code holding tipc_net_lock(read) and a node spin_lock
+ *   is permitted to poke around in both the node itself and its
+ *   subordinate links. I.e, it can update link counters and queues,
+ *   change link state, send protocol messages, and alter the
+ *   "active_links" array in the node; but it can _not_ remove a link
+ *   or a node from the overall structure.
+ *   Correspondingly, individual bearers may change status within a
+ *   tipc_net_lock(read), protected by an individual spin_lock ber bearer
+ *   instance, but it needs tipc_net_lock(write) to remove/add any bearers.
+ *
+ *
+ *  2: The transport level of the protocol.
+ *     This consists of the structures port, (and its user level
+ *     representations, such as user_port and tipc_sock), reference and
+ *     tipc_user (port.c, reg.c, socket.c).
+ *
+ *     This layer has four different locks:
+ *     - The tipc_port spin_lock. This is protecting each port instance
+ *       from parallel data access and removal. Since we can not place
+ *       this lock in the port itself, it has been placed in the
+ *       corresponding reference table entry, which has the same life
+ *       cycle as the module. This entry is difficult to access from
+ *       outside the TIPC core, however, so a pointer to the lock has
+ *       been added in the port instance, -to be used for unlocking
+ *       only.
+ *     - A read/write lock to protect the reference table itself (ref.c). 
+ *       (Nobody is using read-only access to this, so it can just as
+ *       well be changed to a spin_lock)
+ *     - A spin lock to protect the registry of kernel/driver users (reg.c)
+ *     - A global spin_lock (tipc_port_lock), which only task is to ensure
+ *       consistency where more than one port is involved in an operation,
+ *       i.e., whe a port is part of a linked list of ports.
+ *       There are two such lists; 'port_list', which is used for management,
+ *       and 'wait_list', which is used to queue ports during congestion.
+ *
+ *  3: The name table
+ *     - One master read/write lock (tipc_nametbl_lock) protects the overall
+ *       name table structure.  Nothing must be added/removed to this structure
+ *       without holding write access to it.
+ *     - An additional "subsequence update" spin_lock exists per sequence; it
+ *	 is used when the head pointer of one of a subsequence's circular lists
+ *	 of publications is advanced during name table lookup operations.
+ *	 (It could have been implemented as a "per subsequence" lock, but was
+ *	 made "per sequence" to reduce the footprint of the name table entries.)
+ *	 The subsequence update lock can only be used within the scope of
+ *	 a tipc_nametbl_lock(read).
+ */
+
+DEFINE_RWLOCK(tipc_net_lock);
+
+net_element_set_t tipc_local_nodes = { NULL, 0, 0 };
+net_element_set_t tipc_remote_nodes = { NULL, 0, 0 };
+net_element_set_t tipc_regions = { NULL, 0, 0 };
+
+
+static void net_element_set_init(net_element_set_t *set, int num_elements)
+{
+	void *ptr = kcalloc(num_elements, sizeof(struct net_element *),
+			     GFP_ATOMIC);
+
+	if (ptr != NULL) {
+		set->element = ptr;
+		set->max_size = num_elements;
+		set->first_free = 0;
+	}
+}
+
+static void net_element_set_term(net_element_set_t *set)
+{
+	kfree(set->element);
+	memset(set, 0, sizeof(net_element_set_t));
+}
+
+struct net_element *tipc_net_lookup_element(u32 addr, net_element_set_t *set) 
+{
+	int low = 0;
+	int high = set->first_free - 1;
+	int mid;
+
+	while (low <= high) {
+		mid = (low + high) / 2;
+		if (addr < set->element[mid]->addr)
+			high = mid - 1;
+		else if (addr > set->element[mid]->addr)
+			low = mid + 1;
+		else
+			return set->element[mid];
+	}
+	return NULL;
+}
+
+static void net_insert_element(struct net_element *e_ptr, net_element_set_t *set) 
+{
+	int i;
+
+	if (set->first_free >= set->max_size) {
+		warn("Could not add element %x (max %u allowed for this type)\n",
+		     e_ptr->addr, set->max_size);
+		return;
+	}
+
+	for (i = 0; i < set->first_free; i++) {
+		dbg_assert(set->element[i]->addr != e_ptr->addr);
+		if (set->element[i]->addr > e_ptr->addr) {
+			int sz = (set->first_free - i) * sizeof(e_ptr);
+			memmove(&set->element[i+1], &set->element[i], sz);
+			break;
+		}
+	}
+	set->element[i] = e_ptr;
+	set->first_free++;
+}
+
+/**
+ * tipc_net_attach_node - record new neighbor node
+ */
+
+void tipc_net_attach_node(struct tipc_node *n_ptr)
+{
+	if (in_own_cluster(n_ptr->elm.addr)) {
+		net_insert_element(&n_ptr->elm, &tipc_local_nodes);
+	} else {
+		net_insert_element(&n_ptr->elm, &tipc_remote_nodes);
+	}
+}
+
+
+/**
+ * tipc_net_find_node - get specified neighbor node
+ * @addr: network address of node
+ * 
+ * Returns pointer to node (or NULL if node is not a known neighbor)
+ * 
+ * CAUTION: Just because a node is a known neighbor, it does not mean
+ *          there are currently working links to it!
+ */
+
+struct tipc_node *tipc_net_find_node(u32 addr)
+{
+	if (in_own_cluster(addr))
+		return (struct tipc_node *)tipc_net_lookup_element(
+			addr, &tipc_local_nodes);
+	else
+		return (struct tipc_node *)tipc_net_lookup_element(
+			addr, &tipc_remote_nodes);
+}
+
+/**
+ * tipc_net_select_node - select next hop node to use for off-node message
+ * @addr: network address of message destination (may be node, cluster, or zone)
+ * 
+ * Returns pointer to node to use (or NULL if destination is unreachable)
+ */
+
+struct tipc_node *tipc_net_select_node(u32 addr)
+{
+	dbg_assert(addr != tipc_own_addr);
+	dbg_assert(addr != addr_cluster(tipc_own_addr));
+	dbg_assert(addr != addr_zone(tipc_own_addr));
+	dbg_assert(addr != 0);
+
+	if (likely(in_own_cluster(addr)))
+		return (struct tipc_node *)tipc_net_lookup_element(addr,
+							      &tipc_local_nodes);
+	else
+		return tipc_net_find_node(tipc_routetbl_translate(addr));
+}
+
+
+static void net_route_named_msg(struct sk_buff *buf)
+{
+	struct tipc_msg *msg = buf_msg(buf);
+	u32 dnode;
+	u32 dport;
+
+	if (!msg_named(msg)) {
+		msg_dbg(msg, "tipc_net->drop_nam:");
+		buf_discard(buf);
+		return;
+	}
+
+	dnode = addr_domain(msg_lookup_scope(msg));
+	dport = tipc_nametbl_translate(msg_nametype(msg), msg_nameinst(msg),
+				       &dnode);
+
+	if (dport) {
+		msg_set_destnode(msg, dnode);
+		msg_set_destport(msg, dport);
+		tipc_net_route_msg(buf);
+		return;
+	}
+	msg_dbg(msg, "tipc_net->rej:NO NAME: ");
+	tipc_reject_msg(buf, TIPC_ERR_NO_NAME);
+}
+
+void tipc_net_route_msg(struct sk_buff *buf)
+{
+	struct tipc_msg *msg;
+	u32 dnode;
+
+	if (!buf)
+		return;
+	msg = buf_msg(buf);
+
+	msg_incr_reroute_cnt(msg);
+	if (msg_reroute_cnt(msg) > 6) {
+		if (msg_errcode(msg)) {
+			msg_dbg(msg, "NET>DISC>:");
+			buf_discard(buf);
+		} else {
+			msg_dbg(msg, "NET>REJ>:");
+			tipc_reject_msg(buf, msg_destport(msg) ?
+					TIPC_ERR_NO_PORT : TIPC_ERR_NO_NAME);
+		}
+		return;
+	}
+
+	msg_dbg(msg, "tipc_net->rout: ");
+
+	/* Handle message for this node */
+
+	dnode = msg_short(msg) ? msg_destnode_cache(msg) : msg_destnode(msg);
+	if (tipc_in_scope(dnode, tipc_own_addr)) {
+		if (msg_isdata(msg)) {
+			if (msg_mcast(msg))
+				tipc_port_recv_mcast(buf, NULL);
+			else if (msg_destport(msg))
+				tipc_port_recv_msg(buf);
+			else
+				net_route_named_msg(buf);
+			return;
+		}
+		switch (msg_user(msg)) {
+		case NAME_DISTRIBUTOR:
+			tipc_named_recv(buf);
+			break;
+		case ROUTE_DISTRIBUTOR:
+			tipc_route_recv(buf);
+			break;
+		case CONN_MANAGER:
+			tipc_port_recv_proto_msg(buf);
+			break;
+		default:
+			msg_dbg(msg,"DROP/NET/<REC<");
+			buf_discard(buf);
+		}
+		return;
+	}
+
+	/* Handle message for another node */
+
+	msg_dbg(msg, "NET>SEND>: ");
+	pskb_trim(buf, msg_size(msg));
+	tipc_link_send(buf, dnode, msg_link_selector(msg));
+}
+
+static int net_init(void)
+{
+	net_element_set_init(&tipc_local_nodes, tipc_max_nodes);
+	net_element_set_init(&tipc_remote_nodes, tipc_max_remotes);
+	net_element_set_init(&tipc_regions,
+			     (tipc_max_clusters - 1) + (tipc_max_zones - 1));
+
+	if ((tipc_local_nodes.element == NULL) ||
+	    (tipc_remote_nodes.element == NULL) ||
+	    (tipc_regions.element == NULL))
+		return -ENOMEM;
+
+	return 0;
+}
+
+static void net_stop(void)
+{
+	int i;
+
+	for (i = 0; i < tipc_local_nodes.first_free; i++) {
+		tipc_node_delete(
+			(struct tipc_node *)tipc_local_nodes.element[i]);
+	}
+
+	for (i = 0; i < tipc_remote_nodes.first_free; i++) {
+		tipc_node_delete(
+			(struct tipc_node *)tipc_remote_nodes.element[i]);
+	}
+
+	for (i = 0; i < tipc_regions.first_free; i++) {
+		spin_lock_term(&tipc_regions.element[i]->lock);
+		kfree(tipc_regions.element[i]);
+	}
+	
+	net_element_set_term(&tipc_local_nodes);
+	net_element_set_term(&tipc_remote_nodes);
+	net_element_set_term(&tipc_regions);
+}
+
+
+int tipc_net_start(u32 addr)
+{
+	char addr_string[16];
+	int res;
+
+	if (tipc_mode != TIPC_NODE_MODE)
+		return -ENOPROTOOPT;
+
+	tipc_cfg_stop();
+
+	tipc_own_addr = addr;
+	tipc_mode = TIPC_NET_MODE;
+	tipc_port_reinit();
+	tipc_named_reinit();
+
+	if ((res = tipc_bearer_init()) ||
+	    (res = net_init()) ||
+	    (res = tipc_bclink_init())) {
+		return res;
+	}
+
+	tipc_k_signal((Handler)tipc_cfg_init, 0);
+
+	info("Started in network mode\n");
+	tipc_addr_string_fill(addr_string, tipc_own_addr);
+	info("Own node address %s, network identity %u\n",
+	     addr_string, tipc_net_id);
+	return 0;
+}
+
+void tipc_net_stop(void)
+{
+	if (tipc_mode != TIPC_NET_MODE)
+		return;
+
+	write_lock_bh(&tipc_net_lock);
+	tipc_mode = TIPC_NODE_MODE;
+	tipc_bearer_stop();
+	tipc_bclink_stop();
+	net_stop();
+	write_unlock_bh(&tipc_net_lock);
+	info("Left network mode \n");
+}
+
+/**
+ * tipc_netsub_bind - create "element down" subscription
+ */
+
+void tipc_netsub_bind(struct net_subscr *net_sub, u32 addr, 
+		      net_ev_handler handle_down, void *usr_handle)
+{
+	u32 elm_addr;
+
+	net_sub->element = NULL;
+
+	if (in_own_cluster(addr)) {
+		elm_addr = addr;
+		net_sub->element = tipc_net_lookup_element(elm_addr,
+							   &tipc_local_nodes);
+		dbg_assert(net_sub->element);
+	} else {
+		if (!in_own_zone(addr))
+			elm_addr = addr_zone(addr);
+		else
+			elm_addr = addr_cluster(addr);
+
+		net_sub->element = tipc_net_lookup_element(elm_addr,
+							   &tipc_regions);
+		if (net_sub->element == NULL) {
+			struct net_element *region;
+
+			region = kmalloc(sizeof(struct net_element), GFP_ATOMIC);
+			if (region == NULL) {
+				warn("Memory squeeze; unable to record new region\n");
+				return;
+			}
+			region->addr = elm_addr;
+			INIT_LIST_HEAD(&region->nsub);
+			spin_lock_init(&region->lock);
+			net_insert_element(region, &tipc_regions);
+			net_sub->element = region;
+		}
+	}
+
+	net_sub->addr = addr;
+	net_sub->handle_element_down = handle_down;
+	net_sub->usr_handle = usr_handle;
+
+	net_element_lock(net_sub->element);
+	list_add_tail(&net_sub->sub_list, &net_sub->element->nsub);
+	net_element_unlock(net_sub->element);
+}
+
+/**
+ * netsub_unsubscribe - cancel "element down" subscription (if any)
+ */
+
+void tipc_netsub_unbind(struct net_subscr *net_sub)
+{
+	if (!net_sub->element)
+		return;
+
+	net_element_lock(net_sub->element);
+	list_del_init(&net_sub->sub_list);
+	net_element_unlock(net_sub->element);
+}
+
+/** 
+ * netsub_notify - notify subscribers that a network element is unreachable
+ * @e_ptr: network element (either a node or a region)
+ * @affected_addr: address of affected portion of network element
+ *                 (may not be all of it)
+ *                 
+ * Note: network element is locked by caller
+ */
+
+void tipc_netsub_notify(struct net_element *e_ptr, u32 affected_addr)
+{
+	struct net_subscr *ns;
+	struct net_subscr *tns;
+
+	list_for_each_entry_safe(ns, tns, &e_ptr->nsub, sub_list) {
+		if (tipc_in_scope(affected_addr, ns->addr)) {
+			ns->element = NULL;
+			list_del_init(&ns->sub_list);
+			tipc_k_signal((Handler)ns->handle_element_down,
+				      (unsigned long)ns->usr_handle);
+		}
+	}
+}
+
diff -ruN linux-2.6.29/net/tipc/tipc_net.h android_cluster/linux-2.6.29/net/tipc/tipc_net.h
--- linux-2.6.29/net/tipc/tipc_net.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_net.h	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,117 @@
+/*
+ * net/tipc/tipc_net.h: Include file for TIPC network routing code
+ * 
+ * Copyright (c) 1995-2006, Ericsson AB
+ * Copyright (c) 2005-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_NET_H
+#define _TIPC_NET_H
+
+#include "tipc_addr.h"
+
+#define LOWEST_SLAVE  2048u
+#define TIPC_ROUTE 2
+
+/**
+ * struct net_element - generic network element (for node, cluster, or zone)
+ * @addr: address of element (i.e. <Z.C.N>, <Z.C.0>, or <Z.0.0>)
+ * @nsub: list of subscribers to notify when element is unavailable
+ * @lock: lock for exclusive access to element
+ */
+
+struct net_element {
+        u32 addr;
+        struct list_head nsub;
+	spinlock_t lock;
+};
+
+typedef void (*net_ev_handler) (void *usr_handle);
+
+/**
+ * struct net_subscr - network element subscription entry
+ * @addr: network address of entity being monitored
+ * @element: network element containing entity being monitored (NULL, if none)
+ * @sub_list: adjacent entries in list of network element subscriptions
+ * @handle_element_down: routine to invoke when monitored entity is unreachable
+ * @usr_handle: argument to pass to routine
+ */
+
+struct net_subscr {
+	u32 addr;
+	struct net_element *element;
+	struct list_head sub_list;
+	net_ev_handler handle_element_down;
+	void *usr_handle;
+};
+
+typedef struct {
+	struct net_element **element;
+	int max_size;
+	int first_free;
+} net_element_set_t;
+
+extern net_element_set_t tipc_local_nodes;
+extern net_element_set_t tipc_remote_nodes;
+extern net_element_set_t tipc_regions;
+
+DECLARE_RWLOCK(tipc_net_lock);
+
+struct tipc_node;
+
+int tipc_net_start(u32 addr);
+void tipc_net_stop(void);
+void tipc_net_route_msg(struct sk_buff *buf);
+struct tipc_node *tipc_net_find_node(u32 addr);
+struct tipc_node *tipc_net_select_node(u32 addr);
+struct net_element *tipc_net_lookup_element(u32 addr, net_element_set_t *set); 
+void tipc_net_attach_node(struct tipc_node *n_ptr);
+
+void tipc_netsub_bind(struct net_subscr *net_sub, u32 addr,
+		      net_ev_handler handle_down, void *usr_handle);
+void tipc_netsub_unbind(struct net_subscr *net_sub);
+
+void tipc_netsub_notify(struct net_element *element, u32 affected_addr);
+
+
+static inline void net_element_lock(struct net_element *e_ptr)
+{
+	spin_lock_bh(&e_ptr->lock);
+}
+
+static inline void net_element_unlock(struct net_element *e_ptr)
+{
+	spin_unlock_bh(&e_ptr->lock);
+}
+
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_netlink.c android_cluster/linux-2.6.29/net/tipc/tipc_netlink.c
--- linux-2.6.29/net/tipc/tipc_netlink.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_netlink.c	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,143 @@
+/*
+ * net/tipc/tipc_netlink.c: TIPC configuration handling
+ *
+ * Copyright (c) 2005-2006, Ericsson AB
+ * Copyright (c) 2005-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_cfgsrv.h"
+#include <net/genetlink.h>
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+static int handle_cmd(struct sk_buff *skb, struct genl_info *info)
+{
+	struct sk_buff *rep_buf;
+	struct nlmsghdr *rep_nlh;
+	struct nlmsghdr *req_nlh = info->nlhdr;
+	struct tipc_genlmsghdr *req_userhdr = info->userhdr;
+	int hdr_space = NLMSG_SPACE(GENL_HDRLEN + TIPC_GENL_HDRLEN);
+	u16 cmd;
+
+	if ((req_userhdr->cmd & 0xC000) && (!capable(CAP_NET_ADMIN)))
+		cmd = TIPC_CMD_NOT_NET_ADMIN;
+	else
+		cmd = req_userhdr->cmd;
+
+	rep_buf = tipc_cfg_do_cmd(req_userhdr->dest, cmd,
+			NLMSG_DATA(req_nlh) + GENL_HDRLEN + TIPC_GENL_HDRLEN,
+			NLMSG_PAYLOAD(req_nlh, GENL_HDRLEN + TIPC_GENL_HDRLEN),
+			hdr_space);
+
+	if (rep_buf) {
+		skb_push(rep_buf, hdr_space);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+		rep_nlh = nlmsg_hdr(rep_buf);
+#else
+		rep_nlh = (struct nlmsghdr *)rep_buf->data;
+#endif
+		memcpy(rep_nlh, req_nlh, hdr_space);
+		rep_nlh->nlmsg_len = rep_buf->len;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+		genlmsg_unicast(rep_buf, NETLINK_CB(skb).pid);
+#else
+		genlmsg_unicast(rep_buf, req_nlh->nlmsg_pid);
+#endif
+	}
+
+	return 0;
+}
+
+static struct genl_family family = {
+	.id		= GENL_ID_GENERATE,
+	.name		= TIPC_GENL_NAME,
+	.version	= TIPC_GENL_VERSION,
+	.hdrsize	= TIPC_GENL_HDRLEN,
+	.maxattr	= 0,
+};
+
+static struct genl_ops ops = {
+	.cmd		= TIPC_GENL_CMD,
+	.doit		= handle_cmd,
+};
+
+static int family_registered = 0;
+
+int tipc_netlink_start(void)
+{
+
+
+	if (genl_register_family(&family))
+		goto err;
+
+	family_registered = 1;
+
+	if (genl_register_ops(&family, &ops))
+		goto err_unregister;
+
+	return 0;
+
+ err_unregister:
+	genl_unregister_family(&family);
+	family_registered = 0;
+ err:
+	err("Failed to register netlink interface\n");
+	return -EFAULT;
+}
+
+void tipc_netlink_stop(void)
+{
+	if (family_registered) {
+		genl_unregister_family(&family);
+		family_registered = 0;
+	}
+}
+
+#else
+
+/*
+ * Dummy routines used when configuration service is not included
+ */
+
+int tipc_netlink_start(void)
+{
+	return 0;
+
+}
+
+void tipc_netlink_stop(void)
+{
+	/* do nothing */
+}
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_node.c android_cluster/linux-2.6.29/net/tipc/tipc_node.c
--- linux-2.6.29/net/tipc/tipc_node.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_node.c	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,570 @@
+/*
+ * net/tipc/tipc_node.c: TIPC node management routines
+ *
+ * Copyright (c) 2000-2006, Ericsson AB
+ * Copyright (c) 2005-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifdef CONFIG_KRG_HOTPLUG
+#include <kerrighed/sys/types.h>
+#include <kerrighed/hotplug.h>
+#endif
+
+#include "tipc_core.h"
+#include "tipc_cfgsrv.h"
+#include "tipc_node.h"
+#include "tipc_net.h"
+#include "tipc_addr.h"
+#include "tipc_node.h"
+#include "tipc_link.h"
+#include "tipc_port.h"
+#include "tipc_bearer.h"
+#include "tipc_name_distr.h"
+
+static void node_lost_contact(struct tipc_node *n_ptr);
+static void node_established_contact(struct tipc_node *n_ptr);
+
+static LIST_HEAD(nodes_list);	/* sorted list of neighboring nodes */
+static int node_count = 0;     	/* number of neighboring nodes that exist */
+static int link_count = 0;     	/* number of unicast links node currently has */
+
+static DEFINE_SPINLOCK(node_create_lock);
+
+/**
+ * tipc_node_create - create neighboring node
+ *
+ * Currently, this routine is called by neighbor discovery code, which holds
+ * net_lock for reading only.  We must take node_create_lock to ensure a node
+ * isn't created twice if two different bearers discover the node at the same
+ * time.  (It would be preferable to switch to holding net_lock in write mode,
+ * but this is a non-trivial change.)
+ */
+
+struct tipc_node *tipc_node_create(u32 addr)
+{
+	struct tipc_node *n_ptr;
+	struct tipc_node *curr_n_ptr;
+
+	spin_lock_bh(&node_create_lock);
+
+	n_ptr = tipc_net_find_node(addr);
+	if (n_ptr != NULL) {
+		spin_unlock_bh(&node_create_lock);
+		return n_ptr;
+	}
+
+	n_ptr = kzalloc(sizeof(*n_ptr), GFP_ATOMIC);
+	if (n_ptr != NULL) {
+		n_ptr->elm.addr = addr;
+		spin_lock_init(&n_ptr->elm.lock);
+		INIT_LIST_HEAD(&n_ptr->elm.nsub);
+		tipc_net_attach_node(n_ptr);
+
+		list_for_each_entry(curr_n_ptr, &nodes_list, node_list) {
+			if (addr < curr_n_ptr->elm.addr)
+				break;
+		}
+		list_add_tail(&n_ptr->node_list, &curr_n_ptr->node_list);
+
+		node_count++;
+	} else {
+		warn("Node creation failed, no memory\n");
+	}
+
+	spin_unlock_bh(&node_create_lock);
+	return n_ptr;
+}
+
+void tipc_node_delete(struct tipc_node *n_ptr)
+{
+	node_count--;
+	list_del(&n_ptr->node_list);
+	spin_lock_term(&n_ptr->elm.lock);
+	kfree(n_ptr);
+}
+
+
+/**
+ * tipc_node_link_up - handle addition of link
+ *
+ * Link becomes active (alone or shared) or standby, depending on its priority.
+ */
+
+void tipc_node_link_up(struct tipc_node *n_ptr, struct link *l_ptr)
+{
+	struct link **active = &n_ptr->active_links[0];
+
+	n_ptr->working_links++;
+
+	info("Established link <%s> on network plane %c\n",
+	     l_ptr->name, l_ptr->b_ptr->net_plane);
+
+#ifdef CONFIG_TIPC_MULTIPLE_LINKS
+	if (active[0] == NULL) {
+		active[0] = active[1] = l_ptr;
+		node_established_contact(n_ptr);
+		return;
+	}
+	if (l_ptr->priority < active[0]->priority) {
+		info("New link <%s> becomes standby\n", l_ptr->name);
+		return;
+	}
+	tipc_link_send_duplicate(active[0], l_ptr);
+	if (l_ptr->priority == active[0]->priority) {
+		active[0] = l_ptr;
+		return;
+	}
+	info("Old link <%s> becomes standby\n", active[0]->name);
+	if (active[1] != active[0])
+		info("Old link <%s> becomes standby\n", active[1]->name);
+	active[0] = active[1] = l_ptr;
+#else
+	active[0] = active[1] = l_ptr;
+	node_established_contact(n_ptr);
+#endif
+}
+
+#ifdef CONFIG_TIPC_MULTIPLE_LINKS
+/**
+ * node_select_active_links - select active link
+ */
+
+static void node_select_active_links(struct tipc_node *n_ptr)
+{
+	struct link **active = &n_ptr->active_links[0];
+	u32 i;
+	u32 highest_prio = 0;
+
+	active[0] = active[1] = NULL;
+
+	for (i = 0; i < TIPC_MAX_BEARERS; i++) {
+		struct link *l_ptr = n_ptr->links[i];
+
+		if (!l_ptr || !tipc_link_is_up(l_ptr) ||
+		    (l_ptr->priority < highest_prio))
+			continue;
+
+		if (l_ptr->priority > highest_prio) {
+			highest_prio = l_ptr->priority;
+			active[0] = active[1] = l_ptr;
+		} else {
+			active[1] = l_ptr;
+		}
+	}
+}
+#endif
+
+/**
+ * tipc_node_link_down - handle loss of link
+ */
+
+void tipc_node_link_down(struct tipc_node *n_ptr, struct link *l_ptr)
+{
+	struct link **active = &n_ptr->active_links[0];
+
+	n_ptr->working_links--;
+
+#ifdef CONFIG_TIPC_MULTIPLE_LINKS
+	if (!tipc_link_is_active(l_ptr)) {
+		info("Lost standby link <%s> on network plane %c\n",
+		     l_ptr->name, l_ptr->b_ptr->net_plane);
+		return;
+	}
+
+	info("Lost link <%s> on network plane %c\n",
+	     l_ptr->name, l_ptr->b_ptr->net_plane);
+
+	if (active[0] == l_ptr)
+		active[0] = active[1];
+	if (active[1] == l_ptr)
+		active[1] = active[0];
+	if (active[0] == l_ptr)
+		node_select_active_links(n_ptr);
+	if (tipc_node_is_up(n_ptr))
+		tipc_link_changeover(l_ptr);
+	else
+		node_lost_contact(n_ptr);
+#else
+	info("Lost link <%s> on network plane %c\n",
+	     l_ptr->name, l_ptr->b_ptr->net_plane);
+
+	active[0] = active[1] = NULL;
+	node_lost_contact(n_ptr);
+#endif
+}
+
+int tipc_node_is_up(struct tipc_node *n_ptr)
+{
+	return (n_ptr->active_links[0] != NULL);
+}
+
+int tipc_node_has_redundant_links(struct tipc_node *n_ptr)
+{
+#ifdef CONFIG_TIPC_MULTIPLE_LINKS
+	return (n_ptr->working_links > 1);
+#else
+	return 0;
+#endif
+}
+
+struct tipc_node *tipc_node_attach_link(struct link *l_ptr)
+{
+	struct tipc_node *n_ptr = tipc_net_find_node(l_ptr->addr);
+
+	if (!n_ptr)
+		n_ptr = tipc_node_create(l_ptr->addr);
+	if (n_ptr) {
+		u32 bearer_id = l_ptr->b_ptr->identity;
+		char addr_string[16];
+
+		if (n_ptr->link_cnt >= TIPC_MAX_BEARERS) {
+			tipc_addr_string_fill(addr_string, n_ptr->elm.addr);
+			err("Attempt to more than %d links to %s\n",
+			    n_ptr->link_cnt, addr_string);
+			return NULL;
+		}
+
+		if (!n_ptr->links[bearer_id]) {
+			n_ptr->links[bearer_id] = l_ptr;
+			n_ptr->link_cnt++;
+			link_count++;
+			return n_ptr;
+		}
+		tipc_addr_string_fill(addr_string, l_ptr->addr);
+		err("Attempt to establish second link on <%s> to %s \n",
+		    l_ptr->b_ptr->publ.name, addr_string);
+	}
+	return NULL;
+}
+
+void tipc_node_detach_link(struct tipc_node *n_ptr, struct link *l_ptr)
+{
+	n_ptr->links[l_ptr->b_ptr->identity] = NULL;
+	n_ptr->link_cnt--;
+	link_count--;
+}
+
+static void node_established_contact(struct tipc_node *n_ptr)
+{
+	dbg("node_established_contact:-> %x\n", n_ptr->elm.addr);
+
+#ifdef CONFIG_KRG_HOTPLUG
+	krg_node_arrival(tipc_node(n_ptr->elm.addr)-1);
+#endif
+
+	/* Synchronize broadcast acks */
+
+	n_ptr->bclink.acked = tipc_bclink_get_last_sent();
+
+	if (in_own_cluster(n_ptr->elm.addr)) {
+
+		/* Add to multicast destination map, if applicable */
+
+		if (n_ptr->bclink.supported)
+			tipc_bclink_add_node(n_ptr->elm.addr);
+	} else {
+
+		/* Publish new inter-cluster (or inter-zone) route */
+
+		tipc_k_signal((Handler)tipc_routetbl_publish, n_ptr->elm.addr);
+	}
+
+	/* Pass route & name table info to node, if necessary */
+
+	if (in_own_zone(n_ptr->elm.addr)) {
+		if (likely(n_ptr->flags & NF_MULTICLUSTER)) {
+			tipc_k_signal((Handler)tipc_route_node_up,
+				      n_ptr->elm.addr);
+			tipc_k_signal((Handler)tipc_named_node_up,
+				      n_ptr->elm.addr);
+		} else {
+			tipc_k_signal((Handler)tipc_named_node_up_uni,
+				      n_ptr->elm.addr);
+		}
+	}
+}
+
+#ifdef CONFIG_TIPC_MULTIPLE_LINKS
+static inline void node_abort_link_changeover(struct tipc_node *n_ptr)
+{
+	struct link *l_ptr;
+	int i;
+
+	for (i = 0; i < TIPC_MAX_BEARERS; i++) {
+		l_ptr = n_ptr->links[i];
+		if (l_ptr != NULL) {
+			l_ptr->reset_checkpoint = l_ptr->next_in_no;
+			l_ptr->exp_msg_count = 0;
+			tipc_link_reset_fragments(l_ptr);
+		}
+	}
+}
+#endif
+
+static void node_cleanup_finished(unsigned long node_addr)
+{
+	struct tipc_node *n_ptr;
+	 
+	read_lock_bh(&tipc_net_lock);
+	n_ptr = tipc_net_find_node(node_addr);
+	if (n_ptr) {
+		tipc_node_lock(n_ptr);
+		n_ptr->cleanup_required = 0;
+		tipc_node_unlock(n_ptr);
+	}
+	read_unlock_bh(&tipc_net_lock);
+}
+
+static void node_lost_contact(struct tipc_node *n_ptr)
+{
+	char addr_string[16];
+
+#ifdef CONFIG_KRG_HOTPLUG
+	krg_node_departure(tipc_node(n_ptr->elm.addr)-1);
+#endif
+
+	tipc_addr_string_fill(addr_string, n_ptr->elm.addr);
+	info("Lost contact with %s\n", addr_string);
+
+	/* Clean up broadcast reception remains */
+
+	while (n_ptr->bclink.deferred_head) {
+		struct sk_buff *buf = n_ptr->bclink.deferred_head;
+
+		n_ptr->bclink.deferred_head = buf->next;
+		buf_discard(buf);
+	}
+	n_ptr->bclink.deferred_size = 0;
+
+	if (n_ptr->bclink.defragm) {
+		buf_discard(n_ptr->bclink.defragm);
+		n_ptr->bclink.defragm = NULL;
+	}
+
+	if (in_own_cluster(n_ptr->elm.addr) && n_ptr->bclink.supported) { 
+		tipc_bclink_acknowledge(n_ptr, mod(n_ptr->bclink.acked + 10000));
+		tipc_bclink_remove_node(n_ptr->elm.addr);
+	}
+
+#ifdef CONFIG_TIPC_MULTIPLE_LINKS
+	node_abort_link_changeover(n_ptr);
+#endif
+
+	/* 
+	 * For lost node in own cluster:
+	 * - purge all associated name table entries and connections
+	 * - trigger similar purge in all other clusters/zones by notifying
+	 *   them of disappearance of node
+	 *
+	 * For lost node in other cluster (or zone):
+	 * - withdraw route to failed node
+	 */
+
+	if (tipc_mode != TIPC_NET_MODE) {
+		/* TODO: THIS IS A HACK TO PREVENT A KERNEL CRASH IF TIPC
+		   IS UNLOADED WHEN IT HAS ACTIVE INTER-CLUSTER LINKS;
+		   OTHERWISE THE ROUTINES INVOKED VIA SIGNALLING DON'T RUN UNTIL
+		   AFTER STUFF THEY DEPEND ON HAS BEEN SHUT DOWN 
+		   
+		   THE CODE NEEDS TO BE CLEANED UP TO DO THIS BETTER, SINCE
+		   FAILING TO RUN THE CLEANUP CODE COULD LEAVE ENTIRES IN THE
+		   ROUTING TABLE AND NAME TABLE ... */
+		return;
+	}
+
+	if (in_own_cluster(n_ptr->elm.addr)) {
+		tipc_netsub_notify(&n_ptr->elm, n_ptr->elm.addr);
+		tipc_k_signal((Handler)tipc_routetbl_withdraw_node,
+			      n_ptr->elm.addr);
+	} else {
+	       	tipc_k_signal((Handler)tipc_routetbl_withdraw,
+			      n_ptr->elm.addr);
+	}
+
+	/* Prevent re-contact with node until all cleanup is done */
+
+	n_ptr->cleanup_required = 1;
+	tipc_k_signal((Handler)node_cleanup_finished, n_ptr->elm.addr);
+}
+
+#if 0
+void node_print(struct print_buf *buf, struct tipc_node *n_ptr, char *str)
+{
+	u32 i;
+
+	tipc_printf(buf, "\n\n%s", str);
+	for (i = 0; i < TIPC_MAX_BEARERS; i++) {
+		if (!n_ptr->links[i])
+			continue;
+		tipc_printf(buf, "Links[%u]: %x, ", i, n_ptr->links[i]);
+	}
+	tipc_printf(buf, "Active links: [%x,%x]\n",
+		    n_ptr->active_links[0], n_ptr->active_links[1]);
+}
+#endif
+
+u32 tipc_available_nodes(const u32 domain)
+{
+	struct tipc_node *n_ptr;
+	u32 cnt = 0;
+
+	read_lock_bh(&tipc_net_lock);
+	list_for_each_entry(n_ptr, &nodes_list, node_list) {
+		if (!tipc_in_scope(domain, n_ptr->elm.addr))
+			continue;
+		if (tipc_node_is_up(n_ptr))
+			cnt++;
+	}
+	read_unlock_bh(&tipc_net_lock);
+	return cnt;
+}
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+struct sk_buff *tipc_node_get_nodes(const void *req_tlv_area, int req_tlv_space)
+{
+	u32 domain;
+	struct sk_buff *buf;
+	struct tipc_node *n_ptr;
+	struct tipc_node_info node_info;
+	u32 payload_size;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_NET_ADDR))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	domain = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	if (!tipc_addr_domain_valid(domain))
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+						   " (network address)");
+
+	read_lock_bh(&tipc_net_lock);
+	if (!node_count) {
+		read_unlock_bh(&tipc_net_lock);
+		return tipc_cfg_reply_none();
+	}
+
+	/* Get space for all neighboring nodes */
+
+	payload_size = TLV_SPACE(sizeof(node_info)) * node_count;
+	if (payload_size > 32768u) {
+		read_unlock_bh(&tipc_net_lock);
+		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
+						   " (too many nodes)");
+	}
+	buf = tipc_cfg_reply_alloc(payload_size);
+	if (!buf) {
+		read_unlock_bh(&tipc_net_lock);
+		return NULL;
+	}
+
+	/* Add TLVs for all nodes in scope */
+
+	list_for_each_entry(n_ptr, &nodes_list, node_list) {
+		if (!tipc_in_scope(domain, n_ptr->elm.addr))
+			continue;
+		node_info.addr = htonl(n_ptr->elm.addr);
+		node_info.up = htonl(tipc_node_is_up(n_ptr));
+		tipc_cfg_append_tlv(buf, TIPC_TLV_NODE_INFO,
+				    &node_info, sizeof(node_info));
+	}
+
+	read_unlock_bh(&tipc_net_lock);
+	return buf;
+}
+
+struct sk_buff *tipc_node_get_links(const void *req_tlv_area, int req_tlv_space)
+{
+	u32 domain;
+	struct sk_buff *buf;
+	struct tipc_node *n_ptr;
+	struct tipc_link_info link_info;
+	u32 payload_size;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_NET_ADDR))
+		return tipc_cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	domain = ntohl(*(__be32 *)TLV_DATA(req_tlv_area));
+	if (!tipc_addr_domain_valid(domain))
+		return tipc_cfg_reply_error_string(TIPC_CFG_INVALID_VALUE
+						   " (network address)");
+
+	if (tipc_mode != TIPC_NET_MODE)
+		return tipc_cfg_reply_none();
+	
+	read_lock_bh(&tipc_net_lock);
+
+	/* Get space for all unicast links + broadcast link */
+
+	payload_size = TLV_SPACE(sizeof(link_info)) * (link_count + 1);
+	if (payload_size > 32768u) {
+		read_unlock_bh(&tipc_net_lock);
+		return tipc_cfg_reply_error_string(TIPC_CFG_NOT_SUPPORTED
+						   " (too many links)");
+	}
+	buf = tipc_cfg_reply_alloc(payload_size);
+	if (!buf) {
+		read_unlock_bh(&tipc_net_lock);
+		return NULL;
+	}
+
+	/* Add TLV for broadcast link */
+
+	link_info.dest = htonl(tipc_own_addr & 0xfffff00);
+	link_info.up = htonl(1);
+	sprintf(link_info.str, tipc_bclink_name);
+	tipc_cfg_append_tlv(buf, TIPC_TLV_LINK_INFO, &link_info, sizeof(link_info));
+
+	/* Add TLVs for any other links in scope */
+
+	list_for_each_entry(n_ptr, &nodes_list, node_list) {
+		u32 i;
+
+		if (!tipc_in_scope(domain, n_ptr->elm.addr))
+			continue;
+		tipc_node_lock(n_ptr);
+		for (i = 0; i < TIPC_MAX_BEARERS; i++) {
+			if (!n_ptr->links[i])
+				continue;
+			link_info.dest = htonl(n_ptr->elm.addr);
+			link_info.up = htonl(tipc_link_is_up(n_ptr->links[i]));
+			strcpy(link_info.str, n_ptr->links[i]->name);
+			tipc_cfg_append_tlv(buf, TIPC_TLV_LINK_INFO,
+					    &link_info, sizeof(link_info));
+		}
+		tipc_node_unlock(n_ptr);
+	}
+
+	read_unlock_bh(&tipc_net_lock);
+	return buf;
+}
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_node.h android_cluster/linux-2.6.29/net/tipc/tipc_node.h
--- linux-2.6.29/net/tipc/tipc_node.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_node.h	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,116 @@
+/*
+ * net/tipc/tipc_node.h: Include file for TIPC node management routines
+ * 
+ * Copyright (c) 2000-2006, Ericsson AB
+ * Copyright (c) 2005-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_NODE_H
+#define _TIPC_NODE_H
+
+#include "tipc_addr.h"
+#include "tipc_bearer.h"
+#include "tipc_net.h"
+
+/**
+ * struct tipc_node - TIPC node structure
+ * @elm: generic network element structure for node
+ * @node_list: adjacent entries in sorted list of nodes
+ * @active_links: pointers to active links to node
+ * @links: pointers to all links to node
+ * @working_links: number of working links to node (both active and standby)
+ * @link_cnt: number of links to node
+ * @working_links: number of working links to node (both active and standby)
+ * @permit_changeover: non-zero if node has redundant links to this system
+ * @cleanup_required: non-zero if cleaning up after a prior loss of contact
+ * @signature: random node instance identifier (always 0 for a uni-cluster node)
+ * @flags: bit array indicating node's capabilities
+ * @bclink: broadcast-related info
+ *    @supported: non-zero if node supports TIPC b'cast capability
+ *    @acked: sequence # of last outbound b'cast message acknowledged by node
+ *    @last_in: sequence # of last in-sequence b'cast message received from node
+ *    @last_sent: sequence # of last b'cast message sent by node
+ *    @oos_state: state tracker for handling OOS b'cast messages
+ *    @deferred_size: number of OOS b'cast messages in deferred queue
+ *    @deferred_head: oldest OOS b'cast message received from node
+ *    @deferred_tail: newest OOS b'cast message received from node
+ *    @defragm: list of partially reassembled b'cast message fragments from node
+ */
+ 
+struct tipc_node {
+	struct net_element elm;			/* MUST BE FIRST */
+	struct list_head node_list;
+	struct link *active_links[2];
+	struct link *links[TIPC_MAX_BEARERS];
+	int link_cnt;
+	int working_links;
+	int permit_changeover;
+	int cleanup_required;
+	u16 signature;
+	u16 flags;
+	struct {
+		int supported;
+		u32 acked;
+		u32 last_in;
+		u32 last_sent;
+		u32 oos_state;
+		u32 deferred_size;
+		struct sk_buff *deferred_head;
+		struct sk_buff *deferred_tail;
+		struct sk_buff *defragm;
+	} bclink;
+};
+
+struct tipc_node *tipc_node_create(u32 addr);
+void tipc_node_delete(struct tipc_node *n_ptr);
+void tipc_node_link_up(struct tipc_node *n_ptr, struct link *l_ptr);
+void tipc_node_link_down(struct tipc_node *n_ptr, struct link *l_ptr);
+int tipc_node_has_redundant_links(struct tipc_node *n_ptr);
+int tipc_node_is_up(struct tipc_node *n_ptr);
+struct tipc_node *tipc_node_attach_link(struct link *l_ptr);
+void tipc_node_detach_link(struct tipc_node *n_ptr, struct link *l_ptr);
+struct sk_buff *tipc_node_get_nodes(const void *req_tlv_area, int req_tlv_space);
+struct sk_buff *tipc_node_get_links(const void *req_tlv_area, int req_tlv_space);
+
+
+static inline void tipc_node_lock(struct tipc_node *n_ptr)
+{
+        net_element_lock(&n_ptr->elm);	
+}
+
+static inline void tipc_node_unlock(struct tipc_node *n_ptr)
+{
+        net_element_unlock(&n_ptr->elm);	
+}
+
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_port.c android_cluster/linux-2.6.29/net/tipc/tipc_port.c
--- linux-2.6.29/net/tipc/tipc_port.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_port.c	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,1800 @@
+/*
+ * net/tipc/tipc_port.c: TIPC port code
+ *
+ * Copyright (c) 1992-2007, Ericsson AB
+ * Copyright (c) 2004-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_cfgsrv.h"
+#include "tipc_dbg.h"
+#include "tipc_port.h"
+#include "tipc_addr.h"
+#include "tipc_link.h"
+#include "tipc_node.h"
+#include "tipc_name_table.h"
+#include "tipc_user_reg.h"
+#include "tipc_msg.h"
+#include "tipc_bcast.h"
+
+/* Connection management: */
+#define PROBING_INTERVAL 3600000	/* [ms] => 1 h */
+#define CONFIRMED 0
+#define PROBING 1
+
+#define MAX_REJECT_SIZE 1024
+
+static struct sk_buff *msg_queue_head = NULL;
+static struct sk_buff *msg_queue_tail = NULL;
+
+DEFINE_SPINLOCK(tipc_port_list_lock);
+/* Protects global ports list AND each link's waiting ports list */
+
+static DEFINE_SPINLOCK(queue_lock);
+
+static LIST_HEAD(ports);
+static void port_handle_node_down(unsigned long ref);
+static struct sk_buff* port_build_self_abort_msg(struct port *,u32 err);
+static struct sk_buff* port_build_peer_abort_msg(struct port *,u32 err);
+static void port_timeout(unsigned long ref);
+
+
+static u32 port_peernode(struct port *p_ptr)
+{
+	return msg_destnode(&p_ptr->publ.phdr);
+}
+
+static u32 port_peerport(struct port *p_ptr)
+{
+	return msg_destport(&p_ptr->publ.phdr);
+}
+
+static void port_set_msg_importance(struct tipc_msg *msg, u32 importance)
+{
+	/* use port's default if value is TIPC_PORT_IMPORTANCE or invalid */
+
+	if (importance <= TIPC_CRITICAL_IMPORTANCE)
+		msg_set_importance(msg, importance);
+}
+
+/**
+ * tipc_multicast - send a multicast message to local and remote destinations
+ */
+
+int tipc_multicast(u32 ref, struct tipc_name_seq const *seq, u32 domain,
+		   u32 num_sect, struct iovec const *msg_sect)
+{
+	struct tipc_msg *hdr;
+	struct sk_buff *buf;
+	struct sk_buff *ibuf = NULL;
+	struct port_list dports = {0, NULL, };
+	struct port *oport = tipc_port_deref(ref);
+	int ext_targets;
+	int res;
+
+	if (unlikely(!oport))
+		return -EINVAL;
+
+	/* Create multicast message */
+
+	hdr = &oport->publ.phdr;
+	msg_set_hdr_sz(hdr, MCAST_H_SIZE);
+	msg_set_type(hdr, TIPC_MCAST_MSG);
+	msg_set_destport(hdr, 0);
+	msg_set_destnode(hdr, 0);
+	msg_set_nametype(hdr, seq->type);
+	msg_set_namelower(hdr, seq->lower);
+	msg_set_nameupper(hdr, seq->upper);
+	res = tipc_msg_build(hdr, msg_sect, num_sect, MAX_MSG_SIZE,
+			     !oport->user_port, &buf);
+	if (unlikely(!buf))
+		return res;
+
+	/* Figure out where to send multicast message */
+
+	ext_targets = tipc_nametbl_mc_translate(seq->type, seq->lower, seq->upper,
+						TIPC_NODE_SCOPE, &dports);
+
+	/* Send message to destinations (duplicate it only if necessary) */
+
+	if (ext_targets) {
+		if (dports.count != 0) {
+			ibuf = skb_copy(buf, GFP_ATOMIC);
+			if (ibuf == NULL) {
+				tipc_port_list_free(&dports);
+				buf_discard(buf);
+				return -ENOMEM;
+			}
+		}
+		res = tipc_bclink_send_msg(buf);
+		if ((res < 0) && (dports.count != 0)) {
+			buf_discard(ibuf);
+		}
+	} else {
+		ibuf = buf;
+	}
+
+	if (res >= 0) {
+		if (ibuf)
+			tipc_port_recv_mcast(ibuf, &dports);
+	} else {
+		tipc_port_list_free(&dports);
+	}
+	return res;
+}
+
+/**
+ * tipc_port_recv_mcast - deliver multicast message to all destination ports
+ *
+ * If there is no port list, perform a lookup to create one
+ */
+
+void tipc_port_recv_mcast(struct sk_buff *buf, struct port_list *dp)
+{
+	struct tipc_msg* msg;
+	struct port_list dports = {0, NULL, };
+	struct port_list *item = dp;
+	int cnt = 0;
+
+	msg = buf_msg(buf);
+
+	/* Create destination port list, if one wasn't supplied */
+
+	if (dp == NULL) {
+		tipc_nametbl_mc_translate(msg_nametype(msg),
+				     msg_namelower(msg),
+				     msg_nameupper(msg),
+				     TIPC_CLUSTER_SCOPE,
+				     &dports);
+		item = dp = &dports;
+	}
+
+	/* Deliver a copy of message to each destination port */
+
+	if (dp->count != 0) {
+		msg_set_destnode(msg, tipc_own_addr);
+		if (dp->count == 1) {
+			msg_set_destport(msg, dp->ports[0]);
+			tipc_port_recv_msg(buf);
+			tipc_port_list_free(dp);
+			return;
+		}
+		for (; cnt < dp->count; cnt++) {
+			int index = cnt % PLSIZE;
+			struct sk_buff *b = skb_clone(buf, GFP_ATOMIC);
+
+			if (b == NULL) {
+				warn("Unable to deliver multicast message(s)\n");
+				msg_dbg(msg, "LOST:");
+				goto exit;
+			}
+			if ((index == 0) && (cnt != 0)) {
+				item = item->next;
+			}
+			msg_set_destport(buf_msg(b), item->ports[index]);
+			tipc_port_recv_msg(b);
+		}
+	}
+exit:
+	buf_discard(buf);
+	tipc_port_list_free(dp);
+}
+
+/**
+ * tipc_createport_raw - create a generic TIPC port
+ *
+ * Returns pointer to (locked) TIPC port, or NULL if unable to create it
+ */
+
+#ifdef CONFIG_KRGRPC
+struct tipc_port *tipc_createport_raw(void *usr_handle,
+			u32 (*dispatcher)(struct tipc_port *, struct sk_buff *),
+			void (*wakeup)(struct tipc_port *),
+			const u32 importance, void *user_port)
+#else
+struct tipc_port *tipc_createport_raw(void *usr_handle,
+                        u32 (*dispatcher)(struct tipc_port *, struct sk_buff *),
+                        void (*wakeup)(struct tipc_port *),
+                        const u32 importance)
+#endif
+{
+	struct port *p_ptr;
+	struct tipc_msg *msg;
+	u32 ref;
+
+	p_ptr = kzalloc(sizeof(*p_ptr), GFP_ATOMIC);
+	if (!p_ptr) {
+		warn("Port creation failed, no memory\n");
+		return NULL;
+	}
+	ref = tipc_ref_acquire(p_ptr, &p_ptr->publ.lock);
+	if (!ref) {
+		warn("Port creation failed, reference table exhausted\n");
+		kfree(p_ptr);
+		return NULL;
+	}
+
+	p_ptr->publ.usr_handle = usr_handle;
+	p_ptr->publ.max_pkt = MAX_PKT_DEFAULT;
+	p_ptr->publ.ref = ref;
+	p_ptr->sent = 1;
+	INIT_LIST_HEAD(&p_ptr->wait_list);
+	INIT_LIST_HEAD(&p_ptr->subscription.sub_list);
+	p_ptr->dispatcher = dispatcher;
+	p_ptr->wakeup = wakeup;
+#ifdef CONFIG_KRGRPC
+	p_ptr->user_port = user_port;
+#else
+	p_ptr->user_port = NULL;
+#endif
+	k_init_timer(&p_ptr->timer, (Handler)port_timeout, ref);
+	INIT_LIST_HEAD(&p_ptr->publications);
+
+	/*
+	 * Must hold port list lock while initializing message header template
+	 * to ensure node's own network address isn't being altered
+	 */
+	
+	spin_lock_bh(&tipc_port_list_lock);
+	msg = &p_ptr->publ.phdr;
+	tipc_msg_init(msg, importance, TIPC_NAMED_MSG, LONG_H_SIZE, 0);
+	msg_set_origport(msg, ref);
+	list_add_tail(&p_ptr->port_list, &ports);
+	spin_unlock_bh(&tipc_port_list_lock);
+	return &(p_ptr->publ);
+}
+
+int tipc_deleteport(u32 ref)
+{
+	struct port *p_ptr;
+	struct sk_buff *buf = NULL;
+
+	tipc_withdraw(ref, 0, NULL);
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+
+	tipc_ref_discard(ref);
+	tipc_port_unlock(p_ptr);
+
+	k_cancel_timer(&p_ptr->timer);
+	if (p_ptr->publ.connected) {
+		buf = port_build_peer_abort_msg(p_ptr, TIPC_ERR_NO_PORT);
+		tipc_netsub_unbind(&p_ptr->subscription);
+	}
+	if (p_ptr->user_port) {
+		tipc_reg_remove_port(p_ptr->user_port);
+		kfree(p_ptr->user_port);
+	}
+
+	spin_lock_bh(&tipc_port_list_lock);
+	list_del(&p_ptr->port_list);
+	list_del(&p_ptr->wait_list);
+	spin_unlock_bh(&tipc_port_list_lock);
+	k_term_timer(&p_ptr->timer);
+	kfree(p_ptr);
+	dbg("Deleted port %u\n", ref);
+	tipc_net_route_msg(buf);
+	return 0;
+}
+
+/**
+ * tipc_get_port() - return port associated with 'ref'
+ *
+ * Note: Port is not locked.
+ */
+
+struct tipc_port *tipc_get_port(const u32 ref)
+{
+	return (struct tipc_port *)tipc_ref_deref(ref);
+}
+
+/**
+ * tipc_get_handle - return user handle associated to port 'ref'
+ */
+
+void *tipc_get_handle(const u32 ref)
+{
+	struct port *p_ptr;
+	void * handle;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return NULL;
+	handle = p_ptr->publ.usr_handle;
+	tipc_port_unlock(p_ptr);
+	return handle;
+}
+
+static int port_unreliable(struct port *p_ptr)
+{
+	return msg_src_droppable(&p_ptr->publ.phdr);
+}
+
+int tipc_portunreliable(u32 ref, unsigned int *isunreliable)
+{
+	struct port *p_ptr;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+	*isunreliable = port_unreliable(p_ptr);
+	tipc_port_unlock(p_ptr);
+	return 0;
+}
+
+int tipc_set_portunreliable(u32 ref, unsigned int isunreliable)
+{
+	struct port *p_ptr;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+	msg_set_src_droppable(&p_ptr->publ.phdr, (isunreliable != 0));
+	tipc_port_unlock(p_ptr);
+	return 0;
+}
+
+static int port_unreturnable(struct port *p_ptr)
+{
+	return msg_dest_droppable(&p_ptr->publ.phdr);
+}
+
+int tipc_portunreturnable(u32 ref, unsigned int *isunrejectable)
+{
+	struct port *p_ptr;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+	*isunrejectable = port_unreturnable(p_ptr);
+	tipc_port_unlock(p_ptr);
+	return 0;
+}
+
+int tipc_set_portunreturnable(u32 ref, unsigned int isunrejectable)
+{
+	struct port *p_ptr;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+	msg_set_dest_droppable(&p_ptr->publ.phdr, (isunrejectable != 0));
+	tipc_port_unlock(p_ptr);
+	return 0;
+}
+
+/*
+ * port_build_proto_msg(): build a port level protocol
+ * or a connection abortion message. Called with
+ * tipc_port lock on.
+ */
+static struct sk_buff *port_build_proto_msg(u32 destport, u32 destnode,
+					    u32 origport, u32 orignode,
+					    u32 usr, u32 type, u32 err,
+					    u32 ack)
+{
+	struct sk_buff *buf;
+	struct tipc_msg *msg;
+
+	buf = buf_acquire(LONG_H_SIZE);
+	if (buf) {
+		msg = buf_msg(buf);
+		tipc_msg_init(msg, usr, type, LONG_H_SIZE, destnode);
+		msg_set_errcode(msg, err);
+		msg_set_destport(msg, destport);
+		msg_set_origport(msg, origport);
+		msg_set_orignode(msg, orignode);
+		msg_set_msgcnt(msg, ack);
+		msg_dbg(msg, "PORT>SEND>:");
+	}
+	return buf;
+}
+
+int tipc_reject_msg(struct sk_buff *buf, u32 err)
+{
+	struct tipc_msg *msg = buf_msg(buf);
+	u32 data_sz;
+	u32 hdr_sz;
+	u32 msg_imp;
+
+	struct sk_buff *rbuf;
+	struct tipc_msg *rmsg;
+	u32 rmsg_sz;
+
+	msg_dbg(msg, "port->rej: ");
+
+	/* discard rejected message if it shouldn't be returned to sender */
+
+	while ((msg_user(msg) == MSG_FRAGMENTER) && 
+	       (msg_type(msg) == FIRST_FRAGMENT)) {
+		skb_pull(buf, INT_H_SIZE);
+		msg = buf_msg(buf);
+	}
+	data_sz = msg_data_sz(msg);
+
+	if (!msg_isdata(msg) && (msg_user(msg) != CONN_MANAGER)) 
+		goto exit;
+	if (msg_errcode(msg) || msg_dest_droppable(msg))
+		goto exit;
+
+	/* construct returned message */
+
+	msg_imp = msg_importance(msg);
+	hdr_sz = msg_hdr_sz(msg);
+
+	if (data_sz > MAX_REJECT_SIZE)
+		rmsg_sz = MAX_REJECT_SIZE;
+	else
+		rmsg_sz = data_sz;
+	rmsg_sz += hdr_sz;
+
+	rbuf = buf_acquire(rmsg_sz);
+	if (rbuf == NULL)
+		goto exit;
+
+	rmsg = buf_msg(rbuf);
+	skb_copy_to_linear_data(rbuf, msg, rmsg_sz);
+
+	/*
+	 * update fields of returned message header that need to be fixed up
+	 *
+	 * note: the "prev node" field is always updated when the returned
+	 * message is sent, so we don't have to do it here ...
+	 */
+
+	if (msg_connected(msg)) {
+		if (msg_imp < TIPC_CRITICAL_IMPORTANCE)
+			msg_set_importance(rmsg, ++msg_imp);
+	}
+	msg_set_non_seq(rmsg, 0);
+	msg_set_size(rmsg, rmsg_sz); 
+	msg_set_errcode(rmsg, err);
+	msg_reset_reroute_cnt(rmsg);
+	msg_swap_words(rmsg, 4, 5);
+	if (!msg_short(rmsg))
+		msg_swap_words(rmsg, 6, 7);
+	else
+		msg_set_destnode_cache(rmsg, msg_prevnode(rmsg));
+
+	/* 
+	 * notify sender's peer when a connection is broken due to congestion
+	 *
+	 * use original message (with data portion omitted) to notify peer;
+	 * don't have to worry about buffer being cloned, since that only
+	 * happens with multicast messages, which are connectionless ...
+	 */
+
+	if ((err == TIPC_ERR_OVERLOAD) && msg_connected(msg)) {
+		msg_set_importance(msg, msg_imp);
+		msg_set_size(msg, hdr_sz); 
+		skb_trim(buf, hdr_sz);
+		msg_set_errcode(msg, err);
+		msg_set_destnode_cache(msg, tipc_own_addr);
+		tipc_net_route_msg(buf);
+		buf = NULL;
+	}
+
+	/* send returned message & dispose of rejected message */
+
+	tipc_net_route_msg(rbuf);
+exit:
+	buf_discard(buf);
+	return data_sz;
+}
+
+int tipc_port_reject_sections(struct port *p_ptr, struct tipc_msg *hdr,
+			      struct iovec const *msg_sect, u32 num_sect,
+			      int err)
+{
+	struct sk_buff *buf;
+	int res;
+
+	res = tipc_msg_build(hdr, msg_sect, num_sect, MAX_MSG_SIZE, 
+			     !p_ptr->user_port, &buf);
+	if (!buf)
+		return res;
+
+	return tipc_reject_msg(buf, err);
+}
+
+static void port_timeout(unsigned long ref)
+{
+	struct port *p_ptr = tipc_port_lock(ref);
+	struct sk_buff *buf = NULL;
+
+	if (!p_ptr)
+		return;
+
+	if (!p_ptr->publ.connected) {
+		tipc_port_unlock(p_ptr);
+		return;
+	}
+
+	/* Last probe answered ? */
+	if (p_ptr->probing_state == PROBING) {
+		buf = port_build_self_abort_msg(p_ptr, TIPC_ERR_NO_PORT);
+	} else {
+		buf = port_build_proto_msg(port_peerport(p_ptr),
+					   port_peernode(p_ptr),
+					   p_ptr->publ.ref,
+					   tipc_own_addr,
+					   CONN_MANAGER,
+					   CONN_PROBE,
+					   TIPC_OK, 
+					   0);
+		p_ptr->probing_state = PROBING;
+		k_start_timer(&p_ptr->timer, p_ptr->probing_interval);
+	}
+	tipc_port_unlock(p_ptr);
+	tipc_net_route_msg(buf);
+}
+
+
+static void port_handle_node_down(unsigned long ref)
+{
+	struct port *p_ptr = tipc_port_lock(ref);
+	struct sk_buff* buf = NULL;
+
+	if (!p_ptr)
+		return;
+	buf = port_build_self_abort_msg(p_ptr, TIPC_ERR_NO_NODE);
+	tipc_port_unlock(p_ptr);
+	tipc_net_route_msg(buf);
+}
+
+
+static struct sk_buff *port_build_self_abort_msg(struct port *p_ptr, u32 err)
+{
+	u32 imp = msg_importance(&p_ptr->publ.phdr);
+
+	if (!p_ptr->publ.connected)
+		return NULL;
+	if (imp < TIPC_CRITICAL_IMPORTANCE)
+		imp++;
+	return port_build_proto_msg(p_ptr->publ.ref,
+				    tipc_own_addr,
+				    port_peerport(p_ptr),
+				    port_peernode(p_ptr),
+				    imp,
+				    TIPC_CONN_MSG,
+				    err, 
+				    0);
+}
+
+
+static struct sk_buff *port_build_peer_abort_msg(struct port *p_ptr, u32 err)
+{
+	u32 imp = msg_importance(&p_ptr->publ.phdr);
+
+	if (!p_ptr->publ.connected)
+		return NULL;
+	if (imp < TIPC_CRITICAL_IMPORTANCE)
+		imp++;
+	return port_build_proto_msg(port_peerport(p_ptr),
+				    port_peernode(p_ptr),
+				    p_ptr->publ.ref,
+				    tipc_own_addr,
+				    imp,
+				    TIPC_CONN_MSG,
+				    err, 
+				    0);
+}
+
+void tipc_port_recv_proto_msg(struct sk_buff *buf)
+{
+	struct tipc_msg *msg = buf_msg(buf);
+	struct port *p_ptr = tipc_port_lock(msg_destport(msg));
+	struct sk_buff *r_buf = NULL;
+	struct sk_buff *abort_buf = NULL;
+	u32 err = TIPC_OK;
+
+	msg_dbg(msg, "PORT<RECV<:");
+
+	if (!p_ptr) {
+		err = TIPC_ERR_NO_PORT;
+	} else if (p_ptr->publ.connected) {
+		if ((port_peernode(p_ptr) != msg_orignode(msg)) ||
+		    (port_peerport(p_ptr) != msg_origport(msg))) {
+			err = TIPC_ERR_NO_PORT;
+		} else if (msg_type(msg) == CONN_ACK) {
+			int wakeup = tipc_port_congested(p_ptr) && 
+				     p_ptr->publ.congested &&
+				     p_ptr->wakeup;
+			p_ptr->acked += msg_msgcnt(msg);
+			if (tipc_port_congested(p_ptr))
+				goto exit;
+			p_ptr->publ.congested = 0;
+			if (!wakeup)
+				goto exit;
+			p_ptr->wakeup(&p_ptr->publ);
+			goto exit;
+		}
+	} else if (p_ptr->publ.published) {
+		err = TIPC_ERR_NO_PORT;
+	}
+	if (err) {
+		r_buf = port_build_proto_msg(msg_origport(msg),
+					     msg_orignode(msg),
+					     msg_destport(msg),
+					     tipc_own_addr,
+					     TIPC_HIGH_IMPORTANCE,
+					     TIPC_CONN_MSG,
+					     err,
+					     0);
+		goto exit;
+	}
+
+	/* All is fine */
+	if (msg_type(msg) == CONN_PROBE) {
+		r_buf = port_build_proto_msg(msg_origport(msg),
+					     msg_orignode(msg),
+					     msg_destport(msg),
+					     tipc_own_addr,
+					     CONN_MANAGER,
+					     CONN_PROBE_REPLY,
+					     TIPC_OK,
+					     0);
+	}
+	p_ptr->probing_state = CONFIRMED;
+exit:
+	if (p_ptr)
+		tipc_port_unlock(p_ptr);
+	tipc_net_route_msg(r_buf);
+	tipc_net_route_msg(abort_buf);
+	buf_discard(buf);
+}
+
+#ifdef CONFIG_TIPC_CONFIG_SERVICE
+
+static void port_print(struct port *p_ptr, struct print_buf *buf, int full_id)
+{
+	struct publication *publ;
+
+	if (full_id)
+		tipc_printf(buf, "<%u.%u.%u:%u>:",
+			    tipc_zone(tipc_own_addr), tipc_cluster(tipc_own_addr),
+			    tipc_node(tipc_own_addr), p_ptr->publ.ref);
+	else
+		tipc_printf(buf, "%-10u:", p_ptr->publ.ref);
+
+	if (p_ptr->publ.connected) {
+		u32 dport = port_peerport(p_ptr);
+		u32 destnode = port_peernode(p_ptr);
+
+		tipc_printf(buf, " connected to <%u.%u.%u:%u>",
+			    tipc_zone(destnode), tipc_cluster(destnode),
+			    tipc_node(destnode), dport);
+		if (p_ptr->publ.conn_type != 0)
+			tipc_printf(buf, " via {%u,%u}",
+				    p_ptr->publ.conn_type,
+				    p_ptr->publ.conn_instance);
+	}
+	else if (p_ptr->publ.published) {
+		tipc_printf(buf, " bound to");
+		list_for_each_entry(publ, &p_ptr->publications, pport_list) {
+			if (publ->lower == publ->upper)
+				tipc_printf(buf, " {%u,%u}", publ->type,
+					    publ->lower);
+			else
+				tipc_printf(buf, " {%u,%u,%u}", publ->type,
+					    publ->lower, publ->upper);
+		}
+	}
+	tipc_printf(buf, "\n");
+}
+
+#define MAX_PORT_QUERY 32768
+
+struct sk_buff *tipc_port_get_ports(void)
+{
+	struct sk_buff *buf;
+	struct tlv_desc *rep_tlv;
+	struct print_buf pb;
+	struct port *p_ptr;
+	int str_len;
+
+	buf = tipc_cfg_reply_alloc(TLV_SPACE(MAX_PORT_QUERY));
+	if (!buf)
+		return NULL;
+	rep_tlv = (struct tlv_desc *)buf->data;
+
+	tipc_printbuf_init(&pb, TLV_DATA(rep_tlv), MAX_PORT_QUERY);
+	spin_lock_bh(&tipc_port_list_lock);
+	list_for_each_entry(p_ptr, &ports, port_list) {
+		spin_lock_bh(p_ptr->publ.lock);
+		port_print(p_ptr, &pb, 0);
+		spin_unlock_bh(p_ptr->publ.lock);
+	}
+	spin_unlock_bh(&tipc_port_list_lock);
+	str_len = tipc_printbuf_validate(&pb);
+
+	skb_put(buf, TLV_SPACE(str_len));
+	TLV_SET(rep_tlv, TIPC_TLV_ULTRA_STRING, NULL, str_len);
+
+	return buf;
+}
+
+#if 0
+
+#define MAX_PORT_STATS 2000
+
+struct sk_buff *port_show_stats(const void *req_tlv_area, int req_tlv_space)
+{
+	u32 ref;
+	struct port *p_ptr;
+	struct sk_buff *buf;
+	struct tlv_desc *rep_tlv;
+	struct print_buf pb;
+	int str_len;
+
+	if (!TLV_CHECK(req_tlv_area, req_tlv_space, TIPC_TLV_PORT_REF))
+		return cfg_reply_error_string(TIPC_CFG_TLV_ERROR);
+
+	ref = *(u32 *)TLV_DATA(req_tlv_area);
+	ref = ntohl(ref);
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return cfg_reply_error_string("port not found");
+
+	buf = tipc_cfg_reply_alloc(TLV_SPACE(MAX_PORT_STATS));
+	if (!buf) {
+		tipc_port_unlock(p_ptr);
+		return NULL;
+	}
+	rep_tlv = (struct tlv_desc *)buf->data;
+
+	tipc_printbuf_init(&pb, TLV_DATA(rep_tlv), MAX_PORT_STATS);
+	port_print(p_ptr, &pb, 1);
+	/* NEED TO FILL IN ADDITIONAL PORT STATISTICS HERE */
+	tipc_port_unlock(p_ptr);
+	str_len = tipc_printbuf_validate(&pb);
+
+	skb_put(buf, TLV_SPACE(str_len));
+	TLV_SET(rep_tlv, TIPC_TLV_ULTRA_STRING, NULL, str_len);
+
+	return buf;
+}
+
+#endif
+
+#endif
+
+void tipc_port_reinit(void)
+{
+	struct port *p_ptr;
+	struct tipc_msg *msg;
+
+	spin_lock_bh(&tipc_port_list_lock);
+	list_for_each_entry(p_ptr, &ports, port_list) {
+		msg = &p_ptr->publ.phdr;
+		spin_lock_bh(p_ptr->publ.lock);
+		msg_set_prevnode(msg, tipc_own_addr);
+		msg_set_orignode(msg, tipc_own_addr);
+		msg_set_destnode(msg, tipc_own_addr);
+		spin_unlock_bh(p_ptr->publ.lock);
+	}
+	spin_unlock_bh(&tipc_port_list_lock);
+}
+
+
+/*
+ *  port_dispatcher_sigh(): Signal handler for messages destinated
+ *                          to the tipc_port interface.
+ */
+
+static void port_dispatcher_sigh(void *dummy)
+{
+	struct sk_buff *buf;
+
+	spin_lock_bh(&queue_lock);
+	buf = msg_queue_head;
+	msg_queue_head = NULL;
+	spin_unlock_bh(&queue_lock);
+
+	while (buf) {
+		struct port *p_ptr;
+		struct user_port *up_ptr;
+		struct tipc_portid orig;
+		struct tipc_name_seq dseq;
+		void *usr_handle;
+		int connected;
+		int published;
+		u32 message_type;
+
+		struct sk_buff *next = buf->next;
+		struct tipc_msg *msg = buf_msg(buf);
+		u32 dref = msg_destport(msg);
+
+		message_type = msg_type(msg);
+		if (message_type > TIPC_DIRECT_MSG)
+			goto reject;	/* Unsupported message type */
+
+		p_ptr = tipc_port_lock(dref);
+		if (!p_ptr)
+			goto reject;	/* Port deleted while msg in queue */
+
+		orig.ref = msg_origport(msg);
+		orig.node = msg_orignode(msg);
+		up_ptr = p_ptr->user_port;
+		usr_handle = up_ptr->usr_handle;
+		connected = p_ptr->publ.connected;
+		published = p_ptr->publ.published;
+
+		if (unlikely(msg_errcode(msg)))
+			goto err;
+
+		switch (message_type) {
+
+		case TIPC_CONN_MSG:{
+				tipc_conn_msg_event cb = up_ptr->conn_msg_cb;
+				u32 peer_port = port_peerport(p_ptr);
+				u32 peer_node = port_peernode(p_ptr);
+
+				tipc_port_unlock(p_ptr);
+				if (unlikely(!cb))
+					goto reject;
+				if (unlikely(!connected)) {
+					if (tipc_connect2port(dref, &orig))
+						goto reject;
+				} else if ((msg_origport(msg) != peer_port) ||
+					   (msg_orignode(msg) != peer_node))
+					goto reject;
+				/* TODO: Don't access conn_unacked field
+					 while port is unlocked ... */
+				if (unlikely(++p_ptr->publ.conn_unacked >=
+					     TIPC_FLOW_CONTROL_WIN))
+					tipc_acknowledge(dref,
+							 p_ptr->publ.conn_unacked);
+				skb_pull(buf, msg_hdr_sz(msg));
+				cb(usr_handle, dref, &buf, msg_data(msg),
+				   msg_data_sz(msg));
+				break;
+			}
+		case TIPC_DIRECT_MSG:{
+				tipc_msg_event cb = up_ptr->msg_cb;
+
+				tipc_port_unlock(p_ptr);
+				if (unlikely(!cb || connected))
+					goto reject;
+				skb_pull(buf, msg_hdr_sz(msg));
+				cb(usr_handle, dref, &buf, msg_data(msg),
+				   msg_data_sz(msg), msg_importance(msg),
+				   &orig);
+				break;
+			}
+		case TIPC_MCAST_MSG:
+		case TIPC_NAMED_MSG:{
+				tipc_named_msg_event cb = up_ptr->named_msg_cb;
+
+				tipc_port_unlock(p_ptr);
+				if (unlikely(!cb || connected || !published))
+					goto reject;
+				dseq.type =  msg_nametype(msg);
+				dseq.lower = msg_nameinst(msg);
+				dseq.upper = (message_type == TIPC_NAMED_MSG)
+					? dseq.lower : msg_nameupper(msg);
+				skb_pull(buf, msg_hdr_sz(msg));
+				cb(usr_handle, dref, &buf, msg_data(msg),
+				   msg_data_sz(msg), msg_importance(msg),
+				   &orig, &dseq);
+				break;
+			}
+		}
+		if (buf)
+			buf_discard(buf);
+		buf = next;
+		continue;
+err:
+		switch (message_type) {
+
+		case TIPC_CONN_MSG:{
+				tipc_conn_shutdown_event cb =
+					up_ptr->conn_err_cb;
+				u32 peer_port = port_peerport(p_ptr);
+				u32 peer_node = port_peernode(p_ptr);
+
+				tipc_port_unlock(p_ptr);
+				if (!cb || !connected)
+					break;
+				if ((msg_origport(msg) != peer_port) ||
+				    (msg_orignode(msg) != peer_node))
+					break;
+				tipc_disconnect(dref);
+				skb_pull(buf, msg_hdr_sz(msg));
+				cb(usr_handle, dref, &buf, msg_data(msg),
+				   msg_data_sz(msg), msg_errcode(msg));
+				break;
+			}
+		case TIPC_DIRECT_MSG:{
+				tipc_msg_err_event cb = up_ptr->err_cb;
+
+				tipc_port_unlock(p_ptr);
+				if (!cb || connected)
+					break;
+				skb_pull(buf, msg_hdr_sz(msg));
+				cb(usr_handle, dref, &buf, msg_data(msg),
+				   msg_data_sz(msg), msg_errcode(msg), &orig);
+				break;
+			}
+		case TIPC_MCAST_MSG:
+		case TIPC_NAMED_MSG:{
+				tipc_named_msg_err_event cb =
+					up_ptr->named_err_cb;
+
+				tipc_port_unlock(p_ptr);
+				if (!cb || connected)
+					break;
+				dseq.type =  msg_nametype(msg);
+				dseq.lower = msg_nameinst(msg);
+				dseq.upper = (message_type == TIPC_NAMED_MSG)
+					? dseq.lower : msg_nameupper(msg);
+				skb_pull(buf, msg_hdr_sz(msg));
+				cb(usr_handle, dref, &buf, msg_data(msg),
+				   msg_data_sz(msg), msg_errcode(msg), &dseq);
+				break;
+			}
+		}
+		if (buf)
+			buf_discard(buf);
+		buf = next;
+		continue;
+reject:
+		tipc_reject_msg(buf, TIPC_ERR_NO_PORT);
+		buf = next;
+	}
+}
+
+/*
+ *  port_dispatcher(): Dispatcher for messages destinated
+ *  to the tipc_port interface. Called with port locked.
+ */
+
+static u32 port_dispatcher(struct tipc_port *dummy, struct sk_buff *buf)
+{
+	buf->next = NULL;
+	spin_lock_bh(&queue_lock);
+	if (msg_queue_head) {
+		msg_queue_tail->next = buf;
+		msg_queue_tail = buf;
+	} else {
+		msg_queue_tail = msg_queue_head = buf;
+		tipc_k_signal((Handler)port_dispatcher_sigh, 0);
+	}
+	spin_unlock_bh(&queue_lock);
+	return 0;
+}
+
+/*
+ * Wake up port after congestion: Called with port locked,
+ *
+ */
+
+static void port_wakeup_sh(unsigned long ref)
+{
+	struct port *p_ptr;
+	struct user_port *up_ptr;
+	tipc_continue_event cb = NULL;
+	void *uh = NULL;
+
+	p_ptr = tipc_port_lock(ref);
+	if (p_ptr) {
+		up_ptr = p_ptr->user_port;
+		if (up_ptr) {
+			cb = up_ptr->continue_event_cb;
+			uh = up_ptr->usr_handle;
+		}
+		tipc_port_unlock(p_ptr);
+	}
+	if (cb)
+		cb(uh, ref);
+}
+
+
+static void port_wakeup(struct tipc_port *p_ptr)
+{
+	tipc_k_signal((Handler)port_wakeup_sh, p_ptr->ref);
+}
+
+void tipc_acknowledge(u32 ref, u32 ack)
+{
+	struct port *p_ptr;
+	struct sk_buff *buf = NULL;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return;
+	if (p_ptr->publ.connected) {
+		p_ptr->publ.conn_unacked -= ack;
+		buf = port_build_proto_msg(port_peerport(p_ptr),
+					   port_peernode(p_ptr),
+					   ref,
+					   tipc_own_addr,
+					   CONN_MANAGER,
+					   CONN_ACK,
+					   TIPC_OK, 
+					   ack);
+	}
+	tipc_port_unlock(p_ptr);
+	tipc_net_route_msg(buf);
+}
+
+/*
+ * tipc_createport(): user level call. Will add port to
+ *                    registry if non-zero user_ref.
+ */
+
+int tipc_createport(u32 user_ref,
+		    void *usr_handle,
+		    unsigned int importance,
+		    tipc_msg_err_event error_cb,
+		    tipc_named_msg_err_event named_error_cb,
+		    tipc_conn_shutdown_event conn_error_cb,
+		    tipc_msg_event msg_cb,
+		    tipc_named_msg_event named_msg_cb,
+		    tipc_conn_msg_event conn_msg_cb,
+		    tipc_continue_event continue_event_cb,/* May be zero */
+		    u32 *portref)
+{
+	struct user_port *up_ptr;
+	struct port *p_ptr;
+
+	up_ptr = kmalloc(sizeof(*up_ptr), GFP_ATOMIC);
+	if (!up_ptr) {
+		warn("Port creation failed, no memory\n");
+		return -ENOMEM;
+	}
+#ifdef CONFIG_KRGRPC
+	p_ptr = (struct port *)tipc_createport_raw(NULL, port_dispatcher,
+						   port_wakeup, importance, up_ptr);
+#else
+        p_ptr = (struct port *)tipc_createport_raw(NULL, port_dispatcher,
+                                                   port_wakeup, importance);
+#endif
+	if (!p_ptr) {
+		kfree(up_ptr);
+		return -ENOMEM;
+	}
+
+	p_ptr->user_port = up_ptr;
+	up_ptr->user_ref = user_ref;
+	up_ptr->usr_handle = usr_handle;
+	up_ptr->ref = p_ptr->publ.ref;
+	up_ptr->err_cb = error_cb;
+	up_ptr->named_err_cb = named_error_cb;
+	up_ptr->conn_err_cb = conn_error_cb;
+	up_ptr->msg_cb = msg_cb;
+	up_ptr->named_msg_cb = named_msg_cb;
+	up_ptr->conn_msg_cb = conn_msg_cb;
+	up_ptr->continue_event_cb = continue_event_cb;
+	INIT_LIST_HEAD(&up_ptr->uport_list);
+	tipc_reg_add_port(up_ptr);
+	*portref = p_ptr->publ.ref;
+	tipc_port_unlock(p_ptr);
+	return 0;
+}
+
+int tipc_ownidentity(u32 ref, struct tipc_portid *id)
+{
+	id->ref = ref;
+	id->node = tipc_own_addr;
+	return 0;
+}
+
+int tipc_portimportance(u32 ref, unsigned int *importance)
+{
+	struct port *p_ptr;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+	*importance = (unsigned int)msg_importance(&p_ptr->publ.phdr);
+	tipc_port_unlock(p_ptr);
+	return 0;
+}
+
+int tipc_set_portimportance(u32 ref, unsigned int imp)
+{
+	struct port *p_ptr;
+
+	if (imp > TIPC_CRITICAL_IMPORTANCE)
+		return -EINVAL;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+	msg_set_importance(&p_ptr->publ.phdr, (u32)imp);
+	tipc_port_unlock(p_ptr);
+	return 0;
+}
+
+
+int tipc_publish(u32 ref, unsigned int scope, struct tipc_name_seq const *seq)
+{
+	struct port *p_ptr;
+	struct publication *publ;
+	u32 key;
+	int res = -EINVAL;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+	if (p_ptr->publ.connected)
+		goto exit;
+	if (seq->lower > seq->upper)
+		goto exit;
+	if ((scope < TIPC_ZONE_SCOPE) || (scope > TIPC_NODE_SCOPE))
+		goto exit;
+	key = ref + p_ptr->pub_count + 1;
+	if (key == ref) {
+		res = -EADDRINUSE;
+		goto exit;
+	}
+	publ = tipc_nametbl_publish(seq->type, seq->lower, seq->upper,
+				    scope, p_ptr->publ.ref, key);
+	if (publ) {
+		list_add(&publ->pport_list, &p_ptr->publications);
+		p_ptr->pub_count++;
+		p_ptr->publ.published = 1;
+		res = 0;
+	}
+exit:
+	tipc_port_unlock(p_ptr);
+	return res;
+}
+
+int tipc_withdraw(u32 ref, unsigned int scope, struct tipc_name_seq const *seq)
+{
+	struct port *p_ptr;
+	struct publication *publ;
+	struct publication *tpubl;
+	int res = -EINVAL;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+	if (!seq) {
+		list_for_each_entry_safe(publ, tpubl,
+					 &p_ptr->publications, pport_list) {
+			tipc_nametbl_withdraw(publ->type, publ->lower,
+					      publ->ref, publ->key);
+		}
+		res = 0;
+	} else {
+		list_for_each_entry_safe(publ, tpubl,
+					 &p_ptr->publications, pport_list) {
+			if (publ->scope != scope)
+				continue;
+			if (publ->type != seq->type)
+				continue;
+			if (publ->lower != seq->lower)
+				continue;
+			if (publ->upper != seq->upper)
+				break;
+			tipc_nametbl_withdraw(publ->type, publ->lower,
+					      publ->ref, publ->key);
+			res = 0;
+			break;
+		}
+	}
+	if (list_empty(&p_ptr->publications))
+		p_ptr->publ.published = 0;
+	tipc_port_unlock(p_ptr);
+	return res;
+}
+
+int tipc_connect2port(u32 ref, struct tipc_portid const *peer)
+{
+	struct port *p_ptr;
+	struct tipc_msg *msg;
+	int res = -EINVAL;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+	if (p_ptr->publ.published || p_ptr->publ.connected)
+		goto exit;
+	if (!peer->ref)
+		goto exit;
+
+	msg = &p_ptr->publ.phdr;
+	msg_set_destnode(msg, peer->node);
+	msg_set_destport(msg, peer->ref);
+	msg_set_orignode(msg, tipc_own_addr);
+	msg_set_origport(msg, p_ptr->publ.ref);
+	msg_set_type(msg, TIPC_CONN_MSG);
+	if (!may_route(peer->node))
+		msg_set_hdr_sz(msg, SHORT_H_SIZE);
+	else
+		msg_set_hdr_sz(msg, LONG_H_SIZE);
+
+	p_ptr->probing_interval = PROBING_INTERVAL;
+	p_ptr->probing_state = CONFIRMED;
+	p_ptr->publ.connected = 1;
+	k_start_timer(&p_ptr->timer, p_ptr->probing_interval);
+
+	if (!addr_in_node(peer->node))
+		tipc_netsub_bind(&p_ptr->subscription, peer->node,
+				 (net_ev_handler)port_handle_node_down,
+				 (void *)(unsigned long)ref);
+	res = 0;
+exit:
+	tipc_port_unlock(p_ptr);
+	p_ptr->publ.max_pkt = tipc_link_get_max_pkt(peer->node, ref);
+	return res;
+}
+
+/**
+ * tipc_disconnect_port - disconnect port from peer
+ *
+ * Port must be locked.
+ */
+
+int tipc_disconnect_port(struct tipc_port *tp_ptr)
+{
+	int res;
+
+	if (tp_ptr->connected) {
+		tp_ptr->connected = 0;
+		/* let timer expire on it's own to avoid deadlock! */
+		tipc_netsub_unbind(&((struct port *)tp_ptr)->subscription);
+		res = 0;
+	} else {
+		res = -ENOTCONN;
+	}
+	return res;
+}
+
+/*
+ * tipc_disconnect(): Disconnect port from peer.
+ *                    This is a node local operation.
+ */
+
+int tipc_disconnect(u32 ref)
+{
+	struct port *p_ptr;
+	int res;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+	res = tipc_disconnect_port((struct tipc_port *)p_ptr);
+	tipc_port_unlock(p_ptr);
+	return res;
+}
+
+/*
+ * tipc_shutdown(): Send a SHUTDOWN msg to peer and disconnect
+ */
+int tipc_shutdown(u32 ref)
+{
+	struct port *p_ptr;
+	struct sk_buff *buf = NULL;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+
+	if (p_ptr->publ.connected) {
+		u32 imp = msg_importance(&p_ptr->publ.phdr);
+		if (imp < TIPC_CRITICAL_IMPORTANCE)
+			imp++;
+		buf = port_build_proto_msg(port_peerport(p_ptr),
+					   port_peernode(p_ptr),
+					   ref,
+					   tipc_own_addr,
+					   imp,
+					   TIPC_CONN_MSG,
+					   TIPC_CONN_SHUTDOWN, 
+					   0);
+	}
+	tipc_port_unlock(p_ptr);
+	tipc_net_route_msg(buf);
+	return tipc_disconnect(ref);
+}
+
+int tipc_isconnected(u32 ref, int *isconnected)
+{
+	struct port *p_ptr;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+	*isconnected = p_ptr->publ.connected;
+	tipc_port_unlock(p_ptr);
+	return 0;
+}
+
+int tipc_peer(u32 ref, struct tipc_portid *peer)
+{
+	struct port *p_ptr;
+	int res;
+
+	p_ptr = tipc_port_lock(ref);
+	if (!p_ptr)
+		return -EINVAL;
+	if (p_ptr->publ.connected) {
+		peer->ref = port_peerport(p_ptr);
+		peer->node = port_peernode(p_ptr);
+		res = 0;
+	} else
+		res = -ENOTCONN;
+	tipc_port_unlock(p_ptr);
+	return res;
+}
+
+int tipc_ref_valid(u32 ref)
+{
+	/* Works irrespective of type */
+	return !!tipc_ref_deref(ref);
+}
+
+static inline int port_msg_from_peer(struct port *p_ptr, struct tipc_msg *msg)
+{
+	u32 peernode;
+	u32 orignode;
+
+	if (!msg_connected(msg) ||
+	    (msg_origport(msg) != tipc_peer_port(p_ptr)))
+		return 0;
+
+	orignode = msg_orignode(msg);
+	peernode = tipc_peer_node(p_ptr);
+	return (orignode == peernode) ||
+		(!orignode && (peernode == tipc_own_addr)) ||
+		(!peernode && (orignode == tipc_own_addr));
+}
+
+/** 
+ * tipc_port_recv_msg - receive message from lower layer and deliver to port user
+ */
+
+int tipc_port_recv_msg(struct sk_buff *buf)
+{
+	struct port *p_ptr;
+	struct tipc_msg *msg = buf_msg(buf);
+	u32 destport = msg_destport(msg);
+	u32 dsz = msg_data_sz(msg);
+	u32 err;
+	
+	/* forward unresolved named message */
+	if (unlikely(destport == 0)) {
+		tipc_net_route_msg(buf);
+		return dsz;
+	}
+
+	/* validate destination & pass to port, otherwise reject message */
+	p_ptr = tipc_port_lock(destport);
+	if (likely(p_ptr)) {
+		if (likely(p_ptr->publ.connected)) {
+			if (unlikely(!port_msg_from_peer(p_ptr, msg))) {
+				err = TIPC_ERR_NO_PORT;
+				tipc_port_unlock(p_ptr);
+				goto reject;
+			}
+		}
+		err = p_ptr->dispatcher(&p_ptr->publ, buf);
+		tipc_port_unlock(p_ptr);
+		if (likely(!err))
+			return dsz;
+	} else {
+		err = TIPC_ERR_NO_PORT;
+	}
+reject:
+	dbg("port->rejecting, err = %x..\n",err);
+	return tipc_reject_msg(buf, err);
+}
+
+/*
+ *  tipc_port_recv_sections(): Concatenate and deliver sectioned
+ *                        message for this node.
+ */
+
+int tipc_port_recv_sections(struct port *sender, unsigned int num_sect,
+		       struct iovec const *msg_sect)
+{
+	struct sk_buff *buf;
+	int res;
+
+	res = tipc_msg_build(&sender->publ.phdr, msg_sect, num_sect,
+			     MAX_MSG_SIZE, !sender->user_port, &buf);
+	if (likely(buf))
+		tipc_port_recv_msg(buf);
+	return res;
+}
+
+/**
+ * tipc_send - send message sections on connection
+ */
+
+int tipc_send(u32 ref, unsigned int num_sect, struct iovec const *msg_sect)
+{
+	struct port *p_ptr;
+	u32 destnode;
+	int res;
+
+	p_ptr = tipc_port_deref(ref);
+	if (!p_ptr || !p_ptr->publ.connected)
+		return -EINVAL;
+
+	p_ptr->publ.congested = 1;
+	if (!tipc_port_congested(p_ptr)) {
+		destnode = port_peernode(p_ptr);
+		if (!addr_in_node(destnode))
+			res = tipc_link_send_sections_fast(p_ptr, msg_sect,
+							   num_sect, destnode);
+		else
+			res = tipc_port_recv_sections(p_ptr, num_sect,
+						      msg_sect);
+
+		if (likely(res != -ELINKCONG)) {
+			p_ptr->publ.congested = 0;
+			p_ptr->sent++;
+			return res;
+		}
+	}
+	if (port_unreliable(p_ptr)) {
+		p_ptr->publ.congested = 0;
+		/* Just calculate msg length and return */
+		return tipc_msg_calc_data_size(msg_sect, num_sect);
+	}
+	return -ELINKCONG;
+}
+
+/**
+ * tipc_send_buf - send message buffer on connection
+ */
+
+int tipc_send_buf(u32 ref, struct sk_buff *buf, unsigned int dsz)
+{
+	struct port *p_ptr;
+	struct tipc_msg *msg;
+	u32 destnode;
+	u32 hsz;
+	u32 sz;
+	u32 res;
+
+	p_ptr = tipc_port_deref(ref);
+	if (!p_ptr || !p_ptr->publ.connected)
+		return -EINVAL;
+
+	msg = &p_ptr->publ.phdr;
+	hsz = msg_hdr_sz(msg);
+	sz = hsz + dsz;
+	msg_set_size(msg, sz);
+	if (skb_cow(buf, hsz))
+		return -ENOMEM;
+
+	skb_push(buf, hsz);
+	skb_copy_to_linear_data(buf, msg, hsz);
+
+	p_ptr->publ.congested = 1;
+	if (!tipc_port_congested(p_ptr)) {
+		destnode = port_peernode(p_ptr);
+		if (!addr_in_node(destnode))
+			res = tipc_send_buf_fast(buf, destnode);
+		else {
+			tipc_port_recv_msg(buf);
+			res = sz;
+		}
+
+		if (likely(res != -ELINKCONG)) {
+			p_ptr->publ.congested = 0;
+			p_ptr->sent++;
+			return res;
+		}
+	}
+	if (port_unreliable(p_ptr)) {
+		p_ptr->publ.congested = 0;
+		return dsz;
+	}
+	return -ELINKCONG;
+}
+
+/**
+ * tipc_forward2name - forward message sections to port name
+ */
+
+int tipc_forward2name(u32 ref,
+		      struct tipc_name const *name,
+		      u32 domain,
+		      u32 num_sect,
+		      struct iovec const *msg_sect,
+		      struct tipc_portid const *orig,
+		      unsigned int importance)
+{
+	struct port *p_ptr;
+	struct tipc_msg *msg;
+	u32 destnode = domain;
+	u32 destport;
+	int res;
+
+	p_ptr = tipc_port_deref(ref);
+	if (!p_ptr || p_ptr->publ.connected)
+		return -EINVAL;
+
+	destport = tipc_nametbl_translate(name->type, name->instance,
+					  &destnode);
+	msg = &p_ptr->publ.phdr;
+	msg_set_hdr_sz(msg, LONG_H_SIZE);
+	port_set_msg_importance(msg, importance);
+	msg_set_type(msg, TIPC_NAMED_MSG);
+	msg_set_nametype(msg, name->type);
+	msg_set_nameinst(msg, name->instance);
+	msg_set_lookup_scope(msg, addr_scope(domain));
+	msg_set_orignode(msg, orig->node);
+	msg_set_origport(msg, orig->ref);
+	msg_set_destnode(msg, destnode);
+	msg_set_destport(msg, destport);
+
+	if (likely(destport || destnode)) {
+		p_ptr->sent++;
+		if (addr_in_node(destnode))
+			return tipc_port_recv_sections(p_ptr, num_sect,
+						       msg_sect);
+		if (!orig->node)
+			msg_set_orignode(msg, tipc_own_addr);
+		res = tipc_link_send_sections_fast(p_ptr, msg_sect, num_sect,
+						   destnode);
+		if (likely(res != -ELINKCONG))
+			return res;
+		if (port_unreliable(p_ptr)) {
+			/* Just calculate msg length and return */
+			return tipc_msg_calc_data_size(msg_sect, num_sect);
+		}
+		return -ELINKCONG;
+	}
+	return tipc_port_reject_sections(p_ptr, msg, msg_sect, num_sect,
+					 TIPC_ERR_NO_NAME);
+}
+
+/**
+ * tipc_send2name - send message sections to port name
+ */
+
+int tipc_send2name(u32 ref,
+		   struct tipc_name const *name,
+		   unsigned int domain,
+		   unsigned int num_sect,
+		   struct iovec const *msg_sect)
+{
+	struct tipc_portid orig;
+
+	orig.ref = ref;
+	orig.node = tipc_own_addr;
+	return tipc_forward2name(ref, name, domain, num_sect, msg_sect, &orig,
+				 TIPC_PORT_IMPORTANCE);
+}
+
+/**
+ * tipc_forward_buf2name - forward message buffer to port name
+ */
+
+int tipc_forward_buf2name(u32 ref,
+			  struct tipc_name const *name,
+			  u32 domain,
+			  struct sk_buff *buf,
+			  unsigned int dsz,
+			  struct tipc_portid const *orig,
+			  unsigned int importance)
+{
+	struct port *p_ptr;
+	struct tipc_msg *msg;
+	u32 destnode = domain;
+	u32 destport;
+	int res;
+
+	p_ptr = (struct port *)tipc_ref_deref(ref);
+	if (!p_ptr || p_ptr->publ.connected)
+		return -EINVAL;
+
+	if (skb_cow(buf, LONG_H_SIZE))
+		return -ENOMEM;
+
+	destport = tipc_nametbl_translate(name->type, name->instance,
+					  &destnode);
+	msg = &p_ptr->publ.phdr;
+	msg_set_hdr_sz(msg, LONG_H_SIZE);
+	msg_set_size(msg, LONG_H_SIZE + dsz);
+	port_set_msg_importance(msg, importance);
+	msg_set_type(msg, TIPC_NAMED_MSG);
+	msg_set_nametype(msg, name->type);
+	msg_set_nameinst(msg, name->instance);
+	msg_set_lookup_scope(msg, addr_scope(domain));
+	msg_set_orignode(msg, orig->node);
+	msg_set_origport(msg, orig->ref);
+	msg_set_destnode(msg, destnode);
+	msg_set_destport(msg, destport);
+
+	skb_push(buf, LONG_H_SIZE);
+	skb_copy_to_linear_data(buf, msg, LONG_H_SIZE);
+
+	if (likely(destport || destnode)) {
+		p_ptr->sent++;
+		if (addr_in_node(destnode))
+			return tipc_port_recv_msg(buf);
+		if (!orig->node)
+			msg_set_orignode(msg, tipc_own_addr);
+		res = tipc_send_buf_fast(buf, destnode);
+		if (likely(res != -ELINKCONG))
+			return res;
+		if (port_unreliable(p_ptr))
+			return dsz;
+		return -ELINKCONG;
+	}
+	return tipc_reject_msg(buf, TIPC_ERR_NO_NAME);
+}
+
+/**
+ * tipc_send_buf2name - send message buffer to port name
+ */
+
+int tipc_send_buf2name(u32 ref,
+		       struct tipc_name const *dest,
+		       u32 domain,
+		       struct sk_buff *buf,
+		       unsigned int dsz)
+{
+	struct tipc_portid orig;
+
+	orig.ref = ref;
+	orig.node = tipc_own_addr;
+	return tipc_forward_buf2name(ref, dest, domain, buf, dsz, &orig,
+				     TIPC_PORT_IMPORTANCE);
+}
+
+/**
+ * tipc_forward2port - forward message sections to port identity
+ */
+
+int tipc_forward2port(u32 ref,
+		      struct tipc_portid const *dest,
+		      unsigned int num_sect,
+		      struct iovec const *msg_sect,
+		      struct tipc_portid const *orig,
+		      unsigned int importance)
+{
+	struct port *p_ptr;
+	struct tipc_msg *msg;
+	int res;
+
+	p_ptr = tipc_port_deref(ref);
+	if (!p_ptr || p_ptr->publ.connected)
+		return -EINVAL;
+
+	msg = &p_ptr->publ.phdr;
+	msg_set_hdr_sz(msg, DIR_MSG_H_SIZE);
+	port_set_msg_importance(msg, importance);
+	msg_set_type(msg, TIPC_DIRECT_MSG);
+	msg_set_orignode(msg, orig->node);
+	msg_set_origport(msg, orig->ref);
+	msg_set_destnode(msg, dest->node);
+	msg_set_destport(msg, dest->ref);
+
+	p_ptr->sent++;
+	if (addr_in_node(dest->node))
+		return tipc_port_recv_sections(p_ptr, num_sect, msg_sect);
+	if (!orig->node)
+		msg_set_orignode(msg, tipc_own_addr);
+	res = tipc_link_send_sections_fast(p_ptr, msg_sect, num_sect,
+					   dest->node);
+	if (likely(res != -ELINKCONG))
+		return res;
+	if (port_unreliable(p_ptr)) {
+		/* Just calculate msg length and return */
+		return tipc_msg_calc_data_size(msg_sect, num_sect);
+	}
+	return -ELINKCONG;
+}
+
+/**
+ * tipc_send2port - send message sections to port identity
+ */
+
+int tipc_send2port(u32 ref,
+		   struct tipc_portid const *dest,
+		   unsigned int num_sect,
+		   struct iovec const *msg_sect)
+{
+	struct tipc_portid orig;
+
+	orig.ref = ref;
+	orig.node = tipc_own_addr;
+	return tipc_forward2port(ref, dest, num_sect, msg_sect, &orig,
+				 TIPC_PORT_IMPORTANCE);
+}
+
+/**
+ * tipc_forward_buf2port - forward message buffer to port identity
+ */
+int tipc_forward_buf2port(u32 ref,
+			  struct tipc_portid const *dest,
+			  struct sk_buff *buf,
+			  unsigned int dsz,
+			  struct tipc_portid const *orig,
+			  unsigned int importance)
+{
+	struct port *p_ptr;
+	struct tipc_msg *msg;
+	int res;
+
+	p_ptr = (struct port *)tipc_ref_deref(ref);
+	if (!p_ptr || p_ptr->publ.connected)
+		return -EINVAL;
+
+	if (skb_cow(buf, DIR_MSG_H_SIZE))
+		return -ENOMEM;
+
+	msg = &p_ptr->publ.phdr;
+	msg_set_hdr_sz(msg, DIR_MSG_H_SIZE);
+	msg_set_size(msg, DIR_MSG_H_SIZE + dsz);
+	port_set_msg_importance(msg, importance);
+	msg_set_type(msg, TIPC_DIRECT_MSG);
+	msg_set_orignode(msg, orig->node);
+	msg_set_origport(msg, orig->ref);
+	msg_set_destnode(msg, dest->node);
+	msg_set_destport(msg, dest->ref);
+
+	skb_push(buf, DIR_MSG_H_SIZE);
+	skb_copy_to_linear_data(buf, msg, DIR_MSG_H_SIZE);
+
+	p_ptr->sent++;
+	if (addr_in_node(dest->node))
+		return tipc_port_recv_msg(buf);
+	if (!orig->node)
+		msg_set_orignode(msg, tipc_own_addr);
+	res = tipc_send_buf_fast(buf, dest->node);
+	if (likely(res != -ELINKCONG))
+		return res;
+	if (port_unreliable(p_ptr))
+		return dsz;
+	return -ELINKCONG;
+}
+
+/**
+ * tipc_send_buf2port - send message buffer to port identity
+ */
+
+int tipc_send_buf2port(u32 ref,
+		       struct tipc_portid const *dest,
+		       struct sk_buff *buf,
+		       unsigned int dsz)
+{
+	struct tipc_portid orig;
+
+	orig.ref = ref;
+	orig.node = tipc_own_addr;
+	return tipc_forward_buf2port(ref, dest, buf, dsz, &orig,
+				     TIPC_PORT_IMPORTANCE);
+}
+
diff -ruN linux-2.6.29/net/tipc/tipc_port.h android_cluster/linux-2.6.29/net/tipc/tipc_port.h
--- linux-2.6.29/net/tipc/tipc_port.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_port.h	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,163 @@
+/*
+ * net/tipc/tipc_port.h: Include file for TIPC port code
+ *
+ * Copyright (c) 1994-2007, Ericsson AB
+ * Copyright (c) 2004-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_PORT_H
+#define _TIPC_PORT_H
+
+#include "tipc_core.h"
+#include "tipc_ref.h"
+#include "tipc_net.h"
+#include "tipc_msg.h"
+#include "tipc_dbg.h"
+
+/**
+ * struct user_port - TIPC user port (used with native API)
+ * @user_ref: id of user who created user port
+ * @usr_handle: user-specified field
+ * @ref: object reference to associated TIPC port
+ * <various callback routines>
+ * @uport_list: adjacent user ports in list of ports held by user
+ */
+
+struct user_port {
+	u32 user_ref;
+	void *usr_handle;
+	u32 ref;
+	tipc_msg_err_event err_cb;
+	tipc_named_msg_err_event named_err_cb;
+	tipc_conn_shutdown_event conn_err_cb;
+	tipc_msg_event msg_cb;
+	tipc_named_msg_event named_msg_cb;
+	tipc_conn_msg_event conn_msg_cb;
+	tipc_continue_event continue_event_cb;
+	struct list_head uport_list;
+};
+
+/**
+ * struct port - TIPC port structure
+ * @publ: TIPC port info available to privileged users
+ * @port_list: adjacent ports in TIPC's global list of ports
+ * @dispatcher: ptr to routine which handles received messages
+ * @wakeup: ptr to routine to call when port is no longer congested
+ * @user_port: ptr to user port associated with port (if any)
+ * @wait_list: adjacent ports in list of ports waiting on link congestion
+ * @waiting_pkts:
+ * @sent:
+ * @acked:
+ * @publications: list of publications for port
+ * @pub_count: total # of publications port has made during its lifetime
+ * @probing_state:
+ * @probing_interval:
+ * @timer_ref:
+ * @subscription: network element subscription (used to break unreachable connections)
+ */
+
+struct port {
+	struct tipc_port publ;
+	struct list_head port_list;
+	u32 (*dispatcher)(struct tipc_port *, struct sk_buff *);
+	void (*wakeup)(struct tipc_port *);
+	struct user_port *user_port;
+	struct list_head wait_list;
+	u32 waiting_pkts;
+	u32 sent;
+	u32 acked;
+	struct list_head publications;
+	u32 pub_count;
+	u32 probing_state;
+	u32 probing_interval;
+	struct timer_list timer;
+	struct net_subscr subscription;
+};
+
+
+DECLARE_SPINLOCK(tipc_port_list_lock);
+
+struct port_list;
+
+int tipc_port_recv_msg(struct sk_buff *buf);
+int tipc_port_recv_sections(struct port *p_ptr, u32 num_sect,
+			    struct iovec const *msg_sect);
+int tipc_port_reject_sections(struct port *p_ptr, struct tipc_msg *hdr,
+			      struct iovec const *msg_sect, u32 num_sect,
+			      int err);
+struct sk_buff *tipc_port_get_ports(void);
+struct sk_buff *port_show_stats(const void *req_tlv_area, int req_tlv_space);
+void tipc_port_recv_proto_msg(struct sk_buff *buf);
+void tipc_port_recv_mcast(struct sk_buff *buf, struct port_list *dp);
+void tipc_port_reinit(void);
+
+/**
+ * tipc_port_lock - lock port instance referred to and return its pointer
+ */
+
+static inline struct port *tipc_port_lock(u32 ref)
+{
+	return (struct port *)tipc_ref_lock(ref);
+}
+
+/**
+ * tipc_port_unlock - unlock a port instance
+ *
+ * Can use pointer instead of tipc_ref_unlock() since port is already locked.
+ */
+
+static inline void tipc_port_unlock(struct port *p_ptr)
+{
+	spin_unlock_bh(p_ptr->publ.lock);
+}
+
+static inline struct port* tipc_port_deref(u32 ref)
+{
+	return (struct port *)tipc_ref_deref(ref);
+}
+
+static inline u32 tipc_peer_port(struct port *p_ptr)
+{
+	return msg_destport(&p_ptr->publ.phdr);
+}
+
+static inline u32 tipc_peer_node(struct port *p_ptr)
+{
+	return msg_destnode(&p_ptr->publ.phdr);
+}
+
+static inline int tipc_port_congested(struct port *p_ptr)
+{
+	return((p_ptr->sent - p_ptr->acked) >= (TIPC_FLOW_CONTROL_WIN * 2));
+}
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_ref.c android_cluster/linux-2.6.29/net/tipc/tipc_ref.c
--- linux-2.6.29/net/tipc/tipc_ref.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_ref.c	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,317 @@
+/*
+ * net/tipc/tipc_ref.c: TIPC object registry code
+ *
+ * Copyright (c) 1991-2006, Ericsson AB
+ * Copyright (c) 2004-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_ref.h"
+
+/**
+ * struct reference - TIPC object reference entry
+ * @object: pointer to object associated with reference entry
+ * @lock: spinlock controlling access to object
+ * @ref: reference value for object (combines instance & array index info)
+ */
+
+struct reference {
+	void *object;
+	spinlock_t lock;
+	u32 ref;
+};
+
+/**
+ * struct tipc_ref_table - table of TIPC object reference entries
+ * @entries: pointer to array of reference entries
+ * @capacity: array index of first unusable entry
+ * @init_point: array index of first uninitialized entry
+ * @first_free: array index of first unused object reference entry
+ * @last_free: array index of last unused object reference entry
+ * @index_mask: bitmask for array index portion of reference values
+ * @start_mask: initial value for instance value portion of reference values
+ */
+
+struct ref_table {
+	struct reference *entries;
+	u32 capacity;
+	u32 init_point;
+	u32 first_free;
+	u32 last_free;
+	u32 index_mask;
+	u32 start_mask;
+};
+
+/*
+ * Object reference table consists of 2**N entries.
+ *
+ * State	Object ptr	Reference
+ * -----        ----------      ---------
+ * In use        non-NULL       XXXX|own index
+ *				(XXXX changes each time entry is acquired)
+ * Free            NULL         YYYY|next free index
+ *				(YYYY is one more than last used XXXX)
+ * Uninitialized   NULL         0
+ *
+ * Entry 0 is not used; this allows index 0 to denote the end of the free list.
+ *
+ * Note that a reference value of 0 does not necessarily indicate that an
+ * entry is uninitialized, since the last entry in the free list could also
+ * have a reference value of 0 (although this is unlikely).
+ */
+
+static struct ref_table tipc_ref_table = { NULL };
+
+static DEFINE_RWLOCK(ref_table_lock);
+
+/**
+ * tipc_ref_table_init - create reference table for objects
+ */
+
+int tipc_ref_table_init(u32 requested_size, u32 start)
+{
+	struct reference *table;
+	u32 actual_size;
+
+	/* account for unused entry, then round up size to a power of 2 */
+
+	requested_size++;
+	for (actual_size = 16; actual_size < requested_size; actual_size <<= 1)
+		/* do nothing */ ;
+
+	/* allocate table & mark all entries as uninitialized */
+
+	table = __vmalloc(actual_size * sizeof(struct reference),
+			  GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO, PAGE_KERNEL);
+	if (table == NULL)
+		return -ENOMEM;
+
+	tipc_ref_table.entries = table;
+	tipc_ref_table.capacity = requested_size;
+	tipc_ref_table.init_point = 1;
+	tipc_ref_table.first_free = 0;
+	tipc_ref_table.last_free = 0;
+	tipc_ref_table.index_mask = actual_size - 1;
+	tipc_ref_table.start_mask = start & ~tipc_ref_table.index_mask;
+
+	return 0;
+}
+
+/**
+ * tipc_ref_table_stop - destroy reference table for objects
+ */
+
+void tipc_ref_table_stop(void)
+{
+	if (!tipc_ref_table.entries)
+		return;
+
+	while (--tipc_ref_table.init_point > 0) { 
+		spin_lock_term(&tipc_ref_table.entries[tipc_ref_table.init_point].lock);
+	}
+
+	vfree(tipc_ref_table.entries);
+	tipc_ref_table.entries = NULL;
+}
+
+/**
+ * tipc_ref_acquire - create reference to an object
+ *
+ * Register an object pointer in reference table and lock the object.
+ * Returns a unique reference value that is used from then on to retrieve the
+ * object pointer, or to determine that the object has been deregistered.
+ *
+ * Note: The object is returned in the locked state so that the caller can
+ * register a partially initialized object, without running the risk that
+ * the object will be accessed before initialization is complete.
+ */
+
+u32 tipc_ref_acquire(void *object, spinlock_t **lock)
+{
+	struct reference *entry;
+	u32 index;
+	u32 index_mask;
+	u32 next_plus_upper;
+	u32 ref;
+
+	if (!object) {
+		err("Attempt to acquire reference to non-existent object\n");
+		return 0;
+	}
+	if (!tipc_ref_table.entries) {
+		err("Reference table not found during acquisition attempt\n");
+		return 0;
+	}
+
+	/* take a free entry, if available; otherwise initialize a new entry */
+
+	write_lock_bh(&ref_table_lock);
+	if (tipc_ref_table.first_free) {
+		index = tipc_ref_table.first_free;
+		entry = &(tipc_ref_table.entries[index]);
+		index_mask = tipc_ref_table.index_mask;
+		/* take lock in case a previous user of entry still holds it */
+		spin_lock_bh(&entry->lock);
+		next_plus_upper = entry->ref;
+		tipc_ref_table.first_free = next_plus_upper & index_mask;
+		ref = (next_plus_upper & ~index_mask) + index;
+		entry->ref = ref;
+		entry->object = object;
+		*lock = &entry->lock;
+	}
+	else if (tipc_ref_table.init_point < tipc_ref_table.capacity) {
+		index = tipc_ref_table.init_point++;
+		entry = &(tipc_ref_table.entries[index]);
+		spin_lock_init(&entry->lock);
+		spin_lock_bh(&entry->lock);
+		ref = tipc_ref_table.start_mask + index;
+		entry->ref = ref;
+		entry->object = object;
+		*lock = &entry->lock;
+	}
+	else {
+		ref = 0;
+	}
+	write_unlock_bh(&ref_table_lock);
+
+	return ref;
+}
+
+/**
+ * tipc_ref_discard - invalidate references to an object
+ *
+ * Disallow future references to an object and free up the entry for re-use.
+ * Note: The entry's spin_lock may still be busy after discard
+ */
+
+void tipc_ref_discard(u32 ref)
+{
+	struct reference *entry;
+	u32 index;
+	u32 index_mask;
+
+	if (!tipc_ref_table.entries) {
+		err("Reference table not found during discard attempt\n");
+		return;
+	}
+
+	index_mask = tipc_ref_table.index_mask;
+	index = ref & index_mask;
+	entry = &(tipc_ref_table.entries[index]);
+
+	write_lock_bh(&ref_table_lock);
+
+	if (!entry->object) {
+		err("Attempt to discard reference to non-existent object\n");
+		goto exit;
+	}
+	if (entry->ref != ref) {
+		err("Attempt to discard non-existent reference\n");
+		goto exit;
+	}
+
+	/*
+	 * mark entry as unused; increment instance part of entry's reference
+	 * to invalidate any subsequent references
+	 */
+
+	entry->object = NULL;
+	entry->ref = (ref & ~index_mask) + (index_mask + 1);
+
+	/* append entry to free entry list */
+
+	if (tipc_ref_table.first_free == 0)
+		tipc_ref_table.first_free = index;
+	else
+		tipc_ref_table.entries[tipc_ref_table.last_free].ref |= index;
+	tipc_ref_table.last_free = index;
+
+exit:
+	write_unlock_bh(&ref_table_lock);
+}
+
+/**
+ * tipc_ref_lock - lock referenced object and return pointer to it
+ */
+
+void *tipc_ref_lock(u32 ref)
+{
+	if (likely(tipc_ref_table.entries)) {
+		struct reference *entry;
+
+		entry = &tipc_ref_table.entries[ref &
+						tipc_ref_table.index_mask];
+		if (likely(entry->ref != 0)) {
+			spin_lock_bh(&entry->lock);
+			if (likely((entry->ref == ref) && (entry->object)))
+				return entry->object;
+			spin_unlock_bh(&entry->lock);
+		}
+	}
+	return NULL;
+}
+
+/**
+ * tipc_ref_unlock - unlock referenced object
+ */
+
+void tipc_ref_unlock(u32 ref)
+{
+	if (likely(tipc_ref_table.entries)) {
+		struct reference *entry;
+
+		entry = &tipc_ref_table.entries[ref &
+						tipc_ref_table.index_mask];
+		if (likely((entry->ref == ref) && (entry->object)))
+			spin_unlock_bh(&entry->lock);
+		else
+			err("Attempt to unlock non-existent reference\n");
+	}
+}
+
+/**
+ * tipc_ref_deref - return pointer referenced object (without locking it)
+ */
+
+void *tipc_ref_deref(u32 ref)
+{
+	if (likely(tipc_ref_table.entries)) {
+		struct reference *entry;
+
+		entry = &tipc_ref_table.entries[ref &
+						tipc_ref_table.index_mask];
+		if (likely(entry->ref == ref))
+			return entry->object;
+	}
+	return NULL;
+}
+
diff -ruN linux-2.6.29/net/tipc/tipc_ref.h android_cluster/linux-2.6.29/net/tipc/tipc_ref.h
--- linux-2.6.29/net/tipc/tipc_ref.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_ref.h	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,50 @@
+/*
+ * net/tipc/tipc_ref.h: Include file for TIPC object registry code
+ *
+ * Copyright (c) 1991-2006, Ericsson AB
+ * Copyright (c) 2005-2006, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_REF_H
+#define _TIPC_REF_H
+
+int tipc_ref_table_init(u32 requested_size, u32 start);
+void tipc_ref_table_stop(void);
+
+u32 tipc_ref_acquire(void *object, spinlock_t **lock);
+void tipc_ref_discard(u32 ref);
+
+void *tipc_ref_lock(u32 ref);
+void tipc_ref_unlock(u32 ref);
+void *tipc_ref_deref(u32 ref);
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_socket.c android_cluster/linux-2.6.29/net/tipc/tipc_socket.c
--- linux-2.6.29/net/tipc/tipc_socket.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_socket.c	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,1939 @@
+/*
+ * net/tipc/tipc_socket.c: TIPC socket API
+ *
+ * Copyright (c) 2001-2007, Ericsson AB
+ * Copyright (c) 2004-2007, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/net.h>
+#include <linux/socket.h>
+#include <linux/errno.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/poll.h>
+#include <linux/fcntl.h>
+#include <asm/string.h>
+#include <asm/atomic.h>
+#include <net/sock.h>
+
+#include <linux/tipc.h>
+#include <linux/tipc_config.h>
+#include <net/tipc/tipc_plugin_msg.h>
+#include <net/tipc/tipc_plugin_port.h>
+
+#include "tipc_core.h"
+
+#ifdef CONFIG_TIPC_SOCKET_API
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
+#ifndef SHUT_RDWR
+#define SHUT_RDWR 2	/* this is undefined in kernel space for some reason */
+#endif
+#endif
+
+#define SS_LISTENING	-1	/* socket is listening */
+#define SS_READY	-2	/* socket is connectionless */
+
+#define OVERLOAD_LIMIT_BASE	5000
+#define CONN_TIMEOUT_DEFAULT	8000	/* default connect timeout = 8s */
+
+struct tipc_sock {
+	struct sock sk;
+	struct tipc_port *p;
+	struct tipc_portid peer_name;
+};
+
+#define tipc_sk(sk) ((struct tipc_sock *)(sk))
+#define tipc_sk_port(sk) ((struct tipc_port *)(tipc_sk(sk)->p))
+
+static int backlog_rcv(struct sock *sk, struct sk_buff *skb);
+static u32 dispatch(struct tipc_port *tport, struct sk_buff *buf);
+static void wakeupdispatch(struct tipc_port *tport);
+
+static const struct proto_ops packet_ops;
+static const struct proto_ops stream_ops;
+static const struct proto_ops msg_ops;
+
+static struct proto tipc_proto;
+
+static int sockets_enabled = 0;
+
+static atomic_t tipc_queue_size = ATOMIC_INIT(0);
+
+/*
+ * Revised TIPC socket locking policy:
+ *
+ * Most socket operations take the standard socket lock when they start
+ * and hold it until they finish (or until they need to sleep).  Acquiring
+ * this lock grants the owner exclusive access to the fields of the socket
+ * data structures, with the exception of the backlog queue.  A few socket
+ * operations can be done without taking the socket lock because they only
+ * read socket information that never changes during the life of the socket.
+ *
+ * Socket operations may acquire the lock for the associated TIPC port if they
+ * need to perform an operation on the port.  If any routine needs to acquire
+ * both the socket lock and the port lock it must take the socket lock first
+ * to avoid the risk of deadlock.
+ *
+ * The dispatcher handling incoming messages cannot grab the socket lock in
+ * the standard fashion, since invoked it runs at the BH level and cannot block.
+ * Instead, it checks to see if the socket lock is currently owned by someone,
+ * and either handles the message itself or adds it to the socket's backlog
+ * queue; in the latter case the queued message is processed once the process
+ * owning the socket lock releases it.
+ *
+ * NOTE: Releasing the socket lock while an operation is sleeping overcomes
+ * the problem of a blocked socket operation preventing any other operations
+ * from occurring.  However, applications must be careful if they have
+ * multiple threads trying to send (or receive) on the same socket, as these
+ * operations might interfere with each other.  For example, doing a connect
+ * and a receive at the same time might allow the receive to consume the
+ * ACK message meant for the connect.  While additional work could be done
+ * to try and overcome this, it doesn't seem to be worthwhile at the present.
+ *
+ * NOTE: Releasing the socket lock while an operation is sleeping also ensures
+ * that another operation that must be performed in a non-blocking manner is
+ * not delayed for very long because the lock has already been taken.
+ *
+ * NOTE: This code assumes that certain fields of a port/socket pair are
+ * constant over its lifetime; such fields can be examined without taking
+ * the socket lock and/or port lock, and do not need to be re-read even
+ * after resuming processing after waiting.  These fields include:
+ *   - socket type
+ *   - pointer to socket sk structure (aka tipc_sock structure)
+ *   - pointer to port structure
+ *   - port reference
+ */
+
+/**
+ * advance_rx_queue - discard first buffer in socket receive queue
+ *
+ * Caller must hold socket lock
+ */
+
+static void advance_rx_queue(struct sock *sk)
+{
+	buf_discard(__skb_dequeue(&sk->sk_receive_queue));
+	atomic_dec(&tipc_queue_size);
+}
+
+/**
+ * discard_rx_queue - discard all buffers in socket receive queue
+ *
+ * Caller must hold socket lock
+ */
+
+static void discard_rx_queue(struct sock *sk)
+{
+	struct sk_buff *buf;
+
+	while ((buf = __skb_dequeue(&sk->sk_receive_queue))) {
+		atomic_dec(&tipc_queue_size);
+		buf_discard(buf);
+	}
+}
+
+/**
+ * reject_rx_queue - reject all buffers in socket receive queue
+ *
+ * Caller must hold socket lock
+ */
+
+static void reject_rx_queue(struct sock *sk)
+{
+	struct sk_buff *buf;
+
+	while ((buf = __skb_dequeue(&sk->sk_receive_queue))) {
+		tipc_reject_msg(buf, TIPC_ERR_NO_PORT);
+		atomic_dec(&tipc_queue_size);
+	}
+}
+
+/**
+ * tipc_create - create a TIPC socket
+ * @net: network namespace (must be default network) -- only for 2.6.24+
+ * @sock: pre-allocated socket structure
+ * @protocol: protocol indicator (must be 0)
+ *
+ * This routine creates additional data structures used by the TIPC socket,
+ * initializes them, and links them together.
+ *
+ * Returns 0 on success, errno otherwise
+ */
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+static int tipc_create(struct net *net, struct socket *sock, int protocol)
+#else
+static int tipc_create(struct socket *sock, int protocol)
+#endif
+{
+	const struct proto_ops *ops;
+	socket_state state;
+	struct sock *sk;
+	struct tipc_port *tp_ptr;
+
+	/* Validate arguments */
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+	if (net != &init_net)
+		return -EAFNOSUPPORT;
+#endif
+
+	if (unlikely(protocol != 0))
+		return -EPROTONOSUPPORT;
+
+	switch (sock->type) {
+	case SOCK_STREAM:
+		ops = &stream_ops;
+		state = SS_UNCONNECTED;
+		break;
+	case SOCK_SEQPACKET:
+		ops = &packet_ops;
+		state = SS_UNCONNECTED;
+		break;
+	case SOCK_DGRAM:
+	case SOCK_RDM:
+		ops = &msg_ops;
+		state = SS_READY;
+		break;
+	default:
+		return -EPROTOTYPE;
+	}
+
+	/* Allocate socket's protocol area */
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+	sk = sk_alloc(net, AF_TIPC, GFP_KERNEL, &tipc_proto);
+#else
+	sk = sk_alloc(AF_TIPC, GFP_KERNEL, &tipc_proto, 1);
+#endif
+	if (sk == NULL)
+		return -ENOMEM;
+
+	/* Allocate TIPC port for socket to use */
+
+#ifdef CONFIG_KRGRPC
+	tp_ptr = tipc_createport_raw(sk, &dispatch, &wakeupdispatch,
+				     TIPC_LOW_IMPORTANCE, NULL);
+#else
+	tp_ptr = tipc_createport_raw(sk, &dispatch, &wakeupdispatch,
+			 	     TIPC_LOW_IMPORTANCE);
+#endif
+	if (unlikely(!tp_ptr)) {
+		sk_free(sk);
+		return -ENOMEM;
+	}
+
+	/* Finish initializing socket data structures */
+
+	sock->ops = ops;
+	sock->state = state;
+
+	sock_init_data(sock, sk);
+	sk->sk_rcvtimeo = msecs_to_jiffies(CONN_TIMEOUT_DEFAULT);
+	sk->sk_backlog_rcv = backlog_rcv;
+	tipc_sk(sk)->p = tp_ptr;
+
+	spin_unlock_bh(tp_ptr->lock);
+
+	if (sock->state == SS_READY) {
+		tipc_set_portunreturnable(tp_ptr->ref, 1);
+		if (sock->type == SOCK_DGRAM)
+			tipc_set_portunreliable(tp_ptr->ref, 1);
+	}
+
+	atomic_inc(&tipc_user_count);
+	return 0;
+}
+
+/**
+ * release - destroy a TIPC socket
+ * @sock: socket to destroy
+ *
+ * This routine cleans up any messages that are still queued on the socket.
+ * For DGRAM and RDM socket types, all queued messages are rejected.
+ * For SEQPACKET and STREAM socket types, the first message is rejected
+ * and any others are discarded.  (If the first message on a STREAM socket
+ * is partially-read, it is discarded and the next one is rejected instead.)
+ *
+ * NOTE: Rejected messages are not necessarily returned to the sender!  They
+ * are returned or discarded according to the "destination droppable" setting
+ * specified for the message by the sender.
+ *
+ * Returns 0 on success, errno otherwise
+ */
+
+static int release(struct socket *sock)
+{
+	struct sock *sk = sock->sk;
+	struct tipc_port *tport;
+	struct sk_buff *buf;
+	int res;
+
+	/*
+	 * Exit if socket isn't fully initialized (occurs when a failed accept()
+	 * releases a pre-allocated child socket that was never used)
+	 */
+
+	if (sk == NULL)
+		return 0;
+
+	tport = tipc_sk_port(sk);
+	lock_sock(sk);
+
+	/*
+	 * Reject all unreceived messages, except on an active connection
+	 * (which disconnects locally & sends a 'FIN+' to peer)
+	 */
+
+	while (sock->state != SS_DISCONNECTING) {
+		buf = __skb_dequeue(&sk->sk_receive_queue);
+		if (buf == NULL)
+			break;
+		atomic_dec(&tipc_queue_size);
+		if (buf_handle(buf) != msg_data(buf_msg(buf)))
+			buf_discard(buf);
+		else {
+			if ((sock->state == SS_CONNECTING) ||
+			    (sock->state == SS_CONNECTED)) {
+				sock->state = SS_DISCONNECTING;
+				tipc_disconnect(tport->ref);
+			}
+			tipc_reject_msg(buf, TIPC_ERR_NO_PORT);
+		}
+	}
+
+	/*
+	 * Delete TIPC port; this ensures no more messages are queued
+	 * (also disconnects an active connection & sends a 'FIN-' to peer)
+	 */
+
+	res = tipc_deleteport(tport->ref);
+
+	/* Discard any remaining (connection-based) messages in receive queue */
+
+	discard_rx_queue(sk);
+
+	/* Reject any messages that accumulated in backlog queue */
+
+	sock->state = SS_DISCONNECTING;
+	release_sock(sk);
+
+	sock_put(sk);
+	sock->sk = NULL;
+
+	atomic_dec(&tipc_user_count);
+	return res;
+}
+
+/**
+ * bind - associate or disassocate TIPC name(s) with a socket
+ * @sock: socket structure
+ * @uaddr: socket address describing name(s) and desired operation
+ * @uaddr_len: size of socket address data structure
+ *
+ * Name and name sequence binding is indicated using a positive scope value;
+ * a negative scope value unbinds the specified name.  Specifying no name
+ * (i.e. a socket address length of 0) unbinds all names from the socket.
+ *
+ * Returns 0 on success, errno otherwise
+ *
+ * NOTE: This routine doesn't need to take the socket lock since it doesn't
+ *       access any non-constant socket information.
+ */
+
+static int bind(struct socket *sock, struct sockaddr *uaddr, int uaddr_len)
+{
+	struct sockaddr_tipc *addr = (struct sockaddr_tipc *)uaddr;
+	u32 portref = tipc_sk_port(sock->sk)->ref;
+
+	if (unlikely(!uaddr_len))
+		return tipc_withdraw(portref, 0, NULL);
+
+	if (uaddr_len < sizeof(struct sockaddr_tipc))
+		return -EINVAL;
+	if (addr->family != AF_TIPC)
+		return -EAFNOSUPPORT;
+
+	if (addr->addrtype == TIPC_ADDR_NAME)
+		addr->addr.nameseq.upper = addr->addr.nameseq.lower;
+	else if (addr->addrtype != TIPC_ADDR_NAMESEQ)
+		return -EAFNOSUPPORT;
+
+	return (addr->scope > 0) ?
+		tipc_publish(portref, addr->scope, &addr->addr.nameseq) :
+		tipc_withdraw(portref, -addr->scope, &addr->addr.nameseq);
+}
+
+/**
+ * get_name - get port ID of socket or peer socket
+ * @sock: socket structure
+ * @uaddr: area for returned socket address
+ * @uaddr_len: area for returned length of socket address
+ * @peer: 0 = own ID, 1 = current peer ID, 2 = current/former peer ID
+ *
+ * Returns 0 on success, errno otherwise
+ *
+ * NOTE: This routine doesn't need to take the socket lock since it only
+ *       accesses socket information that is unchanging (or which changes in
+ * 	 a completely predictable manner).
+ */
+
+static int get_name(struct socket *sock, struct sockaddr *uaddr,
+		    int *uaddr_len, int peer)
+{
+	struct sockaddr_tipc *addr = (struct sockaddr_tipc *)uaddr;
+	struct tipc_sock *tsock = tipc_sk(sock->sk);
+
+	if (peer) {
+		if ((sock->state != SS_CONNECTED) &&
+			((peer != 2) || (sock->state != SS_DISCONNECTING)))
+			return -ENOTCONN;
+		addr->addr.id.ref = tsock->peer_name.ref;
+		addr->addr.id.node = tsock->peer_name.node;
+	} else {
+		tipc_ownidentity(tsock->p->ref, &addr->addr.id);
+	}
+
+	*uaddr_len = sizeof(*addr);
+	addr->addrtype = TIPC_ADDR_ID;
+	addr->family = AF_TIPC;
+	addr->scope = 0;
+	addr->addr.name.domain = 0;
+
+	return 0;
+}
+
+/**
+ * poll - read and possibly block on pollmask
+ * @file: file structure associated with the socket
+ * @sock: socket for which to calculate the poll bits
+ * @wait: ???
+ *
+ * Returns pollmask value
+ *
+ * COMMENTARY:
+ * It appears that the usual socket locking mechanisms are not useful here
+ * since the pollmask info is potentially out-of-date the moment this routine
+ * exits.  TCP and other protocols seem to rely on higher level poll routines
+ * to handle any preventable race conditions, so TIPC will do the same ...
+ *
+ * TIPC sets the returned events as follows:
+ * a) POLLRDNORM and POLLIN are set if the socket's receive queue is non-empty
+ *    or if a connection-oriented socket is does not have an active connection
+ *    (i.e. a read operation will not block).
+ * b) POLLOUT is set except if the socket's port is in a congested state
+ *    or the socket's connection has been terminated
+ *    (i.e. a write operation will not block).
+ * c) POLLHUP is set when a socket's connection has been terminated.
+ *
+ * IMPORTANT: The fact that a read or write operation will not block does NOT
+ * imply that the operation will succeed!
+ */
+
+static unsigned int poll(struct file *file, struct socket *sock,
+			 poll_table *wait)
+{
+	struct sock *sk = sock->sk;
+	u32 mask;
+
+	poll_wait(file, sk->sk_sleep, wait);
+
+	if (!skb_queue_empty(&sk->sk_receive_queue) ||
+	    (sock->state == SS_UNCONNECTED) ||
+	    (sock->state == SS_DISCONNECTING))
+		mask = (POLLRDNORM | POLLIN);
+	else
+		mask = 0;
+
+	if (sock->state == SS_DISCONNECTING)
+		mask |= POLLHUP;
+	else if (!tipc_sk_port(sk)->congested)
+		mask |= POLLOUT;
+
+	return mask;
+}
+
+/**
+ * dest_name_check - verify user is permitted to send to specified port name
+ * @dest: destination address
+ * @m: descriptor for message to be sent
+ *
+ * Prevents restricted configuration commands from being issued by
+ * unauthorized users.
+ *
+ * Returns 0 if permission is granted, otherwise errno
+ */
+
+static int dest_name_check(struct sockaddr_tipc *dest, struct msghdr *m)
+{
+	struct tipc_cfg_msg_hdr hdr;
+
+	if (likely(dest->addr.name.name.type >= TIPC_RESERVED_TYPES))
+		return 0;
+	if (likely(dest->addr.name.name.type == TIPC_TOP_SRV))
+		return 0;
+	if (likely(dest->addr.name.name.type != TIPC_CFG_SRV))
+		return -EACCES;
+
+	if (copy_from_user(&hdr, m->msg_iov[0].iov_base, sizeof(hdr)))
+		return -EFAULT;
+	if ((ntohs(hdr.tcm_type) & 0xC000) && (!capable(CAP_NET_ADMIN)))
+		return -EACCES;
+
+	return 0;
+}
+
+/**
+ * send_msg - send message in connectionless manner
+ * @iocb: if NULL, indicates that socket lock is already held
+ * @sock: socket structure
+ * @m: message to send
+ * @total_len: length of message
+ *
+ * Message must have an destination specified explicitly.
+ * Used for SOCK_RDM and SOCK_DGRAM messages,
+ * and for 'SYN' messages on SOCK_SEQPACKET and SOCK_STREAM connections.
+ * (Note: 'SYN+' is prohibited on SOCK_STREAM.)
+ *
+ * Returns the number of bytes sent on success, or errno otherwise
+ */
+
+static int send_msg(struct kiocb *iocb, struct socket *sock,
+		    struct msghdr *m, size_t total_len)
+{
+	struct sock *sk = sock->sk;
+	struct tipc_port *tport = tipc_sk_port(sk);
+	struct sockaddr_tipc *dest = (struct sockaddr_tipc *)m->msg_name;
+	int needs_conn;
+	int res = -EINVAL;
+
+	if (unlikely(!dest))
+		return -EDESTADDRREQ;
+	if (unlikely((m->msg_namelen < sizeof(*dest)) ||
+		     (dest->family != AF_TIPC)))
+		return -EINVAL;
+
+	if (iocb)
+		lock_sock(sk);
+
+	needs_conn = (sock->state != SS_READY);
+	if (unlikely(needs_conn)) {
+		if (sock->state == SS_LISTENING) {
+			res = -EPIPE;
+			goto exit;
+		}
+		if (sock->state != SS_UNCONNECTED) {
+			res = -EISCONN;
+			goto exit;
+		}
+		if ((tport->published) ||
+		    ((sock->type == SOCK_STREAM) && (total_len != 0))) {
+			res = -EOPNOTSUPP;
+			goto exit;
+		}
+		if (dest->addrtype == TIPC_ADDR_NAME) {
+			tport->conn_type = dest->addr.name.name.type;
+			tport->conn_instance = dest->addr.name.name.instance;
+		}
+
+		/* Abort any pending connection attempts (very unlikely) */
+
+		reject_rx_queue(sk);
+	}
+
+	do {
+		if (dest->addrtype == TIPC_ADDR_NAME) {
+			if ((res = dest_name_check(dest, m)))
+				break;
+			res = tipc_send2name(tport->ref,
+					     &dest->addr.name.name,
+					     dest->addr.name.domain,
+					     m->msg_iovlen,
+					     m->msg_iov);
+		}
+		else if (dest->addrtype == TIPC_ADDR_ID) {
+			res = tipc_send2port(tport->ref,
+					     &dest->addr.id,
+					     m->msg_iovlen,
+					     m->msg_iov);
+		}
+		else if (dest->addrtype == TIPC_ADDR_MCAST) {
+			if (needs_conn) {
+				res = -EOPNOTSUPP;
+				break;
+			}
+			if ((res = dest_name_check(dest, m)))
+				break;
+			res = tipc_multicast(tport->ref,
+					     &dest->addr.nameseq,
+					     0,
+					     m->msg_iovlen,
+					     m->msg_iov);
+		}
+		if (likely(res != -ELINKCONG)) {
+			if (needs_conn && (res >= 0)) {
+				sock->state = SS_CONNECTING;
+			}
+			break;
+		}
+		if (m->msg_flags & MSG_DONTWAIT) {
+			res = -EWOULDBLOCK;
+			break;
+		}
+		release_sock(sk);
+		res = wait_event_interruptible(*sk->sk_sleep,
+					       !tport->congested);
+		lock_sock(sk);
+		if (res)
+			break;
+	} while (1);
+
+exit:
+	if (iocb)
+		release_sock(sk);
+	return res;
+}
+
+/**
+ * send_packet - send a connection-oriented message
+ * @iocb: if NULL, indicates that socket lock is already held
+ * @sock: socket structure
+ * @m: message to send
+ * @total_len: length of message
+ *
+ * Used for SOCK_SEQPACKET messages and SOCK_STREAM data.
+ *
+ * Returns the number of bytes sent on success, or errno otherwise
+ */
+
+static int send_packet(struct kiocb *iocb, struct socket *sock,
+		       struct msghdr *m, size_t total_len)
+{
+	struct sock *sk = sock->sk;
+	struct tipc_port *tport = tipc_sk_port(sk);
+	struct sockaddr_tipc *dest = (struct sockaddr_tipc *)m->msg_name;
+	int res;
+
+	/* Handle implied connection establishment */
+
+	if (unlikely(dest))
+		return send_msg(iocb, sock, m, total_len);
+
+	if (iocb)
+		lock_sock(sk);
+
+	do {
+		if (unlikely(sock->state != SS_CONNECTED)) {
+			if (sock->state == SS_DISCONNECTING)
+				res = -EPIPE;
+			else
+				res = -ENOTCONN;
+			break;
+		}
+
+		res = tipc_send(tport->ref, m->msg_iovlen, m->msg_iov);
+		if (likely(res != -ELINKCONG)) {
+			break;
+		}
+		if (m->msg_flags & MSG_DONTWAIT) {
+			res = -EWOULDBLOCK;
+			break;
+		}
+		release_sock(sk);
+		res = wait_event_interruptible(*sk->sk_sleep,
+			(!tport->congested || !tport->connected));
+		lock_sock(sk);
+		if (res)
+			break;
+	} while (1);
+
+	if (iocb)
+		release_sock(sk);
+	return res;
+}
+
+/**
+ * send_stream - send stream-oriented data
+ * @iocb: (unused)
+ * @sock: socket structure
+ * @m: data to send
+ * @total_len: total length of data to be sent
+ *
+ * Used for SOCK_STREAM data.
+ *
+ * Returns the number of bytes sent on success (or partial success),
+ * or errno if no data sent
+ */
+
+static int send_stream(struct kiocb *iocb, struct socket *sock,
+		       struct msghdr *m, size_t total_len)
+{
+	struct sock *sk = sock->sk;
+	struct tipc_port *tport = tipc_sk_port(sk);
+	struct msghdr my_msg;
+	struct iovec my_iov;
+	struct iovec *curr_iov;
+	int curr_iovlen;
+	char __user *curr_start;
+	u32 hdr_size;
+	int curr_left;
+	int bytes_to_send;
+	int bytes_sent;
+	int res;
+
+	lock_sock(sk);
+
+	/* Handle special cases where there is no connection */
+
+	if (unlikely(sock->state != SS_CONNECTED)) {
+		if (sock->state == SS_UNCONNECTED) {
+			res = send_packet(NULL, sock, m, total_len);
+			goto exit;
+		} else if (sock->state == SS_DISCONNECTING) {
+			res = -EPIPE;
+			goto exit;
+		} else {
+			res = -ENOTCONN;
+			goto exit;
+		}
+	}
+
+	if (unlikely(m->msg_name)) {
+		res = -EISCONN;
+		goto exit;
+	}
+
+	/*
+	 * Send each iovec entry using one or more messages
+	 *
+	 * Note: This algorithm is good for the most likely case
+	 * (i.e. one large iovec entry), but could be improved to pass sets
+	 * of small iovec entries into send_packet().
+	 */
+
+	curr_iov = m->msg_iov;
+	curr_iovlen = m->msg_iovlen;
+	my_msg.msg_iov = &my_iov;
+	my_msg.msg_iovlen = 1;
+	my_msg.msg_flags = m->msg_flags;
+	my_msg.msg_name = NULL;
+	bytes_sent = 0;
+
+	hdr_size = msg_hdr_sz(&tport->phdr);
+
+	while (curr_iovlen--) {
+		curr_start = curr_iov->iov_base;
+		curr_left = curr_iov->iov_len;
+
+		while (curr_left) {
+			bytes_to_send = tport->max_pkt - hdr_size;
+			if (bytes_to_send > TIPC_MAX_USER_MSG_SIZE)
+				bytes_to_send = TIPC_MAX_USER_MSG_SIZE;
+			if (curr_left < bytes_to_send)
+				bytes_to_send = curr_left;
+			my_iov.iov_base = curr_start;
+			my_iov.iov_len = bytes_to_send;
+			if ((res = send_packet(NULL, sock, &my_msg, 0)) < 0) {
+				if (bytes_sent)
+					res = bytes_sent;
+				goto exit;
+			}
+			curr_left -= bytes_to_send;
+			curr_start += bytes_to_send;
+			bytes_sent += bytes_to_send;
+		}
+
+		curr_iov++;
+	}
+	res = bytes_sent;
+exit:
+	release_sock(sk);
+	return res;
+}
+
+/**
+ * auto_connect - complete connection setup to a remote port
+ * @sock: socket structure
+ * @msg: peer's response message
+ *
+ * Returns 0 on success, errno otherwise
+ */
+
+static int auto_connect(struct socket *sock, struct tipc_msg *msg)
+{
+	struct tipc_sock *tsock = tipc_sk(sock->sk);
+
+	if (msg_errcode(msg)) {
+		sock->state = SS_DISCONNECTING;
+		return -ECONNREFUSED;
+	}
+
+	tsock->peer_name.ref = msg_origport(msg);
+	tsock->peer_name.node = msg_orignode(msg);
+	tipc_connect2port(tsock->p->ref, &tsock->peer_name);
+	tipc_set_portimportance(tsock->p->ref, msg_importance(msg));
+	sock->state = SS_CONNECTED;
+	return 0;
+}
+
+/**
+ * set_orig_addr - capture sender's address for received message
+ * @m: descriptor for message info
+ * @msg: received message header
+ *
+ * Note: Address is not captured if not requested by receiver.
+ */
+
+static void set_orig_addr(struct msghdr *m, struct tipc_msg *msg)
+{
+	struct sockaddr_tipc *addr = (struct sockaddr_tipc *)m->msg_name;
+
+	if (addr) {
+		addr->family = AF_TIPC;
+		addr->addrtype = TIPC_ADDR_ID;
+		addr->addr.id.ref = msg_origport(msg);
+		addr->addr.id.node = msg_orignode(msg);
+		addr->addr.name.domain = 0;   	/* could leave uninitialized */
+		addr->scope = 0;   		/* could leave uninitialized */
+		m->msg_namelen = sizeof(struct sockaddr_tipc);
+	}
+}
+
+/**
+ * anc_data_recv - optionally capture ancillary data for received message
+ * @m: descriptor for message info
+ * @msg: received message header
+ * @tport: TIPC port associated with message
+ *
+ * Note: Ancillary data is not captured if not requested by receiver.
+ *
+ * Returns 0 if successful, otherwise errno
+ */
+
+static int anc_data_recv(struct msghdr *m, struct tipc_msg *msg,
+				struct tipc_port *tport)
+{
+	u32 anc_data[3];
+	u32 err;
+	u32 dest_type;
+	int has_name;
+	int res;
+
+	if (likely(m->msg_controllen == 0))
+		return 0;
+
+	/* Optionally capture errored message object(s) */
+
+	err = msg ? msg_errcode(msg) : 0;
+	if (unlikely(err)) {
+		anc_data[0] = err;
+		anc_data[1] = msg_data_sz(msg);
+		if ((res = put_cmsg(m, SOL_TIPC, TIPC_ERRINFO, 8, anc_data)))
+			return res;
+		if (anc_data[1] &&
+		    (res = put_cmsg(m, SOL_TIPC, TIPC_RETDATA, anc_data[1],
+				    msg_data(msg))))
+			return res;
+	}
+
+	/* Optionally capture message destination object */
+
+	dest_type = msg ? msg_type(msg) : TIPC_DIRECT_MSG;
+	switch (dest_type) {
+	case TIPC_NAMED_MSG:
+		has_name = 1;
+		anc_data[0] = msg_nametype(msg);
+		anc_data[1] = msg_namelower(msg);
+		anc_data[2] = msg_namelower(msg);
+		break;
+	case TIPC_MCAST_MSG:
+		has_name = 1;
+		anc_data[0] = msg_nametype(msg);
+		anc_data[1] = msg_namelower(msg);
+		anc_data[2] = msg_nameupper(msg);
+		break;
+	case TIPC_CONN_MSG:
+		has_name = (tport->conn_type != 0);
+		anc_data[0] = tport->conn_type;
+		anc_data[1] = tport->conn_instance;
+		anc_data[2] = tport->conn_instance;
+		break;
+	default:
+		has_name = 0;
+	}
+	if (has_name &&
+	    (res = put_cmsg(m, SOL_TIPC, TIPC_DESTNAME, 12, anc_data)))
+		return res;
+
+	return 0;
+}
+
+/**
+ * recv_msg - receive packet-oriented message
+ * @iocb: (unused)
+ * @m: descriptor for message info
+ * @buf_len: total size of user buffer area
+ * @flags: receive flags
+ *
+ * Used for SOCK_DGRAM, SOCK_RDM, and SOCK_SEQPACKET messages.
+ * If the complete message doesn't fit in user area, truncate it.
+ *
+ * Returns size of returned message data, errno otherwise
+ */
+
+static int recv_msg(struct kiocb *iocb, struct socket *sock,
+		    struct msghdr *m, size_t buf_len, int flags)
+{
+	struct sock *sk = sock->sk;
+	struct tipc_port *tport = tipc_sk_port(sk);
+	struct sk_buff *buf;
+	struct tipc_msg *msg;
+	unsigned int sz;
+	u32 err;
+	int res;
+
+	/* Catch invalid receive requests */
+
+	if (m->msg_iovlen != 1)
+		return -EOPNOTSUPP;   /* Don't do multiple iovec entries yet */
+
+	if (unlikely(!buf_len))
+		return -EINVAL;
+
+	lock_sock(sk);
+
+	if (unlikely(sock->state == SS_UNCONNECTED)) {
+		res = -ENOTCONN;
+		goto exit;
+	}
+
+restart:
+
+	/* Look for a message in receive queue; wait if necessary */
+
+	while (skb_queue_empty(&sk->sk_receive_queue)) {
+		if (sock->state == SS_DISCONNECTING) {
+			res = -ENOTCONN;
+			goto exit;
+		}
+		if (flags & MSG_DONTWAIT) {
+			res = -EWOULDBLOCK;
+			goto exit;
+		}
+		release_sock(sk);
+		res = wait_event_interruptible(*sk->sk_sleep,
+			(!skb_queue_empty(&sk->sk_receive_queue) ||
+			 (sock->state == SS_DISCONNECTING)));
+		lock_sock(sk);
+		if (res)
+			goto exit;
+	}
+
+	/* Look at first message in receive queue */
+
+	buf = skb_peek(&sk->sk_receive_queue);
+	msg = buf_msg(buf);
+	sz = msg_data_sz(msg);
+	err = msg_errcode(msg);
+
+	/* Complete connection setup for an implied connect */
+
+	if (unlikely(sock->state == SS_CONNECTING)) {
+		res = auto_connect(sock, msg);
+		if (res)
+			goto exit;
+	}
+
+	/* Discard an empty non-errored message & try again */
+
+	if ((!sz) && (!err)) {
+		advance_rx_queue(sk);
+		goto restart;
+	}
+
+	/* Capture sender's address (optional) */
+
+	set_orig_addr(m, msg);
+
+	/* Capture ancillary data (optional) */
+
+	res = anc_data_recv(m, msg, tport);
+	if (res)
+		goto exit;
+
+	/* Capture message data (if valid) & compute return value (always) */
+
+	if (!err) {
+		if (unlikely(buf_len < sz)) {
+			sz = buf_len;
+			m->msg_flags |= MSG_TRUNC;
+		}
+		if (unlikely(copy_to_user(m->msg_iov->iov_base, msg_data(msg),
+					  sz))) {
+			res = -EFAULT;
+			goto exit;
+		}
+		res = sz;
+	} else {
+		if ((sock->state == SS_READY) ||
+		    ((err == TIPC_CONN_SHUTDOWN) || m->msg_control))
+			res = 0;
+		else
+			res = -ECONNRESET;
+	}
+
+	/* Consume received message (optional) */
+
+	if (likely(!(flags & MSG_PEEK))) {
+		if ((sock->state != SS_READY) &&
+		    (++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))
+			tipc_acknowledge(tport->ref, tport->conn_unacked);
+		advance_rx_queue(sk);
+	}
+exit:
+	release_sock(sk);
+	return res;
+}
+
+/**
+ * recv_stream - receive stream-oriented data
+ * @iocb: (unused)
+ * @m: descriptor for message info
+ * @buf_len: total size of user buffer area
+ * @flags: receive flags
+ *
+ * Used for SOCK_STREAM messages only.  If not enough data is available
+ * will optionally wait for more; never truncates data.
+ *
+ * Returns size of returned message data, errno otherwise
+ */
+
+static int recv_stream(struct kiocb *iocb, struct socket *sock,
+		       struct msghdr *m, size_t buf_len, int flags)
+{
+	struct sock *sk = sock->sk;
+	struct tipc_port *tport = tipc_sk_port(sk);
+	struct sk_buff *buf;
+	struct tipc_msg *msg;
+	unsigned int sz;
+	int sz_to_copy;
+	int sz_copied = 0;
+	int needed;
+	char __user *crs = m->msg_iov->iov_base;
+	unsigned char *buf_crs;
+	u32 err;
+	int res = 0;
+
+	/* Catch invalid receive attempts */
+
+	if (m->msg_iovlen != 1)
+		return -EOPNOTSUPP;   /* Don't do multiple iovec entries yet */
+
+	if (unlikely(!buf_len))
+		return -EINVAL;
+
+	lock_sock(sk);
+
+	if (unlikely((sock->state == SS_UNCONNECTED) ||
+		     (sock->state == SS_CONNECTING))) {
+		res = -ENOTCONN;
+		goto exit;
+	}
+
+restart:
+
+	/* Look for a message in receive queue; wait if necessary */
+
+	while (skb_queue_empty(&sk->sk_receive_queue)) {
+		if (sock->state == SS_DISCONNECTING) {
+			res = -ENOTCONN;
+			goto exit;
+		}
+		if (flags & MSG_DONTWAIT) {
+			res = -EWOULDBLOCK;
+			goto exit;
+		}
+		release_sock(sk);
+		res = wait_event_interruptible(*sk->sk_sleep,
+			(!skb_queue_empty(&sk->sk_receive_queue) ||
+			 (sock->state == SS_DISCONNECTING)));
+		lock_sock(sk);
+		if (res)
+			goto exit;
+	}
+
+	/* Look at first message in receive queue */
+
+	buf = skb_peek(&sk->sk_receive_queue);
+	msg = buf_msg(buf);
+	sz = msg_data_sz(msg);
+	err = msg_errcode(msg);
+
+	/* Discard an empty non-errored message & try again */
+
+	if ((!sz) && (!err)) {
+		advance_rx_queue(sk);
+		goto restart;
+	}
+
+	/* Optionally capture sender's address & ancillary data of first msg */
+
+	if (sz_copied == 0) {
+		set_orig_addr(m, msg);
+		res = anc_data_recv(m, msg, tport);
+		if (res)
+			goto exit;
+	}
+
+	/* Capture message data (if valid) & compute return value (always) */
+
+	if (!err) {
+		buf_crs = (unsigned char *)(buf_handle(buf));
+		sz = (unsigned char *)msg + msg_size(msg) - buf_crs;
+
+		needed = (buf_len - sz_copied);
+		sz_to_copy = (sz <= needed) ? sz : needed;
+		if (unlikely(copy_to_user(crs, buf_crs, sz_to_copy))) {
+			res = -EFAULT;
+			goto exit;
+		}
+		sz_copied += sz_to_copy;
+
+		if (sz_to_copy < sz) {
+			if (!(flags & MSG_PEEK))
+				buf_set_handle(buf, buf_crs + sz_to_copy);
+			goto exit;
+		}
+
+		crs += sz_to_copy;
+	} else {
+		if (sz_copied != 0)
+			goto exit; /* can't add error msg to valid data */
+
+		if ((err == TIPC_CONN_SHUTDOWN) || m->msg_control)
+			res = 0;
+		else
+			res = -ECONNRESET;
+	}
+
+	/* Consume received message (optional) */
+
+	if (likely(!(flags & MSG_PEEK))) {
+		if (unlikely(++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))
+			tipc_acknowledge(tport->ref, tport->conn_unacked);
+		advance_rx_queue(sk);
+	}
+
+	/* Loop around if more data is required */
+
+	if ((sz_copied < buf_len)    /* didn't get all requested data */
+	    && (!skb_queue_empty(&sk->sk_receive_queue) ||
+		(flags & MSG_WAITALL))
+				     /* ... and more is ready or required */
+	    && (!(flags & MSG_PEEK)) /* ... and aren't just peeking at data */
+	    && (!err)                /* ... and haven't reached a FIN */
+	    )
+		goto restart;
+
+exit:
+	release_sock(sk);
+	return sz_copied ? sz_copied : res;
+}
+
+/**
+ * rx_queue_full - determine if receive queue can accept another message
+ * @msg: message to be added to queue
+ * @queue_size: current size of queue
+ * @base: nominal maximum size of queue
+ *
+ * Returns 1 if queue is unable to accept message, 0 otherwise
+ */
+
+static int rx_queue_full(struct tipc_msg *msg, u32 queue_size, u32 base)
+{
+	u32 threshold;
+	u32 imp = msg_importance(msg);
+
+	if (imp == TIPC_LOW_IMPORTANCE)
+		threshold = base;
+	else if (imp == TIPC_MEDIUM_IMPORTANCE)
+		threshold = base * 2;
+	else if (imp == TIPC_HIGH_IMPORTANCE)
+		threshold = base * 100;
+	else
+		return 0;
+
+	if (msg_connected(msg))
+		threshold *= 4;
+
+	return (queue_size >= threshold);
+}
+
+/**
+ * filter_rcv - validate incoming message
+ * @sk: socket
+ * @buf: message
+ *
+ * Enqueues message on receive queue if acceptable; optionally handles
+ * disconnect indication for a connected socket.
+ *
+ * Called with socket lock already taken; port lock may also be taken.
+ *
+ * Returns TIPC error status code (TIPC_OK if message is not to be rejected)
+ */
+
+static u32 filter_rcv(struct sock *sk, struct sk_buff *buf)
+{
+	struct socket *sock = sk->sk_socket;
+	struct tipc_msg *msg = buf_msg(buf);
+	u32 recv_q_len;
+
+	/* Reject message if it is wrong sort of message for socket */
+
+	/*
+	 * WOULD IT BE BETTER TO JUST DISCARD THESE MESSAGES INSTEAD?
+	 * "NO PORT" ISN'T REALLY THE RIGHT ERROR CODE, AND THERE MAY
+	 * BE SECURITY IMPLICATIONS INHERENT IN REJECTING INVALID TRAFFIC
+	 */
+
+	if (sock->state == SS_READY) {
+		if (msg_connected(msg)) {
+			msg_dbg(msg, "dispatch filter 1\n");
+			return TIPC_ERR_NO_PORT;
+		}
+	} else {
+		if (msg_mcast(msg)) {
+			msg_dbg(msg, "dispatch filter 2\n");
+			return TIPC_ERR_NO_PORT;
+		}
+		if (sock->state == SS_CONNECTED) {
+			if (!msg_connected(msg)) {
+				msg_dbg(msg, "dispatch filter 3\n");
+				return TIPC_ERR_NO_PORT;
+			}
+		}
+		else if (sock->state == SS_CONNECTING) {
+			if (!msg_connected(msg) && (msg_errcode(msg) == 0)) {
+				msg_dbg(msg, "dispatch filter 4\n");
+				return TIPC_ERR_NO_PORT;
+			}
+		}
+		else if (sock->state == SS_LISTENING) {
+			if (msg_connected(msg) || msg_errcode(msg)) {
+				msg_dbg(msg, "dispatch filter 5\n");
+				return TIPC_ERR_NO_PORT;
+			}
+		}
+		else if (sock->state == SS_DISCONNECTING) {
+			msg_dbg(msg, "dispatch filter 6\n");
+			return TIPC_ERR_NO_PORT;
+		}
+		else /* (sock->state == SS_UNCONNECTED) */ {
+			if (msg_connected(msg) || msg_errcode(msg)) {
+				msg_dbg(msg, "dispatch filter 7\n");
+				return TIPC_ERR_NO_PORT;
+			}
+		}
+	}
+
+	/* Reject message if there isn't room to queue it */
+
+	recv_q_len = (u32)atomic_read(&tipc_queue_size);
+	if (unlikely(recv_q_len >= OVERLOAD_LIMIT_BASE)) {
+		if (rx_queue_full(msg, recv_q_len, OVERLOAD_LIMIT_BASE))
+			return TIPC_ERR_OVERLOAD;
+	}
+	recv_q_len = skb_queue_len(&sk->sk_receive_queue);
+	if (unlikely(recv_q_len >= (OVERLOAD_LIMIT_BASE / 2))) {
+		if (rx_queue_full(msg, recv_q_len, OVERLOAD_LIMIT_BASE / 2))
+			return TIPC_ERR_OVERLOAD;
+	}
+
+	/* Enqueue message (finally!) */
+
+	msg_dbg(msg, "<DISP<: ");
+	buf_set_handle(buf, msg_data(msg));
+	atomic_inc(&tipc_queue_size);
+	__skb_queue_tail(&sk->sk_receive_queue, buf);
+
+	/* Initiate connection termination for an incoming 'FIN' */
+
+	if (unlikely(msg_errcode(msg) && (sock->state == SS_CONNECTED))) {
+		sock->state = SS_DISCONNECTING;
+		tipc_disconnect_port(tipc_sk_port(sk));
+	}
+
+	if (waitqueue_active(sk->sk_sleep))
+		wake_up_interruptible(sk->sk_sleep);
+	return TIPC_OK;
+}
+
+/**
+ * backlog_rcv - handle incoming message from backlog queue
+ * @sk: socket
+ * @buf: message
+ *
+ * Caller must hold socket lock, but not port lock.
+ *
+ * Returns 0
+ */
+
+static int backlog_rcv(struct sock *sk, struct sk_buff *buf)
+{
+	u32 res;
+
+	res = filter_rcv(sk, buf);
+	if (res)
+		tipc_reject_msg(buf, res);
+	return 0;
+}
+
+/**
+ * dispatch - handle incoming message
+ * @tport: TIPC port that received message
+ * @buf: message
+ *
+ * Called with port lock already taken.
+ *
+ * Returns TIPC error status code (TIPC_OK if message is not to be rejected)
+ */
+
+static u32 dispatch(struct tipc_port *tport, struct sk_buff *buf)
+{
+	struct sock *sk = (struct sock *)tport->usr_handle;
+	u32 res;
+
+	/*
+	 * Process message if socket is unlocked; otherwise add to backlog queue
+	 *
+	 * This code is based on sk_receive_skb(), but must be distinct from it
+	 * since a TIPC-specific filter/reject mechanism is utilized
+	 */
+
+	bh_lock_sock(sk);
+	if (!sock_owned_by_user(sk)) {
+		res = filter_rcv(sk, buf);
+	} else {
+		sk_add_backlog(sk, buf);
+		res = TIPC_OK;
+	}
+	bh_unlock_sock(sk);
+
+	return res;
+}
+
+/**
+ * wakeupdispatch - wake up port after congestion
+ * @tport: port to wakeup
+ *
+ * Called with port lock already taken.
+ */
+
+static void wakeupdispatch(struct tipc_port *tport)
+{
+	struct sock *sk = (struct sock *)tport->usr_handle;
+
+	if (waitqueue_active(sk->sk_sleep))
+		wake_up_interruptible(sk->sk_sleep);
+}
+
+/**
+ * connect - establish a connection to another TIPC port
+ * @sock: socket structure
+ * @dest: socket address for destination port
+ * @destlen: size of socket address data structure
+ * @flags: file-related flags associated with socket
+ *
+ * Returns 0 on success, errno otherwise
+ */
+
+static int connect(struct socket *sock, struct sockaddr *dest, int destlen,
+		   int flags)
+{
+	struct sock *sk = sock->sk;
+	struct sockaddr_tipc *dst = (struct sockaddr_tipc *)dest;
+	struct msghdr m = {NULL,};
+	struct sk_buff *buf;
+	struct tipc_msg *msg;
+	int res;
+
+	lock_sock(sk);
+
+	/* For now, TIPC does not allow use of connect() with DGRAM/RDM types */
+
+	if (sock->state == SS_READY) {
+		res = -EOPNOTSUPP;
+		goto exit;
+	}
+
+	/* For now, TIPC does not support the non-blocking form of connect() */
+
+	if (flags & O_NONBLOCK) {
+		res = -EOPNOTSUPP;
+		goto exit;
+	}
+
+	/* Issue Posix-compliant error code if socket is in the wrong state */
+
+	if (sock->state == SS_LISTENING) {
+		res = -EOPNOTSUPP;
+		goto exit;
+	}
+	if (sock->state == SS_CONNECTING) {
+		res = -EALREADY;
+		goto exit;
+	}
+	if (sock->state != SS_UNCONNECTED) {
+		res = -EISCONN;
+		goto exit;
+	}
+
+	/*
+	 * Reject connection attempt using multicast address
+	 *
+	 * Note: send_msg() validates the rest of the address fields,
+	 *       so there's no need to do it here
+	 */
+
+	if (dst->addrtype == TIPC_ADDR_MCAST) {
+		res = -EINVAL;
+		goto exit;
+	}
+
+	/* Reject any messages already in receive queue (very unlikely) */
+
+	reject_rx_queue(sk);
+
+	/* Send a 'SYN-' to destination */
+
+	m.msg_name = dest;
+	m.msg_namelen = destlen;
+	res = send_msg(NULL, sock, &m, 0);
+	if (res < 0) {
+		goto exit;
+	}
+
+	/* Wait until an 'ACK' or 'RST' arrives, or a timeout occurs */
+
+	release_sock(sk);
+	res = wait_event_interruptible_timeout(*sk->sk_sleep,
+			(!skb_queue_empty(&sk->sk_receive_queue) ||
+			(sock->state != SS_CONNECTING)),
+			sk->sk_rcvtimeo);
+	lock_sock(sk);
+
+	if (res > 0) {
+		buf = skb_peek(&sk->sk_receive_queue);
+		if (buf != NULL) {
+			msg = buf_msg(buf);
+			res = auto_connect(sock, msg);
+			if (!res) {
+				if (!msg_data_sz(msg))
+					advance_rx_queue(sk);
+			}
+		} else {
+			if (sock->state == SS_CONNECTED) {
+				res = -EISCONN;
+			} else {
+				res = -ECONNREFUSED;
+			}
+		}
+	} else {
+		if (res == 0)
+			res = -ETIMEDOUT;
+		else
+			; /* leave "res" unchanged */
+		sock->state = SS_DISCONNECTING;
+	}
+
+exit:
+	release_sock(sk);
+	return res;
+}
+
+/**
+ * listen - allow socket to listen for incoming connections
+ * @sock: socket structure
+ * @len: (unused)
+ *
+ * Returns 0 on success, errno otherwise
+ */
+
+static int listen(struct socket *sock, int len)
+{
+	struct sock *sk = sock->sk;
+	int res;
+
+	lock_sock(sk);
+
+	if (sock->state == SS_READY)
+		res = -EOPNOTSUPP;
+	else if (sock->state != SS_UNCONNECTED)
+		res = -EINVAL;
+	else {
+		sock->state = SS_LISTENING;
+		res = 0;
+	}
+
+	release_sock(sk);
+	return res;
+}
+
+/**
+ * accept - wait for connection request
+ * @sock: listening socket
+ * @newsock: new socket that is to be connected
+ * @flags: file-related flags associated with socket
+ *
+ * Returns 0 on success, errno otherwise
+ */
+
+static int accept(struct socket *sock, struct socket *new_sock, int flags)
+{
+	struct sock *sk = sock->sk;
+	struct sk_buff *buf;
+	int res;
+
+	lock_sock(sk);
+
+	if (sock->state == SS_READY) {
+		res = -EOPNOTSUPP;
+		goto exit;
+	}
+	if (sock->state != SS_LISTENING) {
+		res = -EINVAL;
+		goto exit;
+	}
+
+	while (skb_queue_empty(&sk->sk_receive_queue)) {
+		if (flags & O_NONBLOCK) {
+			res = -EWOULDBLOCK;
+			goto exit;
+		}
+		release_sock(sk);
+		res = wait_event_interruptible(*sk->sk_sleep,
+				(!skb_queue_empty(&sk->sk_receive_queue)));
+		lock_sock(sk);
+		if (res)
+			goto exit;
+	}
+
+	buf = skb_peek(&sk->sk_receive_queue);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+	res = tipc_create(sock_net(sock->sk), new_sock, 0);
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+	res = tipc_create(sock->sk->sk_net, new_sock, 0);
+#else
+	res = tipc_create(new_sock, 0);
+#endif
+	if (!res) {
+		struct sock *new_sk = new_sock->sk;
+		struct tipc_sock *new_tsock = tipc_sk(new_sk);
+		struct tipc_port *new_tport = new_tsock->p;
+		u32 new_ref = new_tport->ref;
+		struct tipc_msg *msg = buf_msg(buf);
+
+		lock_sock(new_sk);
+
+		/*
+		 * Reject any stray messages received by new socket
+		 * before the socket lock was taken (very, very unlikely)
+		 */
+
+		reject_rx_queue(new_sk);
+
+		/* Connect new socket to it's peer */
+
+		new_tsock->peer_name.ref = msg_origport(msg);
+		new_tsock->peer_name.node = msg_orignode(msg);
+		tipc_connect2port(new_ref, &new_tsock->peer_name);
+		new_sock->state = SS_CONNECTED;
+
+		tipc_set_portimportance(new_ref, msg_importance(msg));
+		if (msg_named(msg)) {
+			new_tport->conn_type = msg_nametype(msg);
+			new_tport->conn_instance = msg_nameinst(msg);
+		}
+
+		/*
+		 * Respond to 'SYN-' by discarding it & returning 'ACK'-.
+		 * Respond to 'SYN+' by queuing it on new socket.
+		 */
+
+		msg_dbg(msg,"<ACC<: ");
+		if (!msg_data_sz(msg)) {
+			struct msghdr m = {NULL,};
+
+			advance_rx_queue(sk);
+			send_packet(NULL, new_sock, &m, 0);
+		} else {
+			__skb_dequeue(&sk->sk_receive_queue);
+			__skb_queue_head(&new_sk->sk_receive_queue, buf);
+		}
+		release_sock(new_sk);
+	}
+exit:
+	release_sock(sk);
+	return res;
+}
+
+/**
+ * shutdown - shutdown socket connection
+ * @sock: socket structure
+ * @how: direction to close (must be SHUT_RDWR)
+ *
+ * Terminates connection (if necessary), then purges socket's receive queue.
+ *
+ * Returns 0 on success, errno otherwise
+ */
+
+static int shutdown(struct socket *sock, int how)
+{
+	struct sock *sk = sock->sk;
+	struct tipc_port *tport = tipc_sk_port(sk);
+	struct sk_buff *buf;
+	int res;
+
+	if (how != SHUT_RDWR)
+		return -EINVAL;
+
+	lock_sock(sk);
+
+	switch (sock->state) {
+	case SS_CONNECTING:
+	case SS_CONNECTED:
+
+		/* Disconnect and send a 'FIN+' or 'FIN-' message to peer */
+restart:
+		buf = __skb_dequeue(&sk->sk_receive_queue);
+		if (buf) {
+			atomic_dec(&tipc_queue_size);
+			if (buf_handle(buf) != msg_data(buf_msg(buf))) {
+				buf_discard(buf);
+				goto restart;
+			}
+			tipc_disconnect(tport->ref);
+			tipc_reject_msg(buf, TIPC_CONN_SHUTDOWN);
+		} else {
+			tipc_shutdown(tport->ref);
+		}
+
+		sock->state = SS_DISCONNECTING;
+
+		/* fall through */
+
+	case SS_DISCONNECTING:
+
+		/* Discard any unreceived messages; wake up sleeping tasks */
+
+		discard_rx_queue(sk);
+		if (waitqueue_active(sk->sk_sleep))
+			wake_up_interruptible(sk->sk_sleep);
+		res = 0;
+		break;
+
+	default:
+		res = -ENOTCONN;
+	}
+
+	release_sock(sk);
+	return res;
+}
+
+/**
+ * setsockopt - set socket option
+ * @sock: socket structure
+ * @lvl: option level
+ * @opt: option identifier
+ * @ov: pointer to new option value
+ * @ol: length of option value
+ *
+ * For stream sockets only, accepts and ignores all IPPROTO_TCP options
+ * (to ease compatibility).
+ *
+ * Returns 0 on success, errno otherwise
+ */
+
+static int setsockopt(struct socket *sock,
+		      int lvl, int opt, char __user *ov, int ol)
+{
+	struct sock *sk = sock->sk;
+	struct tipc_port *tport = tipc_sk_port(sk);
+	u32 value;
+	int res;
+
+	if ((lvl == IPPROTO_TCP) && (sock->type == SOCK_STREAM))
+		return 0;
+	if (lvl != SOL_TIPC)
+		return -ENOPROTOOPT;
+	if (ol < sizeof(value))
+		return -EINVAL;
+	if ((res = get_user(value, (u32 __user *)ov)))
+		return res;
+
+	lock_sock(sk);
+
+	switch (opt) {
+	case TIPC_IMPORTANCE:
+		res = tipc_set_portimportance(tport->ref, value);
+		break;
+	case TIPC_SRC_DROPPABLE:
+		if (sock->type != SOCK_STREAM)
+			res = tipc_set_portunreliable(tport->ref, value);
+		else
+			res = -ENOPROTOOPT;
+		break;
+	case TIPC_DEST_DROPPABLE:
+		res = tipc_set_portunreturnable(tport->ref, value);
+		break;
+	case TIPC_CONN_TIMEOUT:
+		sk->sk_rcvtimeo = msecs_to_jiffies(value);
+		/* no need to set "res", since already 0 at this point */
+		break;
+	default:
+		res = -EINVAL;
+	}
+
+	release_sock(sk);
+
+	return res;
+}
+
+/**
+ * getsockopt - get socket option
+ * @sock: socket structure
+ * @lvl: option level
+ * @opt: option identifier
+ * @ov: receptacle for option value
+ * @ol: receptacle for length of option value
+ *
+ * For stream sockets only, returns 0 length result for all IPPROTO_TCP options
+ * (to ease compatibility).
+ *
+ * Returns 0 on success, errno otherwise
+ */
+
+static int getsockopt(struct socket *sock,
+		      int lvl, int opt, char __user *ov, int __user *ol)
+{
+	struct sock *sk = sock->sk;
+	struct tipc_port *tport = tipc_sk_port(sk);
+	int len;
+	u32 value;
+	int res;
+
+	if ((lvl == IPPROTO_TCP) && (sock->type == SOCK_STREAM))
+		return put_user(0, ol);
+	if (lvl != SOL_TIPC)
+		return -ENOPROTOOPT;
+	if ((res = get_user(len, ol)))
+		return res;
+
+	lock_sock(sk);
+
+	switch (opt) {
+	case TIPC_IMPORTANCE:
+		res = tipc_portimportance(tport->ref, &value);
+		break;
+	case TIPC_SRC_DROPPABLE:
+		res = tipc_portunreliable(tport->ref, &value);
+		break;
+	case TIPC_DEST_DROPPABLE:
+		res = tipc_portunreturnable(tport->ref, &value);
+		break;
+	case TIPC_CONN_TIMEOUT:
+		value = jiffies_to_msecs(sk->sk_rcvtimeo);
+		/* no need to set "res", since already 0 at this point */
+		break;
+	default:
+		res = -EINVAL;
+	}
+
+	release_sock(sk);
+
+	if (res) {
+		/* "get" failed */
+	}
+	else if (len < sizeof(value)) {
+		res = -EINVAL;
+	}
+	else if (copy_to_user(ov, &value, sizeof(value))) {
+		res = -EFAULT;
+	}
+	else {
+		res = put_user(sizeof(value), ol);
+	}
+
+	return res;
+}
+
+/**
+ * Protocol switches for the various types of TIPC sockets
+ */
+
+static const struct proto_ops msg_ops = {
+	.owner 		= THIS_MODULE,
+	.family		= AF_TIPC,
+	.release	= release,
+	.bind		= bind,
+	.connect	= connect,
+	.socketpair	= sock_no_socketpair,
+	.accept		= accept,
+	.getname	= get_name,
+	.poll		= poll,
+	.ioctl		= sock_no_ioctl,
+	.listen		= listen,
+	.shutdown	= shutdown,
+	.setsockopt	= setsockopt,
+	.getsockopt	= getsockopt,
+	.sendmsg	= send_msg,
+	.recvmsg	= recv_msg,
+	.mmap		= sock_no_mmap,
+	.sendpage	= sock_no_sendpage
+};
+
+static const struct proto_ops packet_ops = {
+	.owner 		= THIS_MODULE,
+	.family		= AF_TIPC,
+	.release	= release,
+	.bind		= bind,
+	.connect	= connect,
+	.socketpair	= sock_no_socketpair,
+	.accept		= accept,
+	.getname	= get_name,
+	.poll		= poll,
+	.ioctl		= sock_no_ioctl,
+	.listen		= listen,
+	.shutdown	= shutdown,
+	.setsockopt	= setsockopt,
+	.getsockopt	= getsockopt,
+	.sendmsg	= send_packet,
+	.recvmsg	= recv_msg,
+	.mmap		= sock_no_mmap,
+	.sendpage	= sock_no_sendpage
+};
+
+static const struct proto_ops stream_ops = {
+	.owner 		= THIS_MODULE,
+	.family		= AF_TIPC,
+	.release	= release,
+	.bind		= bind,
+	.connect	= connect,
+	.socketpair	= sock_no_socketpair,
+	.accept		= accept,
+	.getname	= get_name,
+	.poll		= poll,
+	.ioctl		= sock_no_ioctl,
+	.listen		= listen,
+	.shutdown	= shutdown,
+	.setsockopt	= setsockopt,
+	.getsockopt	= getsockopt,
+	.sendmsg	= send_stream,
+	.recvmsg	= recv_stream,
+	.mmap		= sock_no_mmap,
+	.sendpage	= sock_no_sendpage
+};
+
+static struct net_proto_family tipc_family_ops = {
+	.owner 		= THIS_MODULE,
+	.family		= AF_TIPC,
+	.create		= tipc_create
+};
+
+static struct proto tipc_proto = {
+	.name		= "TIPC",
+	.owner		= THIS_MODULE,
+	.obj_size	= sizeof(struct tipc_sock)
+};
+
+/**
+ * tipc_socket_init - initialize TIPC socket interface
+ *
+ * Returns 0 on success, errno otherwise
+ */
+int tipc_socket_init(void)
+{
+	int res;
+
+	res = proto_register(&tipc_proto, 1);
+	if (res) {
+		err("Failed to register TIPC protocol type\n");
+		goto out;
+	}
+
+	res = sock_register(&tipc_family_ops);
+	if (res) {
+		err("Failed to register TIPC socket type\n");
+		proto_unregister(&tipc_proto);
+		goto out;
+	}
+
+	sockets_enabled = 1;
+ out:
+	return res;
+}
+
+/**
+ * tipc_socket_stop - stop TIPC socket interface
+ */
+
+void tipc_socket_stop(void)
+{
+	if (!sockets_enabled)
+		return;
+
+	sockets_enabled = 0;
+	sock_unregister(tipc_family_ops.family);
+	proto_unregister(&tipc_proto);
+}
+
+#else
+
+/*
+ * Dummy routines used when socket API is not included
+ */
+
+int tipc_socket_init(void)
+{
+	return 0;
+}
+
+void tipc_socket_stop(void)
+{
+	/* do nothing */
+}
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_topsrv.c android_cluster/linux-2.6.29/net/tipc/tipc_topsrv.c
--- linux-2.6.29/net/tipc/tipc_topsrv.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_topsrv.c	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,618 @@
+/*
+ * net/tipc/tipc_topsrv.c: TIPC network topology service
+ *
+ * Copyright (c) 2000-2006, Ericsson AB
+ * Copyright (c) 2005-2008, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_dbg.h"
+#include "tipc_name_table.h"
+#include "tipc_port.h"
+#include "tipc_ref.h"
+#include "tipc_topsrv.h"
+
+/**
+ * struct subscriber - TIPC network topology subscriber
+ * @port_ref: object reference to server port connecting to subscriber
+ * @lock: pointer to spinlock controlling access to subscriber's server port
+ * @subscriber_list: adjacent subscribers in top. server's list of subscribers
+ * @subscription_list: list of subscription objects for this subscriber
+ */
+
+struct subscriber {
+	u32 port_ref;
+	spinlock_t *lock;
+	struct list_head subscriber_list;
+	struct list_head subscription_list;
+};
+
+/**
+ * struct top_srv - TIPC network topology subscription service
+ * @user_ref: TIPC userid of subscription service
+ * @setup_port: reference to TIPC port that handles subscription requests
+ * @subscription_count: number of active subscriptions (not subscribers!)
+ * @subscriber_list: list of ports subscribing to service
+ * @lock: spinlock govering access to subscriber list
+ */
+
+struct top_srv {
+	u32 user_ref;
+	u32 setup_port;
+	atomic_t subscription_count;
+	struct list_head subscriber_list;
+	spinlock_t lock;
+};
+
+static struct top_srv topsrv = { 0 };
+
+/**
+ * htohl - convert value to endianness used by destination
+ * @in: value to convert
+ * @swap: non-zero if endianness must be reversed
+ *
+ * Returns converted value
+ */
+
+static u32 htohl(u32 in, int swap)
+{
+	return swap ? swab32(in) : in;
+}
+
+/**
+ * subscr_send_event - send a message containing a tipc_event to the subscriber
+ *
+ * Note: Must not hold subscriber's server port lock, since tipc_send() will
+ *       try to take the lock if the message is rejected and returned!
+ */
+
+static void subscr_send_event(struct subscription *sub,
+			      u32 found_lower,
+			      u32 found_upper,
+			      u32 event,
+			      u32 port_ref,
+			      u32 node)
+{
+	struct iovec msg_sect;
+
+	msg_sect.iov_base = (void *)&sub->evt;
+	msg_sect.iov_len = sizeof(struct tipc_event);
+
+	sub->evt.event = htohl(event, sub->swap);
+	sub->evt.found_lower = htohl(found_lower, sub->swap);
+	sub->evt.found_upper = htohl(found_upper, sub->swap);
+	sub->evt.port.ref = htohl(port_ref, sub->swap);
+	sub->evt.port.node = htohl(node, sub->swap);
+	tipc_send(sub->server_ref, 1, &msg_sect);
+}
+
+/**
+ * tipc_subscr_overlap - test for subscription overlap with the given values
+ *
+ * Returns 1 if there is overlap, otherwise 0.
+ */
+
+int tipc_subscr_overlap(struct subscription *sub,
+			u32 found_lower,
+			u32 found_upper)
+
+{
+	if (found_lower < sub->seq.lower)
+		found_lower = sub->seq.lower;
+	if (found_upper > sub->seq.upper)
+		found_upper = sub->seq.upper;
+	if (found_lower > found_upper)
+		return 0;
+	return 1;
+}
+
+/**
+ * tipc_subscr_report_overlap - issue event if there is subscription overlap
+ *
+ * Protected by nameseq.lock in name_table.c
+ */
+
+void tipc_subscr_report_overlap(struct subscription *sub,
+				u32 found_lower,
+				u32 found_upper,
+				u32 event,
+				u32 port_ref,
+				u32 node,
+				int must)
+{
+	if (!tipc_subscr_overlap(sub, found_lower, found_upper))
+		return;
+	if (!must && !(sub->filter & TIPC_SUB_PORTS))
+		return;
+
+	sub->event_cb(sub, found_lower, found_upper, event, port_ref, node);
+}
+
+/**
+ * subscr_timeout - subscription timeout has occurred
+ */
+
+static void subscr_timeout(struct subscription *sub)
+{
+	struct port *server_port;
+
+	/* Validate server port reference (in case subscriber is terminating) */
+
+	server_port = tipc_port_lock(sub->server_ref);
+	if (server_port == NULL)
+		return;
+
+	/* Validate timeout (in case subscription is being cancelled) */
+
+	if (sub->timeout == TIPC_WAIT_FOREVER) {
+		tipc_port_unlock(server_port);
+		return;
+	}
+
+	/* Unlink subscription from name table */
+
+	tipc_nametbl_unsubscribe(sub);
+
+	/* Unlink subscription from subscriber */
+
+	list_del(&sub->subscription_list);
+
+	/* Release subscriber's server port */
+
+	tipc_port_unlock(server_port);
+
+	/* Notify subscriber of timeout */
+
+	subscr_send_event(sub, sub->evt.s.seq.lower, sub->evt.s.seq.upper,
+			  TIPC_SUBSCR_TIMEOUT, 0, 0);
+
+	/* Now destroy subscription */
+
+	k_term_timer(&sub->timer);
+	kfree(sub);
+	atomic_dec(&topsrv.subscription_count);
+}
+
+/**
+ * subscr_del - delete a subscription within a subscription list
+ *
+ * Called with subscriber port locked.
+ */
+
+static void subscr_del(struct subscription *sub)
+{
+	tipc_nametbl_unsubscribe(sub);
+	list_del(&sub->subscription_list);
+	kfree(sub);
+	atomic_dec(&topsrv.subscription_count);
+}
+
+/**
+ * subscr_terminate - terminate communication with a subscriber
+ *
+ * Called with subscriber port locked.  Routine must temporarily release lock
+ * to enable subscription timeout routine(s) to finish without deadlocking;
+ * the lock is then reclaimed to allow caller to release it upon return.
+ * (This should work even in the unlikely event some other thread creates
+ * a new object reference in the interim that uses this lock; this routine will
+ * simply wait for it to be released, then claim it.)
+ */
+
+static void subscr_terminate(struct subscriber *subscriber)
+{
+	u32 port_ref;
+	struct subscription *sub;
+	struct subscription *sub_temp;
+
+	/* Invalidate subscriber reference */
+
+	port_ref = subscriber->port_ref;
+	subscriber->port_ref = 0;
+	spin_unlock_bh(subscriber->lock);
+
+	/* Sever connection to subscriber */
+
+	tipc_shutdown(port_ref);
+	tipc_deleteport(port_ref);
+
+	/* Destroy any existing subscriptions for subscriber */
+
+	list_for_each_entry_safe(sub, sub_temp, &subscriber->subscription_list,
+				 subscription_list) {
+		if (sub->timeout != TIPC_WAIT_FOREVER) {
+			k_cancel_timer(&sub->timer);
+			k_term_timer(&sub->timer);
+		}
+		dbg("Term: Removing sub %u,%u,%u from subscriber %x list\n",
+		    sub->seq.type, sub->seq.lower, sub->seq.upper, subscriber);
+		subscr_del(sub);
+	}
+
+	/* Remove subscriber from topology server's subscriber list */
+
+	spin_lock_bh(&topsrv.lock);
+	list_del(&subscriber->subscriber_list);
+	spin_unlock_bh(&topsrv.lock);
+
+	/* Reclaim subscriber lock */
+
+	spin_lock_bh(subscriber->lock);
+
+	/* Now destroy subscriber */
+
+	kfree(subscriber);
+}
+
+/**
+ * subscr_cancel - handle subscription cancellation request
+ *
+ * Called with subscriber port locked.  Routine must temporarily release lock
+ * to enable the subscription timeout routine to finish without deadlocking;
+ * the lock is then reclaimed to allow caller to release it upon return.
+ *
+ * Note that fields of 's' use subscriber's endianness!
+ */
+
+static void subscr_cancel(struct tipc_subscr *s,
+			  struct subscriber *subscriber)
+{
+	struct subscription *sub;
+	struct subscription *sub_temp;
+	int found = 0;
+
+	/* Find first matching subscription, exit if not found */
+
+	list_for_each_entry_safe(sub, sub_temp, &subscriber->subscription_list,
+				 subscription_list) {
+		if (!memcmp(s, &sub->evt.s, sizeof(struct tipc_subscr))) {
+			found = 1;
+			break;
+		}
+	}
+	if (!found)
+		return;
+
+	/* Cancel subscription timer (if used), then delete subscription */
+
+	if (sub->timeout != TIPC_WAIT_FOREVER) {
+		sub->timeout = TIPC_WAIT_FOREVER;
+		spin_unlock_bh(subscriber->lock);
+		k_cancel_timer(&sub->timer);
+		k_term_timer(&sub->timer);
+		spin_lock_bh(subscriber->lock);
+	}
+	dbg("Cancel: removing sub %u,%u,%u from subscriber %x list\n",
+	    sub->seq.type, sub->seq.lower, sub->seq.upper, subscriber);
+	subscr_del(sub);
+}
+
+/**
+ * subscr_subscribe - create subscription for subscriber
+ *
+ * Called with subscriber port locked.
+ */
+
+static struct subscription *subscr_subscribe(struct tipc_subscr *s,
+					     struct subscriber *subscriber)
+{
+	struct subscription *sub;
+	int swap;
+
+	/* Determine subscriber's endianness */
+
+	swap = !(s->filter & (TIPC_SUB_PORTS | TIPC_SUB_SERVICE));
+
+	/* Detect & process a subscription cancellation request */
+
+	if (s->filter & htohl(TIPC_SUB_CANCEL, swap)) {
+		s->filter &= ~htohl(TIPC_SUB_CANCEL, swap);
+		subscr_cancel(s, subscriber);
+		return NULL;
+	}
+
+	/* Refuse subscription if global limit exceeded */
+
+	if (atomic_read(&topsrv.subscription_count) >= tipc_max_subscriptions) {
+		warn("Subscription rejected, subscription limit reached (%u)\n",
+		     tipc_max_subscriptions);
+		subscr_terminate(subscriber);
+		return NULL;
+	}
+
+	/* Allocate subscription object */
+
+	sub = kmalloc(sizeof(*sub), GFP_ATOMIC);
+	if (!sub) {
+		warn("Subscription rejected, no memory\n");
+		subscr_terminate(subscriber);
+		return NULL;
+	}
+
+	/* Initialize subscription object */
+
+	sub->seq.type = htohl(s->seq.type, swap);
+	sub->seq.lower = htohl(s->seq.lower, swap);
+	sub->seq.upper = htohl(s->seq.upper, swap);
+	sub->timeout = htohl(s->timeout, swap);
+	sub->filter = htohl(s->filter, swap);
+	if ((!(sub->filter & TIPC_SUB_PORTS)
+	     == !(sub->filter & TIPC_SUB_SERVICE))
+	    || (sub->seq.lower > sub->seq.upper)) {
+		warn("Subscription rejected, illegal request\n");
+		kfree(sub);
+		subscr_terminate(subscriber);
+		return NULL;
+	}
+	sub->event_cb = subscr_send_event;
+	INIT_LIST_HEAD(&sub->nameseq_list);
+	list_add(&sub->subscription_list, &subscriber->subscription_list);
+	sub->server_ref = subscriber->port_ref;
+	sub->swap = swap;
+	memcpy(&sub->evt.s, s, sizeof(struct tipc_subscr));
+	atomic_inc(&topsrv.subscription_count);
+	if (sub->timeout != TIPC_WAIT_FOREVER) {
+		k_init_timer(&sub->timer,
+			     (Handler)subscr_timeout, (unsigned long)sub);
+		k_start_timer(&sub->timer, sub->timeout);
+	}
+
+	return sub;
+}
+
+/**
+ * subscr_conn_shutdown_event - handle termination request from subscriber
+ *
+ * Called with subscriber's server port unlocked.
+ */
+
+static void subscr_conn_shutdown_event(void *usr_handle,
+				       u32 port_ref,
+				       struct sk_buff **buf,
+				       unsigned char const *data,
+				       unsigned int size,
+				       int reason)
+{
+	struct subscriber *subscriber = usr_handle;
+	spinlock_t *subscriber_lock;
+
+	if (tipc_port_lock(port_ref) == NULL)
+		return;
+
+	subscriber_lock = subscriber->lock;
+	subscr_terminate(subscriber);
+	spin_unlock_bh(subscriber_lock);
+}
+
+/**
+ * subscr_conn_msg_event - handle new subscription request from subscriber
+ *
+ * Called with subscriber's server port unlocked.
+ */
+
+static void subscr_conn_msg_event(void *usr_handle,
+				  u32 port_ref,
+				  struct sk_buff **buf,
+				  const unchar *data,
+				  u32 size)
+{
+	struct subscriber *subscriber = usr_handle;
+	spinlock_t *subscriber_lock;
+	struct subscription *sub;
+
+	/*
+	 * Lock subscriber's server port (& make a local copy of lock pointer,
+	 * in case subscriber is deleted while processing subscription request)
+	 */
+
+	if (tipc_port_lock(port_ref) == NULL)
+		return;
+
+	subscriber_lock = subscriber->lock;
+
+	if (size != sizeof(struct tipc_subscr)) {
+		subscr_terminate(subscriber);
+		spin_unlock_bh(subscriber_lock);
+	} else {
+		sub = subscr_subscribe((struct tipc_subscr *)data, subscriber);
+		spin_unlock_bh(subscriber_lock);
+		if (sub != NULL) {
+
+			/*
+			 * We must release the server port lock before adding a
+			 * subscription to the name table since TIPC needs to be
+			 * able to (re)acquire the port lock if an event message
+			 * issued by the subscription process is rejected and
+			 * returned.  The subscription cannot be deleted while
+			 * it is being added to the name table because:
+			 * a) the single-threading of the native API port code
+			 *    ensures the subscription cannot be cancelled and
+			 *    the subscriber connection cannot be broken, and
+			 * b) the name table lock ensures the subscription
+			 *    timeout code cannot delete the subscription,
+			 * so the subscription object is still protected.
+			 */
+
+			tipc_nametbl_subscribe(sub);
+		}
+	}
+}
+
+/**
+ * subscr_named_msg_event - handle request to establish a new subscriber
+ */
+
+static void subscr_named_msg_event(void *usr_handle,
+				   u32 port_ref,
+				   struct sk_buff **buf,
+				   const unchar *data,
+				   u32 size,
+				   u32 importance,
+				   struct tipc_portid const *orig,
+				   struct tipc_name_seq const *dest)
+{
+	static struct iovec msg_sect = {NULL, 0};
+
+	struct subscriber *subscriber;
+	u32 server_port_ref;
+
+	/* Create subscriber object */
+
+	subscriber = kzalloc(sizeof(struct subscriber), GFP_ATOMIC);
+	if (subscriber == NULL) {
+		warn("Subscriber rejected, no memory\n");
+		return;
+	}
+	INIT_LIST_HEAD(&subscriber->subscription_list);
+	INIT_LIST_HEAD(&subscriber->subscriber_list);
+
+	/* Create server port & establish connection to subscriber */
+
+	tipc_createport(topsrv.user_ref,
+			subscriber,
+			importance,
+			NULL,
+			NULL,
+			subscr_conn_shutdown_event,
+			NULL,
+			NULL,
+			subscr_conn_msg_event,
+			NULL,
+			&subscriber->port_ref);
+	if (subscriber->port_ref == 0) {
+		warn("Subscriber rejected, unable to create port\n");
+		kfree(subscriber);
+		return;
+	}
+	tipc_connect2port(subscriber->port_ref, orig);
+
+	/* Lock server port (& save lock address for future use) */
+
+	subscriber->lock = tipc_port_lock(subscriber->port_ref)->publ.lock;
+
+	/* Add subscriber to topology server's subscriber list */
+
+	spin_lock_bh(&topsrv.lock);
+	list_add(&subscriber->subscriber_list, &topsrv.subscriber_list);
+	spin_unlock_bh(&topsrv.lock);
+
+	/* Unlock server port */
+
+	server_port_ref = subscriber->port_ref;
+	spin_unlock_bh(subscriber->lock);
+
+	/* Send an ACK- to complete connection handshaking */
+
+	tipc_send(server_port_ref, 1, &msg_sect);
+
+	/* Handle optional subscription request */
+
+	if (size != 0) {
+		subscr_conn_msg_event(subscriber, server_port_ref,
+				      buf, data, size);
+	}
+}
+
+int tipc_subscr_start(void)
+{
+	struct tipc_name_seq seq = {TIPC_TOP_SRV, TIPC_TOP_SRV, TIPC_TOP_SRV};
+	int res = -1;
+
+	memset(&topsrv, 0, sizeof (topsrv));
+	spin_lock_init(&topsrv.lock);
+	INIT_LIST_HEAD(&topsrv.subscriber_list);
+
+	spin_lock_bh(&topsrv.lock);
+	res = tipc_attach(&topsrv.user_ref, NULL, NULL);
+	if (res) {
+		spin_unlock_bh(&topsrv.lock);
+		return res;
+	}
+
+	res = tipc_createport(topsrv.user_ref,
+			      NULL,
+			      TIPC_CRITICAL_IMPORTANCE,
+			      NULL,
+			      NULL,
+			      NULL,
+			      NULL,
+			      subscr_named_msg_event,
+			      NULL,
+			      NULL,
+			      &topsrv.setup_port);
+	if (res)
+		goto failed;
+
+ 	res = tipc_publish_rsv(topsrv.setup_port, TIPC_NODE_SCOPE, &seq);
+	if (res)
+		goto failed;
+
+	spin_unlock_bh(&topsrv.lock);
+	return 0;
+
+failed:
+	err("Failed to create subscription service\n");
+	tipc_detach(topsrv.user_ref);
+	topsrv.user_ref = 0;
+	spin_unlock_bh(&topsrv.lock);
+	return res;
+}
+
+void tipc_subscr_stop(void)
+{
+	struct subscriber *subscriber;
+	struct subscriber *subscriber_temp;
+	spinlock_t *subscriber_lock;
+
+	if (topsrv.user_ref) {
+		tipc_deleteport(topsrv.setup_port);
+		list_for_each_entry_safe(subscriber, subscriber_temp,
+					 &topsrv.subscriber_list,
+					 subscriber_list) {
+			subscriber_lock = subscriber->lock;
+			spin_lock_bh(subscriber_lock);
+			subscr_terminate(subscriber);
+			spin_unlock_bh(subscriber_lock);
+		}
+		tipc_detach(topsrv.user_ref);
+		spin_lock_term(&topsrv.lock);
+		topsrv.user_ref = 0;
+	}
+}
+
+
+int tipc_ispublished(struct tipc_name const *name)
+{
+	u32 domain = 0;
+
+	return tipc_nametbl_translate(name->type, name->instance, &domain) != 0;
+}
+
diff -ruN linux-2.6.29/net/tipc/tipc_topsrv.h android_cluster/linux-2.6.29/net/tipc/tipc_topsrv.h
--- linux-2.6.29/net/tipc/tipc_topsrv.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_topsrv.h	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,90 @@
+/*
+ * net/tipc/tipc_topsrv.h: Include file for TIPC network topology service
+ *
+ * Copyright (c) 2003-2006, Ericsson AB
+ * Copyright (c) 2005-2006, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_SUBSCR_H
+#define _TIPC_SUBSCR_H
+
+struct subscription;
+
+typedef void (*tipc_subscr_event) (struct subscription *sub,
+				   u32 found_lower, u32 found_upper,
+				   u32 event, u32 port_ref, u32 node);
+
+/**
+ * struct subscription - TIPC network topology subscription object
+ * @seq: name sequence associated with subscription
+ * @timeout: duration of subscription (in ms)
+ * @filter: event filtering to be done for subscription
+ * @event_cb: routine invoked when a subscription event is detected
+ * @timer: timer governing subscription duration (optional)
+ * @nameseq_list: adjacent subscriptions in name sequence's subscription list
+ * @subscription_list: adjacent subscriptions in subscriber's subscription list
+ * @server_ref: object reference of server port associated with subscription
+ * @swap: indicates if subscriber uses opposite endianness in its messages
+ * @evt: template for events generated by subscription
+ */
+
+struct subscription {
+	struct tipc_name_seq seq;
+	u32 timeout;
+	u32 filter;
+	tipc_subscr_event event_cb;
+	struct timer_list timer;
+	struct list_head nameseq_list;
+	struct list_head subscription_list;
+	u32 server_ref;
+	int swap;
+	struct tipc_event evt;
+};
+
+int tipc_subscr_overlap(struct subscription *sub,
+			u32 found_lower,
+			u32 found_upper);
+
+void tipc_subscr_report_overlap(struct subscription *sub,
+				u32 found_lower,
+				u32 found_upper,
+				u32 event,
+				u32 port_ref,
+				u32 node,
+				int must_report);
+
+int tipc_subscr_start(void);
+
+void tipc_subscr_stop(void);
+
+
+#endif
diff -ruN linux-2.6.29/net/tipc/tipc_user_reg.c android_cluster/linux-2.6.29/net/tipc/tipc_user_reg.c
--- linux-2.6.29/net/tipc/tipc_user_reg.c	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_user_reg.c	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,264 @@
+/*
+ * net/tipc/tipc_user_reg.c: TIPC user registry code
+ * 
+ * Copyright (c) 2000-2006, Ericsson AB
+ * Copyright (c) 2004-2006, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "tipc_core.h"
+#include "tipc_user_reg.h"
+
+/*
+ * TIPC user registry keeps track of users of the tipc_port interface.
+ *
+ * The registry utilizes an array of "TIPC user" entries;
+ * a user's ID is the index of their associated array entry.
+ * Array entry 0 is not used, so userid 0 is not valid;
+ * TIPC sometimes uses this value to denote an anonymous user.
+ * The list of free entries is initially chained from last entry to entry 1.
+ */
+
+/**
+ * struct tipc_user - registered TIPC user info
+ * @next: index of next free registry entry (or -1 for an allocated entry)
+ * @callback: ptr to routine to call when TIPC mode changes (NULL if none)
+ * @usr_handle: user-defined value passed to callback routine
+ * @ports: list of user ports owned by the user
+ */
+
+struct tipc_user {
+	int next;
+	tipc_mode_event callback;
+	void *usr_handle;
+	struct list_head ports;
+};
+
+#define MAX_USERID 64
+#define USER_LIST_SIZE ((MAX_USERID + 1) * sizeof(struct tipc_user))
+
+static struct tipc_user *users = NULL;
+static u32 next_free_user = MAX_USERID + 1;
+static DEFINE_SPINLOCK(reg_lock);
+
+/**
+ * reg_init - create TIPC user registry (but don't activate it)
+ *
+ * If registry has been pre-initialized it is left "as is".
+ * NOTE: This routine may be called when TIPC is inactive.
+ */
+
+static int reg_init(void)
+{
+	u32 i;
+
+	spin_lock_bh(&reg_lock);
+	if (!users) {
+		users = kzalloc(USER_LIST_SIZE, GFP_ATOMIC);
+		if (users) {
+			for (i = 1; i <= MAX_USERID; i++) {
+				users[i].next = i - 1;
+			}
+			next_free_user = MAX_USERID;
+		}
+	}
+	spin_unlock_bh(&reg_lock);
+	return users ? 0 : -ENOMEM;
+}
+
+/**
+ * reg_callback - inform TIPC user about current operating mode
+ */
+
+static void reg_callback(struct tipc_user *user_ptr)
+{
+	tipc_mode_event cb;
+	void *arg;
+
+	spin_lock_bh(&reg_lock);
+	cb = user_ptr->callback;
+	arg = user_ptr->usr_handle;
+	spin_unlock_bh(&reg_lock);
+
+	if (cb)
+		cb(arg, tipc_mode, tipc_own_addr);
+}
+
+/**
+ * tipc_reg_start - activate TIPC user registry
+ */
+
+int tipc_reg_start(void)
+{
+	u32 u;
+	int res;
+
+	if ((res = reg_init()))
+		return res;
+
+	for (u = 1; u <= MAX_USERID; u++) {
+		if (users[u].callback)
+			tipc_k_signal((Handler)reg_callback,
+				      (unsigned long)&users[u]);
+	}
+	return 0;
+}
+
+/**
+ * tipc_reg_stop - shut down & delete TIPC user registry
+ */
+
+void tipc_reg_stop(void)
+{
+	int id;
+
+	if (!users)
+		return;
+
+	for (id = 1; id <= MAX_USERID; id++) {
+		if (users[id].callback)
+			reg_callback(&users[id]);
+	}
+	kfree(users);
+	users = NULL;
+}
+
+/**
+ * tipc_attach - register a TIPC user
+ *
+ * NOTE: This routine may be called when TIPC is inactive.
+ */
+
+int tipc_attach(u32 *userid, tipc_mode_event cb, void *usr_handle)
+{
+	struct tipc_user *user_ptr;
+
+	if ((tipc_mode == TIPC_NOT_RUNNING) && !cb)
+		return -ENOPROTOOPT;
+	if (!users)
+		reg_init();
+
+	spin_lock_bh(&reg_lock);
+	if (!next_free_user) {
+		spin_unlock_bh(&reg_lock);
+		return -EBUSY;
+	}
+	user_ptr = &users[next_free_user];
+	*userid = next_free_user;
+	next_free_user = user_ptr->next;
+	user_ptr->next = -1;
+	spin_unlock_bh(&reg_lock);
+
+	user_ptr->callback = cb;
+	user_ptr->usr_handle = usr_handle;
+	INIT_LIST_HEAD(&user_ptr->ports);
+	atomic_inc(&tipc_user_count);
+
+	if (cb && (tipc_mode != TIPC_NOT_RUNNING))
+		tipc_k_signal((Handler)reg_callback, (unsigned long)user_ptr);
+	return 0;
+}
+
+/**
+ * tipc_detach - deregister a TIPC user
+ */
+
+void tipc_detach(u32 userid)
+{
+	struct tipc_user *user_ptr;
+	struct list_head ports_temp;
+	struct user_port *up_ptr, *temp_up_ptr;
+
+	if ((userid == 0) || (userid > MAX_USERID))
+		return;
+
+	spin_lock_bh(&reg_lock);
+	if ((!users) || (users[userid].next >= 0)) {
+		spin_unlock_bh(&reg_lock);
+		return;
+	}
+
+	user_ptr = &users[userid];
+	user_ptr->callback = NULL;
+	INIT_LIST_HEAD(&ports_temp);
+	list_splice(&user_ptr->ports, &ports_temp);
+	user_ptr->next = next_free_user;
+	next_free_user = userid;
+	spin_unlock_bh(&reg_lock);
+
+	atomic_dec(&tipc_user_count);
+
+	list_for_each_entry_safe(up_ptr, temp_up_ptr, &ports_temp, uport_list) {
+		tipc_deleteport(up_ptr->ref);
+	}
+}
+
+/**
+ * tipc_reg_add_port - register a user's driver port
+ */
+
+int tipc_reg_add_port(struct user_port *up_ptr)
+{
+	struct tipc_user *user_ptr;
+
+	if (up_ptr->user_ref == 0)
+		return 0;
+	if (up_ptr->user_ref > MAX_USERID)
+		return -EINVAL;
+	if ((tipc_mode == TIPC_NOT_RUNNING) || !users )
+		return -ENOPROTOOPT;
+
+	spin_lock_bh(&reg_lock);
+	user_ptr = &users[up_ptr->user_ref];
+	list_add(&up_ptr->uport_list, &user_ptr->ports);
+	spin_unlock_bh(&reg_lock);
+	return 0;
+}
+
+/**
+ * tipc_reg_remove_port - deregister a user's driver port
+ */
+
+int tipc_reg_remove_port(struct user_port *up_ptr)
+{
+	if (up_ptr->user_ref == 0)
+		return 0;
+	if (up_ptr->user_ref > MAX_USERID)
+		return -EINVAL;
+	if (!users )
+		return -ENOPROTOOPT;
+
+	spin_lock_bh(&reg_lock);
+	list_del_init(&up_ptr->uport_list);
+	spin_unlock_bh(&reg_lock);
+	return 0;
+}
+
diff -ruN linux-2.6.29/net/tipc/tipc_user_reg.h android_cluster/linux-2.6.29/net/tipc/tipc_user_reg.h
--- linux-2.6.29/net/tipc/tipc_user_reg.h	1969-12-31 16:00:00.000000000 -0800
+++ android_cluster/linux-2.6.29/net/tipc/tipc_user_reg.h	2014-05-27 23:04:10.638023520 -0700
@@ -0,0 +1,48 @@
+/*
+ * net/tipc/tipc_user_reg.h: Include file for TIPC user registry code
+ *
+ * Copyright (c) 2000-2006, Ericsson AB
+ * Copyright (c) 2005-2006, Wind River Systems
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the names of the copyright holders nor the names of its
+ *    contributors may be used to endorse or promote products derived from
+ *    this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _TIPC_USER_REG_H
+#define _TIPC_USER_REG_H
+
+#include "tipc_port.h"
+
+int tipc_reg_start(void);
+void tipc_reg_stop(void);
+
+int tipc_reg_add_port(struct user_port *up_ptr);
+int tipc_reg_remove_port(struct user_port *up_ptr);
+
+#endif
diff -ruN linux-2.6.29/net/tipc/user_reg.c android_cluster/linux-2.6.29/net/tipc/user_reg.c
--- linux-2.6.29/net/tipc/user_reg.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/user_reg.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,264 +0,0 @@
-/*
- * net/tipc/user_reg.c: TIPC user registry code
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2004-2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "user_reg.h"
-
-/*
- * TIPC user registry keeps track of users of the tipc_port interface.
- *
- * The registry utilizes an array of "TIPC user" entries;
- * a user's ID is the index of their associated array entry.
- * Array entry 0 is not used, so userid 0 is not valid;
- * TIPC sometimes uses this value to denote an anonymous user.
- * The list of free entries is initially chained from last entry to entry 1.
- */
-
-/**
- * struct tipc_user - registered TIPC user info
- * @next: index of next free registry entry (or -1 for an allocated entry)
- * @callback: ptr to routine to call when TIPC mode changes (NULL if none)
- * @usr_handle: user-defined value passed to callback routine
- * @ports: list of user ports owned by the user
- */
-
-struct tipc_user {
-	int next;
-	tipc_mode_event callback;
-	void *usr_handle;
-	struct list_head ports;
-};
-
-#define MAX_USERID 64
-#define USER_LIST_SIZE ((MAX_USERID + 1) * sizeof(struct tipc_user))
-
-static struct tipc_user *users = NULL;
-static u32 next_free_user = MAX_USERID + 1;
-static DEFINE_SPINLOCK(reg_lock);
-
-/**
- * reg_init - create TIPC user registry (but don't activate it)
- *
- * If registry has been pre-initialized it is left "as is".
- * NOTE: This routine may be called when TIPC is inactive.
- */
-
-static int reg_init(void)
-{
-	u32 i;
-
-	spin_lock_bh(&reg_lock);
-	if (!users) {
-		users = kzalloc(USER_LIST_SIZE, GFP_ATOMIC);
-		if (users) {
-			for (i = 1; i <= MAX_USERID; i++) {
-				users[i].next = i - 1;
-			}
-			next_free_user = MAX_USERID;
-		}
-	}
-	spin_unlock_bh(&reg_lock);
-	return users ? 0 : -ENOMEM;
-}
-
-/**
- * reg_callback - inform TIPC user about current operating mode
- */
-
-static void reg_callback(struct tipc_user *user_ptr)
-{
-	tipc_mode_event cb;
-	void *arg;
-
-	spin_lock_bh(&reg_lock);
-	cb = user_ptr->callback;
-	arg = user_ptr->usr_handle;
-	spin_unlock_bh(&reg_lock);
-
-	if (cb)
-		cb(arg, tipc_mode, tipc_own_addr);
-}
-
-/**
- * tipc_reg_start - activate TIPC user registry
- */
-
-int tipc_reg_start(void)
-{
-	u32 u;
-	int res;
-
-	if ((res = reg_init()))
-		return res;
-
-	for (u = 1; u <= MAX_USERID; u++) {
-		if (users[u].callback)
-			tipc_k_signal((Handler)reg_callback,
-				      (unsigned long)&users[u]);
-	}
-	return 0;
-}
-
-/**
- * tipc_reg_stop - shut down & delete TIPC user registry
- */
-
-void tipc_reg_stop(void)
-{
-	int id;
-
-	if (!users)
-		return;
-
-	for (id = 1; id <= MAX_USERID; id++) {
-		if (users[id].callback)
-			reg_callback(&users[id]);
-	}
-	kfree(users);
-	users = NULL;
-}
-
-/**
- * tipc_attach - register a TIPC user
- *
- * NOTE: This routine may be called when TIPC is inactive.
- */
-
-int tipc_attach(u32 *userid, tipc_mode_event cb, void *usr_handle)
-{
-	struct tipc_user *user_ptr;
-
-	if ((tipc_mode == TIPC_NOT_RUNNING) && !cb)
-		return -ENOPROTOOPT;
-	if (!users)
-		reg_init();
-
-	spin_lock_bh(&reg_lock);
-	if (!next_free_user) {
-		spin_unlock_bh(&reg_lock);
-		return -EBUSY;
-	}
-	user_ptr = &users[next_free_user];
-	*userid = next_free_user;
-	next_free_user = user_ptr->next;
-	user_ptr->next = -1;
-	spin_unlock_bh(&reg_lock);
-
-	user_ptr->callback = cb;
-	user_ptr->usr_handle = usr_handle;
-	INIT_LIST_HEAD(&user_ptr->ports);
-	atomic_inc(&tipc_user_count);
-
-	if (cb && (tipc_mode != TIPC_NOT_RUNNING))
-		tipc_k_signal((Handler)reg_callback, (unsigned long)user_ptr);
-	return 0;
-}
-
-/**
- * tipc_detach - deregister a TIPC user
- */
-
-void tipc_detach(u32 userid)
-{
-	struct tipc_user *user_ptr;
-	struct list_head ports_temp;
-	struct user_port *up_ptr, *temp_up_ptr;
-
-	if ((userid == 0) || (userid > MAX_USERID))
-		return;
-
-	spin_lock_bh(&reg_lock);
-	if ((!users) || (users[userid].next >= 0)) {
-		spin_unlock_bh(&reg_lock);
-		return;
-	}
-
-	user_ptr = &users[userid];
-	user_ptr->callback = NULL;
-	INIT_LIST_HEAD(&ports_temp);
-	list_splice(&user_ptr->ports, &ports_temp);
-	user_ptr->next = next_free_user;
-	next_free_user = userid;
-	spin_unlock_bh(&reg_lock);
-
-	atomic_dec(&tipc_user_count);
-
-	list_for_each_entry_safe(up_ptr, temp_up_ptr, &ports_temp, uport_list) {
-		tipc_deleteport(up_ptr->ref);
-	}
-}
-
-/**
- * tipc_reg_add_port - register a user's driver port
- */
-
-int tipc_reg_add_port(struct user_port *up_ptr)
-{
-	struct tipc_user *user_ptr;
-
-	if (up_ptr->user_ref == 0)
-		return 0;
-	if (up_ptr->user_ref > MAX_USERID)
-		return -EINVAL;
-	if ((tipc_mode == TIPC_NOT_RUNNING) || !users )
-		return -ENOPROTOOPT;
-
-	spin_lock_bh(&reg_lock);
-	user_ptr = &users[up_ptr->user_ref];
-	list_add(&up_ptr->uport_list, &user_ptr->ports);
-	spin_unlock_bh(&reg_lock);
-	return 0;
-}
-
-/**
- * tipc_reg_remove_port - deregister a user's driver port
- */
-
-int tipc_reg_remove_port(struct user_port *up_ptr)
-{
-	if (up_ptr->user_ref == 0)
-		return 0;
-	if (up_ptr->user_ref > MAX_USERID)
-		return -EINVAL;
-	if (!users )
-		return -ENOPROTOOPT;
-
-	spin_lock_bh(&reg_lock);
-	list_del_init(&up_ptr->uport_list);
-	spin_unlock_bh(&reg_lock);
-	return 0;
-}
-
diff -ruN linux-2.6.29/net/tipc/user_reg.h android_cluster/linux-2.6.29/net/tipc/user_reg.h
--- linux-2.6.29/net/tipc/user_reg.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/user_reg.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,48 +0,0 @@
-/*
- * net/tipc/user_reg.h: Include file for TIPC user registry code
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_USER_REG_H
-#define _TIPC_USER_REG_H
-
-#include "port.h"
-
-int tipc_reg_start(void);
-void tipc_reg_stop(void);
-
-int tipc_reg_add_port(struct user_port *up_ptr);
-int tipc_reg_remove_port(struct user_port *up_ptr);
-
-#endif
diff -ruN linux-2.6.29/net/tipc/zone.c android_cluster/linux-2.6.29/net/tipc/zone.c
--- linux-2.6.29/net/tipc/zone.c	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/zone.c	1969-12-31 16:00:00.000000000 -0800
@@ -1,173 +0,0 @@
-/*
- * net/tipc/zone.c: TIPC zone management routines
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2005, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "core.h"
-#include "zone.h"
-#include "net.h"
-#include "addr.h"
-#include "node_subscr.h"
-#include "cluster.h"
-#include "node.h"
-
-struct _zone *tipc_zone_create(u32 addr)
-{
-	struct _zone *z_ptr;
-	u32 z_num;
-
-	if (!tipc_addr_domain_valid(addr)) {
-		err("Zone creation failed, invalid domain 0x%x\n", addr);
-		return NULL;
-	}
-
-	z_ptr = kzalloc(sizeof(*z_ptr), GFP_ATOMIC);
-	if (!z_ptr) {
-		warn("Zone creation failed, insufficient memory\n");
-		return NULL;
-	}
-
-	z_num = tipc_zone(addr);
-	z_ptr->addr = tipc_addr(z_num, 0, 0);
-	tipc_net.zones[z_num] = z_ptr;
-	return z_ptr;
-}
-
-void tipc_zone_delete(struct _zone *z_ptr)
-{
-	u32 c_num;
-
-	if (!z_ptr)
-		return;
-	for (c_num = 1; c_num <= tipc_max_clusters; c_num++) {
-		tipc_cltr_delete(z_ptr->clusters[c_num]);
-	}
-	kfree(z_ptr);
-}
-
-void tipc_zone_attach_cluster(struct _zone *z_ptr, struct cluster *c_ptr)
-{
-	u32 c_num = tipc_cluster(c_ptr->addr);
-
-	assert(c_ptr->addr);
-	assert(c_num <= tipc_max_clusters);
-	assert(z_ptr->clusters[c_num] == NULL);
-	z_ptr->clusters[c_num] = c_ptr;
-}
-
-void tipc_zone_remove_as_router(struct _zone *z_ptr, u32 router)
-{
-	u32 c_num;
-
-	for (c_num = 1; c_num <= tipc_max_clusters; c_num++) {
-		if (z_ptr->clusters[c_num]) {
-			tipc_cltr_remove_as_router(z_ptr->clusters[c_num],
-						   router);
-		}
-	}
-}
-
-void tipc_zone_send_external_routes(struct _zone *z_ptr, u32 dest)
-{
-	u32 c_num;
-
-	for (c_num = 1; c_num <= tipc_max_clusters; c_num++) {
-		if (z_ptr->clusters[c_num]) {
-			if (in_own_cluster(z_ptr->addr))
-				continue;
-			tipc_cltr_send_ext_routes(z_ptr->clusters[c_num], dest);
-		}
-	}
-}
-
-struct tipc_node *tipc_zone_select_remote_node(struct _zone *z_ptr, u32 addr, u32 ref)
-{
-	struct cluster *c_ptr;
-	struct tipc_node *n_ptr;
-	u32 c_num;
-
-	if (!z_ptr)
-		return NULL;
-	c_ptr = z_ptr->clusters[tipc_cluster(addr)];
-	if (!c_ptr)
-		return NULL;
-	n_ptr = tipc_cltr_select_node(c_ptr, ref);
-	if (n_ptr)
-		return n_ptr;
-
-	/* Links to any other clusters within this zone ? */
-	for (c_num = 1; c_num <= tipc_max_clusters; c_num++) {
-		c_ptr = z_ptr->clusters[c_num];
-		if (!c_ptr)
-			return NULL;
-		n_ptr = tipc_cltr_select_node(c_ptr, ref);
-		if (n_ptr)
-			return n_ptr;
-	}
-	return NULL;
-}
-
-u32 tipc_zone_select_router(struct _zone *z_ptr, u32 addr, u32 ref)
-{
-	struct cluster *c_ptr;
-	u32 c_num;
-	u32 router;
-
-	if (!z_ptr)
-		return 0;
-	c_ptr = z_ptr->clusters[tipc_cluster(addr)];
-	router = c_ptr ? tipc_cltr_select_router(c_ptr, ref) : 0;
-	if (router)
-		return router;
-
-	/* Links to any other clusters within the zone? */
-	for (c_num = 1; c_num <= tipc_max_clusters; c_num++) {
-		c_ptr = z_ptr->clusters[c_num];
-		router = c_ptr ? tipc_cltr_select_router(c_ptr, ref) : 0;
-		if (router)
-			return router;
-	}
-	return 0;
-}
-
-
-u32 tipc_zone_next_node(u32 addr)
-{
-	struct cluster *c_ptr = tipc_cltr_find(addr);
-
-	if (c_ptr)
-		return tipc_cltr_next_node(c_ptr, addr);
-	return 0;
-}
-
diff -ruN linux-2.6.29/net/tipc/zone.h android_cluster/linux-2.6.29/net/tipc/zone.h
--- linux-2.6.29/net/tipc/zone.h	2009-03-23 16:12:14.000000000 -0700
+++ android_cluster/linux-2.6.29/net/tipc/zone.h	1969-12-31 16:00:00.000000000 -0800
@@ -1,71 +0,0 @@
-/*
- * net/tipc/zone.h: Include file for TIPC zone management routines
- *
- * Copyright (c) 2000-2006, Ericsson AB
- * Copyright (c) 2005-2006, Wind River Systems
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the names of the copyright holders nor the names of its
- *    contributors may be used to endorse or promote products derived from
- *    this software without specific prior written permission.
- *
- * Alternatively, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") version 2 as published by the Free
- * Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _TIPC_ZONE_H
-#define _TIPC_ZONE_H
-
-#include "node_subscr.h"
-#include "net.h"
-
-
-/**
- * struct _zone - TIPC zone structure
- * @addr: network address of zone
- * @clusters: array of pointers to all clusters within zone
- * @links: number of (unicast) links to zone
- */
-
-struct _zone {
-	u32 addr;
-	struct cluster *clusters[2]; /* currently limited to just 1 cluster */
-	u32 links;
-};
-
-struct tipc_node *tipc_zone_select_remote_node(struct _zone *z_ptr, u32 addr, u32 ref);
-u32 tipc_zone_select_router(struct _zone *z_ptr, u32 addr, u32 ref);
-void tipc_zone_remove_as_router(struct _zone *z_ptr, u32 router);
-void tipc_zone_send_external_routes(struct _zone *z_ptr, u32 dest);
-struct _zone *tipc_zone_create(u32 addr);
-void tipc_zone_delete(struct _zone *z_ptr);
-void tipc_zone_attach_cluster(struct _zone *z_ptr, struct cluster *c_ptr);
-u32 tipc_zone_next_node(u32 addr);
-
-static inline struct _zone *tipc_zone_find(u32 addr)
-{
-	return tipc_net.zones[tipc_zone(addr)];
-}
-
-#endif
